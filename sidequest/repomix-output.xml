This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: logs/**, node_modules/**, *.log, directory-scan-reports/**, document-enhancement-impact-measurement/**, **/go/pkg/mod/**, **/pyenv/**, **/python/pyenv/**, **/vim/bundle/**, **/vim/autoload/**, repomix-output.xml, repomix-output.txt, **/README.md, **/README.MD, **/*.md
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.claude/
  settings.local.json
bug-fixes/
  bugfix-audit-worker.js
  index.js
  launch-tonight.sh
  package.json
core/
  config.js
  database.js
  index.js
  server.js
pipeline-core/
  .claude/
    settings.local.json
  cache/
    cached-scanner.js
    git-tracker.js
    scan-cache.js
  config/
    repository-config-loader.js
  errors/
    error-classifier.js
    error-types.d.ts
    types.d.ts
  extractors/
    extract_blocks.py
  git/
    branch-manager.js
    migration-transformer.js
    pr-creator.js
  models/
    __init__.py
    code_block.py
    consolidation_suggestion.py
    duplicate_group.py
    scan_report.py
    test_models.py
  reports/
    html-report-generator.js
    json-report-generator.js
    markdown-report-generator.js
    report-coordinator.js
  scanners/
    ast-grep-detector.js
    codebase-health-scanner.js
    repository-scanner.js
    root-directory-analyzer.js
    timeout_detector.py
    timeout-pattern-detector.js
  similarity/
    __init__.py
    config.py
    grouping.py
    semantic.py
    structural.py
  types/
    scan-orchestrator-types.js
    scan-orchestrator-types.ts
  utils/
    error-helpers.js
  doppler-health-monitor.js
  inter-project-scanner.js
  scan-orchestrator.d.ts
  scan-orchestrator.js
  scan-orchestrator.ts
pipeline-runners/
  claude-health-pipeline.js
  collect_git_activity.py
  duplicate-detection-pipeline.js
  duplicate-detection-pipeline.ts
  git-activity-pipeline.js
  gitignore-pipeline.js
  plugin-management-pipeline.js
  repo-cleanup-pipeline.js
  schema-enhancement-pipeline.js
  test-refactor-pipeline.ts
  universal-repo-cleanup.sh
types/
  duplicate-detection-types.js
  duplicate-detection-types.ts
utils/
  directory-scanner.js
  doppler-resilience.example.js
  doppler-resilience.js
  gitignore-repomix-updater.js
  logger.d.ts
  logger.js
  pipeline-names.js
  plugin-manager.js
  refactor-test-suite.ts
  report-generator.js
  schema-mcp-tools.js
workers/
  claude-health-worker.js
  duplicate-detection-worker.js
  git-activity-worker.js
  gitignore-worker.js
  repo-cleanup-worker.js
  repomix-worker.js
  schema-enhancement-worker.js
  test-refactor-worker.ts
.env.example
.gitignore
git-report-config.json
plugin-management-audit.sh
repomix.config.json
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "Bash(node gitignore-repomix-updater.js:*)",
      "Bash(mv:*)",
      "Bash(chmod:*)",
      "Bash(doppler run:*)",
      "Bash(npm run cleanup:dryrun:*)",
      "Bash(node:*)",
      "Bash(npm run cleanup:once:*)",
      "Bash(find:*)",
      "Bash(npm run typecheck:*)",
      "Bash(curl:*)",
      "Bash(pm2 logs:*)",
      "Bash(pm2 info:*)",
      "Bash(npm test:*)",
      "Bash(npm run test:integration:*)"
    ],
    "deny": [],
    "ask": []
  }
}
</file>

<file path="bug-fixes/bugfix-audit-worker.js">
// @ts-nocheck
import { SidequestServer } from '../core/server.js';
import { spawn } from 'child_process';
import fs from 'fs/promises';
import path from 'path';
import { createComponentLogger } from '../utils/logger.js';
import { config } from '../core/config.js';

const logger = createComponentLogger('BugfixAuditWorker');

/**
 * BugfixAuditWorker - Automated bug detection and fixing workflow
 *
 * Orchestrates multiple tools in sequence:
 * 1. bugfix-planner agent - Create comprehensive bug fix plans
 * 2. bug-detective plugin - Systematic debugging
 * 3. audit plugin - Security audit
 * 4. ceo-quality-controller-agent - Quality validation
 * 5. refractor plugin - Implement fixes
 *
 * Git workflow:
 * - Creates feature branch before changes
 * - Commits after each stage
 * - Creates PR to main when complete
 */
export class BugfixAuditWorker extends SidequestServer {
  constructor(options = {}) {
    super(options);
    this.activeDocsDir = options.activeDocsDir || path.join(process.env.HOME, 'dev', 'active');
    this.outputBaseDir = options.outputBaseDir || path.join(process.env.HOME, 'code', 'jobs', 'sidequest', 'bug-fixes', 'output');

    logger.info({
      activeDocsDir: this.activeDocsDir,
      outputBaseDir: this.outputBaseDir
    }, 'BugfixAuditWorker initialized');
  }

  /**
   * Find all markdown files in active docs directory
   */
  async findMarkdownFiles() {
    logger.info({ dir: this.activeDocsDir }, 'Scanning for markdown files');

    const markdownFiles = [];

    async function scanDirectory(dir) {
      const entries = await fs.readdir(dir, { withFileTypes: true });

      for (const entry of entries) {
        const fullPath = path.join(dir, entry.name);

        if (entry.isDirectory()) {
          await scanDirectory(fullPath);
        } else if (entry.isFile() && entry.name.endsWith('.md')) {
          markdownFiles.push(fullPath);
        }
      }
    }

    await scanDirectory(this.activeDocsDir);

    logger.info({ count: markdownFiles.length }, 'Markdown files found');
    return markdownFiles;
  }

  /**
   * Extract repository path from markdown file location
   */
  getRepositoryFromPath(markdownPath) {
    // Extract the project directory from path like:
    // /Users/*/dev/active/bugfix-project-name/context.md
    const parts = markdownPath.split(path.sep);
    const activeIndex = parts.indexOf('active');

    if (activeIndex !== -1 && parts.length > activeIndex + 1) {
      const projectName = parts[activeIndex + 1];

      // Try to find corresponding repo in ~/code/
      const possibleRepoPaths = [
        path.join(process.env.HOME, 'code', projectName),
        path.join(process.env.HOME, 'code', 'jobs', projectName),
        // Extract repo name from bugfix- prefix
        path.join(process.env.HOME, 'code', projectName.replace(/^bugfix-/, '').replace(/-\d{4}-\d{2}-\d{2}$/, ''))
      ];

      return { projectName, possibleRepoPaths };
    }

    return { projectName: null, possibleRepoPaths: [] };
  }

  /**
   * Run Claude Code agent via CLI
   */
  async runClaudeAgent(agentType, prompt, cwd) {
    logger.info({ agentType, cwd }, 'Running Claude Code agent');

    return new Promise((resolve, reject) => {
      // Use claude CLI to run agent
      // Note: This is a placeholder - actual implementation would use Claude Code API
      const child = spawn('claude', ['--agent', agentType, '--prompt', prompt], {
        cwd,
        stdio: 'pipe',
        shell: true
      });

      let stdout = '';
      let stderr = '';

      child.stdout.on('data', (data) => {
        stdout += data.toString();
      });

      child.stderr.on('data', (data) => {
        stderr += data.toString();
      });

      child.on('close', (code) => {
        if (code === 0) {
          resolve({ stdout, stderr });
        } else {
          reject(new Error(`Agent ${agentType} failed with code ${code}: ${stderr}`));
        }
      });

      child.on('error', (error) => {
        reject(error);
      });
    });
  }

  /**
   * Run Claude Code slash command
   */
  async runSlashCommand(command, cwd) {
    logger.info({ command, cwd }, 'Running slash command');

    return new Promise((resolve, reject) => {
      const child = spawn('claude', ['--command', command], {
        cwd,
        stdio: 'pipe',
        shell: true
      });

      let stdout = '';
      let stderr = '';

      child.stdout.on('data', (data) => {
        stdout += data.toString();
      });

      child.stderr.on('data', (data) => {
        stderr += data.toString();
      });

      child.on('close', (code) => {
        if (code === 0) {
          resolve({ stdout, stderr });
        } else {
          reject(new Error(`Command ${command} failed with code ${code}: ${stderr}`));
        }
      });

      child.on('error', (error) => {
        reject(error);
      });
    });
  }

  /**
   * Create a git branch for bug fixes
   */
  async createGitBranch(repoPath, branchName) {
    logger.info({ repoPath, branchName }, 'Creating git branch');

    return new Promise((resolve, reject) => {
      const child = spawn('git', ['checkout', '-b', branchName], {
        cwd: repoPath,
        stdio: 'pipe'
      });

      let stderr = '';

      child.stderr.on('data', (data) => {
        stderr += data.toString();
      });

      child.on('close', (code) => {
        if (code === 0) {
          resolve();
        } else {
          reject(new Error(`Git branch creation failed: ${stderr}`));
        }
      });

      child.on('error', (error) => {
        reject(error);
      });
    });
  }

  /**
   * Commit changes with descriptive message
   */
  async commitChanges(repoPath, message) {
    logger.info({ repoPath, message }, 'Committing changes');

    // Stage all changes
    await new Promise((resolve, reject) => {
      const child = spawn('git', ['add', '.'], {
        cwd: repoPath,
        stdio: 'pipe'
      });

      child.on('close', (code) => {
        if (code === 0) resolve();
        else reject(new Error('Git add failed'));
      });

      child.on('error', reject);
    });

    // Commit
    return new Promise((resolve, reject) => {
      const child = spawn('git', ['commit', '-m', message], {
        cwd: repoPath,
        stdio: 'pipe'
      });

      child.on('close', (code) => {
        if (code === 0) resolve();
        else reject(new Error('Git commit failed'));
      });

      child.on('error', reject);
    });
  }

  /**
   * Create pull request using gh CLI
   */
  async createPullRequest(repoPath, title, body) {
    logger.info({ repoPath, title }, 'Creating pull request');

    return new Promise((resolve, reject) => {
      const child = spawn('gh', [
        'pr', 'create',
        '--title', title,
        '--body', body,
        '--base', 'main'
      ], {
        cwd: repoPath,
        stdio: 'pipe'
      });

      let stdout = '';
      let stderr = '';

      child.stdout.on('data', (data) => {
        stdout += data.toString();
      });

      child.stderr.on('data', (data) => {
        stderr += data.toString();
      });

      child.on('close', (code) => {
        if (code === 0) {
          resolve({ url: stdout.trim() });
        } else {
          reject(new Error(`PR creation failed: ${stderr}`));
        }
      });

      child.on('error', (error) => {
        reject(error);
      });
    });
  }

  /**
   * Main job handler - orchestrates the entire bug fix workflow
   */
  async runJobHandler(job) {
    const { markdownFile, projectName, repoPath } = job.data;

    logger.info({
      jobId: job.id,
      markdownFile,
      projectName,
      repoPath
    }, 'Starting bug fix audit workflow');

    const outputDir = path.join(this.outputBaseDir, projectName, new Date().toISOString().split('T')[0]);
    await fs.mkdir(outputDir, { recursive: true });

    const results = {
      markdownFile,
      projectName,
      repoPath,
      stages: [],
      branchName: null,
      pullRequestUrl: null,
      timestamp: new Date().toISOString()
    };

    try {
      // Read markdown content
      const markdownContent = await fs.readFile(markdownFile, 'utf-8');

      // Stage 1: bugfix-planner agent
      logger.info({ stage: 1 }, 'Running bugfix-planner agent');
      const plannerPrompt = `Analyze this markdown file and create a comprehensive bug fix plan:\n\n${markdownContent}`;

      try {
        const plannerResult = await this.runClaudeAgent('bugfix-planner', plannerPrompt, repoPath);
        results.stages.push({
          name: 'bugfix-planner',
          status: 'completed',
          output: plannerResult.stdout
        });

        // Save plan to output
        const planPath = path.join(outputDir, '01-bugfix-plan.md');
        await fs.writeFile(planPath, plannerResult.stdout);
        logger.info({ planPath }, 'Bug fix plan saved');
      } catch (error) {
        logger.error({ error }, 'bugfix-planner failed');
        results.stages.push({
          name: 'bugfix-planner',
          status: 'failed',
          error: error.message
        });
        throw error;
      }

      // Stage 2: bug-detective plugin
      logger.info({ stage: 2 }, 'Running bug-detective plugin');
      try {
        const detectiveResult = await this.runSlashCommand('/bug-detective:bug-detective', repoPath);
        results.stages.push({
          name: 'bug-detective',
          status: 'completed',
          output: detectiveResult.stdout
        });

        const detectivePath = path.join(outputDir, '02-bug-detective-report.md');
        await fs.writeFile(detectivePath, detectiveResult.stdout);
        logger.info({ detectivePath }, 'Bug detective report saved');
      } catch (error) {
        logger.error({ error }, 'bug-detective failed');
        results.stages.push({
          name: 'bug-detective',
          status: 'failed',
          error: error.message
        });
        throw error;
      }

      // Stage 3: audit plugin
      logger.info({ stage: 3 }, 'Running audit plugin');
      try {
        const auditResult = await this.runSlashCommand('/audit:audit', repoPath);
        results.stages.push({
          name: 'audit',
          status: 'completed',
          output: auditResult.stdout
        });

        const auditPath = path.join(outputDir, '03-security-audit.md');
        await fs.writeFile(auditPath, auditResult.stdout);
        logger.info({ auditPath }, 'Security audit saved');
      } catch (error) {
        logger.error({ error }, 'audit failed');
        results.stages.push({
          name: 'audit',
          status: 'failed',
          error: error.message
        });
        throw error;
      }

      // Commit audit results
      await this.commitChanges(repoPath, 'üîç Automated audit: bugfix plan, detective report, security audit\n\nü§ñ Generated with Claude Code AlephAuto\n\nCo-Authored-By: Claude <noreply@anthropic.com>');

      // Stage 4: ceo-quality-controller-agent
      logger.info({ stage: 4 }, 'Running ceo-quality-controller-agent');
      try {
        const qualityPrompt = 'Review all audit reports and validate quality standards before implementing fixes';
        const qualityResult = await this.runClaudeAgent('ceo-quality-controller-agent', qualityPrompt, repoPath);
        results.stages.push({
          name: 'ceo-quality-controller',
          status: 'completed',
          output: qualityResult.stdout
        });

        const qualityPath = path.join(outputDir, '04-quality-control.md');
        await fs.writeFile(qualityPath, qualityResult.stdout);
        logger.info({ qualityPath }, 'Quality control report saved');
      } catch (error) {
        logger.error({ error }, 'quality-controller failed');
        results.stages.push({
          name: 'ceo-quality-controller',
          status: 'failed',
          error: error.message
        });
        throw error;
      }

      // Commit quality control
      await this.commitChanges(repoPath, '‚úÖ Quality control validation complete\n\nü§ñ Generated with Claude Code AlephAuto\n\nCo-Authored-By: Claude <noreply@anthropic.com>');

      // Stage 5: refractor plugin - implement fixes
      logger.info({ stage: 5 }, 'Running refractor plugin to implement fixes');
      try {
        const refractorResult = await this.runSlashCommand('/refractor:refractor', repoPath);
        results.stages.push({
          name: 'refractor',
          status: 'completed',
          output: refractorResult.stdout
        });

        const refractorPath = path.join(outputDir, '05-refactor-implementation.md');
        await fs.writeFile(refractorPath, refractorResult.stdout);
        logger.info({ refractorPath }, 'Refactor implementation saved');
      } catch (error) {
        logger.error({ error }, 'refractor failed');
        results.stages.push({
          name: 'refractor',
          status: 'failed',
          error: error.message
        });
        throw error;
      }

      // Commit refactored code
      await this.commitChanges(repoPath, '‚ôªÔ∏è Automated refactoring and bug fixes implemented\n\nü§ñ Generated with Claude Code AlephAuto\n\nCo-Authored-By: Claude <noreply@anthropic.com>');

      // Create pull request
      const prTitle = `ü§ñ Automated Bug Fixes: ${projectName}`;
      const prBody = `## Automated Bug Fix Workflow

This PR was automatically generated by the BugfixAuditWorker (AlephAuto framework).

### Workflow Stages Completed:
${results.stages.map((stage, i) => `${i + 1}. **${stage.name}**: ${stage.status}`).join('\n')}

### Reports Generated:
- Bug Fix Plan
- Bug Detective Analysis
- Security Audit
- Quality Control Validation
- Refactor Implementation

### Testing:
- [ ] All automated tests pass
- [ ] Manual review of security audit findings
- [ ] Code quality meets standards

---
ü§ñ Generated with [Claude Code](https://claude.com/claude-code) AlephAuto

Co-Authored-By: Claude <noreply@anthropic.com>`;

      const pr = await this.createPullRequest(repoPath, prTitle, prBody);
      results.pullRequestUrl = pr.url;

      logger.info({ prUrl: pr.url }, 'Pull request created');

      // Save final results summary
      const summaryPath = path.join(outputDir, 'workflow-summary.json');
      await fs.writeFile(summaryPath, JSON.stringify(results, null, 2));

      return results;

    } catch (error) {
      logger.error({ error, jobId: job.id }, 'Bug fix workflow failed');

      // Save partial results
      const errorPath = path.join(outputDir, 'workflow-error.json');
      await fs.writeFile(errorPath, JSON.stringify({
        ...results,
        error: error.message,
        stack: error.stack
      }, null, 2));

      throw error;
    }
  }

  /**
   * Create jobs for all markdown files
   */
  async createJobsForAllMarkdownFiles() {
    const markdownFiles = await this.findMarkdownFiles();

    logger.info({ count: markdownFiles.length }, 'Creating jobs for markdown files');

    const jobs = [];

    for (const markdownFile of markdownFiles) {
      const { projectName, possibleRepoPaths } = this.getRepositoryFromPath(markdownFile);

      if (!projectName) {
        logger.warn({ markdownFile }, 'Could not determine project name, skipping');
        continue;
      }

      // Find the first existing repo path
      let repoPath = null;
      for (const possiblePath of possibleRepoPaths) {
        try {
          await fs.access(possiblePath);
          repoPath = possiblePath;
          break;
        } catch {
          // Path doesn't exist, try next
        }
      }

      if (!repoPath) {
        logger.warn({ markdownFile, possibleRepoPaths }, 'Could not find repository, skipping');
        continue;
      }

      // Check if it's a git repository
      try {
        await fs.access(path.join(repoPath, '.git'));
      } catch {
        logger.warn({ repoPath }, 'Not a git repository, skipping');
        continue;
      }

      // Create feature branch name
      const branchName = `bugfix/automated-audit-${Date.now()}`;

      // Create branch before adding job
      try {
        await this.createGitBranch(repoPath, branchName);
        logger.info({ repoPath, branchName }, 'Branch created');
      } catch (error) {
        logger.error({ error, repoPath }, 'Failed to create branch, skipping');
        continue;
      }

      const job = this.createJob({
        markdownFile,
        projectName,
        repoPath,
        branchName
      });

      jobs.push(job);
    }

    logger.info({ jobCount: jobs.length }, 'Jobs created');
    return jobs;
  }
}

export default BugfixAuditWorker;
</file>

<file path="bug-fixes/index.js">
// @ts-nocheck
import cron from 'node-cron';
import { BugfixAuditWorker } from './bugfix-audit-worker.js';
import { config } from '../core/config.js';
import path from 'path';
import fs from 'fs/promises';
import { createComponentLogger } from '../utils/logger.js';

const logger = createComponentLogger('BugfixAuditApp');

/**
 * BugfixAuditApp - Automated bug detection and fixing application
 *
 * Orchestrates the bugfix workflow:
 * 1. Scans ~/dev/active for .md files
 * 2. Analyzes with bugfix-planner, bug-detective, audit, quality-controller
 * 3. Implements fixes with refractor
 * 4. Creates git branches, commits at each stage, creates PRs
 *
 * Scheduled to run at 1 AM daily
 */
class BugfixAuditApp {
  constructor() {
    this.worker = new BugfixAuditWorker({
      maxConcurrent: 3, // Process 3 projects in parallel
      activeDocsDir: path.join(process.env.HOME, 'dev', 'active'),
      outputBaseDir: path.join(process.env.HOME, 'code', 'jobs', 'sidequest', 'bug-fixes', 'output'),
      logDir: config.logDir,
      sentryDsn: config.sentryDsn,
    });

    this.setupEventListeners();
  }

  /**
   * Setup event listeners for job events
   */
  setupEventListeners() {
    this.worker.on('job:created', (job) => {
      logger.info({
        jobId: job.id,
        projectName: job.data.projectName,
        markdownFile: job.data.markdownFile
      }, 'Job created');
    });

    this.worker.on('job:started', (job) => {
      logger.info({
        jobId: job.id,
        projectName: job.data.projectName
      }, 'Job started');
    });

    this.worker.on('job:completed', (job) => {
      const duration = job.completedAt - job.startedAt;
      logger.info({
        jobId: job.id,
        projectName: job.data.projectName,
        pullRequestUrl: job.result?.pullRequestUrl,
        duration
      }, 'Job completed');
    });

    this.worker.on('job:failed', (job) => {
      logger.error({
        jobId: job.id,
        projectName: job.data.projectName,
        error: job.error
      }, 'Job failed');
    });
  }

  /**
   * Run bug fix audit on all active projects
   */
  async runBugfixAudit() {
    logger.info({ timestamp: new Date().toISOString() }, 'Starting automated bugfix audit');

    const startTime = Date.now();

    try {
      // Create jobs for all markdown files
      const jobs = await this.worker.createJobsForAllMarkdownFiles();

      if (jobs.length === 0) {
        logger.info('No eligible projects found for bug fix audit');
        return;
      }

      logger.info({ jobCount: jobs.length }, 'Jobs created, waiting for completion');

      // Wait for all jobs to complete
      await this.waitForCompletion();

      const duration = Date.now() - startTime;
      const stats = this.worker.getStats();

      logger.info({
        durationSeconds: Math.round(duration / 1000),
        totalJobs: stats.total,
        completed: stats.completed,
        failed: stats.failed
      }, 'Bugfix audit complete');

      // Save run summary
      await this.saveRunSummary(stats, duration, jobs);

    } catch (error) {
      logger.error({ err: error }, 'Error during bugfix audit');
      throw error;
    }
  }

  /**
   * Wait for all jobs to complete
   */
  async waitForCompletion() {
    return new Promise((resolve) => {
      const checkInterval = setInterval(() => {
        const stats = this.worker.getStats();
        if (stats.active === 0 && stats.queued === 0) {
          clearInterval(checkInterval);
          resolve();
        }
      }, 1000);
    });
  }

  /**
   * Save run summary to logs
   */
  async saveRunSummary(stats, duration, jobs) {
    const summary = {
      timestamp: new Date().toISOString(),
      duration,
      stats,
      jobs: jobs.map(job => ({
        id: job.id,
        projectName: job.data.projectName,
        markdownFile: job.data.markdownFile,
        repoPath: job.data.repoPath,
        branchName: job.data.branchName,
        status: job.status,
        pullRequestUrl: job.result?.pullRequestUrl
      }))
    };

    const logsDir = path.join(process.env.HOME, 'code', 'jobs', 'sidequest', 'bug-fixes', 'logs');
    await fs.mkdir(logsDir, { recursive: true });

    const summaryPath = path.join(logsDir, `run-summary-${Date.now()}.json`);
    await fs.writeFile(summaryPath, JSON.stringify(summary, null, 2));

    logger.info({ summaryPath }, 'Run summary saved');
  }

  /**
   * Setup cron job to run at 1 AM
   */
  setupCronJob(schedule = '0 1 * * *') {
    logger.info({ schedule }, 'Setting up cron job');

    cron.schedule(schedule, async () => {
      logger.info({ triggerTime: new Date().toISOString() }, 'Cron job triggered');
      try {
        await this.runBugfixAudit();
      } catch (error) {
        logger.error({ err: error }, 'Cron job failed');
      }
    });

    logger.info('Cron job scheduled successfully');
  }

  /**
   * Setup one-time job for tonight at 1 AM
   */
  setupOneTimeJob() {
    const now = new Date();
    const tonight1AM = new Date();
    tonight1AM.setHours(1, 0, 0, 0);

    // If 1 AM has passed today, schedule for tomorrow
    if (now.getHours() >= 1) {
      tonight1AM.setDate(tonight1AM.getDate() + 1);
    }

    const delay = tonight1AM.getTime() - now.getTime();
    const hoursUntil = Math.floor(delay / (1000 * 60 * 60));
    const minutesUntil = Math.floor((delay % (1000 * 60 * 60)) / (1000 * 60));

    logger.info({
      scheduledTime: tonight1AM.toISOString(),
      hoursUntil,
      minutesUntil
    }, 'One-time job scheduled for tonight at 1 AM');

    setTimeout(async () => {
      logger.info({ triggerTime: new Date().toISOString() }, 'One-time job triggered');
      try {
        await this.runBugfixAudit();
      } catch (error) {
        logger.error({ err: error }, 'One-time job failed');
      }
    }, delay);
  }

  /**
   * Start the application
   */
  async start(options = {}) {
    logger.info({
      activeDocsDir: this.worker.activeDocsDir,
      outputBaseDir: this.worker.outputBaseDir
    }, 'BugfixAuditApp starting');

    if (options.oneTime) {
      // Run once tonight at 1 AM
      this.setupOneTimeJob();
      logger.info('One-time execution scheduled. Server will keep running.');
    } else if (options.recurring) {
      // Setup recurring cron job (1 AM daily)
      this.setupCronJob(options.schedule || '0 1 * * *');
      logger.info('Recurring cron job scheduled');
    }

    // Run immediately if requested
    if (options.runNow) {
      logger.info('Running immediately (runNow option)');
      await this.runBugfixAudit();
    }

    if (!options.runNow) {
      logger.info('Server running. Press Ctrl+C to exit');
    }
  }
}

// Parse command line arguments
const args = process.argv.slice(2);
const options = {
  oneTime: args.includes('--once') || args.includes('--one-time'),
  recurring: args.includes('--recurring') || args.includes('--cron'),
  runNow: args.includes('--now') || args.includes('--immediate'),
  schedule: null
};

// Extract custom schedule if provided
const scheduleIndex = args.indexOf('--schedule');
if (scheduleIndex !== -1 && args[scheduleIndex + 1]) {
  options.schedule = args[scheduleIndex + 1];
}

// Default to one-time execution tonight at 1 AM
if (!options.oneTime && !options.recurring && !options.runNow) {
  options.oneTime = true;
}

// Start the application
const app = new BugfixAuditApp();
app.start(options).catch((error) => {
  logger.error({ err: error }, 'Fatal error');
  process.exit(1);
});

// Keep process alive
process.on('SIGTERM', () => {
  logger.info('Received SIGTERM, shutting down gracefully');
  process.exit(0);
});

process.on('SIGINT', () => {
  logger.info('Received SIGINT, shutting down gracefully');
  process.exit(0);
});
</file>

<file path="bug-fixes/launch-tonight.sh">
#!/bin/bash

# Launch script for BugfixAudit - Automated Bug Detection & Fixing
# Schedules execution for tonight at 1 AM

set -e

SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
cd "$SCRIPT_DIR"

echo "ü§ñ BugfixAudit - Automated Bug Detection & Fixing"
echo "=================================================="
echo ""

# Check if npm packages are installed
if [ ! -d "node_modules" ]; then
    echo "üì¶ Installing dependencies..."
    npm install
    echo "‚úÖ Dependencies installed"
    echo ""
fi

# Check for required tools
echo "üîç Checking required tools..."

if ! command -v git &> /dev/null; then
    echo "‚ùå git is not installed"
    exit 1
fi
echo "  ‚úÖ git"

if ! command -v gh &> /dev/null; then
    echo "‚ùå gh CLI is not installed"
    echo "   Install with: brew install gh"
    exit 1
fi
echo "  ‚úÖ gh"

if ! command -v doppler &> /dev/null; then
    echo "‚ùå doppler is not installed"
    echo "   Install with: brew install dopplerhq/cli/doppler"
    exit 1
fi
echo "  ‚úÖ doppler"

if ! command -v node &> /dev/null; then
    echo "‚ùå node is not installed"
    exit 1
fi
echo "  ‚úÖ node $(node --version)"

echo ""

# Calculate time until 1 AM
current_hour=$(date +%H)
current_minute=$(date +%M)

if [ "$current_hour" -lt 1 ]; then
    hours_until=$((1 - current_hour))
    minutes_until=$((60 - current_minute))
else
    hours_until=$((25 - current_hour))
    minutes_until=$((60 - current_minute))
fi

echo "‚è∞ Scheduling execution for tonight at 1:00 AM"
echo "   Current time: $(date '+%H:%M')"
echo "   Time until execution: ${hours_until}h ${minutes_until}m"
echo ""
echo "üìã Workflow:"
echo "   1. Scan ~/dev/active for markdown files"
echo "   2. Create git branches for each project"
echo "   3. Run analysis: bugfix-planner, bug-detective, audit, quality-controller"
echo "   4. Implement fixes with refractor"
echo "   5. Commit after each stage"
echo "   6. Create pull requests"
echo ""
echo "üìÇ Output will be saved to:"
echo "   ~/code/jobs/sidequest/bug-fixes/output/"
echo ""
echo "üöÄ Starting background process..."
echo ""

# Run in background and detach
nohup npm run start:once > logs/bugfix-audit-$(date +%Y%m%d-%H%M%S).log 2>&1 &

PID=$!

echo "‚úÖ Process started with PID: $PID"
echo ""
echo "üìä Monitor progress:"
echo "   tail -f logs/bugfix-audit-*.log"
echo ""
echo "üõë Stop execution:"
echo "   kill $PID"
echo ""
echo "üí§ The process will run at 1 AM and then exit."
echo "   You can close this terminal."
echo ""
</file>

<file path="bug-fixes/package.json">
{
  "name": "@alephauto/bugfix-audit",
  "version": "1.0.0",
  "description": "Automated bug detection and fixing using Claude Code agents and plugins",
  "type": "module",
  "main": "index.js",
  "scripts": {
    "start": "doppler run -- node index.js",
    "start:now": "doppler run -- node index.js --now",
    "start:once": "doppler run -- node index.js --once",
    "start:recurring": "doppler run -- node index.js --recurring",
    "dev": "doppler run -- node --watch index.js --now",
    "test": "echo \"No tests yet\" && exit 0"
  },
  "keywords": [
    "automation",
    "bugfix",
    "audit",
    "claude-code",
    "alephauto"
  ],
  "author": "Alyshia Ledlie",
  "license": "MIT",
  "dependencies": {
    "node-cron": "^3.0.3",
    "pino": "^8.17.2",
    "pino-pretty": "^10.3.1"
  },
  "engines": {
    "node": ">=18.0.0"
  }
}
</file>

<file path="core/config.js">
import path from 'path';
import os from 'os';
import { fileURLToPath } from 'url';
import { dirname } from 'path';

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

/**
 * Centralized configuration for AlephAuto
 * All paths are resolved to absolute paths for consistency
 */
export const config = {
  // Base directories
  codeBaseDir: process.env.CODE_BASE_DIR || path.join(os.homedir(), 'code'),

  // Output directories (relative to project root or absolute)
  outputBaseDir: process.env.OUTPUT_BASE_DIR
    ? path.resolve(process.env.OUTPUT_BASE_DIR)
    : path.resolve(__dirname, 'output', 'condense'),

  logDir: process.env.LOG_DIR
    ? path.resolve(process.env.LOG_DIR)
    : path.resolve(__dirname, 'logs'),

  scanReportsDir: process.env.SCAN_REPORTS_DIR
    ? path.resolve(process.env.SCAN_REPORTS_DIR)
    : path.resolve(__dirname, 'output', 'directory-scan-reports'),

  // Job processing
  maxConcurrent: parseInt(process.env.MAX_CONCURRENT || '5', 10),

  // Sentry monitoring
  sentryDsn: process.env.SENTRY_DSN,
  nodeEnv: process.env.NODE_ENV || 'production',

  // Cron schedules (default: 2 AM for repomix, 3 AM for docs)
  repomixSchedule: process.env.CRON_SCHEDULE || '0 2 * * *',
  docSchedule: process.env.DOC_CRON_SCHEDULE || '0 3 * * *',

  // Feature flags
  runOnStartup: process.env.RUN_ON_STARTUP === 'true',
  forceEnhancement: process.env.FORCE_ENHANCEMENT === 'true',

  // Git workflow feature flags
  enableGitWorkflow: process.env.ENABLE_GIT_WORKFLOW === 'true',
  gitBaseBranch: process.env.GIT_BASE_BRANCH || 'main',
  gitBranchPrefix: process.env.GIT_BRANCH_PREFIX || 'automated',
  gitDryRun: process.env.GIT_DRY_RUN === 'true', // Skip push/PR in dry run mode

  // Repomix settings
  repomixTimeout: parseInt(process.env.REPOMIX_TIMEOUT || '600000', 10), // 10 minutes
  repomixMaxBuffer: parseInt(process.env.REPOMIX_MAX_BUFFER || '52428800', 10), // 50MB
  repomixIgnorePatterns: process.env.REPOMIX_IGNORE_PATTERNS
    ? process.env.REPOMIX_IGNORE_PATTERNS.split(',')
    : ['**/README.md', '**/README.MD', '**/*.md'], // Skip README and markdown files by default

  // Schema.org MCP integration / Documentation Enhancement
  schemaMcpUrl: process.env.SCHEMA_MCP_URL,
  skipDocEnhancement: process.env.SKIP_DOC_ENHANCEMENT === 'true', // Skip README Schema.org enhancement

  // Logging
  logLevel: process.env.LOG_LEVEL || 'info',

  // Directory scanner exclusions
  excludeDirs: [
    'node_modules',
    '.git',
    'dist',
    'build',
    'coverage',
    '.next',
    '.nuxt',
    'vendor',
    '__pycache__',
    '.venv',
    'venv',
    'target',
    '.idea',
    '.vscode',
    'jobs',
    '.DS_Store',
    'Thumbs.db',
    '*.swp',
    '*.swo'
  ],

  // Health check server
  healthCheckPort: parseInt(process.env.HEALTH_CHECK_PORT || '3000', 10),

  // API server port
  apiPort: parseInt(process.env.JOBS_API_PORT || '8080', 10),

  // Doppler resilience configuration
  doppler: {
    // Circuit breaker settings
    failureThreshold: parseInt(process.env.DOPPLER_FAILURE_THRESHOLD || '3', 10),
    successThreshold: parseInt(process.env.DOPPLER_SUCCESS_THRESHOLD || '2', 10),
    timeout: parseInt(process.env.DOPPLER_TIMEOUT || '5000', 10), // 5s before attempting recovery

    // Exponential backoff settings
    baseDelayMs: parseInt(process.env.DOPPLER_BASE_DELAY_MS || '1000', 10), // 1s
    backoffMultiplier: parseFloat(process.env.DOPPLER_BACKOFF_MULTIPLIER || '2.0'),
    maxBackoffMs: parseInt(process.env.DOPPLER_MAX_BACKOFF_MS || '10000', 10), // 10s

    // Cache settings
    cacheFile: process.env.DOPPLER_CACHE_FILE || path.join(os.homedir(), '.doppler', '.fallback.json')
  },

  // Project root directory
  projectRoot: __dirname,
};

/**
 * Validate configuration on import
 */
function validateConfig() {
  const errors = [];

  if (config.maxConcurrent < 1 || config.maxConcurrent > 50) {
    errors.push('MAX_CONCURRENT must be between 1 and 50');
  }

  if (config.repomixTimeout < 1000) {
    errors.push('REPOMIX_TIMEOUT must be at least 1000ms');
  }

  if (config.repomixMaxBuffer < 1024) {
    errors.push('REPOMIX_MAX_BUFFER must be at least 1024 bytes');
  }

  // Doppler resilience validation
  if (config.doppler.failureThreshold < 1 || config.doppler.failureThreshold > 10) {
    errors.push('DOPPLER_FAILURE_THRESHOLD must be between 1 and 10');
  }

  if (config.doppler.successThreshold < 1 || config.doppler.successThreshold > 10) {
    errors.push('DOPPLER_SUCCESS_THRESHOLD must be between 1 and 10');
  }

  if (config.doppler.timeout < 1000) {
    errors.push('DOPPLER_TIMEOUT must be at least 1000ms');
  }

  if (config.doppler.baseDelayMs < 100) {
    errors.push('DOPPLER_BASE_DELAY_MS must be at least 100ms');
  }

  if (config.doppler.backoffMultiplier < 1.0 || config.doppler.backoffMultiplier > 5.0) {
    errors.push('DOPPLER_BACKOFF_MULTIPLIER must be between 1.0 and 5.0');
  }

  if (config.doppler.maxBackoffMs < 1000) {
    errors.push('DOPPLER_MAX_BACKOFF_MS must be at least 1000ms');
  }

  if (errors.length > 0) {
    throw new Error(`Configuration validation failed:\n${errors.join('\n')}`);
  }
}

// Validate on import
validateConfig();

export default config;
</file>

<file path="core/database.js">
/**
 * SQLite Database for Job History Persistence
 *
 * Stores job history to survive server restarts.
 * Located at: data/jobs.db
 */

import Database from 'better-sqlite3';
import path from 'path';
import fs from 'fs';
import { fileURLToPath } from 'url';
import { createComponentLogger } from '../utils/logger.js';

const __dirname = path.dirname(fileURLToPath(import.meta.url));
const logger = createComponentLogger('Database');

// Database path - in project root/data directory
const DB_PATH = path.join(__dirname, '../../data/jobs.db');

let db = null;

/**
 * Initialize the database connection and create tables
 */
export function initDatabase() {
  if (db) return db;

  // Ensure data directory exists
  const dataDir = path.dirname(DB_PATH);
  if (!fs.existsSync(dataDir)) {
    fs.mkdirSync(dataDir, { recursive: true });
  }

  db = new Database(DB_PATH);

  // Enable WAL mode for better concurrent access
  db.pragma('journal_mode = WAL');

  // Create jobs table
  db.exec(`
    CREATE TABLE IF NOT EXISTS jobs (
      id TEXT PRIMARY KEY,
      pipeline_id TEXT NOT NULL,
      status TEXT NOT NULL DEFAULT 'queued',
      created_at TEXT NOT NULL,
      started_at TEXT,
      completed_at TEXT,
      data TEXT,
      result TEXT,
      error TEXT,
      git TEXT
    );

    CREATE INDEX IF NOT EXISTS idx_jobs_pipeline_id ON jobs(pipeline_id);
    CREATE INDEX IF NOT EXISTS idx_jobs_status ON jobs(status);
    CREATE INDEX IF NOT EXISTS idx_jobs_created_at ON jobs(created_at DESC);
  `);

  logger.info({ dbPath: DB_PATH }, 'Database initialized');

  return db;
}

/**
 * Get database instance (initializes if needed)
 */
export function getDatabase() {
  if (!db) {
    return initDatabase();
  }
  return db;
}

/**
 * Save a job to the database
 */
export function saveJob(job) {
  const db = getDatabase();

  const stmt = db.prepare(`
    INSERT OR REPLACE INTO jobs
    (id, pipeline_id, status, created_at, started_at, completed_at, data, result, error, git)
    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
  `);

  stmt.run(
    job.id,
    job.pipelineId || 'duplicate-detection',
    job.status,
    job.createdAt || new Date().toISOString(),
    job.startedAt || null,
    job.completedAt || null,
    job.data ? JSON.stringify(job.data) : null,
    job.result ? JSON.stringify(job.result) : null,
    job.error ? JSON.stringify(job.error) : null,
    job.git ? JSON.stringify(job.git) : null
  );

  logger.debug({ jobId: job.id, status: job.status }, 'Job saved to database');
}

/**
 * Get jobs for a pipeline with filtering and pagination
 *
 * @param {string} pipelineId - Pipeline identifier
 * @param {Object} options - Query options
 * @param {string} [options.status] - Filter by status
 * @param {number} [options.limit=10] - Max results per page
 * @param {number} [options.offset=0] - Pagination offset
 * @param {string} [options.tab] - Tab context (failed, recent, all)
 * @param {boolean} [options.includeTotal=false] - Include total count in response
 * @returns {Array|Object} Array of jobs, or {jobs: Array, total: number} if includeTotal=true
 */
export function getJobs(pipelineId, options = {}) {
  const db = getDatabase();
  const { status, limit = 10, offset = 0, tab, includeTotal = false } = options;

  // Build count query (only if includeTotal requested)
  let totalCount = null;
  if (includeTotal) {
    let countQuery = 'SELECT COUNT(*) as count FROM jobs WHERE pipeline_id = ?';
    const countParams = [pipelineId];

    if (status) {
      countQuery += ' AND status = ?';
      countParams.push(status);
    } else if (tab === 'failed') {
      countQuery += ' AND status = ?';
      countParams.push('failed');
    }

    const countStmt = db.prepare(countQuery);
    const countResult = countStmt.get(...countParams);
    totalCount = countResult.count;
  }

  // Build data query
  let query = 'SELECT * FROM jobs WHERE pipeline_id = ?';
  /** @type {Array<string|number>} */
  const params = [pipelineId];

  // Filter by status
  if (status) {
    query += ' AND status = ?';
    params.push(status);
  } else if (tab === 'failed') {
    query += ' AND status = ?';
    params.push('failed');
  }

  // Order by created_at descending (newest first)
  query += ' ORDER BY created_at DESC';

  // Apply pagination
  query += ' LIMIT ? OFFSET ?';
  params.push(String(limit), String(offset));

  const stmt = db.prepare(query);
  const rows = stmt.all(...params);

  // Parse JSON fields
  const jobs = rows.map(row => ({
    id: row.id,
    pipelineId: row.pipeline_id,
    status: row.status,
    createdAt: row.created_at,
    startedAt: row.started_at,
    completedAt: row.completed_at,
    data: row.data ? JSON.parse(row.data) : null,
    result: row.result ? JSON.parse(row.result) : null,
    error: row.error ? JSON.parse(row.error) : null,
    git: row.git ? JSON.parse(row.git) : null
  }));

  // Return with or without total count based on includeTotal option
  if (includeTotal) {
    return { jobs, total: totalCount };
  } else {
    return jobs;  // Backward compatible - just return array
  }
}

/**
 * Get job counts for a pipeline
 */
export function getJobCounts(pipelineId) {
  const db = getDatabase();

  const stmt = db.prepare(`
    SELECT
      COUNT(*) as total,
      SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) as completed,
      SUM(CASE WHEN status = 'failed' THEN 1 ELSE 0 END) as failed,
      SUM(CASE WHEN status = 'running' THEN 1 ELSE 0 END) as running,
      SUM(CASE WHEN status = 'queued' THEN 1 ELSE 0 END) as queued
    FROM jobs
    WHERE pipeline_id = ?
  `);

  return stmt.get(pipelineId);
}

/**
 * Get the most recent job for a pipeline
 */
export function getLastJob(pipelineId) {
  const db = getDatabase();

  const stmt = db.prepare(`
    SELECT * FROM jobs
    WHERE pipeline_id = ?
    ORDER BY created_at DESC
    LIMIT 1
  `);

  const row = stmt.get(pipelineId);

  if (!row) return null;

  return {
    id: row.id,
    pipelineId: row.pipeline_id,
    status: row.status,
    createdAt: row.created_at,
    startedAt: row.started_at,
    completedAt: row.completed_at,
    data: row.data ? JSON.parse(row.data) : null,
    result: row.result ? JSON.parse(row.result) : null,
    error: row.error ? JSON.parse(row.error) : null,
    git: row.git ? JSON.parse(row.git) : null
  };
}

/**
 * Get all pipelines with job statistics
 *
 * Returns statistics for ALL pipelines in the database, including job counts
 * by status and last run timestamp.
 *
 * @returns {Array<{pipeline_id: string, total: number, completed: number, failed: number, running: number, queued: number, last_run: string|null}>} Array of pipeline statistics with pipeline_id, total, completed, failed, running, queued job counts, and last_run ISO timestamp
 *
 * @example
 * const stats = getAllPipelineStats();
 * // Returns: [
 * //   { pipeline_id: 'duplicate-detection', total: 10, completed: 8, failed: 2, ... },
 * //   { pipeline_id: 'repomix', total: 201, completed: 80, failed: 121, ... },
 * //   ...
 * // ]
 */
export function getAllPipelineStats() {
  const db = getDatabase();

  const stmt = db.prepare(`
    SELECT
      pipeline_id,
      COUNT(*) as total,
      SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) as completed,
      SUM(CASE WHEN status = 'failed' THEN 1 ELSE 0 END) as failed,
      SUM(CASE WHEN status = 'running' THEN 1 ELSE 0 END) as running,
      SUM(CASE WHEN status = 'queued' THEN 1 ELSE 0 END) as queued,
      MAX(completed_at) as last_run
    FROM jobs
    GROUP BY pipeline_id
    ORDER BY pipeline_id
  `);

  return stmt.all();
}

/**
 * Import existing reports into the database
 */
export async function importReportsToDatabase(reportsDir) {
  const fsModule = await import('fs');
  const db = getDatabase();

  if (!fsModule.existsSync(reportsDir)) {
    logger.warn({ reportsDir }, 'Reports directory not found');
    return 0;
  }

  const files = fsModule.readdirSync(reportsDir)
    .filter(f => f.endsWith('-summary.json'));

  let imported = 0;

  for (const file of files) {
    try {
      const filePath = path.join(reportsDir, file);
      const content = JSON.parse(fsModule.readFileSync(filePath, 'utf8'));
      const stats = fsModule.statSync(filePath);

      // Extract date from filename (e.g., inter-project-scan-2repos-2025-11-24-summary.json)
      const dateMatch = file.match(/(\d{4}-\d{2}-\d{2})/);
      const dateStr = dateMatch ? dateMatch[1] : stats.mtime.toISOString().split('T')[0];

      // Create job ID from filename
      const jobId = file.replace('-summary.json', '');

      // Check if already imported
      const existing = db.prepare('SELECT id FROM jobs WHERE id = ?').get(jobId);
      if (existing) continue;

      // Import as completed job
      saveJob({
        id: jobId,
        pipelineId: 'duplicate-detection',
        status: 'completed',
        createdAt: `${dateStr}T00:00:00.000Z`,
        startedAt: `${dateStr}T00:00:00.000Z`,
        completedAt: stats.mtime.toISOString(),
        data: {
          scanType: content.scanType || 'inter-project',
          repositories: content.repositories || []
        },
        result: {
          totalDuplicates: content.totalDuplicates || 0,
          totalBlocks: content.totalBlocks || 0,
          scanDuration: content.scanDuration || null,
          reportPath: filePath
        }
      });

      imported++;
    } catch (err) {
      logger.error({ file, error: err.message }, 'Failed to import report');
    }
  }

  logger.info({ imported, total: files.length }, 'Imported existing reports');
  return imported;
}

/**
 * Import job logs from sidequest/logs directory
 */
export async function importLogsToDatabase(logsDir) {
  const fsModule = await import('fs');
  const db = getDatabase();

  if (!fsModule.existsSync(logsDir)) {
    logger.warn({ logsDir }, 'Logs directory not found');
    return 0;
  }

  const files = fsModule.readdirSync(logsDir)
    .filter(f => f.endsWith('.json'));

  let imported = 0;

  // Map filename prefixes to pipeline IDs
  const pipelineMap = {
    'git-activity': 'git-activity',
    'claude-health': 'claude-health',
    'plugin-audit': 'plugin-manager',
    'gitignore': 'gitignore-manager',
    'schema-enhancement': 'schema-enhancement',
    'doc-enhancement': 'schema-enhancement', // Legacy compatibility
    'repomix': 'repomix'
  };

  for (const file of files) {
    try {
      const filePath = path.join(logsDir, file);
      const content = JSON.parse(fsModule.readFileSync(filePath, 'utf8'));
      const stats = fsModule.statSync(filePath);

      // Extract pipeline type from filename
      let pipelineId = 'unknown';
      for (const [prefix, id] of Object.entries(pipelineMap)) {
        if (file.startsWith(prefix)) {
          pipelineId = id;
          break;
        }
      }

      // Create job ID from filename
      const jobId = file.replace('.json', '');

      // Check if already imported
      const existing = db.prepare('SELECT id FROM jobs WHERE id = ?').get(jobId);
      if (existing) continue;

      // Determine status from content
      const status = content.error ? 'failed' : 'completed';

      // Import job
      saveJob({
        id: jobId,
        pipelineId,
        status,
        createdAt: content.startTime || stats.mtime.toISOString(),
        startedAt: content.startTime || stats.mtime.toISOString(),
        completedAt: content.endTime || stats.mtime.toISOString(),
        data: content.parameters || content.config || {},
        result: content.result || content.summary || content,
        error: content.error || null
      });

      imported++;
    } catch (err) {
      logger.error({ file, error: err.message }, 'Failed to import log');
    }
  }

  logger.info({ imported, total: files.length }, 'Imported existing logs');
  return imported;
}

/**
 * Close the database connection
 */
export function closeDatabase() {
  if (db) {
    db.close();
    db = null;
    logger.info('Database closed');
  }
}

export default {
  initDatabase,
  getDatabase,
  saveJob,
  getJobs,
  getJobCounts,
  getLastJob,
  importReportsToDatabase,
  importLogsToDatabase,
  closeDatabase
};
</file>

<file path="core/index.js">
import cron from 'node-cron';
import { RepomixWorker } from '../workers/repomix-worker.js';
import { DirectoryScanner } from '../utils/directory-scanner.js';
import { config } from './config.js';
import path from 'path';
import fs from 'fs/promises';
import { createComponentLogger } from '../utils/logger.js';

const logger = createComponentLogger('RepomixCronApp');

/**
 * Main application entry point
 */
class RepomixCronApp {
  constructor() {
    this.worker = new RepomixWorker({
      maxConcurrent: config.maxConcurrent,
      outputBaseDir: config.outputBaseDir,
      codeBaseDir: config.codeBaseDir,
      logDir: config.logDir,
      sentryDsn: config.sentryDsn,
    });

    this.scanner = new DirectoryScanner({
      baseDir: config.codeBaseDir,
      outputDir: config.scanReportsDir,
      excludeDirs: config.excludeDirs,
    });

    this.setupEventListeners();
  }

  /**
   * Setup event listeners for job events
   */
  setupEventListeners() {
    this.worker.on('job:created', (job) => {
      logger.info({ jobId: job.id }, 'Job created');
    });

    this.worker.on('job:started', (job) => {
      logger.info({ jobId: job.id, relativePath: job.data.relativePath }, 'Job started');
    });

    this.worker.on('job:completed', (job) => {
      const duration = job.completedAt - job.startedAt;
      logger.info({
        jobId: job.id,
        relativePath: job.data.relativePath,
        duration
      }, 'Job completed');
    });

    this.worker.on('job:failed', (job) => {
      logger.error({
        jobId: job.id,
        relativePath: job.data.relativePath,
        error: job.error
      }, 'Job failed');
    });
  }

  /**
   * Run repomix on all directories
   */
  async runRepomixOnAllDirectories() {
    logger.info({ baseDir: this.scanner.baseDir }, 'Starting repomix run');

    const startTime = Date.now();

    try {
      // Scan all directories
      const directories = await this.scanner.scanDirectories();
      logger.info({ directoryCount: directories.length }, 'Directories found');

      // Save scan results
      logger.info('Saving scan results');
      const scanResults = await this.scanner.generateAndSaveScanResults(directories);
      logger.info({
        reportPath: scanResults.reportPath,
        treePath: scanResults.treePath,
        summaryPath: scanResults.summaryPath,
        maxDepth: scanResults.summary.maxDepth,
        topDirectories: scanResults.summary.stats.topDirectoryNames.slice(0, 3).map(d => d.name)
      }, 'Scan results saved');

      // Create jobs for each directory
      let jobCount = 0;
      for (const dir of directories) {
        this.worker.createRepomixJob(dir.fullPath, dir.relativePath);
        jobCount++;
      }

      logger.info({ jobCount }, 'Jobs created');

      // Wait for all jobs to complete
      await this.waitForCompletion();

      const duration = Date.now() - startTime;
      const stats = this.worker.getStats();

      logger.info({
        durationSeconds: Math.round(duration / 1000),
        totalJobs: stats.total,
        completed: stats.completed,
        failed: stats.failed
      }, 'Repomix run complete');

      // Save run summary
      await this.saveRunSummary(stats, duration);

    } catch (error) {
      logger.error({ err: error }, 'Error during repomix run');
      throw error;
    }
  }

  /**
   * Wait for all jobs to complete
   */
  async waitForCompletion() {
    return new Promise((resolve) => {
      const checkInterval = setInterval(() => {
        const stats = this.worker.getStats();
        if (stats.active === 0 && stats.queued === 0) {
          clearInterval(checkInterval);
          resolve();
        }
      }, 1000);
    });
  }

  /**
   * Save run summary to logs
   */
  async saveRunSummary(stats, duration) {
    const summary = {
      timestamp: new Date().toISOString(),
      duration,
      stats,
    };

    const summaryPath = path.join('../logs', `run-summary-${Date.now()}.json`);
    await fs.writeFile(summaryPath, JSON.stringify(summary, null, 2));
  }

  /**
   * Setup cron job
   */
  setupCronJob(schedule = '0 2 * * *') {
    // Default: Run at 2 AM every day
    logger.info({ schedule }, 'Setting up cron job');

    cron.schedule(schedule, async () => {
      logger.info({ triggerTime: new Date().toISOString() }, 'Cron job triggered');
      try {
        await this.runRepomixOnAllDirectories();
      } catch (error) {
        logger.error({ err: error }, 'Cron job failed');
      }
    });

    logger.info('Cron job scheduled successfully');
  }

  /**
   * Start the application
   */
  async start() {
    logger.info({
      codeDirectory: this.scanner.baseDir,
      outputDirectory: this.worker.outputBaseDir,
      logDirectory: this.worker.logDir
    }, 'Repomix Cron Sidequest Server starting');

    // Setup cron job
    // Schedule: '0 2 * * *' = 2 AM daily
    // For testing: '*/5 * * * *' = every 5 minutes
    this.setupCronJob(config.repomixSchedule);

    // Run immediately on startup if requested
    if (config.runOnStartup) {
      logger.info('Running immediately (RUN_ON_STARTUP=true)');
      await this.runRepomixOnAllDirectories();
    }

    logger.info('Server running. Press Ctrl+C to exit');
  }
}

// Start the application
const app = new RepomixCronApp();
app.start().catch((error) => {
  logger.error({ err: error }, 'Fatal error');
  process.exit(1);
});
</file>

<file path="core/server.js">
import { EventEmitter } from 'events';
import * as Sentry from '@sentry/node';
import { config } from './config.js';
import fs from 'fs/promises';
import path from 'path';
import { createComponentLogger } from '../utils/logger.js';
import { safeErrorMessage } from '../pipeline-core/utils/error-helpers.js';
import { BranchManager } from '../pipeline-core/git/branch-manager.js';
import { saveJob, getJobCounts, getLastJob, initDatabase } from './database.js';

const logger = createComponentLogger('SidequestServer');

/**
 * SidequestServer - Manages job execution with Sentry logging and optional Git workflow
 */
export class SidequestServer extends EventEmitter {
  constructor(options = {}) {
    super();
    this.jobs = new Map();
    this.jobHistory = [];
    this.maxConcurrent = options.maxConcurrent ?? 5;
    this.activeJobs = 0;
    this.queue = [];
    this.logDir = options.logDir || './logs';

    // Git workflow options
    this.gitWorkflowEnabled = options.gitWorkflowEnabled ?? false;
    this.gitBranchPrefix = options.gitBranchPrefix || 'automated';
    this.gitBaseBranch = options.gitBaseBranch || 'main';
    this.gitDryRun = options.gitDryRun ?? false;
    this.jobType = options.jobType || 'job'; // Used for branch naming

    // Initialize BranchManager if git workflow is enabled
    if (this.gitWorkflowEnabled) {
      this.branchManager = new BranchManager({
        baseBranch: this.gitBaseBranch,
        branchPrefix: this.gitBranchPrefix,
        dryRun: this.gitDryRun
      });
      logger.info({
        enabled: true,
        baseBranch: this.gitBaseBranch,
        branchPrefix: this.gitBranchPrefix,
        dryRun: this.gitDryRun
      }, 'Git workflow enabled');
    }

    // Initialize SQLite database for job persistence
    try {
      initDatabase();
      logger.info('Job history database initialized');
    } catch (err) {
      logger.error({ error: err.message }, 'Failed to initialize database');
    }

    // Initialize Sentry
    Sentry.init({
      dsn: options.sentryDsn || config.sentryDsn,
      environment: config.nodeEnv,
      tracesSampleRate: 1.0,
    });
  }

  /**
   * Create a new job
   */
  createJob(jobId, jobData) {
    const job = {
      id: jobId,
      status: 'queued',
      data: jobData,
      createdAt: new Date(),
      startedAt: null,
      completedAt: null,
      error: null,
      result: null,
      // Git workflow metadata
      git: {
        branchName: null,
        originalBranch: null,
        commitSha: null,
        prUrl: null,
        changedFiles: []
      }
    };

    this.jobs.set(jobId, job);
    this.queue.push(jobId);

    Sentry.addBreadcrumb({
      category: 'job',
      message: `Job ${jobId} created`,
      level: 'info',
      data: { jobId, jobData },
    });

    this.emit('job:created', job);
    this.processQueue();

    return job;
  }

  /**
   * Process the job queue
   */
  async processQueue() {
    while (this.queue.length > 0 && this.activeJobs < this.maxConcurrent) {
      const jobId = this.queue.shift();
      const job = this.jobs.get(jobId);

      if (!job) continue;

      this.activeJobs++;
      this.executeJob(jobId).catch(error => {
        logger.error({ err: error, jobId }, 'Error executing job');
      });
    }
  }

  /**
   * Execute a job
   */
  async executeJob(jobId) {
    const job = this.jobs.get(jobId);
    if (!job) return;

    // Use Sentry v8 API
    return await Sentry.startSpan({
      op: 'job.execute',
      name: `Execute Job: ${jobId}`,
    }, async () => {
      let branchCreated = false;

      try {
        job.status = 'running';
        job.startedAt = new Date();
        this.emit('job:started', job);

        Sentry.addBreadcrumb({
          category: 'job',
          message: `Job ${jobId} started`,
          level: 'info',
        });

        // Git workflow: Create branch before job execution
        if (this.gitWorkflowEnabled && job.data.repositoryPath) {
          try {
            const branchInfo = await this.branchManager.createJobBranch(
              job.data.repositoryPath,
              {
                jobId: job.id,
                jobType: this.jobType,
                description: job.data.description || job.data.repository
              }
            );

            if (branchInfo.branchName) {
              job.git.branchName = branchInfo.branchName;
              job.git.originalBranch = branchInfo.originalBranch;
              branchCreated = true;

              logger.info({
                jobId: job.id,
                branchName: branchInfo.branchName,
                repositoryPath: job.data.repositoryPath
              }, 'Created job branch');

              Sentry.addBreadcrumb({
                category: 'git',
                message: `Created branch ${branchInfo.branchName}`,
                level: 'info',
              });
            }
          } catch (gitError) {
            logger.warn({ err: gitError, jobId }, 'Failed to create branch, continuing without git workflow');
            // Continue job execution even if branch creation fails
          }
        }

        // Execute the job's handler
        const result = await this.runJobHandler(job);

        job.status = 'completed';
        job.completedAt = new Date();
        job.result = result;

        // Git workflow: Commit, push, and create PR after successful job completion
        if (branchCreated && this.gitWorkflowEnabled) {
          try {
            await this._handleGitWorkflowSuccess(job);
          } catch (gitError) {
            logger.error({ err: gitError, jobId }, 'Git workflow failed, but job completed successfully');
            // Don't fail the job if git operations fail
            Sentry.captureException(gitError, {
              tags: {
                component: 'git-workflow',
                jobId: job.id
              }
            });
          }
        }

        this.emit('job:completed', job);
        this.jobHistory.push({ ...job });

        // Persist to SQLite
        try {
          saveJob({
            id: job.id,
            pipelineId: this.jobType,
            status: job.status,
            createdAt: job.createdAt?.toISOString?.() || job.createdAt,
            startedAt: job.startedAt?.toISOString?.() || job.startedAt,
            completedAt: job.completedAt?.toISOString?.() || job.completedAt,
            data: job.data,
            result: job.result,
            error: job.error,
            git: job.git
          });
        } catch (dbErr) {
          logger.error({ error: dbErr.message, jobId }, 'Failed to persist job to database');
        }

        // Log to file
        await this.logJobCompletion(job);

        Sentry.addBreadcrumb({
          category: 'job',
          message: `Job ${jobId} completed`,
          level: 'info',
        });

      } catch (error) {
        job.status = 'failed';
        job.completedAt = new Date();
        job.error = safeErrorMessage(error);

        // Git workflow: Cleanup branch on failure
        if (branchCreated && this.gitWorkflowEnabled) {
          try {
            await this.branchManager.cleanupBranch(
              job.data.repositoryPath,
              job.git.branchName,
              job.git.originalBranch
            );
            logger.info({ jobId, branchName: job.git.branchName }, 'Cleaned up branch after job failure');
          } catch (cleanupError) {
            logger.warn({ err: cleanupError, jobId }, 'Failed to cleanup branch');
          }
        }

        this.emit('job:failed', job, error);
        this.jobHistory.push({ ...job });

        // Persist to SQLite
        try {
          saveJob({
            id: job.id,
            pipelineId: this.jobType,
            status: job.status,
            createdAt: job.createdAt?.toISOString?.() || job.createdAt,
            startedAt: job.startedAt?.toISOString?.() || job.startedAt,
            completedAt: job.completedAt?.toISOString?.() || job.completedAt,
            data: job.data,
            result: job.result,
            error: job.error,
            git: job.git
          });
        } catch (dbErr) {
          logger.error({ error: dbErr.message, jobId }, 'Failed to persist job to database');
        }

        // Log error to Sentry
        Sentry.captureException(error, {
          tags: {
            jobId: job.id,
            jobType: this.jobType,
          },
          contexts: {
            job: {
              id: job.id,
              data: job.data,
              startedAt: job.startedAt,
            },
          },
        });

        // Log to file
        await this.logJobFailure(job, error);

        logger.error({ err: error, jobId, jobData: job.data }, 'Job failed');
      } finally {
        this.activeJobs--;
        this.processQueue();
      }
    });
  }

  /**
   * Handle git workflow after successful job completion
   * @private
   */
  async _handleGitWorkflowSuccess(job) {
    const repositoryPath = job.data.repositoryPath;

    // Check if there are changes
    const hasChanges = await this.branchManager.hasChanges(repositoryPath);

    if (!hasChanges) {
      logger.info({ jobId: job.id }, 'No changes to commit, cleaning up branch');

      // Cleanup branch if no changes
      await this.branchManager.cleanupBranch(
        repositoryPath,
        job.git.branchName,
        job.git.originalBranch
      );

      return;
    }

    // Get changed files
    job.git.changedFiles = await this.branchManager.getChangedFiles(repositoryPath);

    logger.info({
      jobId: job.id,
      filesChanged: job.git.changedFiles.length
    }, 'Changes detected, committing');

    // Commit changes
    const commitMessage = await this._generateCommitMessage(job);
    const commitContext = {
      message: commitMessage.title,
      description: commitMessage.body,
      jobId: job.id
    };

    job.git.commitSha = await this.branchManager.commitChanges(
      repositoryPath,
      commitContext
    );

    // Push branch
    const pushed = await this.branchManager.pushBranch(
      repositoryPath,
      job.git.branchName
    );

    if (!pushed) {
      logger.warn({ jobId: job.id }, 'Failed to push branch, skipping PR creation');
      return;
    }

    // Create PR
    const prContext = await this._generatePRContext(job);
    job.git.prUrl = await this.branchManager.createPullRequest(
      repositoryPath,
      prContext
    );

    if (job.git.prUrl) {
      logger.info({
        jobId: job.id,
        prUrl: job.git.prUrl
      }, 'Pull request created');

      Sentry.addBreadcrumb({
        category: 'git',
        message: `Created PR: ${job.git.prUrl}`,
        level: 'info',
      });
    }
  }

  /**
   * Generate commit message for job
   * Override this method to customize commit messages
   * @protected
   */
  async _generateCommitMessage(job) {
    return {
      title: `${this.jobType}: automated changes from job ${job.id}`,
      body: `Automated changes generated by ${this.jobType} job.\n\nFiles changed: ${job.git.changedFiles.length}`
    };
  }

  /**
   * Generate PR context for job
   * Override this method to customize PR details
   * @protected
   */
  async _generatePRContext(job) {
    const commitMessage = await this._generateCommitMessage(job);

    return {
      branchName: job.git.branchName,
      title: commitMessage.title,
      body: `## Automated Changes\n\n${commitMessage.body}\n\n### Job Details\n- **Job ID**: ${job.id}\n- **Job Type**: ${this.jobType}\n- **Files Changed**: ${job.git.changedFiles.length}\n\n### Changed Files\n${job.git.changedFiles.map(f => `- \`${f}\``).join('\n')}\n\n---\n\nü§ñ Generated with [Claude Code](https://claude.com/claude-code)`,
      labels: ['automated', this.jobType]
    };
  }

  /**
   * Override this method to define job execution logic
   * @param {any} job - The job to execute
   * @returns {Promise<any>} - The result of the job execution
   */
  async runJobHandler(job) {
    throw new Error('runJobHandler must be implemented by subclass');
  }

  /**
   * Log job completion to file
   */
  async logJobCompletion(job) {
    const logPath = path.join(this.logDir, `${job.id}.json`);
    await fs.writeFile(logPath, JSON.stringify(job, null, 2));
  }

  /**
   * Log job failure to file
   */
  async logJobFailure(job, error) {
    const logPath = path.join(this.logDir, `${job.id}.error.json`);
    await fs.writeFile(logPath, JSON.stringify({
      ...job,
      error: {
        message: error.message,
        stack: error.stack,
      },
    }, null, 2));
  }

  /**
   * Get job status
   */
  getJob(jobId) {
    return this.jobs.get(jobId);
  }

  /**
   * Get all jobs
   */
  getAllJobs() {
    return Array.from(this.jobs.values());
  }

  /**
   * Get job statistics
   */
  getStats() {
    return {
      total: this.jobs.size,
      queued: this.queue.length,
      active: this.activeJobs,
      completed: this.jobHistory.filter(j => j.status === 'completed').length,
      failed: this.jobHistory.filter(j => j.status === 'failed').length,
    };
  }
}
</file>

<file path="pipeline-core/.claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "Bash(rm:*)"
    ],
    "deny": [],
    "ask": []
  }
}
</file>

<file path="pipeline-core/cache/cached-scanner.js">
/**
 * Cached Scanner
 *
 * Integrates Git commit tracking with scan result caching.
 * Automatically uses cached results when repository hasn't changed.
 */

import { GitCommitTracker } from './git-tracker.js';
import { ScanResultCache } from './scan-cache.js';
import { ScanOrchestrator } from '../scan-orchestrator.js';
import { createComponentLogger } from '../../utils/logger.js';
import * as Sentry from '@sentry/node';

const logger = createComponentLogger('CachedScanner');

export class CachedScanner {
  /**
   * @param {Object} options - Scanner options
   */
  constructor(options = {}) {
    this.gitTracker = new GitCommitTracker();
    this.cache = options.cache || null; // ScanResultCache instance (optional)
    this.scanner = new ScanOrchestrator(options.scannerOptions || {});

    this.cacheEnabled = options.cacheEnabled !== false;
    this.forceRefresh = options.forceRefresh || false;
    this.trackUncommitted = options.trackUncommitted !== false;

    logger.info({
      cacheEnabled: this.cacheEnabled,
      forceRefresh: this.forceRefresh,
      trackUncommitted: this.trackUncommitted
    }, 'Cached scanner initialized');
  }

  /**
   * Initialize cache connection
   * @param {Object} redisClient - Redis MCP client
   * @param {Object} cacheOptions - Cache options
   */
  initializeCache(redisClient, cacheOptions = {}) {
    this.cache = new ScanResultCache(redisClient, {
      enabled: this.cacheEnabled,
      ...cacheOptions
    });

    logger.info('Cache initialized with Redis client');
  }

  /**
   * Scan repository with intelligent caching
   * @param {string} repoPath - Path to repository
   * @param {Object} options - Scan options
   * @returns {Promise<Object>} - Scan result
   */
  async scanRepository(repoPath, options = {}) {
    const startTime = Date.now();

    try {
      logger.info({ repoPath, options }, 'Starting cached repository scan');

      // Get repository Git status
      const repoStatus = await this.gitTracker.getRepositoryStatus(repoPath);

      // Determine if we should use cache
      const shouldUseCache = await this._shouldUseCache(repoPath, repoStatus, options);

      if (shouldUseCache) {
        // Try to get from cache
        const cachedResult = await this._getCachedResult(repoPath, repoStatus.current_commit);

        if (cachedResult) {
          const duration = (Date.now() - startTime) / 1000;

          logger.info({
            repoPath,
            fromCache: true,
            duration,
            cacheAge: cachedResult.cache_metadata?.age
          }, 'Scan completed from cache');

          return {
            ...cachedResult,
            scan_metadata: {
              ...cachedResult.scan_metadata,
              duration_seconds: duration,
              git_status: repoStatus
            }
          };
        }
      }

      // Cache miss or forced refresh - run actual scan
      logger.info({ repoPath, reason: options.forceRefresh ? 'forced_refresh' : 'cache_miss' }, 'Running fresh scan');

      const scanResult = await this.scanner.scanRepository(repoPath, options);

      // Cache the result if caching is enabled
      if (this.cacheEnabled && this.cache && repoStatus.current_commit) {
        await this._cacheResult(repoPath, repoStatus.current_commit, scanResult);
      }

      const duration = (Date.now() - startTime) / 1000;

      logger.info({
        repoPath,
        fromCache: false,
        duration,
        duplicates: scanResult.metrics?.total_duplicate_groups || 0
      }, 'Scan completed from fresh scan');

      return {
        ...scanResult,
        scan_metadata: {
          ...scanResult.scan_metadata,
          duration_seconds: duration,
          git_status: repoStatus,
          from_cache: false
        }
      };
    } catch (error) {
      logger.error({ error, repoPath }, 'Cached scan failed');
      Sentry.captureException(error, {
        tags: {
          component: 'cached-scanner',
          repository: repoPath
        }
      });
      throw error;
    }
  }

  /**
   * Determine if cache should be used
   * @param {string} repoPath - Repository path
   * @param {Object} repoStatus - Repository Git status
   * @param {Object} options - Scan options
   * @returns {Promise<boolean>} - True if should use cache
   * @private
   */
  async _shouldUseCache(repoPath, repoStatus, options) {
    // Never use cache if forced refresh
    if (options.forceRefresh || this.forceRefresh) {
      logger.debug({ repoPath }, 'Skipping cache: forced refresh');
      return false;
    }

    // Never use cache if caching disabled
    if (!this.cacheEnabled || !this.cache) {
      logger.debug({ repoPath }, 'Skipping cache: caching disabled');
      return false;
    }

    // Never use cache if not a Git repository
    if (!repoStatus.is_git_repository) {
      logger.debug({ repoPath }, 'Skipping cache: not a git repository');
      return false;
    }

    // Check for uncommitted changes
    if (this.trackUncommitted && repoStatus.has_uncommitted_changes) {
      logger.debug({ repoPath }, 'Skipping cache: has uncommitted changes');
      return false;
    }

    return true;
  }

  /**
   * Get cached result
   * @param {string} repoPath - Repository path
   * @param {string} commitHash - Commit hash
   * @returns {Promise<Object|null>} - Cached result or null
   * @private
   */
  async _getCachedResult(repoPath, commitHash) {
    try {
      const cachedResult = await this.cache.getCachedScan(repoPath, commitHash);

      if (cachedResult) {
        // Calculate cache age
        const cacheAge = await this.cache.getCacheAge(repoPath, commitHash);

        return {
          ...cachedResult,
          cache_metadata: {
            ...cachedResult.cache_metadata,
            age: cacheAge,
            age_hours: cacheAge / (60 * 60 * 1000),
            age_days: cacheAge / (24 * 60 * 60 * 1000)
          }
        };
      }

      return null;
    } catch (error) {
      logger.warn({ error, repoPath }, 'Failed to get cached result');
      return null;
    }
  }

  /**
   * Cache scan result
   * @param {string} repoPath - Repository path
   * @param {string} commitHash - Commit hash
   * @param {Object} scanResult - Scan result
   * @private
   */
  async _cacheResult(repoPath, commitHash, scanResult) {
    try {
      await this.cache.cacheScan(repoPath, commitHash, scanResult);
    } catch (error) {
      logger.warn({ error, repoPath }, 'Failed to cache scan result (non-fatal)');
      // Don't throw - caching failure shouldn't fail the scan
    }
  }

  /**
   * Invalidate cache for a repository
   * @param {string} repoPath - Repository path
   * @returns {Promise<number>} - Number of cache entries invalidated
   */
  async invalidateCache(repoPath) {
    if (!this.cache) {
      logger.warn({ repoPath }, 'Cannot invalidate cache: cache not initialized');
      return 0;
    }

    try {
      logger.info({ repoPath }, 'Invalidating cache for repository');

      const deletedCount = await this.cache.invalidateCache(repoPath);

      logger.info({
        repoPath,
        entriesDeleted: deletedCount
      }, 'Cache invalidated successfully');

      return deletedCount;
    } catch (error) {
      logger.error({ error, repoPath }, 'Failed to invalidate cache');
      throw error;
    }
  }

  /**
   * Check if repository scan is cached
   * @param {string} repoPath - Repository path
   * @returns {Promise<Object>} - Cache status
   */
  async getCacheStatus(repoPath) {
    try {
      const repoStatus = await this.gitTracker.getRepositoryStatus(repoPath);

      if (!repoStatus.current_commit) {
        return {
          is_cached: false,
          reason: 'not_a_git_repository',
          repository_status: repoStatus
        };
      }

      const isCached = await this.cache?.isCached(repoPath, repoStatus.current_commit);
      const cacheAge = isCached ? await this.cache.getCacheAge(repoPath, repoStatus.current_commit) : null;
      const metadata = isCached ? await this.cache.getCacheMetadata(repoPath, repoStatus.current_commit) : null;

      return {
        is_cached: isCached || false,
        cache_age_ms: cacheAge,
        cache_age_hours: cacheAge ? cacheAge / (60 * 60 * 1000) : null,
        cache_age_days: cacheAge ? cacheAge / (24 * 60 * 60 * 1000) : null,
        metadata,
        repository_status: repoStatus
      };
    } catch (error) {
      logger.error({ error, repoPath }, 'Failed to get cache status');
      throw error;
    }
  }

  /**
   * Get scanner statistics
   * @returns {Promise<Object>} - Scanner statistics
   */
  async getStats() {
    const cacheStats = this.cache ? await this.cache.getStats() : null;

    return {
      cache_enabled: this.cacheEnabled,
      force_refresh: this.forceRefresh,
      track_uncommitted: this.trackUncommitted,
      cache_initialized: this.cache !== null,
      cache_stats: cacheStats
    };
  }

  /**
   * Warm cache by scanning repositories
   * @param {Array<string>} repoPaths - Array of repository paths to scan
   * @param {Object} options - Scan options
   * @returns {Promise<Object>} - Warm-up results
   */
  async warmCache(repoPaths, options = {}) {
    logger.info({ repositoryCount: repoPaths.length }, 'Starting cache warm-up');

    const results = {
      total: repoPaths.length,
      successful: 0,
      failed: 0,
      errors: []
    };

    for (const repoPath of repoPaths) {
      try {
        await this.scanRepository(repoPath, {
          ...options,
          forceRefresh: true // Force scan to populate cache
        });

        results.successful++;

        logger.info({
          repoPath,
          progress: `${results.successful}/${results.total}`
        }, 'Repository scanned for cache warm-up');
      } catch (error) {
        results.failed++;
        results.errors.push({
          repository: repoPath,
          error: error.message
        });

        logger.error({ error, repoPath }, 'Failed to scan repository for cache warm-up');
      }
    }

    logger.info(results, 'Cache warm-up completed');

    return results;
  }
}
</file>

<file path="pipeline-core/cache/git-tracker.js">
/**
 * Git Commit Tracker
 *
 * Tracks Git commit hashes to detect repository changes.
 * Used by caching layer to determine if scans need to be refreshed.
 */

// @ts-nocheck
import { exec } from 'child_process';
import { promisify } from 'util';
import fs from 'fs/promises';
import path from 'path';
import { createComponentLogger } from '../../utils/logger.js';

// @ts-ignore - Promisified exec has correct signature
const execPromise = promisify(exec);
const logger = createComponentLogger('GitCommitTracker');

export class GitCommitTracker {
  /**
   * Get the current HEAD commit hash for a repository
   * @param {string} repoPath - Path to repository
   * @returns {Promise<string|null>} - Commit hash or null if not a git repo
   */
  async getRepositoryCommit(repoPath) {
    try {
      const { stdout } = await execPromise('git rev-parse HEAD', {
        cwd: repoPath
      });

      const commitHash = stdout.trim();
      logger.info({ repoPath, commitHash }, 'Retrieved repository commit hash');

      return commitHash;
    } catch (error) {
      logger.warn({ repoPath, error: error.message }, 'Not a git repository or git not available');
      return null;
    }
  }

  /**
   * Get short commit hash (first 7 characters)
   * @param {string} repoPath - Path to repository
   * @returns {Promise<string|null>} - Short commit hash
   */
  async getShortCommit(repoPath) {
    try {
      const { stdout } = await execPromise('git rev-parse --short HEAD', {
        cwd: repoPath
      });

      const shortHash = stdout.trim();
      logger.debug({ repoPath, shortHash }, 'Retrieved short commit hash');

      return shortHash;
    } catch (error) {
      logger.warn({ repoPath, error: error.message }, 'Failed to get short commit hash');
      return null;
    }
  }

  /**
   * Check if repository has changed since a given commit
   * @param {string} repoPath - Path to repository
   * @param {string} lastCommit - Last known commit hash
   * @returns {Promise<boolean>} - True if changed
   */
  async hasChanged(repoPath, lastCommit) {
    if (!lastCommit) {
      return true; // No previous commit, treat as changed
    }

    const currentCommit = await this.getRepositoryCommit(repoPath);

    if (!currentCommit) {
      logger.warn({ repoPath }, 'Cannot determine if repository changed (not a git repo)');
      return true; // Assume changed if not a git repo
    }

    const changed = currentCommit !== lastCommit;

    logger.info({
      repoPath,
      lastCommit: lastCommit.substring(0, 7),
      currentCommit: currentCommit.substring(0, 7),
      changed
    }, 'Checked repository for changes');

    return changed;
  }

  /**
   * Get list of files changed since a given commit
   * @param {string} repoPath - Path to repository
   * @param {string} fromCommit - Starting commit hash
   * @returns {Promise<string[]>} - Array of changed file paths
   */
  async getChangedFiles(repoPath, fromCommit) {
    try {
      const currentCommit = await this.getRepositoryCommit(repoPath);

      if (!currentCommit || !fromCommit) {
        logger.warn({ repoPath }, 'Cannot get changed files (missing commit hash)');
        return [];
      }

      // Get diff between commits
      const { stdout } = await execPromise(
        `git diff --name-only ${fromCommit} ${currentCommit}`,
        { cwd: repoPath }
      );

      const changedFiles = stdout
        .trim()
        .split('\n')
        .filter(file => file.length > 0);

      logger.info({
        repoPath,
        fromCommit: fromCommit.substring(0, 7),
        toCommit: currentCommit.substring(0, 7),
        filesChanged: changedFiles.length
      }, 'Retrieved changed files');

      return changedFiles;
    } catch (error) {
      logger.error({ repoPath, error }, 'Failed to get changed files');
      return [];
    }
  }

  /**
   * Get commit metadata (author, date, message)
   * @param {string} repoPath - Path to repository
   * @param {string} commitHash - Commit hash (defaults to HEAD)
   * @returns {Promise<Object|null>} - Commit metadata
   */
  async getCommitMetadata(repoPath, commitHash = 'HEAD') {
    try {
      const { stdout } = await execPromise(
        `git show -s --format='%H|%an|%ae|%at|%s' ${commitHash}`,
        { cwd: repoPath }
      );

      const [hash, author, email, timestamp, message] = stdout.trim().split('|');

      const metadata = {
        hash,
        shortHash: hash.substring(0, 7),
        author,
        email,
        date: new Date(parseInt(timestamp) * 1000).toISOString(),
        message
      };

      logger.debug({ repoPath, metadata }, 'Retrieved commit metadata');

      return metadata;
    } catch (error) {
      logger.error({ repoPath, commitHash, error }, 'Failed to get commit metadata');
      return null;
    }
  }

  /**
   * Get repository branch name
   * @param {string} repoPath - Path to repository
   * @returns {Promise<string|null>} - Branch name
   */
  async getBranchName(repoPath) {
    try {
      const { stdout } = await execPromise('git rev-parse --abbrev-ref HEAD', {
        cwd: repoPath
      });

      const branchName = stdout.trim();
      logger.debug({ repoPath, branchName }, 'Retrieved branch name');

      return branchName;
    } catch (error) {
      logger.warn({ repoPath, error: error.message }, 'Failed to get branch name');
      return null;
    }
  }

  /**
   * Check if repository has uncommitted changes
   * @param {string} repoPath - Path to repository
   * @returns {Promise<boolean>} - True if has uncommitted changes
   */
  async hasUncommittedChanges(repoPath) {
    try {
      const { stdout } = await execPromise('git status --porcelain', {
        cwd: repoPath
      });

      const hasChanges = stdout.trim().length > 0;

      logger.info({
        repoPath,
        hasUncommittedChanges: hasChanges
      }, 'Checked for uncommitted changes');

      return hasChanges;
    } catch (error) {
      logger.warn({ repoPath, error: error.message }, 'Failed to check for uncommitted changes');
      return false;
    }
  }

  /**
   * Get repository remote URL
   * @param {string} repoPath - Path to repository
   * @param {string} remoteName - Remote name (default: origin)
   * @returns {Promise<string|null>} - Remote URL
   */
  async getRemoteUrl(repoPath, remoteName = 'origin') {
    try {
      const { stdout } = await execPromise(`git remote get-url ${remoteName}`, {
        cwd: repoPath
      });

      const remoteUrl = stdout.trim();
      logger.debug({ repoPath, remoteName, remoteUrl }, 'Retrieved remote URL');

      return remoteUrl;
    } catch (error) {
      logger.warn({ repoPath, remoteName, error: error.message }, 'Failed to get remote URL');
      return null;
    }
  }

  /**
   * Get total number of commits in repository
   * @param {string} repoPath - Path to repository
   * @returns {Promise<number>} - Number of commits
   */
  async getCommitCount(repoPath) {
    try {
      const { stdout } = await execPromise('git rev-list --count HEAD', {
        cwd: repoPath
      });

      const count = parseInt(stdout.trim());
      logger.debug({ repoPath, commitCount: count }, 'Retrieved commit count');

      return count;
    } catch (error) {
      logger.warn({ repoPath, error: error.message }, 'Failed to get commit count');
      return 0;
    }
  }

  /**
   * Check if path is a git repository
   * @param {string} repoPath - Path to check
   * @returns {Promise<boolean>} - True if is a git repository
   */
  async isGitRepository(repoPath) {
    try {
      const gitDir = path.join(repoPath, '.git');
      const stats = await fs.stat(gitDir);
      const isGit = stats.isDirectory();

      logger.debug({ repoPath, isGitRepository: isGit }, 'Checked if path is git repository');

      return isGit;
    } catch (error) {
      logger.debug({ repoPath }, 'Path is not a git repository');
      return false;
    }
  }

  /**
   * Get repository status summary
   * @param {string} repoPath - Path to repository
   * @returns {Promise<Object>} - Repository status
   */
  async getRepositoryStatus(repoPath) {
    const [
      isGit,
      currentCommit,
      branchName,
      hasUncommitted,
      remoteUrl
    ] = await Promise.all([
      this.isGitRepository(repoPath),
      this.getRepositoryCommit(repoPath),
      this.getBranchName(repoPath),
      this.hasUncommittedChanges(repoPath),
      this.getRemoteUrl(repoPath)
    ]);

    const status = {
      is_git_repository: isGit,
      current_commit: currentCommit,
      short_commit: currentCommit ? currentCommit.substring(0, 7) : null,
      branch: branchName,
      has_uncommitted_changes: hasUncommitted,
      remote_url: remoteUrl,
      scanned_at: new Date().toISOString()
    };

    logger.info({ repoPath, status }, 'Retrieved repository status');

    return status;
  }

  /**
   * Get commit history for a repository
   * @param {string} repoPath - Path to repository
   * @param {number} limit - Number of commits to retrieve
   * @returns {Promise<Array>} - Array of commit objects
   */
  async getCommitHistory(repoPath, limit = 10) {
    try {
      const { stdout } = await execPromise(
        `git log -${limit} --format='%H|%an|%ae|%at|%s'`,
        { cwd: repoPath }
      );

      const commits = stdout
        .trim()
        .split('\n')
        .map(line => {
          const [hash, author, email, timestamp, message] = line.split('|');
          return {
            hash,
            shortHash: hash.substring(0, 7),
            author,
            email,
            date: new Date(parseInt(timestamp) * 1000).toISOString(),
            message
          };
        });

      logger.info({
        repoPath,
        commitsRetrieved: commits.length
      }, 'Retrieved commit history');

      return commits;
    } catch (error) {
      logger.error({ repoPath, error }, 'Failed to get commit history');
      return [];
    }
  }
}
</file>

<file path="pipeline-core/cache/scan-cache.js">
/**
 * Scan Result Cache
 *
 * Caches duplicate detection scan results in Redis using Git commit hashes as keys.
 * Provides fast retrieval of scan results for unchanged repositories.
 */

import { createComponentLogger } from '../../utils/logger.js';
import crypto from 'crypto';

const logger = createComponentLogger('ScanResultCache');

export class ScanResultCache {
  /**
   * @param {Object} redisClient - Redis MCP client
   * @param {Object} options - Cache options
   */
  constructor(redisClient, options = {}) {
    this.redis = redisClient;
    this.ttl = options.ttl || (30 * 24 * 60 * 60); // Default: 30 days in seconds
    this.keyPrefix = options.keyPrefix || 'scan:';
    this.enabled = options.enabled !== false;
  }

  /**
   * Generate cache key for a repository scan
   * @param {string} repoPath - Repository path
   * @param {string} commitHash - Git commit hash
   * @returns {string} - Cache key
   */
  _generateCacheKey(repoPath, commitHash) {
    // Create a stable hash of repository path
    const pathHash = crypto
      .createHash('sha256')
      .update(repoPath)
      .digest('hex')
      .substring(0, 16);

    const shortCommit = commitHash ? commitHash.substring(0, 7) : 'no-git';

    return `${this.keyPrefix}${pathHash}:${shortCommit}`;
  }

  /**
   * Get cached scan result
   * @param {string} repoPath - Repository path
   * @param {string} commitHash - Git commit hash
   * @returns {Promise<Object|null>} - Cached scan result or null
   */
  async getCachedScan(repoPath, commitHash) {
    if (!this.enabled) {
      logger.debug('Cache disabled, skipping lookup');
      return null;
    }

    const cacheKey = this._generateCacheKey(repoPath, commitHash);

    try {
      logger.debug({ cacheKey, repoPath, commitHash }, 'Looking up cached scan');

      // Check if key exists
      const exists = await this.redis.hexists({ name: cacheKey, key: 'scan_result' });

      if (!exists) {
        logger.info({ cacheKey, repoPath }, 'Cache miss');
        return null;
      }

      // Retrieve scan result
      const cachedData = await this.redis.hget({ name: cacheKey, key: 'scan_result' });

      if (!cachedData) {
        logger.warn({ cacheKey }, 'Cache key exists but no data found');
        return null;
      }

      const scanResult = JSON.parse(cachedData);

      // Retrieve metadata
      const metadataStr = await this.redis.hget({ name: cacheKey, key: 'metadata' });
      const metadata = metadataStr ? JSON.parse(metadataStr) : {};

      logger.info({
        cacheKey,
        repoPath,
        cachedAt: metadata.cached_at,
        age: Date.now() - new Date(metadata.cached_at).getTime()
      }, 'Cache hit');

      return {
        ...scanResult,
        cache_metadata: {
          ...metadata,
          from_cache: true,
          cache_key: cacheKey
        }
      };
    } catch (error) {
      logger.error({ error, cacheKey, repoPath }, 'Failed to retrieve cached scan');
      return null;
    }
  }

  /**
   * Cache a scan result
   * @param {string} repoPath - Repository path
   * @param {string} commitHash - Git commit hash
   * @param {Object} scanResult - Scan result to cache
   * @param {Object} options - Caching options
   * @returns {Promise<boolean>} - True if cached successfully
   */
  async cacheScan(repoPath, commitHash, scanResult, options = {}) {
    if (!this.enabled) {
      logger.debug('Cache disabled, skipping storage');
      return false;
    }

    const cacheKey = this._generateCacheKey(repoPath, commitHash);

    try {
      const metadata = {
        repository_path: repoPath,
        commit_hash: commitHash,
        short_commit: commitHash ? commitHash.substring(0, 7) : null,
        cached_at: new Date().toISOString(),
        ttl: this.ttl,
        scan_type: scanResult.scan_type || 'unknown',
        total_duplicates: scanResult.metrics?.total_duplicate_groups || 0,
        total_suggestions: scanResult.metrics?.total_suggestions || 0
      };

      // Store scan result
      await this.redis.hset({
        name: cacheKey,
        key: 'scan_result',
        value: JSON.stringify(scanResult),
        expire_seconds: this.ttl
      });

      // Store metadata
      await this.redis.hset({
        name: cacheKey,
        key: 'metadata',
        value: JSON.stringify(metadata),
        expire_seconds: this.ttl
      });

      // Store repository path for reverse lookup
      await this.redis.hset({
        name: cacheKey,
        key: 'repository_path',
        value: repoPath,
        expire_seconds: this.ttl
      });

      // Add to index for listing cached scans
      await this._addToIndex(cacheKey, repoPath, commitHash);

      logger.info({
        cacheKey,
        repoPath,
        commitHash: commitHash ? commitHash.substring(0, 7) : null,
        ttl: this.ttl,
        duplicates: metadata.total_duplicates
      }, 'Scan result cached successfully');

      return true;
    } catch (error) {
      logger.error({ error, cacheKey, repoPath }, 'Failed to cache scan result');
      return false;
    }
  }

  /**
   * Add cache key to index for listing
   * @param {string} cacheKey - Cache key
   * @param {string} repoPath - Repository path
   * @param {string} commitHash - Commit hash
   * @private
   */
  async _addToIndex(cacheKey, repoPath, commitHash) {
    try {
      const indexKey = `${this.keyPrefix}index`;
      const indexEntry = JSON.stringify({
        cache_key: cacheKey,
        repository_path: repoPath,
        commit_hash: commitHash,
        indexed_at: new Date().toISOString()
      });

      await this.redis.lpush({
        name: indexKey,
        value: indexEntry,
        expire: this.ttl
      });

      // Keep only the 100 most recent entries
      await this.redis.lrange({
        name: indexKey,
        start: 0,
        stop: 99
      });
    } catch (error) {
      logger.warn({ error, cacheKey }, 'Failed to update cache index');
    }
  }

  /**
   * Invalidate cache for a repository
   * @param {string} repoPath - Repository path
   * @returns {Promise<number>} - Number of keys invalidated
   */
  async invalidateCache(repoPath) {
    try {
      const pathHash = crypto
        .createHash('sha256')
        .update(repoPath)
        .digest('hex')
        .substring(0, 16);

      const pattern = `${this.keyPrefix}${pathHash}:*`;

      logger.info({ repoPath, pattern }, 'Invalidating cache for repository');

      // Scan for matching keys
      const keys = await this.redis.scan_all_keys({ pattern });

      if (keys.length === 0) {
        logger.info({ repoPath }, 'No cached scans found to invalidate');
        return 0;
      }

      // Delete all matching keys
      let deletedCount = 0;
      for (const key of keys) {
        await this.redis.delete({ key });
        deletedCount++;
      }

      logger.info({
        repoPath,
        keysDeleted: deletedCount
      }, 'Cache invalidated successfully');

      return deletedCount;
    } catch (error) {
      logger.error({ error, repoPath }, 'Failed to invalidate cache');
      return 0;
    }
  }

  /**
   * Get cache statistics
   * @returns {Promise<Object>} - Cache statistics
   */
  async getStats() {
    try {
      const pattern = `${this.keyPrefix}*`;
      const allKeys = await this.redis.scan_all_keys({ pattern });

      // Filter out index keys
      const cacheKeys = allKeys.filter(key => !key.endsWith(':index'));

      const stats = {
        total_cached_scans: cacheKeys.length,
        cache_enabled: this.enabled,
        ttl_seconds: this.ttl,
        ttl_days: Math.floor(this.ttl / (24 * 60 * 60)),
        prefix: this.keyPrefix
      };

      logger.info(stats, 'Retrieved cache statistics');

      return stats;
    } catch (error) {
      logger.error({ error }, 'Failed to get cache statistics');
      return {
        total_cached_scans: 0,
        cache_enabled: this.enabled,
        error: error.message
      };
    }
  }

  /**
   * List cached scans
   * @param {number} limit - Maximum number of scans to list
   * @returns {Promise<Array>} - Array of cached scan info
   */
  async listCachedScans(limit = 10) {
    try {
      const indexKey = `${this.keyPrefix}index`;
      const entries = await this.redis.lrange({
        name: indexKey,
        start: 0,
        stop: limit - 1
      });

      const cachedScans = entries.map(entry => JSON.parse(entry));

      logger.info({ count: cachedScans.length }, 'Retrieved cached scan list');

      return cachedScans;
    } catch (error) {
      logger.error({ error }, 'Failed to list cached scans');
      return [];
    }
  }

  /**
   * Get cache metadata for a specific scan
   * @param {string} repoPath - Repository path
   * @param {string} commitHash - Commit hash
   * @returns {Promise<Object|null>} - Cache metadata
   */
  async getCacheMetadata(repoPath, commitHash) {
    const cacheKey = this._generateCacheKey(repoPath, commitHash);

    try {
      const metadataStr = await this.redis.hget({ name: cacheKey, key: 'metadata' });

      if (!metadataStr) {
        return null;
      }

      const metadata = JSON.parse(metadataStr);

      logger.debug({ cacheKey, repoPath }, 'Retrieved cache metadata');

      return metadata;
    } catch (error) {
      logger.error({ error, cacheKey, repoPath }, 'Failed to get cache metadata');
      return null;
    }
  }

  /**
   * Clear all cached scans
   * @returns {Promise<number>} - Number of keys cleared
   */
  async clearAll() {
    try {
      logger.warn('Clearing all cached scans');

      const pattern = `${this.keyPrefix}*`;
      const allKeys = await this.redis.scan_all_keys({ pattern });

      let deletedCount = 0;
      for (const key of allKeys) {
        await this.redis.delete({ key });
        deletedCount++;
      }

      logger.warn({ keysDeleted: deletedCount }, 'All cached scans cleared');

      return deletedCount;
    } catch (error) {
      logger.error({ error }, 'Failed to clear all cached scans');
      return 0;
    }
  }

  /**
   * Check if a scan is cached
   * @param {string} repoPath - Repository path
   * @param {string} commitHash - Commit hash
   * @returns {Promise<boolean>} - True if cached
   */
  async isCached(repoPath, commitHash) {
    const cacheKey = this._generateCacheKey(repoPath, commitHash);

    try {
      const exists = await this.redis.hexists({ name: cacheKey, key: 'scan_result' });

      logger.debug({ cacheKey, repoPath, exists }, 'Checked if scan is cached');

      return exists;
    } catch (error) {
      logger.error({ error, cacheKey, repoPath }, 'Failed to check if scan is cached');
      return false;
    }
  }

  /**
   * Get cache age in milliseconds
   * @param {string} repoPath - Repository path
   * @param {string} commitHash - Commit hash
   * @returns {Promise<number|null>} - Age in milliseconds, or null if not cached
   */
  async getCacheAge(repoPath, commitHash) {
    const metadata = await this.getCacheMetadata(repoPath, commitHash);

    if (!metadata || !metadata.cached_at) {
      return null;
    }

    const age = Date.now() - new Date(metadata.cached_at).getTime();

    logger.debug({ repoPath, ageMs: age, ageDays: age / (24 * 60 * 60 * 1000) }, 'Calculated cache age');

    return age;
  }
}
</file>

<file path="pipeline-core/config/repository-config-loader.js">
/**
 * Repository Configuration Loader
 *
 * Loads and validates repository scanning configuration from JSON files.
 * Provides API for repository selection, priority sorting, and frequency filtering.
 */

import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';
import os from 'os';
import { createComponentLogger } from '../../utils/logger.js';

const __dirname = path.dirname(fileURLToPath(import.meta.url));
const logger = createComponentLogger('RepositoryConfigLoader');

/**
 * Loads and manages repository scanning configuration
 */
export class RepositoryConfigLoader {
  constructor(configPath = null) {
    this.configPath = configPath || path.join(process.cwd(), 'config', 'scan-repositories.json');
    this.config = null;
    this.lastLoaded = null;
  }

  /**
   * Load configuration from file
   */
  async load() {
    try {
      const configContent = await fs.readFile(this.configPath, 'utf-8');
      this.config = JSON.parse(configContent);
      this.lastLoaded = new Date();

      // Expand paths (handle ~ for home directory)
      this._expandPaths();

      logger.info({
        configPath: this.configPath,
        repositoryCount: this.config.repositories.length,
        groupCount: this.config.repositoryGroups?.length || 0
      }, 'Configuration loaded');

      return this.config;
    } catch (error) {
      logger.error({ error, configPath: this.configPath }, 'Failed to load configuration');
      throw new Error(`Failed to load configuration from ${this.configPath}: ${error.message}`);
    }
  }

  /**
   * Reload configuration from file
   */
  async reload() {
    logger.info('Reloading configuration');
    return await this.load();
  }

  /**
   * Get scan configuration
   */
  getScanConfig() {
    this._ensureLoaded();
    return this.config.scanConfig;
  }

  /**
   * Get all repositories
   */
  getAllRepositories() {
    this._ensureLoaded();
    return this.config.repositories;
  }

  /**
   * Get enabled repositories
   */
  getEnabledRepositories() {
    this._ensureLoaded();
    return this.config.repositories.filter(repo => repo.enabled);
  }

  /**
   * Get repositories by priority
   */
  getRepositoriesByPriority(priority) {
    this._ensureLoaded();
    return this.config.repositories.filter(
      repo => repo.enabled && repo.priority === priority
    );
  }

  /**
   * Get repositories by frequency
   */
  getRepositoriesByFrequency(frequency) {
    this._ensureLoaded();
    return this.config.repositories.filter(
      repo => repo.enabled && repo.scanFrequency === frequency
    );
  }

  /**
   * Get repositories by tag
   */
  getRepositoriesByTag(tag) {
    this._ensureLoaded();
    return this.config.repositories.filter(
      repo => repo.enabled && repo.tags && repo.tags.includes(tag)
    );
  }

  /**
   * Get repository by name
   */
  getRepository(name) {
    this._ensureLoaded();
    return this.config.repositories.find(repo => repo.name === name);
  }

  /**
   * Get all repository groups
   */
  getAllGroups() {
    this._ensureLoaded();
    return this.config.repositoryGroups || [];
  }

  /**
   * Get enabled repository groups
   */
  getEnabledGroups() {
    this._ensureLoaded();
    return (this.config.repositoryGroups || []).filter(group => group.enabled);
  }

  /**
   * Get group by name
   */
  getGroup(name) {
    this._ensureLoaded();
    return (this.config.repositoryGroups || []).find(group => group.name === name);
  }

  /**
   * Get repositories for a group
   */
  getGroupRepositories(groupName) {
    const group = this.getGroup(groupName);
    if (!group) {
      throw new Error(`Group '${groupName}' not found`);
    }

    return group.repositories
      .map(repoName => this.getRepository(repoName))
      .filter(repo => repo && repo.enabled);
  }

  /**
   * Get repositories to scan tonight (based on priority and frequency)
   */
  getRepositoriesToScanTonight(maxRepos = null) {
    this._ensureLoaded();

    const maxRepositories = maxRepos || this.config.scanConfig.maxRepositoriesPerNight;
    const now = new Date();
    const dayOfWeek = now.getDay(); // 0 = Sunday, 1 = Monday, ...
    const dayOfMonth = now.getDate();

    // Filter repositories by frequency
    const eligibleRepositories = this.getEnabledRepositories().filter(repo => {
      // Daily frequency always eligible
      if (repo.scanFrequency === 'daily') return true;

      // Weekly frequency: scan on Sundays
      if (repo.scanFrequency === 'weekly' && dayOfWeek === 0) return true;

      // Monthly frequency: scan on 1st of month
      if (repo.scanFrequency === 'monthly' && dayOfMonth === 1) return true;

      // On-demand: skip
      if (repo.scanFrequency === 'on-demand') return false;

      return false;
    });

    // Sort by priority (critical > high > medium > low)
    const priorityOrder = { critical: 4, high: 3, medium: 2, low: 1 };
    const sortedRepositories = eligibleRepositories.sort((a, b) => {
      const priorityDiff = priorityOrder[b.priority] - priorityOrder[a.priority];
      if (priorityDiff !== 0) return priorityDiff;

      // If same priority, sort by last scanned (oldest first)
      const aLastScanned = a.lastScannedAt ? new Date(a.lastScannedAt).getTime() : 0;
      const bLastScanned = b.lastScannedAt ? new Date(b.lastScannedAt).getTime() : 0;
      return aLastScanned - bLastScanned;
    });

    // Limit to maxRepositories
    return sortedRepositories.slice(0, maxRepositories);
  }

  /**
   * Get scan defaults
   */
  getScanDefaults() {
    this._ensureLoaded();
    return this.config.scanDefaults || {};
  }

  /**
   * Get notification settings
   */
  getNotificationSettings() {
    this._ensureLoaded();
    return this.config.notifications || {};
  }

  /**
   * Update repository last scanned timestamp
   */
  async updateLastScanned(repoName, timestamp = null) {
    this._ensureLoaded();

    const repo = this.getRepository(repoName);
    if (!repo) {
      throw new Error(`Repository '${repoName}' not found`);
    }

    repo.lastScannedAt = (timestamp || new Date()).toISOString();

    // Save updated configuration
    await this.save();

    logger.info({ repoName, timestamp: repo.lastScannedAt }, 'Updated last scanned timestamp');
  }

  /**
   * Add scan history entry
   */
  async addScanHistory(repoName, historyEntry) {
    this._ensureLoaded();

    const repo = this.getRepository(repoName);
    if (!repo) {
      throw new Error(`Repository '${repoName}' not found`);
    }

    if (!repo.scanHistory) {
      repo.scanHistory = [];
    }

    repo.scanHistory.unshift({
      timestamp: new Date().toISOString(),
      ...historyEntry
    });

    // Keep only last 10 entries
    repo.scanHistory = repo.scanHistory.slice(0, 10);

    // Save updated configuration
    await this.save();

    logger.info({ repoName, status: historyEntry.status }, 'Added scan history entry');
  }

  /**
   * Save configuration to file
   */
  async save() {
    try {
      await fs.writeFile(
        this.configPath,
        JSON.stringify(this.config, null, 2),
        'utf-8'
      );

      logger.info({ configPath: this.configPath }, 'Configuration saved');
    } catch (error) {
      logger.error({ error, configPath: this.configPath }, 'Failed to save configuration');
      throw new Error(`Failed to save configuration: ${error.message}`);
    }
  }

  /**
   * Validate configuration
   */
  validate() {
    this._ensureLoaded();

    const errors = [];

    // Validate scan config
    if (!this.config.scanConfig) {
      errors.push('scanConfig is required');
    } else {
      if (!this.config.scanConfig.schedule) {
        errors.push('scanConfig.schedule is required');
      }
    }

    // Validate repositories
    if (!Array.isArray(this.config.repositories)) {
      errors.push('repositories must be an array');
    } else {
      const names = new Set();
      this.config.repositories.forEach((repo, index) => {
        if (!repo.name) {
          errors.push(`repositories[${index}]: name is required`);
        } else if (names.has(repo.name)) {
          errors.push(`repositories[${index}]: duplicate name '${repo.name}'`);
        } else {
          names.add(repo.name);
        }

        if (!repo.path) {
          errors.push(`repositories[${index}]: path is required`);
        }
      });
    }

    // Validate repository groups
    if (this.config.repositoryGroups) {
      this.config.repositoryGroups.forEach((group, index) => {
        if (!group.name) {
          errors.push(`repositoryGroups[${index}]: name is required`);
        }

        if (!Array.isArray(group.repositories)) {
          errors.push(`repositoryGroups[${index}]: repositories must be an array`);
        } else {
          group.repositories.forEach(repoName => {
            if (!this.getRepository(repoName)) {
              errors.push(`repositoryGroups[${index}]: repository '${repoName}' not found`);
            }
          });
        }
      });
    }

    if (errors.length > 0) {
      throw new Error(`Configuration validation failed:\n${errors.join('\n')}`);
    }

    logger.info('Configuration validated successfully');
    return true;
  }

  /**
   * Get statistics
   */
  getStats() {
    this._ensureLoaded();

    const totalRepos = this.config.repositories.length;
    const enabledRepos = this.config.repositories.filter(r => r.enabled).length;
    const disabledRepos = totalRepos - enabledRepos;

    const byPriority = {
      critical: this.getRepositoriesByPriority('critical').length,
      high: this.getRepositoriesByPriority('high').length,
      medium: this.getRepositoriesByPriority('medium').length,
      low: this.getRepositoriesByPriority('low').length
    };

    const byFrequency = {
      daily: this.getRepositoriesByFrequency('daily').length,
      weekly: this.getRepositoriesByFrequency('weekly').length,
      monthly: this.getRepositoriesByFrequency('monthly').length,
      onDemand: this.getRepositoriesByFrequency('on-demand').length
    };

    return {
      totalRepositories: totalRepos,
      enabledRepositories: enabledRepos,
      disabledRepositories: disabledRepos,
      byPriority,
      byFrequency,
      groups: (this.config.repositoryGroups || []).length,
      lastLoaded: this.lastLoaded
    };
  }

  /**
   * Private: Ensure configuration is loaded
   */
  _ensureLoaded() {
    if (!this.config) {
      throw new Error('Configuration not loaded. Call load() first.');
    }
  }

  /**
   * Private: Expand ~ in repository paths
   */
  _expandPaths() {
    const homeDir = os.homedir();

    this.config.repositories.forEach(repo => {
      if (repo.path.startsWith('~')) {
        repo.path = repo.path.replace('~', homeDir);
      }
    });
  }
}
</file>

<file path="pipeline-core/errors/error-classifier.js">
/**
 * Error Classification System
 *
 * Classifies errors as retryable (transient) or non-retryable (permanent)
 * to prevent wasted retry attempts on errors that will never succeed.
 *
 * @module lib/errors/error-classifier
 */

// @ts-check
/** @typedef {import('./types').HTTPError} HTTPError */
/** @typedef {import('./types').ClassifiedError} ClassifiedError */
/** @typedef {import('./error-types').ExtendedError} ExtendedError */

/**
 * Error categories
 */
export const ErrorCategory = {
  RETRYABLE: 'retryable',
  NON_RETRYABLE: 'non_retryable'
};

/**
 * Error types that should NOT be retried
 * These represent permanent failures that won't be fixed by retrying
 */
const NON_RETRYABLE_ERROR_CODES = new Set([
  // Filesystem errors
  'ENOENT',      // No such file or directory
  'ENOTDIR',     // Not a directory
  'EISDIR',      // Is a directory (when file expected)
  'EACCES',      // Permission denied
  'EPERM',       // Operation not permitted
  'EINVAL',      // Invalid argument
  'EEXIST',      // File already exists

  // Network errors - permanent
  'ENOTFOUND',   // DNS resolution failed
  'ECONNREFUSED',// Connection refused (server not listening)

  // Application errors
  'ERR_INVALID_ARG_TYPE',
  'ERR_INVALID_ARG_VALUE',
  'ERR_MODULE_NOT_FOUND',
  'ERR_REQUIRE_ESM',
  'ERR_UNKNOWN_FILE_EXTENSION',

  // HTTP errors - client errors (4xx)
  'ERR_HTTP_400', // Bad Request
  'ERR_HTTP_401', // Unauthorized
  'ERR_HTTP_403', // Forbidden
  'ERR_HTTP_404', // Not Found
  'ERR_HTTP_405', // Method Not Allowed
  'ERR_HTTP_409', // Conflict
  'ERR_HTTP_422', // Unprocessable Entity
]);

/**
 * Error types that SHOULD be retried
 * These represent transient failures that may succeed on retry
 */
const RETRYABLE_ERROR_CODES = new Set([
  // Network errors - transient
  'ETIMEDOUT',   // Connection timed out
  'ECONNRESET',  // Connection reset by peer
  'EHOSTUNREACH',// Host unreachable
  'ENETUNREACH', // Network unreachable
  'EPIPE',       // Broken pipe
  'EAGAIN',      // Resource temporarily unavailable
  'EBUSY',       // Resource busy

  // HTTP errors - server errors (5xx)
  'ERR_HTTP_500', // Internal Server Error
  'ERR_HTTP_502', // Bad Gateway
  'ERR_HTTP_503', // Service Unavailable
  'ERR_HTTP_504', // Gateway Timeout
]);

/**
 * Error messages that indicate non-retryable errors
 * (case-insensitive matching)
 */
const NON_RETRYABLE_MESSAGES = [
  'invalid repository path',
  'not a git repository',
  'permission denied',
  'access denied',
  'authentication failed',
  'invalid credentials',
  'malformed',
  'invalid format',
  'parse error',
  'syntax error',
  'validation error',
  'schema error',
];

/**
 * Error messages that indicate retryable errors
 */
const RETRYABLE_MESSAGES = [
  'timeout',
  'timed out',
  'connection reset',
  'service unavailable',
  'temporarily unavailable',
  'try again',
  'rate limit',
  'too many requests',
];

/**
 * Classify an error as retryable or non-retryable
 *
 * @param {Error} error - The error to classify
 * @returns {{category: string, reason: string, suggestedDelay: number}} Classification result with category, reason, and suggested retry delay
 */
export function classifyError(error) {
  if (!error) {
    return {
      category: ErrorCategory.NON_RETRYABLE,
      reason: 'No error provided',
      suggestedDelay: 0
    };
  }

  // Extract error properties
  const extError = /** @type {ExtendedError} */ (error);
  const errorCode = extError.code || extError.errno;
  const errorMessage = error.message?.toLowerCase() || '';
  const httpError = /** @type {HTTPError} */ (error);
  const statusCode = httpError.statusCode || httpError.status;

  // Check error code against known sets
  if (errorCode) {
    // Convert errorCode to string for Set operations
    const errorCodeStr = String(errorCode);

    if (NON_RETRYABLE_ERROR_CODES.has(errorCodeStr)) {
      return {
        category: ErrorCategory.NON_RETRYABLE,
        reason: `Error code '${errorCodeStr}' indicates permanent failure`,
        suggestedDelay: 0
      };
    }

    if (RETRYABLE_ERROR_CODES.has(errorCodeStr)) {
      return {
        category: ErrorCategory.RETRYABLE,
        reason: `Error code '${errorCodeStr}' indicates transient failure`,
        suggestedDelay: calculateRetryDelay(errorCodeStr, statusCode)
      };
    }
  }

  // Check HTTP status codes
  if (statusCode) {
    if (statusCode >= 400 && statusCode < 500 && statusCode !== 429) {
      // 4xx errors (except 429 Too Many Requests) are non-retryable
      return {
        category: ErrorCategory.NON_RETRYABLE,
        reason: `HTTP ${statusCode} indicates client error`,
        suggestedDelay: 0
      };
    }

    if (statusCode >= 500 || statusCode === 429) {
      // 5xx errors and 429 are retryable
      return {
        category: ErrorCategory.RETRYABLE,
        reason: `HTTP ${statusCode} indicates server error or rate limit`,
        suggestedDelay: statusCode === 429 ? 60000 : 10000 // 1min for rate limit, 10s otherwise
      };
    }
  }

  // Check error message patterns
  for (const pattern of NON_RETRYABLE_MESSAGES) {
    if (errorMessage.includes(pattern)) {
      return {
        category: ErrorCategory.NON_RETRYABLE,
        reason: `Error message contains '${pattern}'`,
        suggestedDelay: 0
      };
    }
  }

  for (const pattern of RETRYABLE_MESSAGES) {
    if (errorMessage.includes(pattern)) {
      return {
        category: ErrorCategory.RETRYABLE,
        reason: `Error message contains '${pattern}'`,
        suggestedDelay: pattern.includes('rate limit') ? 60000 : 5000
      };
    }
  }

  // Default to retryable for unknown errors (conservative approach)
  return {
    category: ErrorCategory.RETRYABLE,
    reason: 'Unknown error type - defaulting to retryable',
    suggestedDelay: 5000
  };
}

/**
 * Check if an error is retryable
 *
 * @param {Error} error - The error to check
 * @returns {boolean} True if the error should be retried
 */
export function isRetryable(error) {
  const classification = classifyError(error);
  return classification.category === ErrorCategory.RETRYABLE;
}

/**
 * Calculate suggested retry delay based on error type
 *
 * @param {string} errorCode - Error code
 * @param {number} [statusCode] - HTTP status code (if applicable)
 * @returns {number} Suggested delay in milliseconds
 * @private
 */
function calculateRetryDelay(errorCode, statusCode) {
  // Rate limiting
  if (statusCode === 429) {
    return 60000; // 1 minute
  }

  // Network timeouts
  if (errorCode === 'ETIMEDOUT') {
    return 10000; // 10 seconds
  }

  // Connection errors
  if (['ECONNRESET', 'ECONNREFUSED', 'EPIPE'].includes(errorCode)) {
    return 5000; // 5 seconds
  }

  // Server errors
  if (statusCode && statusCode >= 500) {
    return 10000; // 10 seconds
  }

  // Default
  return 5000; // 5 seconds
}

/**
 * Get detailed error information
 *
 * @param {Error} error - The error to analyze
 * @returns {Object} Detailed error information
 */
export function getErrorInfo(error) {
  const classification = classifyError(error);
  const extError = /** @type {ExtendedError} */ (error);
  const httpError = /** @type {HTTPError} */ (error);

  return {
    name: error.name || 'Error',
    message: error.message || 'Unknown error',
    code: extError.code || extError.errno,
    statusCode: httpError.statusCode || httpError.status,
    category: classification.category,
    reason: classification.reason,
    suggestedDelay: classification.suggestedDelay,
    retryable: classification.category === ErrorCategory.RETRYABLE,
    stack: error.stack,
    cause: error.cause
  };
}

/**
 * Create a ScanError with classification
 *
 * @param {string} message - Error message
 * @param {Error} cause - Original error
 * @returns {Error} Classified error
 */
export function createScanError(message, cause) {
  const error = new Error(message);
  error.name = 'ScanError';
  error.cause = cause;

  // Cast to typed error interfaces for property assignment
  const httpError = /** @type {HTTPError} */ (error);
  const classifiedError = /** @type {ClassifiedError} */ (error);

  // Preserve original error properties
  if (cause) {
    const extCause = /** @type {ExtendedError} */ (cause);
    const httpCause = /** @type {HTTPError} */ (cause);
    const extError = /** @type {ExtendedError} */ (error);
    extError.code = extCause.code || extCause.errno;
    httpError.statusCode = httpCause.statusCode || httpCause.status;
  }

  // Add classification
  const classification = classifyError(cause || error);
  classifiedError.retryable = classification.category === ErrorCategory.RETRYABLE;
  classifiedError.classification = {
    category: classification.reason.includes('network') ? 'network' :
              classification.reason.includes('HTTP') ? 'http' :
              classification.reason.includes('file') ? 'file_system' : 'unknown',
    retryable: classification.category === ErrorCategory.RETRYABLE,
    severity: classification.category === ErrorCategory.NON_RETRYABLE ? 'high' : 'medium'
  };

  return error;
}
</file>

<file path="pipeline-core/errors/error-types.d.ts">
/**
 * Extended Error type definitions
 *
 * TypeScript's built-in Error type doesn't include common properties
 * that are added by Node.js and other libraries.
 */

/**
 * Node.js system error with code and errno
 */
export interface NodeError extends Error {
  code?: string;
  errno?: number | string;
  syscall?: string;
  path?: string;
}

/**
 * Child process error with stdout/stderr
 */
export interface ProcessError extends Error {
  code?: number | string;
  stdout?: string;
  stderr?: string;
}

/**
 * HTTP error with status code
 */
export interface HTTPError extends Error {
  statusCode?: number;
  status?: number;
  response?: {
    status?: number;
    statusText?: string;
    data?: any;
  };
}

/**
 * Combined error type with all possible properties
 */
export interface ExtendedError extends Error {
  code?: string | number;
  errno?: number | string;
  syscall?: string;
  path?: string;
  stdout?: string;
  stderr?: string;
  statusCode?: number;
  status?: number;
  response?: {
    status?: number;
    statusText?: string;
    data?: any;
  };
  cause?: Error;
}
</file>

<file path="pipeline-core/errors/types.d.ts">
/**
 * Extended Error Types for Error Classification System
 *
 * These types extend the standard Error class with additional properties
 * used for error classification and retry logic.
 */

/**
 * HTTP Error with status code
 */
export interface HTTPError extends Error {
  statusCode?: number;
  status?: number;
}

/**
 * Classified Error with retry information
 */
export interface ClassifiedError extends Error {
  retryable?: boolean;
  classification?: {
    category: 'network' | 'file_system' | 'http' | 'database' | 'unknown';
    retryable: boolean;
    severity: 'low' | 'medium' | 'high';
  };
  statusCode?: number;
  status?: number;
}

/**
 * Type guard to check if error has status code
 */
export function isHTTPError(error: Error): error is HTTPError;

/**
 * Type guard to check if error is classified
 */
export function isClassifiedError(error: Error): error is ClassifiedError;
</file>

<file path="pipeline-core/extractors/extract_blocks.py">
#!/usr/bin/env python3
"""
Code Block Extraction Pipeline

Reads JSON from stdin containing:
- repository_info
- pattern_matches

Outputs JSON to stdout containing:
- code_blocks
- duplicate_groups
- suggestions
- metrics
"""

import sys
import json
import hashlib
import re
import os
from pathlib import Path
from typing import List, Dict, Any, Optional

# Debug mode - set PIPELINE_DEBUG=1 to enable verbose output
DEBUG = os.environ.get('PIPELINE_DEBUG', '').lower() in ('1', 'true', 'yes')

# Add lib/models and lib/similarity to Python path
sys.path.insert(0, str(Path(__file__).parent.parent / 'models'))
sys.path.insert(0, str(Path(__file__).parent.parent))

from code_block import CodeBlock, SourceLocation, ASTNode
from duplicate_group import DuplicateGroup
from consolidation_suggestion import ConsolidationSuggestion, MigrationStep
from scan_report import ScanReport, RepositoryInfo, ScanConfiguration, ScanMetrics
from similarity.grouping import group_by_similarity


def extract_function_name(source_code: str, file_path: Optional[str] = None, line_start: Optional[int] = None, repo_path: Optional[str] = None) -> Optional[str]:
    """
    Extract function name from source code using regex patterns.

    Priority 1: Function-Level Extraction
    This enables proper matching between detected and expected duplicates.

    If function name can't be found in source_code, reads the actual file
    to get more context (lines before the match).
    """
    if DEBUG:
        print(f"DEBUG extract_function_name called: file_path={file_path}, line_start={line_start}, repo_path={repo_path}", file=sys.stderr)

    if not source_code:
        return None

    # Try various patterns to extract function name
    patterns = [
        r'function\s+(\w+)\s*\(',          # function name(
        r'const\s+(\w+)\s*=\s*(?:async\s+)?function',  # const name = function
        r'const\s+(\w+)\s*=\s*(?:async\s+)?\(',        # const name = ( or const name = async (
        r'let\s+(\w+)\s*=\s*(?:async\s+)?function',    # let name = function
        r'let\s+(\w+)\s*=\s*(?:async\s+)?\(',          # let name = (
        r'var\s+(\w+)\s*=\s*(?:async\s+)?function',    # var name = function
        r'var\s+(\w+)\s*=\s*(?:async\s+)?\(',          # var name = (
        r'async\s+function\s+(\w+)\s*\(',  # async function name(
        r'(\w+)\s*:\s*function',           # name: function
        r'(\w+)\s*:\s*async\s+function',   # name: async function
        r'export\s+function\s+(\w+)',      # export function name
        r'export\s+const\s+(\w+)\s*=',     # export const name =
    ]

    for pattern in patterns:
        match = re.search(pattern, source_code, re.MULTILINE)
        if match and match.group(1):
            return match.group(1)

    # If not found in matched text, try reading more context from file
    if file_path and line_start and repo_path:
        try:
            full_file_path = Path(repo_path) / file_path
            if DEBUG:
                print(f"DEBUG attempting to read {full_file_path} at line {line_start}", file=sys.stderr)

            if full_file_path.exists():
                with open(full_file_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()

                # Search BACKWARDS from match line to find the CLOSEST function declaration
                # This prevents finding previous functions in the file
                search_start = max(0, line_start - 11)  # line_start is 1-indexed
                search_end = line_start  # Don't search past the match

                # Iterate backwards through lines to find closest function declaration
                for i in range(search_end - 1, search_start - 1, -1):
                    if i < 0 or i >= len(lines):
                        continue

                    line = lines[i]

                    # Try each pattern on this line
                    for pattern in patterns:
                        match = re.search(pattern, line)
                        if match and match.group(1):
                            func_name = match.group(1)
                            if DEBUG:
                                print(f"DEBUG found function name '{func_name}' at line {i+1} (match was at {line_start})", file=sys.stderr)
                            return func_name

                if DEBUG:
                    print(f"DEBUG no function name found in lines {search_start+1}-{search_end} for {file_path}:{line_start}", file=sys.stderr)
            else:
                if DEBUG:
                    print(f"DEBUG file does not exist: {full_file_path}", file=sys.stderr)
        except Exception as e:
            print(f"Warning: Could not read file context for {file_path}: {e}", file=sys.stderr)

    return None

def deduplicate_blocks(blocks: List[CodeBlock]) -> List[CodeBlock]:
    """
    Remove duplicate code blocks from the same location and function.

    Priority 4: Deduplicate Pattern Matches
    ast-grep patterns can match the same code multiple times within a function.
    This removes duplicates based on file:function_name, keeping only the earliest match.
    """
    seen_locations = set()
    seen_functions = {}  # file:function -> earliest block
    unique_blocks = []

    for i, block in enumerate(blocks):
        # Extract function name from tags
        function_name = None
        for tag in block.tags:
            if tag.startswith('function:'):
                function_name = tag[9:]  # Remove 'function:' prefix
                break

        # Debug: Show first 10 blocks being processed
        if DEBUG and i < 10:
            print(f"DEBUG dedup block {i}: {block.location.file_path}:{block.location.line_start}, func={function_name}, code_len={len(block.source_code)}", file=sys.stderr)

        # Strategy 1: Deduplicate by function name (preferred)
        if function_name:
            function_key = f"{block.location.file_path}:{function_name}"

            if function_key not in seen_functions:
                # First occurrence of this function - keep it
                seen_functions[function_key] = block
                unique_blocks.append(block)
            else:
                # Already seen this function - check if this is earlier in file
                existing_block = seen_functions[function_key]
                if block.location.line_start < existing_block.location.line_start:
                    # This block is earlier, replace the existing one
                    unique_blocks.remove(existing_block)
                    seen_functions[function_key] = block
                    unique_blocks.append(block)
                else:
                    # This is a later occurrence of same function - skip it
                    if DEBUG:
                        print(f"DEBUG dedup: skipping duplicate {function_name} at line {block.location.line_start} (kept line {existing_block.location.line_start})", file=sys.stderr)
        else:
            # Strategy 2: Fall back to line-based deduplication for blocks without function names
            location_key = f"{block.location.file_path}:{block.location.line_start}"

            if location_key not in seen_locations:
                seen_locations.add(location_key)
                unique_blocks.append(block)

    if len(blocks) != len(unique_blocks):
        removed = len(blocks) - len(unique_blocks)
        print(f"Deduplication: Removed {removed} duplicate blocks ({len(seen_functions)} unique functions, {len(seen_locations)} unique locations)", file=sys.stderr)

    return unique_blocks


def extract_code_blocks(pattern_matches: List[Dict], repository_info: Dict) -> List[CodeBlock]:
    """
    Extract CodeBlock models from pattern matches
    """
    if DEBUG:
        print(f"DEBUG extract_code_blocks: repository_info={repository_info}", file=sys.stderr)
        print(f"DEBUG extract_code_blocks: got {len(pattern_matches)} pattern matches", file=sys.stderr)
    blocks = []

    for i, match in enumerate(pattern_matches):
        if DEBUG and i == 0:  # Debug first match only
            print(f"DEBUG first match: file_path={match.get('file_path')}, line_start={match.get('line_start')}", file=sys.stderr)
        try:
            # Generate unique block ID
            block_id = f"cb_{hashlib.sha256(f"{match['file_path']}:{match['line_start']}".encode()).hexdigest()[:12]}"

            # Map pattern_id to category (must match SemanticCategory enum)
            category_map = {
                'object-manipulation': 'utility',
                'array-map-filter': 'utility',
                'string-manipulation': 'utility',
                'type-checking': 'utility',
                'validation': 'validator',
                'express-route-handlers': 'api_handler',
                'auth-checks': 'auth_check',
                'error-responses': 'error_handler',
                'request-validation': 'validator',
                'prisma-operations': 'database_operation',
                'query-builders': 'database_operation',
                'connection-handling': 'database_operation',
                'await-patterns': 'async_pattern',
                'promise-chains': 'async_pattern',
                'env-variables': 'config_access',
                'config-objects': 'config_access',
                'console-statements': 'logger',
                'logger-patterns': 'logger'
            }

            category = category_map.get(match['rule_id'], 'utility')

            # Extract function name from source code (Priority 1: Function-Level Extraction)
            source_code = match.get('matched_text', '')
            function_name = extract_function_name(
                source_code,
                file_path=match['file_path'],
                line_start=match['line_start'],
                repo_path=repository_info['path']
            )

            # Create CodeBlock
            # Note: file_path from ast-grep is already relative to repository root
            block = CodeBlock(
                block_id=block_id,
                pattern_id=match['rule_id'],
                location=SourceLocation(
                    file_path=match['file_path'],
                    line_start=match['line_start'],
                    line_end=match.get('line_end', match['line_start'])
                ),
                relative_path=match['file_path'],  # Already relative from ast-grep
                source_code=source_code,
                language='javascript',  # TODO: Detect from file extension
                category=category,
                repository_path=repository_info['path'],
                line_count=match.get('line_end', match['line_start']) - match['line_start'] + 1,
                # Store function name in tags field
                tags=[f"function:{function_name}"] if function_name else []
            )

            # Debug: confirm tags are set
            if DEBUG and i < 3:  # Only log first 3 blocks
                print(f"DEBUG block created: file={block.relative_path}, line={block.location.line_start}, tags={block.tags}", file=sys.stderr)

            blocks.append(block)

        except Exception as e:
            print(f"Warning: Failed to extract block {i} from {match.get('file_path', 'unknown')}: {e}", file=sys.stderr)
            import traceback
            traceback.print_exc(file=sys.stderr)
            continue

    return blocks


def group_duplicates(blocks: List[CodeBlock]) -> List[DuplicateGroup]:
    """
    Group similar code blocks using multi-layer similarity algorithm.

    Priority 2: Structural Similarity
    Uses the enhanced grouping algorithm that combines:
    - Layer 1: Exact matching (hash-based)
    - Layer 2: Structural similarity (AST-based)
    - Layer 3: Semantic equivalence (TODO)
    """
    # Use the multi-layer grouping algorithm
    groups = group_by_similarity(blocks, similarity_threshold=0.85)

    return groups


def generate_suggestions(groups: List[DuplicateGroup]) -> List[ConsolidationSuggestion]:
    """
    Generate consolidation suggestions with enhanced strategy logic
    """
    suggestions = []

    for group in groups:
        # Determine strategy based on multiple factors
        strategy, rationale, complexity, risk = _determine_strategy(group)

        # Generate migration steps
        migration_steps = _generate_migration_steps(group, strategy)

        # Generate code example
        code_example = _generate_code_example(group, strategy)

        # Calculate ROI score (higher for simpler, lower-risk refactoring)
        roi_score = _calculate_roi(group, complexity, risk)

        # Determine if this is a breaking change
        breaking_changes = _is_breaking_change(group, strategy)

        suggestion = ConsolidationSuggestion(
            suggestion_id=f"cs_{group.group_id}",
            duplicate_group_id=group.group_id,
            strategy=strategy,
            strategy_rationale=rationale,
            target_location=_suggest_target_location(group, strategy),
            migration_steps=migration_steps,
            code_example=code_example,
            impact_score=min(group.impact_score, 100.0),
            complexity=complexity,
            migration_risk=risk,
            estimated_effort_hours=_estimate_effort(group, complexity),
            breaking_changes=breaking_changes,
            affected_files_count=len(group.affected_files),
            affected_repositories_count=len(group.affected_repositories),
            confidence=0.9 if group.similarity_score >= 0.95 else 0.7,
            roi_score=roi_score
        )

        suggestions.append(suggestion)

    return suggestions


def _determine_strategy(group: DuplicateGroup) -> tuple[str, str, str, str]:
    """
    Determine consolidation strategy based on group characteristics

    Returns: (strategy, rationale, complexity, risk)
    """
    occurrences = group.occurrence_count
    files = len(group.affected_files)
    category = group.category

    # Single file duplicates - simplest case
    if files == 1:
        return (
            'local_util',
            f"All {occurrences} occurrences in same file - extract to local function",
            'trivial',
            'minimal'
        )

    # Logger patterns - special handling
    if category in ['logger', 'config_access']:
        if occurrences <= 5:
            return (
                'local_util',
                f"Logger/config pattern used {occurrences} times - extract to module constant",
                'trivial',
                'minimal'
            )
        else:
            return (
                'shared_package',
                f"Logger/config pattern used {occurrences} times across {files} files - centralize configuration",
                'simple',
                'low'
            )

    # API handlers and auth checks - medium complexity
    if category in ['api_handler', 'auth_check', 'error_handler']:
        if occurrences <= 3:
            return (
                'local_util',
                f"API pattern used {occurrences} times - extract to middleware/util",
                'simple',
                'low'
            )
        elif occurrences <= 10:
            return (
                'shared_package',
                f"API pattern used {occurrences} times across {files} files - create shared middleware",
                'moderate',
                'medium'
            )
        else:
            return (
                'mcp_server',
                f"API pattern used {occurrences} times - candidate for framework/MCP abstraction",
                'complex',
                'high'
            )

    # Database operations - handle carefully
    if category == 'database_operation':
        if occurrences <= 3:
            return (
                'local_util',
                f"Database pattern used {occurrences} times - extract to repository method",
                'moderate',
                'medium'
            )
        else:
            return (
                'shared_package',
                f"Database pattern used {occurrences} times - create shared query builder",
                'complex',
                'high'
            )

    # General utilities and helpers
    if occurrences <= 3:
        return (
            'local_util',
            f"Utility pattern used {occurrences} times in {files} files - extract to local util",
            'trivial' if files == 2 else 'simple',
            'minimal'
        )
    elif occurrences <= 8:
        return (
            'shared_package',
            f"Utility pattern used {occurrences} times across {files} files - create shared utility",
            'simple',
            'low'
        )
    else:
        return (
            'mcp_server',
            f"Utility pattern used {occurrences} times - consider MCP tool or shared package",
            'moderate',
            'medium'
        )


def _generate_migration_steps(group: DuplicateGroup, strategy: str) -> List[MigrationStep]:
    """Generate specific migration steps based on strategy"""

    if strategy == 'local_util':
        steps = [
            ("Create utility function in local utils module", True, "15min"),
            ("Extract common logic from duplicate blocks", False, "30min"),
            ("Replace each occurrence with function call", True, "20min"),
            ("Add unit tests for extracted function", False, "30min"),
            ("Run existing tests to verify behavior", True, "10min")
        ]
    elif strategy == 'shared_package':
        steps = [
            ("Create shared package/module for utility", False, "1h"),
            ("Extract and parameterize common logic", False, "1h"),
            ("Add comprehensive tests to shared package", False, "45min"),
            ("Update each file to import from shared package", True, "30min"),
            ("Replace duplicates with shared function calls", True, "30min"),
            ("Update package.json/requirements.txt dependencies", False, "15min"),
            ("Run full test suite across affected projects", True, "20min")
        ]
    elif strategy == 'mcp_server':
        steps = [
            ("Design MCP tool interface for functionality", False, "2h"),
            ("Create MCP server with tool implementation", False, "4h"),
            ("Add MCP tool schema and documentation", False, "1h"),
            ("Test MCP tool independently", False, "1h"),
            ("Update projects to use MCP client", False, "2h"),
            ("Replace duplicates with MCP tool calls", True, "1h"),
            ("Add integration tests", False, "2h"),
            ("Document MCP tool usage", False, "1h")
        ]
    else:  # autonomous_agent
        steps = [
            ("Define agent capabilities and workflow", False, "3h"),
            ("Design agent prompt and tool access", False, "2h"),
            ("Implement agent logic and orchestration", False, "8h"),
            ("Create agent tests and safety checks", False, "3h"),
            ("Integrate agent with existing systems", False, "4h"),
            ("Replace complex duplicate logic with agent calls", False, "2h"),
            ("Monitor agent performance and behavior", False, "ongoing"),
            ("Document agent usage and limitations", False, "2h")
        ]

    # Convert to MigrationStep objects
    return [
        MigrationStep(
            step_number=i + 1,
            description=desc,
            automated=automated,
            estimated_time=time
        )
        for i, (desc, automated, time) in enumerate(steps)
    ]


def _generate_code_example(group: DuplicateGroup, strategy: str) -> str:
    """Generate example code showing the refactoring"""

    pattern = group.pattern_id
    category = group.category

    if strategy == 'local_util':
        if category == 'logger':
            return """// Before:
logger.info({ userId }, 'User action');
logger.info({ userId }, 'User action');

// After:
const logUserAction = (userId) => logger.info({ userId }, 'User action');
logUserAction(userId);
logUserAction(userId);"""

        return """// Before: Duplicated code in multiple places
function foo() {
  // ... duplicate logic ...
}

// After: Extracted to utility function
import { sharedUtil } from './utils';
function foo() {
  sharedUtil();
}"""

    elif strategy == 'shared_package':
        return """// Before: Duplicated across files
// file1.js: { check logic }
// file2.js: { check logic }

// After: Shared package
import { validateInput } from '@shared/validators';
validateInput(data);"""

    elif strategy == 'mcp_server':
        return """// Before: Complex duplicated logic
async function processData() {
  // ... complex logic ...
}

// After: MCP tool
const result = await mcp.callTool('process-data', { input });"""

    return "// Refactoring example not available"


def _calculate_roi(group: DuplicateGroup, complexity: str, risk: str) -> float:
    """Calculate return on investment score (0-100)"""

    # Start with impact score
    roi = group.impact_score

    # Adjust based on complexity (simpler = higher ROI)
    complexity_multipliers = {
        'trivial': 1.3,
        'simple': 1.1,
        'moderate': 0.9,
        'complex': 0.7
    }
    roi *= complexity_multipliers.get(complexity, 1.0)

    # Adjust based on risk (lower risk = higher ROI)
    risk_multipliers = {
        'minimal': 1.2,
        'low': 1.1,
        'medium': 0.9,
        'high': 0.7
    }
    roi *= risk_multipliers.get(risk, 1.0)

    return min(roi, 100.0)


def _is_breaking_change(group: DuplicateGroup, strategy: str) -> bool:
    """Determine if consolidation would be a breaking change"""

    # Local utils are not breaking
    if strategy == 'local_util':
        return False

    # Shared packages might be breaking if they change APIs
    if strategy == 'shared_package':
        return group.category in ['api_handler', 'auth_check']

    # MCP servers and agents are potentially breaking
    return True


def _suggest_target_location(group: DuplicateGroup, strategy: str) -> str:
    """Suggest where the consolidated code should live"""

    if strategy == 'local_util':
        # Extract to utils in same directory
        first_file = group.affected_files[0] if group.affected_files else ''
        if '/' in first_file:
            dir_path = '/'.join(first_file.split('/')[:-1])
            return f"{dir_path}/utils.js"
        return "utils.js"

    elif strategy == 'shared_package':
        category = group.category
        if category == 'logger':
            return "shared/logging/logger-utils.js"
        elif category in ['api_handler', 'auth_check']:
            return "shared/middleware/auth-middleware.js"
        elif category == 'database_operation':
            return "shared/database/query-builder.js"
        elif category == 'validator':
            return "shared/validation/validators.js"
        else:
            return f"shared/utils/{category}.js"

    elif strategy == 'mcp_server':
        return f"mcp-servers/{group.pattern_id}-server/"

    else:  # autonomous_agent
        return f"agents/{group.pattern_id}-agent/"


def _estimate_effort(group: DuplicateGroup, complexity: str) -> float:
    """Estimate effort in hours"""

    base_hours = {
        'trivial': 0.5,
        'simple': 1.0,
        'moderate': 3.0,
        'complex': 8.0
    }

    hours = base_hours.get(complexity, 2.0)

    # Add time per affected file (more files = more refactoring)
    hours += len(group.affected_files) * 0.25

    # Add time for testing
    hours += 0.5

    return round(hours, 1)


def main():
    """
    Main pipeline execution
    """
    try:
        # Read input from stdin
        input_data = json.load(sys.stdin)

        repository_info = input_data['repository_info']
        pattern_matches = input_data['pattern_matches']

        # Stage 3: Extract code blocks
        blocks = extract_code_blocks(pattern_matches, repository_info)

        # Stage 3.5: Deduplicate blocks (Priority 4)
        blocks = deduplicate_blocks(blocks)

        # Stage 4: Semantic annotation (TODO: Implement full annotator)
        # For now, blocks already have basic category from extraction

        # Stage 5: Group duplicates
        groups = group_duplicates(blocks)

        # Stage 6: Generate suggestions
        suggestions = generate_suggestions(groups)

        # Stage 7: Calculate metrics
        metrics = {
            'total_code_blocks': len(blocks),
            'total_duplicate_groups': len(groups),
            'exact_duplicates': len([g for g in groups if g.similarity_method == 'exact']),
            'structural_duplicates': len([g for g in groups if g.similarity_method == 'structural']),
            'semantic_duplicates': 0,  # TODO: Implement semantic grouping
            'total_duplicated_lines': sum(g.total_lines for g in groups),
            'potential_loc_reduction': sum(g.total_lines - g.total_lines // g.occurrence_count for g in groups),
            'duplication_percentage': 0.0,  # TODO: Calculate properly
            'total_suggestions': len(suggestions),
            'quick_wins': len([s for s in suggestions if s.complexity == 'trivial']),
            'high_priority_suggestions': len([s for s in suggestions if s.impact_score >= 75])
        }

        # Output result as JSON (use mode='json' to serialize datetime objects)
        result = {
            'code_blocks': [b.model_dump(mode='json') for b in blocks],
            'duplicate_groups': [g.model_dump(mode='json') for g in groups],
            'suggestions': [s.model_dump(mode='json') for s in suggestions],
            'metrics': metrics
        }

        json.dump(result, sys.stdout, indent=2)

    except Exception as e:
        print(f"Error in extraction pipeline: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc(file=sys.stderr)
        sys.exit(1)


if __name__ == '__main__':
    main()
</file>

<file path="pipeline-core/git/branch-manager.js">
/**
 * Branch Manager - Generic Git branch and PR management for AlephAuto jobs
 *
 * Provides branch creation, change detection, commit, push, and PR creation
 * for any AlephAuto worker that modifies code.
 *
 * Features:
 * - Create feature branches with job context
 * - Detect git changes automatically
 * - Commit with descriptive messages
 * - Push branches to remote
 * - Create PRs with job details
 * - Clean up on errors
 * - Sentry error tracking
 */

// @ts-check
/** @typedef {import('../errors/error-types').ProcessError} ProcessError */

import { spawn } from 'child_process';
import { createComponentLogger } from '../../utils/logger.js';
import * as Sentry from '@sentry/node';

const logger = createComponentLogger('BranchManager');

/**
 * Branch Manager for Git operations
 */
export class BranchManager {
  /**
   * @param {Object} options - Configuration options
   * @param {string} [options.baseBranch='main'] - Base branch for PRs
   * @param {string} [options.branchPrefix='automated'] - Prefix for branch names
   * @param {boolean} [options.dryRun=false] - Skip push and PR creation
   */
  constructor(options = {}) {
    this.baseBranch = options.baseBranch || 'main';
    this.branchPrefix = options.branchPrefix || 'automated';
    this.dryRun = options.dryRun ?? false;
  }

  /**
   * Check if repository has uncommitted changes
   *
   * @param {string} repositoryPath - Path to repository
   * @returns {Promise<boolean>} True if changes exist
   */
  async hasChanges(repositoryPath) {
    try {
      const status = await this._runGitCommand(repositoryPath, ['status', '--porcelain']);
      return status.trim().length > 0;
    } catch (error) {
      logger.warn({ error, repositoryPath }, 'Failed to check git status');
      return false;
    }
  }

  /**
   * Get list of changed files
   *
   * @param {string} repositoryPath - Path to repository
   * @returns {Promise<string[]>} List of changed file paths
   */
  async getChangedFiles(repositoryPath) {
    try {
      const status = await this._runGitCommand(repositoryPath, ['status', '--porcelain']);

      // Parse git status output
      // Format: XY filename
      // X = index status, Y = working tree status
      const files = status
        .split('\n')
        .filter(line => line.trim().length > 0)
        .map(line => {
          // Remove status characters and trim
          return line.substring(3).trim();
        });

      return files;
    } catch (error) {
      logger.warn({ error, repositoryPath }, 'Failed to get changed files');
      return [];
    }
  }

  /**
   * Get current branch name
   *
   * @param {string} repositoryPath - Path to repository
   * @returns {Promise<string>} Current branch name
   */
  async getCurrentBranch(repositoryPath) {
    try {
      const branch = await this._runGitCommand(repositoryPath, ['rev-parse', '--abbrev-ref', 'HEAD']);
      return branch.trim();
    } catch (error) {
      logger.warn({ error, repositoryPath }, 'Failed to get current branch');
      return '';
    }
  }

  /**
   * Check if repository is a git repository
   *
   * @param {string} repositoryPath - Path to check
   * @returns {Promise<boolean>} True if git repository
   */
  async isGitRepository(repositoryPath) {
    try {
      await this._runGitCommand(repositoryPath, ['rev-parse', '--git-dir']);
      return true;
    } catch (error) {
      return false;
    }
  }

  /**
   * Create and checkout a new branch for job changes
   *
   * @param {string} repositoryPath - Path to repository
   * @param {Object} jobContext - Job context for branch naming
   * @param {string} jobContext.jobId - Job ID
   * @param {string} jobContext.jobType - Type of job (duplicate-detection, schema-enhancement, etc.)
   * @param {string} [jobContext.description] - Optional description for branch name
   * @returns {Promise<{branchName: string, originalBranch: string}>} Branch info
   */
  async createJobBranch(repositoryPath, jobContext) {
    const span = Sentry.startInactiveSpan({
      op: 'git.create_branch',
      name: 'Create Job Branch',
    });

    try {
      // Check if it's a git repository
      const isGitRepo = await this.isGitRepository(repositoryPath);
      if (!isGitRepo) {
        logger.info({ repositoryPath }, 'Not a git repository, skipping branch creation');
        return { branchName: '', originalBranch: '' };
      }

      // Get current branch
      const originalBranch = await this.getCurrentBranch(repositoryPath);

      logger.info({
        repositoryPath,
        originalBranch,
        jobId: jobContext.jobId
      }, 'Creating job branch');

      // Ensure we're on base branch
      await this._runGitCommand(repositoryPath, ['checkout', this.baseBranch]);

      // Pull latest changes (skip in dry-run)
      if (!this.dryRun) {
        try {
          await this._runGitCommand(repositoryPath, ['pull', 'origin', this.baseBranch]);
        } catch (error) {
          // Non-critical if pull fails (might be local-only repo)
          logger.warn({ error }, 'Failed to pull latest changes, continuing anyway');
        }
      }

      // Generate branch name
      const branchName = this._generateBranchName(jobContext);

      // Create and checkout new branch
      await this._runGitCommand(repositoryPath, ['checkout', '-b', branchName]);

      logger.info({ branchName, repositoryPath }, 'Branch created and checked out');

      span.setStatus('ok');
      return { branchName, originalBranch };

    } catch (error) {
      span.setStatus('internal_error');
      logger.error({ error, repositoryPath, jobContext }, 'Failed to create job branch');

      Sentry.captureException(error, {
        tags: {
          component: 'branch-manager',
          operation: 'create_branch',
          jobType: jobContext.jobType
        },
        extra: {
          repositoryPath,
          jobContext
        }
      });

      throw error;
    } finally {
      span.end();
    }
  }

  /**
   * Commit changes with job context
   *
   * @param {string} repositoryPath - Path to repository
   * @param {Object} commitContext - Commit context
   * @param {string} commitContext.message - Commit message
   * @param {string} commitContext.jobId - Job ID
   * @param {string} [commitContext.description] - Optional detailed description
   * @returns {Promise<string>} Commit SHA
   */
  async commitChanges(repositoryPath, commitContext) {
    const span = Sentry.startInactiveSpan({
      op: 'git.commit',
      name: 'Commit Changes',
    });

    try {
      logger.info({ repositoryPath, jobId: commitContext.jobId }, 'Committing changes');

      // Check if there are changes
      const hasChanges = await this.hasChanges(repositoryPath);
      if (!hasChanges) {
        logger.info({ repositoryPath }, 'No changes to commit');
        span.setStatus('ok');
        return '';
      }

      // Get changed files for logging
      const changedFiles = await this.getChangedFiles(repositoryPath);
      logger.info({ changedFiles, count: changedFiles.length }, 'Files changed');

      // Stage all changes
      await this._runGitCommand(repositoryPath, ['add', '.']);

      // Create commit message
      const commitMessage = this._generateCommitMessage(commitContext, changedFiles);

      // Commit
      await this._runGitCommand(repositoryPath, ['commit', '-m', commitMessage]);

      // Get commit SHA
      const commitSha = await this._runGitCommand(repositoryPath, ['rev-parse', 'HEAD']);

      logger.info({ commitSha: commitSha.trim(), filesCount: changedFiles.length }, 'Changes committed');

      span.setStatus('ok');
      return commitSha.trim();

    } catch (error) {
      span.setStatus('internal_error');
      logger.error({ error, repositoryPath, commitContext }, 'Failed to commit changes');

      Sentry.captureException(error, {
        tags: {
          component: 'branch-manager',
          operation: 'commit',
          jobId: commitContext.jobId
        },
        extra: {
          repositoryPath,
          commitContext
        }
      });

      throw error;
    } finally {
      span.end();
    }
  }

  /**
   * Push branch to remote
   *
   * @param {string} repositoryPath - Path to repository
   * @param {string} branchName - Branch name to push
   * @returns {Promise<boolean>} True if pushed successfully
   */
  async pushBranch(repositoryPath, branchName) {
    const span = Sentry.startInactiveSpan({
      op: 'git.push',
      name: 'Push Branch',
    });

    try {
      if (this.dryRun) {
        logger.info({ branchName }, 'Dry run: Skipping branch push');
        span.setStatus('ok');
        return false;
      }

      logger.info({ repositoryPath, branchName }, 'Pushing branch to remote');

      await this._runGitCommand(repositoryPath, ['push', '-u', 'origin', branchName]);

      logger.info({ branchName }, 'Branch pushed successfully');

      span.setStatus('ok');
      return true;

    } catch (error) {
      span.setStatus('internal_error');
      logger.error({ error, repositoryPath, branchName }, 'Failed to push branch');

      Sentry.captureException(error, {
        tags: {
          component: 'branch-manager',
          operation: 'push'
        },
        extra: {
          repositoryPath,
          branchName
        }
      });

      // Don't throw - PR creation can still fail gracefully
      return false;
    } finally {
      span.end();
    }
  }

  /**
   * Create pull request using gh CLI
   *
   * @param {string} repositoryPath - Path to repository
   * @param {Object} prContext - PR context
   * @param {string} prContext.branchName - Branch name
   * @param {string} prContext.title - PR title
   * @param {string} prContext.body - PR description
   * @param {string[]} [prContext.labels] - Optional labels to add
   * @returns {Promise<string|null>} PR URL or null if failed
   */
  async createPullRequest(repositoryPath, prContext) {
    const span = Sentry.startInactiveSpan({
      op: 'git.create_pr',
      name: 'Create Pull Request',
    });

    try {
      if (this.dryRun) {
        logger.info({ branch: prContext.branchName }, 'Dry run: Skipping PR creation');
        span.setStatus('ok');
        return `dry-run-${prContext.branchName}`;
      }

      logger.info({
        repositoryPath,
        branchName: prContext.branchName,
        title: prContext.title
      }, 'Creating pull request');

      // Build gh pr create command
      const args = [
        'pr',
        'create',
        '--title', prContext.title,
        '--body', prContext.body,
        '--base', this.baseBranch,
        '--head', prContext.branchName
      ];

      // Add labels if provided
      if (prContext.labels && prContext.labels.length > 0) {
        args.push('--label', prContext.labels.join(','));
      }

      const prUrl = await this._runCommand(repositoryPath, 'gh', args);

      logger.info({ prUrl: prUrl.trim() }, 'Pull request created');

      span.setStatus('ok');
      return prUrl.trim();

    } catch (error) {
      span.setStatus('internal_error');
      logger.error({ error, repositoryPath, prContext }, 'Failed to create pull request');

      Sentry.captureException(error, {
        tags: {
          component: 'branch-manager',
          operation: 'create_pr'
        },
        extra: {
          repositoryPath,
          prContext
        }
      });

      return null;
    } finally {
      span.end();
    }
  }

  /**
   * Clean up branch (checkout original and delete job branch)
   *
   * @param {string} repositoryPath - Path to repository
   * @param {string} branchName - Branch to delete
   * @param {string} originalBranch - Original branch to checkout
   * @returns {Promise<void>}
   */
  async cleanupBranch(repositoryPath, branchName, originalBranch) {
    try {
      logger.info({ repositoryPath, branchName, originalBranch }, 'Cleaning up branch');

      // Checkout original branch (or base branch if original not available)
      const targetBranch = originalBranch || this.baseBranch;
      await this._runGitCommand(repositoryPath, ['checkout', targetBranch]);

      // Delete job branch
      await this._runGitCommand(repositoryPath, ['branch', '-D', branchName]);

      logger.info({ branchName }, 'Branch cleaned up');

    } catch (error) {
      logger.warn({ error, branchName }, 'Failed to cleanup branch (non-critical)');
      // Don't throw - cleanup is best-effort
    }
  }

  /**
   * Generate branch name from job context
   *
   * @param {Object} jobContext - Job context
   * @returns {string} Branch name
   * @private
   */
  _generateBranchName(jobContext) {
    const timestamp = Date.now();
    const jobType = jobContext.jobType || 'job';
    const description = jobContext.description
      ? `-${jobContext.description.toLowerCase().replace(/[^a-z0-9]+/g, '-').substring(0, 30)}`
      : '';

    return `${this.branchPrefix}/${jobType}${description}-${timestamp}`;
  }

  /**
   * Generate commit message from context
   *
   * @param {Object} commitContext - Commit context
   * @param {string[]} changedFiles - Changed files
   * @returns {string} Commit message
   * @private
   */
  _generateCommitMessage(commitContext, changedFiles) {
    const lines = [
      commitContext.message,
      '',
    ];

    if (commitContext.description) {
      lines.push(commitContext.description, '');
    }

    lines.push(
      `Job ID: ${commitContext.jobId}`,
      `Files changed: ${changedFiles.length}`,
      '',
      'ü§ñ Generated with [Claude Code](https://claude.com/claude-code)',
      '',
      'Co-Authored-By: Claude <noreply@anthropic.com>'
    );

    return lines.join('\n');
  }

  /**
   * Run a Git command
   *
   * @param {string} cwd - Working directory
   * @param {string[]} args - Git arguments
   * @returns {Promise<string>} Command output
   * @private
   */
  async _runGitCommand(cwd, args) {
    return this._runCommand(cwd, 'git', args);
  }

  /**
   * Run a shell command
   *
   * @param {string} cwd - Working directory
   * @param {string} command - Command to run
   * @param {string[]} args - Command arguments
   * @returns {Promise<string>} Command output
   * @private
   */
  async _runCommand(cwd, command, args) {
    return new Promise((resolve, reject) => {
      const proc = spawn(command, args, { cwd });

      let stdout = '';
      let stderr = '';

      proc.stdout.on('data', (data) => {
        stdout += data.toString();
      });

      proc.stderr.on('data', (data) => {
        stderr += data.toString();
      });

      proc.on('close', (code) => {
        if (code === 0) {
          resolve(stdout.trim());
        } else {
          const error = /** @type {ProcessError} */ (new Error(`Command failed: ${command} ${args.join(' ')}\n${stderr}`));
          error.code = code;
          error.stdout = stdout;
          error.stderr = stderr;
          reject(error);
        }
      });

      proc.on('error', (error) => {
        reject(error);
      });
    });
  }
}
</file>

<file path="pipeline-core/git/migration-transformer.js">
/**
 * Migration Transformer - AST-based code transformation for consolidation migrations
 *
 * Applies migration steps to affected files using AST manipulation:
 * - Updates import statements to point to consolidated files
 * - Updates function calls to use new consolidated locations
 * - Removes old duplicate code
 * - Creates backups before transformations
 * - Provides rollback capability
 *
 * Features:
 * - Babel AST manipulation for JavaScript/TypeScript
 * - Safe transformation with validation
 * - Atomic operations with rollback
 * - Comprehensive error handling
 * - Sentry error tracking
 */

// @ts-check
import { parse } from '@babel/parser';
import traverse from '@babel/traverse';
import generate from '@babel/generator';
import * as t from '@babel/types';
import fs from 'fs/promises';
import path from 'path';
import { createComponentLogger } from '../../utils/logger.js';
import * as Sentry from '@sentry/node';

const logger = createComponentLogger('MigrationTransformer');

/**
 * Parse migration step description to extract transformation details
 *
 * Migration steps follow patterns like:
 * - "Update import from './utils/json.js' to '../shared/json-utils.js'"
 * - "Replace calls to writeJsonFile with jsonUtils.writeJsonFile"
 * - "Remove duplicate function writeJsonFile from src/utils/legacy.js"
 *
 * @param {string} description - Migration step description
 * @returns {Object|null} Parsed transformation details or null if not parseable
 */
function parseMigrationStep(description) {
  // Pattern 1: Update import
  const importPattern = /Update import.*?from ['"]([^'"]+)['"].*?to ['"]([^'"]+)['"]/i;
  const importMatch = description.match(importPattern);
  if (importMatch) {
    return {
      type: 'update-import',
      oldPath: importMatch[1],
      newPath: importMatch[2]
    };
  }

  // Pattern 2: Replace function calls
  const callPattern = /Replace calls to (\w+) with ([\w.]+)/i;
  const callMatch = description.match(callPattern);
  if (callMatch) {
    return {
      type: 'replace-call',
      oldName: callMatch[1],
      newName: callMatch[2]
    };
  }

  // Pattern 3: Remove duplicate code
  const removePattern = /Remove duplicate (?:function|class|const|let|var) (\w+)/i;
  const removeMatch = description.match(removePattern);
  if (removeMatch) {
    return {
      type: 'remove-declaration',
      name: removeMatch[1]
    };
  }

  // Pattern 4: Add import
  const addImportPattern = /Add import.*?['"]([^'"]+)['"].*?from ['"]([^'"]+)['"]/i;
  const addImportMatch = description.match(addImportPattern);
  if (addImportMatch) {
    return {
      type: 'add-import',
      imported: addImportMatch[1],
      source: addImportMatch[2]
    };
  }

  logger.debug({ description }, 'Could not parse migration step');
  return null;
}

/**
 * Migration Transformer for applying consolidation changes
 */
export class MigrationTransformer {
  constructor(options = {}) {
    this.backupDir = options.backupDir || '.migration-backups';
    this.dryRun = options.dryRun ?? false;
  }

  /**
   * Apply migration steps to affected files
   *
   * @param {Object} suggestion - Consolidation suggestion with migration_steps
   * @param {string} repositoryPath - Repository path
   * @returns {Promise<Object>} Transformation results
   */
  async applyMigrationSteps(suggestion, repositoryPath) {
    logger.info({
      suggestionId: suggestion.suggestion_id,
      stepsCount: suggestion.migration_steps?.length || 0,
      repositoryPath
    }, 'Applying migration steps');

    if (!suggestion.migration_steps || suggestion.migration_steps.length === 0) {
      logger.info('No migration steps to apply');
      return {
        filesModified: [],
        transformations: [],
        backupPath: null
      };
    }

    const results = {
      filesModified: [],
      transformations: [],
      errors: [],
      backupPath: null
    };

    // Create backup directory
    const backupPath = await this._createBackup(repositoryPath);
    results.backupPath = backupPath;

    try {
      // Parse all migration steps
      const parsedSteps = suggestion.migration_steps
        .map((step, index) => ({
          ...step,
          parsed: parseMigrationStep(step.description),
          index
        }))
        .filter(step => step.parsed !== null);

      logger.info({
        totalSteps: suggestion.migration_steps.length,
        parseableSteps: parsedSteps.length
      }, 'Parsed migration steps');

      // Group steps by affected file (extracted from code_example or inferred)
      const fileGroups = this._groupStepsByFile(parsedSteps, suggestion);

      // Apply transformations to each file
      for (const [filePath, steps] of Object.entries(fileGroups)) {
        try {
          const absolutePath = path.join(repositoryPath, filePath);

          // Check file exists
          try {
            await fs.access(absolutePath);
          } catch {
            logger.warn({ filePath }, 'File does not exist, skipping');
            continue;
          }

          const transformResult = await this._transformFile(
            absolutePath,
            steps,
            suggestion
          );

          if (transformResult.modified) {
            results.filesModified.push(filePath);
            results.transformations.push({
              file: filePath,
              ...transformResult
            });
          }

        } catch (error) {
          logger.error({ error, filePath }, 'Failed to transform file');
          results.errors.push({
            file: filePath,
            error: error.message
          });

          Sentry.captureException(error, {
            tags: {
              component: 'migration-transformer',
              file: filePath
            },
            extra: {
              suggestionId: suggestion.suggestion_id,
              steps: steps.map(s => s.description)
            }
          });
        }
      }

      logger.info({
        filesModified: results.filesModified.length,
        transformations: results.transformations.length,
        errors: results.errors.length
      }, 'Migration steps applied');

      return results;

    } catch (error) {
      logger.error({ error }, 'Failed to apply migration steps');

      // Attempt rollback on error
      if (results.filesModified.length > 0) {
        logger.info('Attempting rollback due to error');
        await this.rollback(backupPath, repositoryPath);
      }

      throw error;
    }
  }

  /**
   * Transform a single file by applying migration steps
   *
   * @param {string} filePath - Absolute file path
   * @param {Array} steps - Migration steps to apply
   * @param {Object} suggestion - Full suggestion context
   * @returns {Promise<Object>} Transformation result
   * @private
   */
  async _transformFile(filePath, steps, suggestion) {
    logger.debug({ filePath, stepsCount: steps.length }, 'Transforming file');

    // Read original source
    const originalSource = await fs.readFile(filePath, 'utf-8');

    // Parse to AST
    let ast;
    try {
      ast = parse(originalSource, {
        sourceType: 'module',
        plugins: [
          'typescript',
          'jsx',
          'decorators-legacy',
          'classProperties',
          'objectRestSpread',
          'asyncGenerators',
          'dynamicImport',
          'optionalChaining',
          'nullishCoalescingOperator'
        ]
      });
    } catch (parseError) {
      logger.warn({ filePath, parseError }, 'Failed to parse file as JavaScript/TypeScript');
      return {
        modified: false,
        reason: 'parse-error',
        error: parseError.message
      };
    }

    let modified = false;
    const appliedTransformations = [];

    // Apply each transformation
    for (const step of steps) {
      const transformation = step.parsed;

      try {
        switch (transformation.type) {
          case 'update-import':
            if (this._updateImport(ast, transformation.oldPath, transformation.newPath)) {
              modified = true;
              appliedTransformations.push({
                type: 'update-import',
                from: transformation.oldPath,
                to: transformation.newPath
              });
            }
            break;

          case 'add-import':
            if (this._addImport(ast, transformation.imported, transformation.source)) {
              modified = true;
              appliedTransformations.push({
                type: 'add-import',
                imported: transformation.imported,
                from: transformation.source
              });
            }
            break;

          case 'replace-call':
            if (this._replaceCallExpression(ast, transformation.oldName, transformation.newName)) {
              modified = true;
              appliedTransformations.push({
                type: 'replace-call',
                from: transformation.oldName,
                to: transformation.newName
              });
            }
            break;

          case 'remove-declaration':
            if (this._removeDeclaration(ast, transformation.name)) {
              modified = true;
              appliedTransformations.push({
                type: 'remove-declaration',
                name: transformation.name
              });
            }
            break;

          default:
            logger.warn({ type: transformation.type }, 'Unknown transformation type');
        }
      } catch (transformError) {
        logger.error({
          transformError,
          transformation
        }, 'Transformation failed');
      }
    }

    // Generate new code if modified
    if (modified) {
      const output = generate(ast, {
        retainLines: false,
        comments: true
      });

      if (!this.dryRun) {
        await fs.writeFile(filePath, output.code, 'utf-8');
        logger.info({ filePath }, 'File transformed successfully');
      } else {
        logger.info({ filePath }, 'Dry run: Would transform file');
      }

      return {
        modified: true,
        transformations: appliedTransformations,
        originalLength: originalSource.length,
        newLength: output.code.length
      };
    }

    return {
      modified: false,
      reason: 'no-transformations-applied'
    };
  }

  /**
   * Update import statement from old path to new path
   *
   * @param {Object} ast - Babel AST
   * @param {string} oldPath - Old import path
   * @param {string} newPath - New import path
   * @returns {boolean} True if modified
   * @private
   */
  _updateImport(ast, oldPath, newPath) {
    let modified = false;

    traverse(ast, {
      ImportDeclaration(path) {
        if (path.node.source.value === oldPath) {
          path.node.source.value = newPath;
          modified = true;
          logger.debug({ oldPath, newPath }, 'Updated import statement');
        }
      }
    });

    return modified;
  }

  /**
   * Add new import statement
   *
   * @param {Object} ast - Babel AST
   * @param {string} imported - What to import (e.g., 'writeJsonFile' or '{ writeJsonFile }')
   * @param {string} source - Import source path
   * @returns {boolean} True if modified
   * @private
   */
  _addImport(ast, imported, source) {
    // Check if import already exists
    let alreadyExists = false;
    traverse(ast, {
      ImportDeclaration(path) {
        if (path.node.source.value === source) {
          alreadyExists = true;
          path.stop();
        }
      }
    });

    if (alreadyExists) {
      logger.debug({ imported, source }, 'Import already exists');
      return false;
    }

    // Parse imported name(s)
    let specifiers;
    if (imported.startsWith('{') && imported.endsWith('}')) {
      // Named import: { foo, bar }
      const names = imported.slice(1, -1).split(',').map(n => n.trim());
      specifiers = names.map(name =>
        t.importSpecifier(t.identifier(name), t.identifier(name))
      );
    } else if (imported === '*') {
      // Namespace import: * as foo
      specifiers = [t.importNamespaceSpecifier(t.identifier('imported'))];
    } else {
      // Default import: foo
      specifiers = [t.importDefaultSpecifier(t.identifier(imported))];
    }

    // Create import declaration
    const importDeclaration = t.importDeclaration(
      specifiers,
      t.stringLiteral(source)
    );

    // Add to top of program
    ast.program.body.unshift(importDeclaration);
    logger.debug({ imported, source }, 'Added import statement');

    return true;
  }

  /**
   * Replace function call expressions
   *
   * @param {Object} ast - Babel AST
   * @param {string} oldName - Old function name
   * @param {string} newName - New function name (supports dot notation)
   * @returns {boolean} True if modified
   * @private
   */
  _replaceCallExpression(ast, oldName, newName) {
    let modified = false;

    traverse(ast, {
      CallExpression(path) {
        // Handle simple identifier calls
        if (t.isIdentifier(path.node.callee, { name: oldName })) {
          // Parse new name (supports dot notation like 'utils.writeJson')
          if (newName.includes('.')) {
            const parts = newName.split('.');
            let memberExpr = t.identifier(parts[0]);
            for (let i = 1; i < parts.length; i++) {
              memberExpr = t.memberExpression(memberExpr, t.identifier(parts[i]));
            }
            path.node.callee = memberExpr;
          } else {
            path.node.callee = t.identifier(newName);
          }
          modified = true;
          logger.debug({ oldName, newName }, 'Replaced call expression');
        }
      }
    });

    return modified;
  }

  /**
   * Remove declaration (function, class, const, let, var)
   *
   * @param {Object} ast - Babel AST
   * @param {string} name - Name of declaration to remove
   * @returns {boolean} True if modified
   * @private
   */
  _removeDeclaration(ast, name) {
    let modified = false;

    traverse(ast, {
      // Function declarations
      FunctionDeclaration(path) {
        if (path.node.id && path.node.id.name === name) {
          path.remove();
          modified = true;
          logger.debug({ name, type: 'function' }, 'Removed declaration');
        }
      },

      // Class declarations
      ClassDeclaration(path) {
        if (path.node.id && path.node.id.name === name) {
          path.remove();
          modified = true;
          logger.debug({ name, type: 'class' }, 'Removed declaration');
        }
      },

      // Variable declarations (const, let, var)
      VariableDeclarator(path) {
        if (t.isIdentifier(path.node.id, { name })) {
          // Remove the declarator
          const parent = path.parentPath;
          if (parent.node.declarations.length === 1) {
            // Only one declaration, remove entire statement
            parent.remove();
          } else {
            // Multiple declarations, remove just this one
            path.remove();
          }
          modified = true;
          logger.debug({ name, type: 'variable' }, 'Removed declaration');
        }
      }
    });

    return modified;
  }

  /**
   * Group migration steps by affected file
   *
   * Tries to infer file from code_example or uses suggestion context
   *
   * @param {Array} parsedSteps - Parsed migration steps
   * @param {Object} suggestion - Full suggestion
   * @returns {Object} Map of file paths to steps
   * @private
   */
  _groupStepsByFile(parsedSteps, suggestion) {
    const fileGroups = {};

    for (const step of parsedSteps) {
      // Try to extract file path from code_example (e.g., "// src/utils/json.js")
      let filePath = null;

      if (step.code_example) {
        const fileCommentMatch = step.code_example.match(/^\/\/\s*(.+?\.(?:js|ts|jsx|tsx))/);
        if (fileCommentMatch) {
          filePath = fileCommentMatch[1];
        }
      }

      // Fallback: Use transformation type to infer
      if (!filePath) {
        switch (step.parsed.type) {
          case 'update-import':
          case 'add-import':
            // Assume these apply to files that import the old code
            filePath = '__inferred_importers__';
            break;
          case 'remove-declaration':
            // Assume this applies to files where duplicates exist
            filePath = '__inferred_duplicate_locations__';
            break;
          case 'replace-call':
            // Assume this applies to all files using the old function
            filePath = '__inferred_callers__';
            break;
        }
      }

      if (!fileGroups[filePath]) {
        fileGroups[filePath] = [];
      }
      fileGroups[filePath].push(step);
    }

    logger.debug({ fileCount: Object.keys(fileGroups).length }, 'Grouped steps by file');
    return fileGroups;
  }

  /**
   * Create backup of current state
   *
   * @param {string} repositoryPath - Repository path
   * @returns {Promise<string>} Backup directory path
   * @private
   */
  async _createBackup(repositoryPath) {
    const timestamp = Date.now();
    const backupPath = path.join(repositoryPath, this.backupDir, `backup-${timestamp}`);

    if (this.dryRun) {
      logger.info({ backupPath }, 'Dry run: Would create backup');
      return backupPath;
    }

    await fs.mkdir(backupPath, { recursive: true });
    logger.info({ backupPath }, 'Created backup directory');

    return backupPath;
  }

  /**
   * Backup a file before transformation
   *
   * @param {string} filePath - Original file path
   * @param {string} backupPath - Backup directory
   * @returns {Promise<void>}
   * @private
   */
  async _backupFile(filePath, backupPath) {
    if (this.dryRun) {
      return;
    }

    const fileName = path.basename(filePath);
    const backupFilePath = path.join(backupPath, fileName);

    await fs.copyFile(filePath, backupFilePath);
    logger.debug({ filePath, backupFilePath }, 'Backed up file');
  }

  /**
   * Rollback transformations using backup
   *
   * @param {string} backupPath - Backup directory path
   * @param {string} repositoryPath - Repository path
   * @returns {Promise<void>}
   */
  async rollback(backupPath, repositoryPath) {
    logger.info({ backupPath }, 'Rolling back transformations');

    if (this.dryRun) {
      logger.info('Dry run: Would rollback transformations');
      return;
    }

    try {
      // Read backup directory
      const files = await fs.readdir(backupPath);

      for (const file of files) {
        const backupFilePath = path.join(backupPath, file);
        const originalFilePath = path.join(repositoryPath, file);

        await fs.copyFile(backupFilePath, originalFilePath);
        logger.debug({ file }, 'Restored file from backup');
      }

      logger.info({ filesRestored: files.length }, 'Rollback completed');

    } catch (error) {
      logger.error({ error, backupPath }, 'Rollback failed');
      Sentry.captureException(error, {
        tags: { component: 'migration-transformer', operation: 'rollback' },
        extra: { backupPath, repositoryPath }
      });
      throw error;
    }
  }
}
</file>

<file path="pipeline-core/git/pr-creator.js">
/**
 * PR Creator - Automated Pull Request creation for consolidation suggestions
 *
 * Automatically creates Git branches and pull requests for duplicate code consolidation.
 * Uses gh CLI for PR creation and Git commands for branch management.
 *
 * Features:
 * - Creates feature branches for consolidation suggestions
 * - Applies automated code changes
 * - Commits changes with descriptive messages
 * - Creates PRs with detailed descriptions
 * - Handles multi-file consolidations
 * - Sentry error tracking
 */

// @ts-check
/** @typedef {import('../errors/error-types').ProcessError} ProcessError */

import { spawn } from 'child_process';
import fs from 'fs/promises';
import path from 'path';
import { createComponentLogger } from '../../utils/logger.js';
import * as Sentry from '@sentry/node';
import { MigrationTransformer } from './migration-transformer.js';

const logger = createComponentLogger('PRCreator');

/**
 * PR Creator for automated consolidation suggestions
 */
export class PRCreator {
  constructor(options = {}) {
    this.baseBranch = options.baseBranch || 'main';
    this.branchPrefix = options.branchPrefix || 'consolidate';
    this.dryRun = options.dryRun ?? false;
    this.maxSuggestionsPerPR = options.maxSuggestionsPerPR || 5;
    this.migrationTransformer = new MigrationTransformer({
      dryRun: this.dryRun
    });
  }

  /**
   * Create PRs for consolidation suggestions
   *
   * @param {Object} scanResult - Scan result with suggestions
   * @param {string} repositoryPath - Path to repository
   * @param {Object} options - PR creation options
   * @returns {Promise<Object>} PR creation results
   */
  async createPRsForSuggestions(scanResult, repositoryPath, options = {}) {
    logger.info({
      repositoryPath,
      totalSuggestions: scanResult.suggestions?.length || 0
    }, 'Creating PRs for consolidation suggestions');

    if (!scanResult.suggestions || scanResult.suggestions.length === 0) {
      logger.info('No suggestions to process');
      return {
        prsCreated: 0,
        errors: [],
        skipped: 0
      };
    }

    const results = {
      prsCreated: 0,
      prUrls: [],
      errors: [],
      skipped: 0
    };

    // Filter suggestions that can be automated
    const automatableSuggestions = scanResult.suggestions.filter(
      s => s.automated_refactor_possible && s.impact_score >= 50
    );

    logger.info({
      total: scanResult.suggestions.length,
      automatable: automatableSuggestions.length
    }, 'Filtered automatable suggestions');

    if (automatableSuggestions.length === 0) {
      logger.info('No automatable suggestions found');
      results.skipped = scanResult.suggestions.length;
      return results;
    }

    // Group suggestions for batching (max 5 per PR to keep PRs manageable)
    const suggestionBatches = this._batchSuggestions(automatableSuggestions);

    logger.info({
      batches: suggestionBatches.length
    }, 'Created suggestion batches');

    // Create PR for each batch
    for (let i = 0; i < suggestionBatches.length; i++) {
      const batch = suggestionBatches[i];

      try {
        const prUrl = await this._createPRForBatch(
          batch,
          repositoryPath,
          i + 1,
          options
        );

        if (prUrl) {
          results.prsCreated++;
          results.prUrls.push(prUrl);
        } else {
          results.skipped += batch.length;
        }
      } catch (error) {
        logger.error({ error, batch: i + 1 }, 'Failed to create PR for batch');
        results.errors.push({
          batch: i + 1,
          error: error.message,
          suggestions: batch.map(s => s.suggestion_id)
        });

        Sentry.captureException(error, {
          tags: {
            component: 'pr-creator',
            batch: i + 1
          },
          extra: {
            repositoryPath,
            suggestions: batch.map(s => s.suggestion_id)
          }
        });
      }
    }

    logger.info(results, 'PR creation completed');
    return results;
  }

  /**
   * Create a PR for a batch of suggestions
   *
   * @param {Array} suggestions - Suggestions to include in PR
   * @param {string} repositoryPath - Repository path
   * @param {number} batchNumber - Batch number
   * @param {Object} options - Creation options
   * @returns {Promise<string|null>} PR URL or null
   * @private
   */
  async _createPRForBatch(suggestions, repositoryPath, batchNumber, options) {
    const branchName = this._generateBranchName(suggestions, batchNumber);

    logger.info({
      branchName,
      suggestions: suggestions.length,
      batchNumber
    }, 'Creating PR for suggestion batch');

    try {
      // 1. Ensure we're on base branch
      await this._runGitCommand(repositoryPath, ['checkout', this.baseBranch]);

      // 2. Pull latest changes (skip in dry-run to allow testing without remote)
      if (!this.dryRun) {
        await this._runGitCommand(repositoryPath, ['pull', 'origin', this.baseBranch]);
      }

      // 3. Create new branch
      await this._runGitCommand(repositoryPath, ['checkout', '-b', branchName]);

      // 4. Apply suggested changes
      const filesModified = await this._applySuggestions(suggestions, repositoryPath);

      if (filesModified.length === 0) {
        logger.warn({ branchName }, 'No files were modified, skipping PR creation');
        // Cleanup branch
        await this._runGitCommand(repositoryPath, ['checkout', this.baseBranch]);
        await this._runGitCommand(repositoryPath, ['branch', '-D', branchName]);
        return null;
      }

      // 5. Stage changes
      await this._runGitCommand(repositoryPath, ['add', '.']);

      // 6. Commit changes
      const commitMessage = this._generateCommitMessage(suggestions, filesModified);
      await this._runGitCommand(repositoryPath, ['commit', '-m', commitMessage]);

      if (this.dryRun) {
        logger.info({ branchName }, 'Dry run: Skipping push and PR creation');
        // Cleanup branch
        await this._runGitCommand(repositoryPath, ['checkout', this.baseBranch]);
        await this._runGitCommand(repositoryPath, ['branch', '-D', branchName]);
        return `dry-run-${branchName}`;
      }

      // 7. Push branch
      await this._runGitCommand(repositoryPath, ['push', '-u', 'origin', branchName]);

      // 8. Create PR
      const prDescription = this._generatePRDescription(suggestions, filesModified);
      const prTitle = this._generatePRTitle(suggestions, batchNumber);
      const prUrl = await this._createPR(repositoryPath, branchName, prTitle, prDescription);

      logger.info({ prUrl, branchName }, 'PR created successfully');
      return prUrl;

    } catch (error) {
      logger.error({ error, branchName }, 'Failed to create PR');

      // Attempt cleanup
      try {
        await this._runGitCommand(repositoryPath, ['checkout', this.baseBranch]);
        await this._runGitCommand(repositoryPath, ['branch', '-D', branchName]);
      } catch (cleanupError) {
        logger.warn({ error: cleanupError }, 'Failed to cleanup branch');
      }

      throw error;
    }
  }

  /**
   * Apply consolidation suggestions to files
   *
   * @param {Array} suggestions - Suggestions to apply
   * @param {string} repositoryPath - Repository path
   * @returns {Promise<Array>} List of modified files
   * @private
   */
  async _applySuggestions(suggestions, repositoryPath) {
    const filesModified = [];

    for (const suggestion of suggestions) {
      try {
        // Create consolidated file if target_location is specified
        if (suggestion.target_location && suggestion.proposed_implementation) {
          const targetPath = path.join(repositoryPath, suggestion.target_location);

          // Ensure directory exists
          await fs.mkdir(path.dirname(targetPath), { recursive: true });

          // Write proposed implementation
          await fs.writeFile(targetPath, suggestion.proposed_implementation, 'utf-8');
          filesModified.push(suggestion.target_location);

          logger.info({
            file: suggestion.target_location,
            suggestionId: suggestion.suggestion_id
          }, 'Created consolidated file');
        }

        // Apply migration steps to affected files
        if (suggestion.migration_steps && suggestion.migration_steps.length > 0) {
          try {
            const migrationResult = await this.migrationTransformer.applyMigrationSteps(
              suggestion,
              repositoryPath
            );

            // Add migrated files to modified list
            for (const file of migrationResult.filesModified) {
              if (!filesModified.includes(file)) {
                filesModified.push(file);
              }
            }

            logger.info({
              suggestionId: suggestion.suggestion_id,
              filesModified: migrationResult.filesModified.length,
              transformations: migrationResult.transformations.length,
              errors: migrationResult.errors.length,
              backupPath: migrationResult.backupPath
            }, 'Applied migration steps');

            // Log any transformation errors
            if (migrationResult.errors.length > 0) {
              logger.warn({
                errors: migrationResult.errors
              }, 'Some migration transformations failed');
            }

          } catch (migrationError) {
            // Log migration error but don't fail the entire suggestion
            // The consolidated file has still been created
            logger.error({
              error: migrationError,
              suggestionId: suggestion.suggestion_id
            }, 'Failed to apply migration steps');

            Sentry.captureException(migrationError, {
              tags: {
                component: 'pr-creator',
                operation: 'apply-migration-steps'
              },
              extra: {
                suggestionId: suggestion.suggestion_id,
                repositoryPath
              }
            });
          }
        }

      } catch (error) {
        logger.error({
          error,
          suggestionId: suggestion.suggestion_id
        }, 'Failed to apply suggestion');

        // Continue with other suggestions
      }
    }

    return filesModified;
  }

  /**
   * Run a Git command
   *
   * @param {string} cwd - Working directory
   * @param {Array} args - Git arguments
   * @returns {Promise<string>} Command output
   * @private
   */
  async _runGitCommand(cwd, args) {
    return new Promise((resolve, reject) => {
      const proc = spawn('git', args, { cwd });

      let stdout = '';
      let stderr = '';

      proc.stdout.on('data', (data) => {
        stdout += data.toString();
      });

      proc.stderr.on('data', (data) => {
        stderr += data.toString();
      });

      proc.on('close', (code) => {
        if (code === 0) {
          resolve(stdout.trim());
        } else {
          const error = /** @type {ProcessError} */ (new Error(`Git command failed: git ${args.join(' ')}`));
          error.code = code;
          error.stdout = stdout;
          error.stderr = stderr;
          reject(error);
        }
      });

      proc.on('error', (error) => {
        reject(error);
      });
    });
  }

  /**
   * Create PR using gh CLI
   *
   * @param {string} cwd - Working directory
   * @param {string} branch - Branch name
   * @param {string} title - PR title
   * @param {string} body - PR description
   * @returns {Promise<string>} PR URL
   * @private
   */
  async _createPR(cwd, branch, title, body) {
    return new Promise((resolve, reject) => {
      const proc = spawn('gh', [
        'pr',
        'create',
        '--title', title,
        '--body', body,
        '--base', this.baseBranch,
        '--head', branch
      ], { cwd });

      let stdout = '';
      let stderr = '';

      proc.stdout.on('data', (data) => {
        stdout += data.toString();
      });

      proc.stderr.on('data', (data) => {
        stderr += data.toString();
      });

      proc.on('close', (code) => {
        if (code === 0) {
          // gh CLI returns the PR URL in stdout
          const prUrl = stdout.trim();
          resolve(prUrl);
        } else {
          const error = /** @type {ProcessError} */ (new Error(`Failed to create PR: ${stderr}`));
          error.code = code;
          error.stdout = stdout;
          error.stderr = stderr;
          reject(error);
        }
      });

      proc.on('error', (error) => {
        reject(error);
      });
    });
  }

  /**
   * Batch suggestions for PR creation
   *
   * @param {Array} suggestions - All suggestions
   * @returns {Array<Array>} Batches of suggestions
   * @private
   */
  _batchSuggestions(suggestions) {
    const batches = [];

    for (let i = 0; i < suggestions.length; i += this.maxSuggestionsPerPR) {
      batches.push(suggestions.slice(i, i + this.maxSuggestionsPerPR));
    }

    return batches;
  }

  /**
   * Generate branch name
   *
   * @param {Array} suggestions - Suggestions in batch
   * @param {number} batchNumber - Batch number
   * @returns {string} Branch name
   * @private
   */
  _generateBranchName(suggestions, batchNumber) {
    const timestamp = Date.now();
    return `${this.branchPrefix}/batch-${batchNumber}-${timestamp}`;
  }

  /**
   * Generate commit message
   *
   * @param {Array} suggestions - Suggestions in batch
   * @param {Array} filesModified - Modified files
   * @returns {string} Commit message
   * @private
   */
  _generateCommitMessage(suggestions, filesModified) {
    const consolidationCount = suggestions.length;
    const fileCount = filesModified.length;

    const message = [
      `refactor: consolidate ${consolidationCount} duplicate code pattern${consolidationCount > 1 ? 's' : ''}`,
      '',
      `This commit consolidates ${consolidationCount} identified duplicate code pattern${consolidationCount > 1 ? 's' : ''} across ${fileCount} file${fileCount > 1 ? 's' : ''}.`,
      '',
      'Consolidations:',
      ...suggestions.map((s, i) => `${i + 1}. ${s.target_name || s.suggestion_id}: ${s.strategy_rationale.substring(0, 80)}...`),
      '',
      'Files created:',
      ...filesModified.map(f => `- ${f}`),
      '',
      'ü§ñ Generated with [Claude Code](https://claude.com/claude-code)',
      '',
      'Co-Authored-By: Claude <noreply@anthropic.com>'
    ].join('\n');

    return message;
  }

  /**
   * Generate PR title
   *
   * @param {Array} suggestions - Suggestions in batch
   * @param {number} batchNumber - Batch number
   * @returns {string} PR title
   * @private
   */
  _generatePRTitle(suggestions, batchNumber) {
    const consolidationCount = suggestions.length;
    return `refactor: consolidate ${consolidationCount} duplicate code pattern${consolidationCount > 1 ? 's' : ''} (batch ${batchNumber})`;
  }

  /**
   * Generate PR description
   *
   * @param {Array} suggestions - Suggestions in batch
   * @param {Array} filesModified - Modified files
   * @returns {string} PR description
   * @private
   */
  _generatePRDescription(suggestions, filesModified) {
    const consolidationCount = suggestions.length;

    const description = [
      '## Summary',
      '',
      `This PR consolidates ${consolidationCount} identified duplicate code pattern${consolidationCount > 1 ? 's' : ''} to improve code maintainability and reduce duplication.`,
      '',
      '## Consolidations',
      '',
      ...suggestions.map((s, i) => {
        return [
          `### ${i + 1}. ${s.target_name || s.suggestion_id}`,
          '',
          `**Strategy:** ${s.strategy}`,
          `**Impact Score:** ${s.impact_score}/100`,
          `**Complexity:** ${s.complexity}`,
          `**Risk:** ${s.migration_risk}`,
          '',
          `**Rationale:** ${s.strategy_rationale}`,
          '',
          `**Target Location:** \`${s.target_location}\``,
          '',
          s.migration_steps.length > 0 ? '**Migration Steps:**' : '',
          ...s.migration_steps.map(step => `${step.step_number}. ${step.description}`),
          ''
        ].join('\n');
      }),
      '## Files Modified',
      '',
      ...filesModified.map(f => `- \`${f}\``),
      '',
      '## Testing',
      '',
      '- [ ] Unit tests pass',
      '- [ ] Integration tests pass',
      '- [ ] Manual testing completed',
      '- [ ] Code review completed',
      '',
      '## Migration Notes',
      '',
      '‚ö†Ô∏è **Important:** This PR creates consolidated utility files. The actual migration of existing code to use these utilities should be done in follow-up PRs to keep changes manageable and reviewable.',
      '',
      suggestions.map((s, i) => {
        if (s.usage_example) {
          return [
            `### ${s.target_name || s.suggestion_id} Usage`,
            '',
            '```javascript',
            s.usage_example,
            '```',
            ''
          ].join('\n');
        }
        return '';
      }).filter(Boolean).join('\n'),
      '---',
      '',
      'ü§ñ Generated with [Claude Code](https://claude.com/claude-code)'
    ].join('\n');

    return description;
  }
}
</file>

<file path="pipeline-core/models/__init__.py">
"""
Pydantic Models for Code Consolidation System

This package contains the data models for structuring duplicate detection
and consolidation results using Pydantic v2.

Models:
- CodeBlock: Individual code pattern/block detected by ast-grep
- DuplicateGroup: Group of similar code blocks
- ConsolidationSuggestion: Recommendation for consolidating duplicates
- ScanReport: Complete scan results with metrics and recommendations
"""

from .code_block import (
    CodeBlock,
    SourceLocation,
    ASTNode,
    LanguageType,
    SemanticCategory,
)

from .duplicate_group import (
    DuplicateGroup,
    SimilarityMethod,
)

from .consolidation_suggestion import (
    ConsolidationSuggestion,
    ConsolidationStrategy,
    ImplementationComplexity,
    MigrationRisk,
    MigrationStep,
)

from .scan_report import (
    ScanReport,
    RepositoryInfo,
    ScanConfiguration,
    ScanMetrics,
)

__all__ = [
    # code_block
    'CodeBlock',
    'SourceLocation',
    'ASTNode',
    'LanguageType',
    'SemanticCategory',

    # duplicate_group
    'DuplicateGroup',
    'SimilarityMethod',

    # consolidation_suggestion
    'ConsolidationSuggestion',
    'ConsolidationStrategy',
    'ImplementationComplexity',
    'MigrationRisk',
    'MigrationStep',

    # scan_report
    'ScanReport',
    'RepositoryInfo',
    'ScanConfiguration',
    'ScanMetrics',
]

__version__ = '1.0.0'
</file>

<file path="pipeline-core/models/code_block.py">
"""
CodeBlock Model - Represents a detected code pattern or block

This model captures all metadata about a code block found by ast-grep,
including its location, AST structure, semantic categorization, and hash
for similarity comparison.
"""

from datetime import datetime
from enum import Enum
from typing import Optional, Dict, Any, List
from pydantic import BaseModel, Field, computed_field, field_validator
import hashlib
import json


class LanguageType(str, Enum):
    """Supported programming languages"""
    JAVASCRIPT = "javascript"
    TYPESCRIPT = "typescript"
    PYTHON = "python"
    JAVA = "java"
    GO = "go"
    RUST = "rust"
    C = "c"
    CPP = "cpp"
    CSHARP = "csharp"
    PHP = "php"
    RUBY = "ruby"
    # Add more as needed


class SemanticCategory(str, Enum):
    """Semantic categorization of code blocks"""
    UTILITY = "utility"
    HELPER = "helper"
    VALIDATOR = "validator"
    API_HANDLER = "api_handler"
    AUTH_CHECK = "auth_check"
    DATABASE_OPERATION = "database_operation"
    ERROR_HANDLER = "error_handler"
    LOGGER = "logger"
    CONFIG_ACCESS = "config_access"
    FILE_OPERATION = "file_operation"
    ASYNC_PATTERN = "async_pattern"
    UNKNOWN = "unknown"


class SourceLocation(BaseModel):
    """Precise location of code in source file"""
    file_path: str = Field(..., description="Absolute path to source file")
    line_start: int = Field(..., ge=1, description="Starting line number (1-indexed)")
    line_end: int = Field(..., ge=1, description="Ending line number (1-indexed)")
    column_start: Optional[int] = Field(None, ge=0, description="Starting column (0-indexed)")
    column_end: Optional[int] = Field(None, ge=0, description="Ending column (0-indexed)")

    @field_validator('line_end')
    @classmethod
    def validate_line_range(cls, v, info):
        """Ensure line_end >= line_start"""
        if 'line_start' in info.data and v < info.data['line_start']:
            raise ValueError('line_end must be >= line_start')
        return v

    def __str__(self) -> str:
        return f"{self.file_path}:{self.line_start}"


class ASTNode(BaseModel):
    """Representation of AST node structure"""
    node_type: str = Field(..., description="Type of AST node (e.g., 'CallExpression')")
    children: List['ASTNode'] = Field(default_factory=list, description="Child nodes")
    properties: Dict[str, Any] = Field(default_factory=dict, description="Node properties")

    model_config = {
        'frozen': False,  # Allow mutation for building AST
    }


# Enable self-referencing for AST tree structure
ASTNode.model_rebuild()


class CodeBlock(BaseModel):
    """
    Represents a code block detected by ast-grep

    This is the primary data structure for code pattern matching.
    Each CodeBlock contains:
    - Location information (file, lines)
    - AST structure representation
    - Semantic categorization
    - Hash for similarity comparison
    - Pattern metadata from ast-grep
    """

    # Core identification
    block_id: str = Field(..., description="Unique identifier for this code block")
    pattern_id: str = Field(..., description="ast-grep rule ID that matched this block")

    # Location
    location: SourceLocation = Field(..., description="Source code location")
    relative_path: str = Field(..., description="Repository-relative path")

    # Code content
    source_code: str = Field(..., description="Raw source code of the block")
    normalized_code: Optional[str] = Field(None, description="Normalized/formatted code")

    # AST representation
    ast_structure: Optional[ASTNode] = Field(None, description="AST node tree")
    ast_hash: Optional[str] = Field(None, description="Hash of AST structure")

    # Semantic information
    language: LanguageType = Field(..., description="Programming language")
    category: SemanticCategory = Field(..., description="Semantic category")
    tags: List[str] = Field(default_factory=list, description="Additional semantic tags")

    # Metadata from ast-grep
    match_context: Dict[str, Any] = Field(
        default_factory=dict,
        description="Context from ast-grep match (meta-variables, etc.)"
    )

    # Repository context
    repository_path: str = Field(..., description="Absolute path to repository root")
    repository_name: Optional[str] = Field(None, description="Repository name/identifier")
    git_commit: Optional[str] = Field(None, description="Git commit hash when scanned")

    # Timestamps
    detected_at: datetime = Field(
        default_factory=datetime.utcnow,
        description="When this block was detected"
    )

    # Complexity metrics
    line_count: int = Field(..., ge=1, description="Number of lines in block")
    complexity_score: Optional[float] = Field(
        None,
        ge=0,
        description="Cyclomatic complexity or similar metric"
    )

    model_config = {
        'json_schema_extra': {
            'example': {
                'block_id': 'cb_12345',
                'pattern_id': 'object-manipulation',
                'location': {
                    'file_path': '/path/to/repo/src/utils.js',
                    'line_start': 42,
                    'line_end': 44,
                },
                'relative_path': 'src/utils.js',
                'source_code': 'JSON.stringify(data, null, 2)',
                'language': 'javascript',
                'category': 'utility',
                'repository_path': '/path/to/repo',
                'line_count': 3,
            }
        }
    }

    @computed_field
    @property
    def content_hash(self) -> str:
        """
        Generate hash of source code for exact duplicate detection

        Uses SHA-256 hash of normalized source code
        """
        # Normalize: remove extra whitespace, consistent formatting
        normalized = ' '.join(self.source_code.split())
        return hashlib.sha256(normalized.encode()).hexdigest()[:16]

    @computed_field
    @property
    def structural_hash(self) -> str:
        """
        Generate hash of AST structure for structural similarity

        Uses AST hash if available, otherwise falls back to content hash
        """
        if self.ast_hash:
            return self.ast_hash
        return self.content_hash

    def to_dict_for_comparison(self) -> Dict[str, Any]:
        """
        Export minimal data for similarity comparison

        Returns dict suitable for clustering/grouping algorithms
        """
        return {
            'block_id': self.block_id,
            'pattern_id': self.pattern_id,
            'category': self.category.value,
            'language': self.language.value,
            'content_hash': self.content_hash,
            'structural_hash': self.structural_hash,
            'line_count': self.line_count,
            'tags': self.tags,
        }

    def __hash__(self) -> int:
        """Enable use in sets and as dict keys"""
        return hash(self.block_id)

    def __eq__(self, other: object) -> bool:
        """Compare blocks by ID"""
        if not isinstance(other, CodeBlock):
            return NotImplemented
        return self.block_id == other.block_id
</file>

<file path="pipeline-core/models/consolidation_suggestion.py">
"""
ConsolidationSuggestion Model - Recommendation for consolidating duplicates

Represents a specific recommendation for how to consolidate a DuplicateGroup,
including the strategy tier, implementation steps, and impact assessment.
"""

from datetime import datetime
from enum import Enum
from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field, computed_field, field_validator


class ConsolidationStrategy(str, Enum):
    """Consolidation tier/strategy"""
    LOCAL_UTIL = "local_util"           # Utility within single project
    SHARED_PACKAGE = "shared_package"   # Shared library across 2-3 projects
    MCP_SERVER = "mcp_server"           # MCP server for cross-language/tool
    AUTONOMOUS_AGENT = "autonomous_agent"  # Complex orchestration requiring AI
    NO_ACTION = "no_action"             # Not worth consolidating


class ImplementationComplexity(str, Enum):
    """Estimated implementation effort"""
    TRIVIAL = "trivial"      # < 1 hour
    SIMPLE = "simple"        # 1-4 hours
    MODERATE = "moderate"    # 1-2 days
    COMPLEX = "complex"      # 1+ weeks
    VERY_COMPLEX = "very_complex"  # Multiple weeks


class MigrationRisk(str, Enum):
    """Risk level for migration"""
    MINIMAL = "minimal"      # No breaking changes expected
    LOW = "low"             # Minor breaking changes possible
    MEDIUM = "medium"       # Some breaking changes likely
    HIGH = "high"           # Significant breaking changes
    CRITICAL = "critical"   # High risk of system breakage


class MigrationStep(BaseModel):
    """Single step in migration path"""
    step_number: int = Field(..., ge=1, description="Step order")
    description: str = Field(..., description="What to do in this step")
    code_example: Optional[str] = Field(None, description="Example code")
    automated: bool = Field(False, description="Can this step be automated?")
    estimated_time: Optional[str] = Field(None, description="Estimated time (e.g., '30min', '2h')")


class ConsolidationSuggestion(BaseModel):
    """
    Recommendation for consolidating a duplicate group

    Provides actionable guidance on how to consolidate a DuplicateGroup,
    including the recommended strategy tier, implementation steps,
    impact assessment, and migration guidance.
    """

    # Core identification
    suggestion_id: str = Field(..., description="Unique identifier for this suggestion")
    duplicate_group_id: str = Field(..., description="ID of DuplicateGroup being addressed")

    # Consolidation strategy
    strategy: ConsolidationStrategy = Field(..., description="Recommended consolidation tier")
    strategy_rationale: str = Field(..., description="Why this strategy was chosen")

    # Impact assessment
    impact_score: float = Field(
        ...,
        ge=0,
        le=100,
        description="Overall impact score (0-100, higher is more beneficial)"
    )

    # Implementation details
    complexity: ImplementationComplexity = Field(..., description="Implementation complexity")
    migration_risk: MigrationRisk = Field(..., description="Migration risk level")
    breaking_changes: bool = Field(..., description="Will this introduce breaking changes?")

    # Migration path
    migration_steps: List[MigrationStep] = Field(
        default_factory=list,
        description="Step-by-step migration guide"
    )

    # Target implementation
    target_location: Optional[str] = Field(
        None,
        description="Where to create the consolidated code (path or package name)"
    )
    target_name: Optional[str] = Field(
        None,
        description="Suggested name for consolidated function/class/package"
    )

    # Code examples
    proposed_implementation: Optional[str] = Field(
        None,
        description="Proposed consolidated code"
    )
    usage_example: Optional[str] = Field(
        None,
        description="Example of how to use the consolidated code"
    )

    # Metrics
    estimated_effort_hours: Optional[float] = Field(
        None,
        ge=0,
        description="Estimated implementation effort in hours"
    )
    loc_reduction: Optional[int] = Field(
        None,
        ge=0,
        description="Lines of code that will be eliminated"
    )
    affected_files_count: int = Field(..., ge=1, description="Number of files to modify")
    affected_repositories_count: int = Field(..., ge=1, description="Number of repos affected")

    # Dependencies
    dependencies: List[str] = Field(
        default_factory=list,
        description="Required dependencies for consolidation"
    )
    prerequisite_suggestions: List[str] = Field(
        default_factory=list,
        description="Other suggestions that should be completed first"
    )

    # Testing
    test_strategy: Optional[str] = Field(
        None,
        description="How to test the consolidated code"
    )
    rollback_plan: Optional[str] = Field(
        None,
        description="How to rollback if consolidation fails"
    )

    # Metadata
    confidence: float = Field(
        ...,
        ge=0.0,
        le=1.0,
        description="Confidence in this suggestion (0.0-1.0)"
    )
    automated_refactor_possible: bool = Field(
        False,
        description="Can this be automated with codemod?"
    )
    requires_human_review: bool = Field(
        True,
        description="Requires human review before implementation"
    )

    # Additional context
    benefits: List[str] = Field(
        default_factory=list,
        description="List of benefits from this consolidation"
    )
    drawbacks: List[str] = Field(
        default_factory=list,
        description="Potential drawbacks or concerns"
    )
    notes: Optional[str] = Field(None, description="Additional notes")

    # Timestamps
    created_at: datetime = Field(
        default_factory=datetime.utcnow,
        description="When this suggestion was created"
    )

    model_config = {
        'json_schema_extra': {
            'example': {
                'suggestion_id': 'cs_001',
                'duplicate_group_id': 'dg_001',
                'strategy': 'local_util',
                'strategy_rationale': 'Used within single project, simple utility function',
                'impact_score': 75.0,
                'complexity': 'trivial',
                'migration_risk': 'low',
                'breaking_changes': False,
                'target_location': 'src/utils/json.js',
                'target_name': 'writeJsonFile',
                'loc_reduction': 15,
                'affected_files_count': 5,
                'affected_repositories_count': 1,
            }
        }
    }

    @field_validator('impact_score')
    @classmethod
    def round_impact_score(cls, v):
        """Round impact score to 2 decimal places"""
        return round(v, 2)

    @computed_field
    @property
    def priority(self) -> str:
        """
        Determine priority level based on impact and complexity

        Returns: 'critical', 'high', 'medium', 'low'
        """
        # High impact, low complexity = highest priority
        # Low impact, high complexity = lowest priority

        if self.impact_score >= 75 and self.complexity in ['trivial', 'simple']:
            return 'critical'
        elif self.impact_score >= 50 and self.complexity in ['trivial', 'simple', 'moderate']:
            return 'high'
        elif self.impact_score >= 25:
            return 'medium'
        else:
            return 'low'

    @computed_field
    @property
    def roi_score(self) -> float:
        """
        Calculate Return on Investment score

        Higher ROI = better impact relative to effort
        """
        # Complexity to hours mapping
        complexity_hours = {
            ImplementationComplexity.TRIVIAL: 0.5,
            ImplementationComplexity.SIMPLE: 2.5,
            ImplementationComplexity.MODERATE: 12,
            ImplementationComplexity.COMPLEX: 40,
            ImplementationComplexity.VERY_COMPLEX: 80,
        }

        effort = self.estimated_effort_hours or complexity_hours.get(self.complexity, 10)

        # ROI = impact / effort (normalized to 0-100)
        if effort == 0:
            return 100.0

        roi = (self.impact_score / effort) * 10
        return min(round(roi, 2), 100.0)

    @computed_field
    @property
    def is_quick_win(self) -> bool:
        """
        Check if this is a 'quick win' consolidation

        Quick win = high impact, low effort, low risk
        """
        return (
            self.impact_score >= 60 and
            self.complexity in [ImplementationComplexity.TRIVIAL, ImplementationComplexity.SIMPLE] and
            self.migration_risk in [MigrationRisk.MINIMAL, MigrationRisk.LOW]
        )

    def add_migration_step(
        self,
        description: str,
        code_example: Optional[str] = None,
        automated: bool = False,
        estimated_time: Optional[str] = None
    ) -> None:
        """Add a step to the migration path"""
        step_number = len(self.migration_steps) + 1
        step = MigrationStep(
            step_number=step_number,
            description=description,
            code_example=code_example,
            automated=automated,
            estimated_time=estimated_time
        )
        self.migration_steps.append(step)

    def add_benefit(self, benefit: str) -> None:
        """Add a benefit to the list"""
        if benefit not in self.benefits:
            self.benefits.append(benefit)

    def add_drawback(self, drawback: str) -> None:
        """Add a drawback to the list"""
        if drawback not in self.drawbacks:
            self.drawbacks.append(drawback)

    def to_markdown_summary(self) -> str:
        """Generate a markdown summary of this suggestion"""
        return f"""## {self.target_name or 'Consolidation Suggestion'}

**Strategy:** {self.strategy.value.replace('_', ' ').title()}
**Priority:** {self.priority.upper()}
**Impact Score:** {self.impact_score}/100
**ROI Score:** {self.roi_score}/100

### Rationale
{self.strategy_rationale}

### Metrics
- **Complexity:** {self.complexity.value}
- **Risk:** {self.migration_risk.value}
- **LOC Reduction:** {self.loc_reduction or 'Unknown'}
- **Files Affected:** {self.affected_files_count}
- **Breaking Changes:** {'Yes' if self.breaking_changes else 'No'}

### Benefits
{chr(10).join(f'- {b}' for b in self.benefits) if self.benefits else 'None specified'}

### Migration Steps
{chr(10).join(f'{i+1}. {step.description}' for i, step in enumerate(self.migration_steps)) if self.migration_steps else 'Not specified'}
"""

    def __hash__(self) -> int:
        """Enable use in sets and as dict keys"""
        return hash(self.suggestion_id)

    def __eq__(self, other: object) -> bool:
        """Compare suggestions by ID"""
        if not isinstance(other, ConsolidationSuggestion):
            return NotImplemented
        return self.suggestion_id == other.suggestion_id
</file>

<file path="pipeline-core/models/duplicate_group.py">
"""
DuplicateGroup Model - Represents a group of similar code blocks

Groups together CodeBlocks that are similar enough to be considered
duplicates or candidates for consolidation.
"""

from datetime import datetime
from enum import Enum
from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field, computed_field, field_validator


class SimilarityMethod(str, Enum):
    """Method used to determine similarity"""
    EXACT_MATCH = "exact_match"  # Identical code
    STRUCTURAL = "structural"    # Same AST structure
    SEMANTIC = "semantic"        # Similar logic/behavior
    HYBRID = "hybrid"            # Combination of methods


class DuplicateGroup(BaseModel):
    """
    Group of similar code blocks detected as duplicates

    Represents a cluster of CodeBlocks that share structural or semantic
    similarity and are candidates for consolidation into a single
    abstraction.
    """

    # Core identification
    group_id: str = Field(..., description="Unique identifier for this duplicate group")
    pattern_id: str = Field(..., description="ast-grep pattern that matched these blocks")

    # Member blocks (references to CodeBlock IDs)
    member_block_ids: List[str] = Field(
        ...,
        min_length=2,
        description="IDs of CodeBlocks in this group"
    )

    # Similarity metrics
    similarity_score: float = Field(
        ...,
        ge=0.0,
        le=1.0,
        description="Similarity score (0.0 = completely different, 1.0 = identical)"
    )
    similarity_method: SimilarityMethod = Field(
        ...,
        description="Method used to calculate similarity"
    )

    # Consolidation target
    canonical_block_id: Optional[str] = Field(
        None,
        description="ID of the 'best' representative block for this group"
    )

    # Analysis metadata
    category: str = Field(..., description="Semantic category of duplicates")
    language: str = Field(..., description="Programming language")

    # Statistics
    occurrence_count: int = Field(..., ge=2, description="Number of occurrences")
    total_lines: int = Field(..., ge=1, description="Total lines of duplicated code")
    affected_files: List[str] = Field(
        default_factory=list,
        description="List of files containing duplicates"
    )
    affected_repositories: List[str] = Field(
        default_factory=list,
        description="List of repositories containing duplicates"
    )

    # Consolidation analysis
    consolidation_complexity: Optional[str] = Field(
        None,
        description="Estimated complexity: 'trivial', 'moderate', 'complex'"
    )
    breaking_changes_risk: Optional[str] = Field(
        None,
        description="Risk level: 'low', 'medium', 'high'"
    )

    # Timestamps
    created_at: datetime = Field(
        default_factory=datetime.utcnow,
        description="When this group was created"
    )
    updated_at: datetime = Field(
        default_factory=datetime.utcnow,
        description="Last update timestamp"
    )

    # Additional context
    notes: Optional[str] = Field(None, description="Analysis notes")
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description="Additional metadata"
    )

    model_config = {
        'json_schema_extra': {
            'example': {
                'group_id': 'dg_001',
                'pattern_id': 'object-manipulation',
                'member_block_ids': ['cb_12345', 'cb_12346', 'cb_12347'],
                'similarity_score': 0.95,
                'similarity_method': 'structural',
                'category': 'utility',
                'language': 'javascript',
                'occurrence_count': 3,
                'total_lines': 9,
                'affected_files': ['src/utils.js', 'src/helpers.js'],
            }
        }
    }

    @field_validator('member_block_ids')
    @classmethod
    def validate_min_members(cls, v):
        """Ensure at least 2 members (a duplicate must have multiple instances)"""
        if len(v) < 2:
            raise ValueError('A duplicate group must have at least 2 members')
        return v

    @field_validator('canonical_block_id')
    @classmethod
    def validate_canonical_in_members(cls, v, info):
        """Ensure canonical block ID is one of the members"""
        if v is not None and 'member_block_ids' in info.data:
            if v not in info.data['member_block_ids']:
                raise ValueError('canonical_block_id must be one of member_block_ids')
        return v

    @computed_field
    @property
    def deduplication_potential(self) -> int:
        """
        Calculate lines of code that could be removed

        If consolidated, we keep one instance and remove the rest
        """
        if self.occurrence_count <= 1:
            return 0
        avg_lines_per_instance = self.total_lines / self.occurrence_count
        return int((self.occurrence_count - 1) * avg_lines_per_instance)

    @computed_field
    @property
    def impact_score(self) -> float:
        """
        Calculate consolidation impact score (0-100)

        Higher score = higher priority for consolidation
        Factors:
        - Number of occurrences
        - Similarity score
        - Lines of code affected
        """
        # Normalize occurrence count (cap at 20 for scoring)
        occurrence_factor = min(self.occurrence_count / 20.0, 1.0)

        # Similarity weight
        similarity_factor = self.similarity_score

        # Lines of code factor (cap at 100 lines for scoring)
        loc_factor = min(self.total_lines / 100.0, 1.0)

        # Weighted average
        score = (
            occurrence_factor * 40 +  # Frequency is most important
            similarity_factor * 35 +   # Similarity is very important
            loc_factor * 25            # Size is moderately important
        )

        return round(score, 2)

    @computed_field
    @property
    def is_cross_repository(self) -> bool:
        """Check if duplicates span multiple repositories"""
        return len(self.affected_repositories) > 1

    @computed_field
    @property
    def priority_level(self) -> str:
        """
        Determine consolidation priority level

        Returns: 'critical', 'high', 'medium', 'low'
        """
        if self.impact_score >= 75:
            return 'critical'
        elif self.impact_score >= 50:
            return 'high'
        elif self.impact_score >= 25:
            return 'medium'
        else:
            return 'low'

    def add_member(self, block_id: str) -> None:
        """Add a new member to this group"""
        if block_id not in self.member_block_ids:
            self.member_block_ids.append(block_id)
            self.occurrence_count = len(self.member_block_ids)
            self.updated_at = datetime.utcnow()

    def remove_member(self, block_id: str) -> None:
        """Remove a member from this group"""
        if block_id in self.member_block_ids:
            self.member_block_ids.remove(block_id)
            self.occurrence_count = len(self.member_block_ids)
            self.updated_at = datetime.utcnow()

            # Reset canonical if it was the removed block
            if self.canonical_block_id == block_id:
                self.canonical_block_id = None

    def set_canonical(self, block_id: str) -> None:
        """Set the canonical (representative) block for this group"""
        if block_id not in self.member_block_ids:
            raise ValueError(f"Block {block_id} is not a member of this group")
        self.canonical_block_id = block_id
        self.updated_at = datetime.utcnow()

    def __hash__(self) -> int:
        """Enable use in sets and as dict keys"""
        return hash(self.group_id)

    def __eq__(self, other: object) -> bool:
        """Compare groups by ID"""
        if not isinstance(other, DuplicateGroup):
            return NotImplemented
        return self.group_id == other.group_id
</file>

<file path="pipeline-core/models/scan_report.py">
"""
ScanReport Model - Complete duplicate detection scan results

Top-level model representing the complete results of a duplicate detection
scan across one or more repositories, including all code blocks, duplicate
groups, consolidation suggestions, and summary metrics.
"""

from datetime import datetime
from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field, computed_field

# Import our other models
# Note: In actual implementation, these would be proper imports
# from .code_block import CodeBlock
# from .duplicate_group import DuplicateGroup
# from .consolidation_suggestion import ConsolidationSuggestion


class RepositoryInfo(BaseModel):
    """Information about a scanned repository"""
    repository_path: str = Field(..., description="Absolute path to repository")
    repository_name: str = Field(..., description="Repository name/identifier")
    git_remote: Optional[str] = Field(None, description="Git remote URL")
    git_branch: Optional[str] = Field(None, description="Current branch")
    git_commit: Optional[str] = Field(None, description="Current commit hash")
    total_files: int = Field(..., ge=0, description="Total files scanned")
    total_lines: int = Field(..., ge=0, description="Total lines of code scanned")
    languages: List[str] = Field(default_factory=list, description="Languages detected")


class ScanConfiguration(BaseModel):
    """Configuration used for the scan"""
    rules_used: List[str] = Field(default_factory=list, description="ast-grep rules applied")
    excluded_paths: List[str] = Field(default_factory=list, description="Paths excluded from scan")
    min_similarity_threshold: float = Field(0.8, ge=0.0, le=1.0, description="Minimum similarity for grouping")
    min_duplicate_size: int = Field(3, ge=1, description="Minimum lines for duplicate detection")


class ScanMetrics(BaseModel):
    """Statistical metrics from the scan"""

    # Code blocks
    total_code_blocks: int = Field(..., ge=0, description="Total code blocks detected")
    code_blocks_by_category: Dict[str, int] = Field(
        default_factory=dict,
        description="Count of blocks by semantic category"
    )
    code_blocks_by_language: Dict[str, int] = Field(
        default_factory=dict,
        description="Count of blocks by programming language"
    )

    # Duplicate groups
    total_duplicate_groups: int = Field(..., ge=0, description="Total duplicate groups found")
    exact_duplicates: int = Field(..., ge=0, description="Groups with 100% similarity")
    structural_duplicates: int = Field(..., ge=0, description="Groups with structural similarity")
    semantic_duplicates: int = Field(..., ge=0, description="Groups with semantic similarity")

    # Impact metrics
    total_duplicated_lines: int = Field(..., ge=0, description="Total lines in duplicate groups")
    potential_loc_reduction: int = Field(..., ge=0, description="Potential lines that could be removed")
    duplication_percentage: float = Field(..., ge=0.0, le=100.0, description="Percentage of code that's duplicated")

    # Consolidation suggestions
    total_suggestions: int = Field(..., ge=0, description="Total consolidation suggestions")
    quick_wins: int = Field(..., ge=0, description="Number of quick win suggestions")
    high_priority_suggestions: int = Field(..., ge=0, description="High priority suggestions")

    # Cross-repository analysis (if multiple repos scanned)
    cross_repository_duplicates: int = Field(0, ge=0, description="Duplicates spanning multiple repos")


class ScanReport(BaseModel):
    """
    Complete duplicate detection scan report

    This is the top-level model that contains all results from a
    duplicate detection scan, including code blocks, duplicate groups,
    consolidation suggestions, and summary metrics.
    """

    # Core identification
    report_id: str = Field(..., description="Unique identifier for this report")
    scan_name: Optional[str] = Field(None, description="Descriptive name for this scan")

    # Scan metadata
    scanned_at: datetime = Field(default_factory=datetime.utcnow, description="Scan timestamp")
    scan_duration_seconds: Optional[float] = Field(None, ge=0, description="How long the scan took")
    scanner_version: str = Field("1.0.0", description="Version of duplicate detection pipeline")

    # Configuration
    configuration: ScanConfiguration = Field(..., description="Scan configuration")

    # Repository information
    repositories: List[RepositoryInfo] = Field(..., description="Repositories scanned")

    # Results (IDs only for performance, actual objects stored separately)
    code_block_ids: List[str] = Field(default_factory=list, description="IDs of detected code blocks")
    duplicate_group_ids: List[str] = Field(default_factory=list, description="IDs of duplicate groups")
    suggestion_ids: List[str] = Field(default_factory=list, description="IDs of consolidation suggestions")

    # Summary metrics
    metrics: ScanMetrics = Field(..., description="Statistical metrics")

    # Analysis summary
    executive_summary: Optional[str] = Field(None, description="High-level summary of findings")
    recommendations: List[str] = Field(default_factory=list, description="Top-level recommendations")
    warnings: List[str] = Field(default_factory=list, description="Warnings or issues encountered")

    # Output paths
    output_directory: str = Field(..., description="Directory where detailed results are saved")
    report_files: Dict[str, str] = Field(
        default_factory=dict,
        description="Paths to generated report files (HTML, JSON, etc.)"
    )

    # Additional metadata
    tags: List[str] = Field(default_factory=list, description="Tags for categorization")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Additional metadata")

    model_config = {
        'json_schema_extra': {
            'example': {
                'report_id': 'scan_20250111_001',
                'scan_name': 'Sidequest Repository Scan',
                'repositories': [{
                    'repository_name': 'sidequest',
                    'repository_path': '/Users/user/code/jobs/sidequest',
                    'total_files': 15,
                    'total_lines': 2500,
                    'languages': ['javascript', 'typescript']
                }],
                'metrics': {
                    'total_code_blocks': 150,
                    'total_duplicate_groups': 12,
                    'total_duplicated_lines': 300,
                    'potential_loc_reduction': 250,
                    'duplication_percentage': 12.0,
                    'total_suggestions': 12,
                    'quick_wins': 5
                }
            }
        }
    }

    @computed_field
    @property
    def is_multi_repository(self) -> bool:
        """Check if this scan covers multiple repositories"""
        return len(self.repositories) > 1

    @computed_field
    @property
    def total_scanned_files(self) -> int:
        """Total files across all repositories"""
        return sum(repo.total_files for repo in self.repositories)

    @computed_field
    @property
    def total_scanned_lines(self) -> int:
        """Total lines of code across all repositories"""
        return sum(repo.total_lines for repo in self.repositories)

    @computed_field
    @property
    def duplication_severity(self) -> str:
        """
        Assess overall duplication severity

        Returns: 'minimal', 'low', 'moderate', 'high', 'critical'
        """
        dup_pct = self.metrics.duplication_percentage

        if dup_pct < 5:
            return 'minimal'
        elif dup_pct < 10:
            return 'low'
        elif dup_pct < 20:
            return 'moderate'
        elif dup_pct < 40:
            return 'high'
        else:
            return 'critical'

    @computed_field
    @property
    def consolidation_opportunity_score(self) -> float:
        """
        Calculate overall consolidation opportunity (0-100)

        Higher score = more benefit from consolidation
        """
        # Factors: duplication %, number of quick wins, potential LOC reduction
        dup_factor = min(self.metrics.duplication_percentage / 40 * 100, 100)  # Cap at 40%

        quick_win_factor = min(self.metrics.quick_wins / 10 * 100, 100)  # Cap at 10 quick wins

        if self.total_scanned_lines > 0:
            loc_reduction_factor = (
                self.metrics.potential_loc_reduction / self.total_scanned_lines * 100
            )
        else:
            loc_reduction_factor = 0

        # Weighted average
        score = (
            dup_factor * 0.35 +
            quick_win_factor * 0.40 +
            loc_reduction_factor * 0.25
        )

        return round(min(score, 100), 2)

    def add_repository(self, repo_info: RepositoryInfo) -> None:
        """Add a repository to this scan"""
        self.repositories.append(repo_info)

    def add_code_block_id(self, block_id: str) -> None:
        """Register a code block in this scan"""
        if block_id not in self.code_block_ids:
            self.code_block_ids.append(block_id)

    def add_duplicate_group_id(self, group_id: str) -> None:
        """Register a duplicate group in this scan"""
        if group_id not in self.duplicate_group_ids:
            self.duplicate_group_ids.append(group_id)

    def add_suggestion_id(self, suggestion_id: str) -> None:
        """Register a consolidation suggestion in this scan"""
        if suggestion_id not in self.suggestion_ids:
            self.suggestion_ids.append(suggestion_id)

    def generate_executive_summary(self) -> str:
        """
        Auto-generate executive summary based on metrics

        Returns a human-readable summary of the scan results
        """
        repos_text = f"{len(self.repositories)} repository" if len(self.repositories) == 1 else f"{len(self.repositories)} repositories"

        summary = f"""
# Duplicate Detection Scan Report

Scanned {repos_text} containing {self.total_scanned_files:,} files and {self.total_scanned_lines:,} lines of code.

## Key Findings

- **Duplicate Groups Found:** {self.metrics.total_duplicate_groups}
- **Duplicated Code:** {self.metrics.total_duplicated_lines:,} lines ({self.metrics.duplication_percentage:.1f}% of total)
- **Duplication Severity:** {self.duplication_severity.upper()}
- **Potential Reduction:** {self.metrics.potential_loc_reduction:,} lines could be eliminated

## Consolidation Opportunities

- **Total Suggestions:** {self.metrics.total_suggestions}
- **Quick Wins:** {self.metrics.quick_wins} high-impact, low-effort consolidations
- **High Priority:** {self.metrics.high_priority_suggestions} high-priority suggestions
- **Opportunity Score:** {self.consolidation_opportunity_score}/100

## Recommendation

{'üöÄ Immediate action recommended - many quick wins available!' if self.metrics.quick_wins >= 5 else
 '‚ö†Ô∏è Moderate duplication detected - consider prioritizing high-impact consolidations' if self.metrics.duplication_percentage >= 10 else
 '‚úÖ Low duplication - focus on preventing new duplicates'}
"""
        return summary.strip()

    def to_summary_dict(self) -> Dict[str, Any]:
        """Export summary data for dashboards/APIs"""
        return {
            'report_id': self.report_id,
            'scanned_at': self.scanned_at.isoformat(),
            'repositories_count': len(self.repositories),
            'total_files': self.total_scanned_files,
            'total_lines': self.total_scanned_lines,
            'duplication_percentage': self.metrics.duplication_percentage,
            'duplication_severity': self.duplication_severity,
            'duplicate_groups': self.metrics.total_duplicate_groups,
            'potential_loc_reduction': self.metrics.potential_loc_reduction,
            'suggestions_total': self.metrics.total_suggestions,
            'quick_wins': self.metrics.quick_wins,
            'opportunity_score': self.consolidation_opportunity_score,
        }

    def __hash__(self) -> int:
        """Enable use in sets and as dict keys"""
        return hash(self.report_id)

    def __eq__(self, other: object) -> bool:
        """Compare reports by ID"""
        if not isinstance(other, ScanReport):
            return NotImplemented
        return self.report_id == other.report_id
</file>

<file path="pipeline-core/models/test_models.py">
"""
Test script for Pydantic models

Run with: python test_models.py

Tests basic functionality of all models without requiring
the actual pydantic package (design validation).
"""

def test_model_structure():
    """Test that model files have correct structure"""
    import importlib.util
    import sys
    from pathlib import Path

    models_dir = Path(__file__).parent

    models = [
        'code_block',
        'duplicate_group',
        'consolidation_suggestion',
        'scan_report',
    ]

    print("Testing model file structure...")
    print("-" * 60)

    for model_name in models:
        model_path = models_dir / f"{model_name}.py"

        if not model_path.exists():
            print(f"‚ùå {model_name}.py - File not found")
            continue

        # Check file can be read
        try:
            with open(model_path, 'r') as f:
                content = f.read()

            # Basic validation checks
            checks = {
                'Has docstring': '"""' in content,
                'Imports BaseModel': 'from pydantic import BaseModel' in content,
                'Imports Field': 'Field' in content,
                'Defines class': 'class ' in content and '(BaseModel)' in content,
                'Has model_config': 'model_config' in content,
            }

            all_passed = all(checks.values())
            symbol = "‚úÖ" if all_passed else "‚ö†Ô∏è"

            print(f"{symbol} {model_name}.py")

            for check, passed in checks.items():
                status = "‚úì" if passed else "‚úó"
                print(f"  {status} {check}")

        except Exception as e:
            print(f"‚ùå {model_name}.py - Error: {e}")

    print("-" * 60)


def test_sample_data():
    """Test with sample data structure (without pydantic)"""
    print("\nTesting sample data structures...")
    print("-" * 60)

    # Sample CodeBlock data
    code_block_data = {
        'block_id': 'cb_test_001',
        'pattern_id': 'object-manipulation',
        'location': {
            'file_path': '/test/file.js',
            'line_start': 10,
            'line_end': 12,
        },
        'relative_path': 'src/test/file.js',
        'source_code': 'JSON.stringify(data, null, 2)',
        'language': 'javascript',
        'category': 'utility',
        'repository_path': '/test',
        'line_count': 3,
    }

    # Sample DuplicateGroup data
    duplicate_group_data = {
        'group_id': 'dg_test_001',
        'pattern_id': 'object-manipulation',
        'member_block_ids': ['cb_test_001', 'cb_test_002', 'cb_test_003'],
        'similarity_score': 0.95,
        'similarity_method': 'structural',
        'category': 'utility',
        'language': 'javascript',
        'occurrence_count': 3,
        'total_lines': 9,
        'affected_files': ['file1.js', 'file2.js'],
        'affected_repositories': ['/test'],
    }

    # Sample ConsolidationSuggestion data
    consolidation_data = {
        'suggestion_id': 'cs_test_001',
        'duplicate_group_id': 'dg_test_001',
        'strategy': 'local_util',
        'strategy_rationale': 'Simple utility function used within single project',
        'impact_score': 75.0,
        'complexity': 'trivial',
        'migration_risk': 'low',
        'breaking_changes': False,
        'affected_files_count': 3,
        'affected_repositories_count': 1,
        'confidence': 0.9,
    }

    # Sample ScanReport data
    scan_report_data = {
        'report_id': 'scan_test_001',
        'scan_name': 'Test Scan',
        'configuration': {
            'rules_used': ['object-manipulation', 'array-operations'],
            'min_similarity_threshold': 0.8,
            'min_duplicate_size': 3,
        },
        'repositories': [{
            'repository_path': '/test',
            'repository_name': 'test-repo',
            'total_files': 10,
            'total_lines': 1000,
        }],
        'metrics': {
            'total_code_blocks': 50,
            'total_duplicate_groups': 5,
            'exact_duplicates': 2,
            'structural_duplicates': 3,
            'semantic_duplicates': 0,
            'total_duplicated_lines': 100,
            'potential_loc_reduction': 75,
            'duplication_percentage': 10.0,
            'total_suggestions': 5,
            'quick_wins': 2,
            'high_priority_suggestions': 3,
        },
        'output_directory': '/test/output',
    }

    samples = [
        ('CodeBlock', code_block_data),
        ('DuplicateGroup', duplicate_group_data),
        ('ConsolidationSuggestion', consolidation_data),
        ('ScanReport', scan_report_data),
    ]

    for name, data in samples:
        required_fields = len(data)
        print(f"‚úÖ {name} - {required_fields} fields defined")
        print(f"   Sample keys: {', '.join(list(data.keys())[:5])}")

    print("-" * 60)


def test_computed_fields_logic():
    """Test the logic of computed fields (without pydantic)"""
    print("\nTesting computed field logic...")
    print("-" * 60)

    # Test DuplicateGroup impact_score calculation
    def calculate_impact_score(occurrence_count, similarity_score, total_lines):
        occurrence_factor = min(occurrence_count / 20.0, 1.0)
        similarity_factor = similarity_score
        loc_factor = min(total_lines / 100.0, 1.0)
        score = (
            occurrence_factor * 40 +
            similarity_factor * 35 +
            loc_factor * 25
        )
        return round(score, 2)

    test_cases = [
        (3, 0.95, 9, "Low occurrence, high similarity, small size"),
        (10, 0.9, 50, "Moderate occurrence, high similarity, moderate size"),
        (20, 0.8, 100, "High occurrence, good similarity, large size"),
    ]

    print("Impact Score Calculation Tests:")
    for occurrence, similarity, lines, desc in test_cases:
        score = calculate_impact_score(occurrence, similarity, lines)
        print(f"  {desc}")
        print(f"    Occurrence: {occurrence}, Similarity: {similarity}, Lines: {lines}")
        print(f"    ‚Üí Impact Score: {score}/100")

    # Test ROI score calculation
    def calculate_roi_score(impact_score, estimated_hours):
        if estimated_hours == 0:
            return 100.0
        roi = (impact_score / estimated_hours) * 10
        return min(round(roi, 2), 100.0)

    print("\nROI Score Calculation Tests:")
    roi_tests = [
        (75.0, 0.5, "High impact, trivial effort"),
        (50.0, 2.0, "Medium impact, simple effort"),
        (25.0, 40.0, "Low impact, complex effort"),
    ]

    for impact, hours, desc in roi_tests:
        roi = calculate_roi_score(impact, hours)
        print(f"  {desc}")
        print(f"    Impact: {impact}, Effort: {hours}h")
        print(f"    ‚Üí ROI Score: {roi}/100")

    print("-" * 60)


def main():
    """Run all tests"""
    print("\n" + "=" * 60)
    print("Pydantic Models Structure Test")
    print("=" * 60)

    test_model_structure()
    test_sample_data()
    test_computed_fields_logic()

    print("\n" + "=" * 60)
    print("All structure tests completed!")
    print("=" * 60)
    print("\nNote: To fully test with Pydantic validation:")
    print("  1. Install pydantic: pip install pydantic")
    print("  2. Run: python -m pytest test_models.py")
    print("=" * 60 + "\n")


if __name__ == '__main__':
    main()
</file>

<file path="pipeline-core/reports/html-report-generator.js">
/**
 * HTML Report Generator
 *
 * Generates interactive HTML dashboards for duplicate detection results
 */

import fs from 'fs/promises';
import path from 'path';

export class HTMLReportGenerator {
  /**
   * Generate HTML report from scan results
   *
   * @param {Object} scanResult - Scan results (intra-project or inter-project)
   * @param {Object} options - Report options
   * @returns {string} - HTML content
   */
  static generateReport(scanResult, options = {}) {
    const title = options.title || 'Duplicate Detection Report';
    const isInterProject = scanResult.scan_type === 'inter-project';

    const html = `<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>${this._escapeHtml(title)}</title>
    <style>
        ${this._getStyles()}
    </style>
</head>
<body>
    <div class="container">
        ${this._generateHeader(scanResult, title, isInterProject)}
        ${this._generateMetrics(scanResult, isInterProject)}
        ${this._generateSummaryCharts(scanResult, isInterProject)}
        ${isInterProject ? this._generateCrossRepoSection(scanResult) : ''}
        ${this._generateDuplicateGroups(scanResult, isInterProject)}
        ${this._generateSuggestions(scanResult, isInterProject)}
        ${this._generateFooter(scanResult)}
    </div>
    <script>
        ${this._getScripts()}
    </script>
</body>
</html>`;

    return html;
  }

  /**
   * Save HTML report to file
   */
  static async saveReport(scanResult, outputPath, options = {}) {
    const html = this.generateReport(scanResult, options);
    await fs.mkdir(path.dirname(outputPath), { recursive: true });
    await fs.writeFile(outputPath, html);
    return outputPath;
  }

  /**
   * Generate header section
   * @private
   */
  static _generateHeader(scanResult, title, isInterProject) {
    const scanDate = new Date(scanResult.scan_metadata?.scanned_at || Date.now());
    const duration = scanResult.scan_metadata?.duration_seconds || 0;

    return `
    <header>
        <h1>üîç ${this._escapeHtml(title)}</h1>
        <div class="header-meta">
            <span class="meta-item">
                <strong>Scan Type:</strong> ${isInterProject ? 'Inter-Project' : 'Intra-Project'}
            </span>
            <span class="meta-item">
                <strong>Date:</strong> ${scanDate.toLocaleString()}
            </span>
            <span class="meta-item">
                <strong>Duration:</strong> ${duration.toFixed(2)}s
            </span>
        </div>
    </header>`;
  }

  /**
   * Generate metrics cards
   * @private
   */
  static _generateMetrics(scanResult, isInterProject) {
    const metrics = scanResult.metrics || {};

    if (isInterProject) {
      return `
    <section class="metrics">
        <h2>üìä Scan Metrics</h2>
        <div class="metrics-grid">
            <div class="metric-card">
                <div class="metric-value">${metrics.total_repositories_scanned || 0}</div>
                <div class="metric-label">Repositories Scanned</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">${metrics.total_code_blocks || 0}</div>
                <div class="metric-label">Code Blocks</div>
            </div>
            <div class="metric-card highlight">
                <div class="metric-value">${metrics.total_cross_repository_groups || 0}</div>
                <div class="metric-label">Cross-Repo Duplicates</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">${metrics.cross_repository_duplicated_lines || 0}</div>
                <div class="metric-label">Duplicated Lines</div>
            </div>
            <div class="metric-card success">
                <div class="metric-value">${metrics.shared_package_candidates || 0}</div>
                <div class="metric-label">Shared Package Candidates</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">${metrics.mcp_server_candidates || 0}</div>
                <div class="metric-label">MCP Server Candidates</div>
            </div>
        </div>
    </section>`;
    }

    return `
    <section class="metrics">
        <h2>üìä Scan Metrics</h2>
        <div class="metrics-grid">
            <div class="metric-card">
                <div class="metric-value">${metrics.total_code_blocks || 0}</div>
                <div class="metric-label">Code Blocks Detected</div>
            </div>
            <div class="metric-card highlight">
                <div class="metric-value">${metrics.total_duplicate_groups || 0}</div>
                <div class="metric-label">Duplicate Groups</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">${metrics.exact_duplicates || 0}</div>
                <div class="metric-label">Exact Duplicates</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">${metrics.total_duplicated_lines || 0}</div>
                <div class="metric-label">Duplicated Lines</div>
            </div>
            <div class="metric-card success">
                <div class="metric-value">${metrics.potential_loc_reduction || 0}</div>
                <div class="metric-label">Potential LOC Reduction</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">${metrics.quick_wins || 0}</div>
                <div class="metric-label">Quick Wins</div>
            </div>
        </div>
    </section>`;
  }

  /**
   * Generate summary charts
   * @private
   */
  static _generateSummaryCharts(scanResult, isInterProject) {
    const suggestions = isInterProject
      ? (scanResult.cross_repository_suggestions || [])
      : (scanResult.suggestions || []);

    // Count by strategy
    const strategyCounts = {};
    suggestions.forEach(s => {
      strategyCounts[s.strategy] = (strategyCounts[s.strategy] || 0) + 1;
    });

    // Count by complexity
    const complexityCounts = {};
    suggestions.forEach(s => {
      complexityCounts[s.complexity] = (complexityCounts[s.complexity] || 0) + 1;
    });

    return `
    <section class="charts">
        <h2>üìà Distribution</h2>
        <div class="charts-grid">
            <div class="chart-card">
                <h3>By Strategy</h3>
                <div class="chart-bars">
                    ${Object.entries(strategyCounts).map(([strategy, count]) => `
                    <div class="chart-bar-row">
                        <span class="chart-label">${strategy.replace('_', ' ')}</span>
                        <div class="chart-bar-container">
                            <div class="chart-bar strategy-${strategy}" style="width: ${(count / suggestions.length * 100).toFixed(1)}%"></div>
                        </div>
                        <span class="chart-count">${count}</span>
                    </div>
                    `).join('')}
                </div>
            </div>
            <div class="chart-card">
                <h3>By Complexity</h3>
                <div class="chart-bars">
                    ${Object.entries(complexityCounts).map(([complexity, count]) => `
                    <div class="chart-bar-row">
                        <span class="chart-label">${complexity}</span>
                        <div class="chart-bar-container">
                            <div class="chart-bar complexity-${complexity}" style="width: ${(count / suggestions.length * 100).toFixed(1)}%"></div>
                        </div>
                        <span class="chart-count">${count}</span>
                    </div>
                    `).join('')}
                </div>
            </div>
        </div>
    </section>`;
  }

  /**
   * Generate cross-repository section
   * @private
   */
  static _generateCrossRepoSection(scanResult) {
    const repos = scanResult.scanned_repositories || [];

    return `
    <section class="cross-repo">
        <h2>üîó Scanned Repositories</h2>
        <div class="repo-grid">
            ${repos.map(repo => `
            <div class="repo-card ${repo.error ? 'error' : ''}">
                <div class="repo-name">${this._escapeHtml(repo.name)}</div>
                ${repo.error ? `
                <div class="repo-error">‚ùå ${this._escapeHtml(repo.error)}</div>
                ` : `
                <div class="repo-stats">
                    <span>${repo.code_blocks} blocks</span>
                    <span>${repo.duplicate_groups} groups</span>
                </div>
                `}
            </div>
            `).join('')}
        </div>
    </section>`;
  }

  /**
   * Generate duplicate groups section
   * @private
   */
  static _generateDuplicateGroups(scanResult, isInterProject) {
    const groups = isInterProject
      ? (scanResult.cross_repository_duplicates || [])
      : (scanResult.duplicate_groups || []);

    const topGroups = groups
      .sort((a, b) => b.impact_score - a.impact_score)
      .slice(0, 10);

    if (topGroups.length === 0) {
      return `
    <section class="duplicates">
        <h2>üéØ Duplicate Groups</h2>
        <p class="empty-state">No duplicate groups detected.</p>
    </section>`;
    }

    return `
    <section class="duplicates">
        <h2>üéØ Top Duplicate Groups</h2>
        <div class="duplicates-list">
            ${topGroups.map((group, index) => `
            <div class="duplicate-card">
                <div class="duplicate-header">
                    <div class="duplicate-rank">#${index + 1}</div>
                    <div class="duplicate-title">
                        <strong>${this._escapeHtml(group.group_id)}</strong>
                        <span class="duplicate-pattern">${this._escapeHtml(group.pattern_id)}</span>
                    </div>
                    <div class="duplicate-impact ${this._getImpactClass(group.impact_score)}">
                        ${group.impact_score.toFixed(0)}% impact
                    </div>
                </div>
                <div class="duplicate-stats">
                    <span class="stat-badge">
                        üì¶ ${group.occurrence_count} occurrences
                    </span>
                    ${isInterProject ? `
                    <span class="stat-badge">
                        üîó ${group.repository_count} repositories
                    </span>
                    ` : ''}
                    <span class="stat-badge">
                        üìÑ ${group.affected_files?.length || 0} files
                    </span>
                    <span class="stat-badge">
                        üìè ${group.total_lines} lines
                    </span>
                </div>
                <div class="duplicate-files">
                    <strong>Affected files:</strong>
                    <ul>
                        ${(group.affected_files || []).slice(0, 5).map(file => `
                        <li><code>${this._escapeHtml(file)}</code></li>
                        `).join('')}
                        ${group.affected_files?.length > 5 ? `<li><em>... and ${group.affected_files.length - 5} more</em></li>` : ''}
                    </ul>
                </div>
            </div>
            `).join('')}
        </div>
    </section>`;
  }

  /**
   * Generate suggestions section
   * @private
   */
  static _generateSuggestions(scanResult, isInterProject) {
    const suggestions = isInterProject
      ? (scanResult.cross_repository_suggestions || [])
      : (scanResult.suggestions || []);

    const topSuggestions = suggestions
      .sort((a, b) => b.roi_score - a.roi_score)
      .slice(0, 10);

    if (topSuggestions.length === 0) {
      return `
    <section class="suggestions">
        <h2>üí° Consolidation Suggestions</h2>
        <p class="empty-state">No suggestions generated.</p>
    </section>`;
    }

    return `
    <section class="suggestions">
        <h2>üí° Top Consolidation Suggestions</h2>
        <div class="suggestions-list">
            ${topSuggestions.map((suggestion, index) => `
            <div class="suggestion-card ${suggestion.breaking_changes ? 'breaking' : ''}">
                <div class="suggestion-header">
                    <div class="suggestion-rank">#${index + 1}</div>
                    <div class="suggestion-title">
                        <strong>${this._escapeHtml(suggestion.suggestion_id)}</strong>
                        <span class="suggestion-strategy strategy-${suggestion.strategy}">
                            ${this._escapeHtml(suggestion.strategy.replace('_', ' '))}
                        </span>
                    </div>
                    <div class="suggestion-roi ${this._getROIClass(suggestion.roi_score)}">
                        ROI: ${suggestion.roi_score.toFixed(0)}%
                    </div>
                </div>
                <div class="suggestion-body">
                    <p class="suggestion-rationale">${this._escapeHtml(suggestion.strategy_rationale)}</p>
                    ${suggestion.target_location ? `
                    <p class="suggestion-target">
                        <strong>Target:</strong> <code>${this._escapeHtml(suggestion.target_location)}</code>
                    </p>
                    ` : ''}
                    <div class="suggestion-metrics">
                        <span class="metric-badge">
                            Impact: ${suggestion.impact_score.toFixed(0)}%
                        </span>
                        <span class="metric-badge complexity-${suggestion.complexity}">
                            ${suggestion.complexity} complexity
                        </span>
                        <span class="metric-badge risk-${suggestion.migration_risk}">
                            ${suggestion.migration_risk} risk
                        </span>
                        ${suggestion.estimated_effort_hours ? `
                        <span class="metric-badge">
                            ~${suggestion.estimated_effort_hours}h effort
                        </span>
                        ` : ''}
                    </div>
                    ${suggestion.breaking_changes ? `
                    <div class="warning-box">
                        ‚ö†Ô∏è <strong>Breaking Change:</strong> This consolidation may require API changes
                    </div>
                    ` : ''}
                </div>
            </div>
            `).join('')}
        </div>
    </section>`;
  }

  /**
   * Generate footer
   * @private
   */
  static _generateFooter(scanResult) {
    return `
    <footer>
        <p>Generated by Duplicate Detection Pipeline | ${new Date().toLocaleString()}</p>
    </footer>`;
  }

  /**
   * Get CSS styles
   * @private
   */
  static _getStyles() {
    return `
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f7fa;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            border-radius: 10px;
            margin-bottom: 30px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        header h1 { margin-bottom: 15px; font-size: 2em; }
        .header-meta {
            display: flex;
            gap: 20px;
            flex-wrap: wrap;
        }
        .meta-item {
            background: rgba(255,255,255,0.2);
            padding: 8px 15px;
            border-radius: 5px;
        }
        section {
            background: white;
            padding: 25px;
            margin-bottom: 20px;
            border-radius: 10px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.05);
        }
        section h2 {
            margin-bottom: 20px;
            color: #2d3748;
            border-bottom: 2px solid #e2e8f0;
            padding-bottom: 10px;
        }
        .metrics-grid, .charts-grid, .repo-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
        }
        .metric-card {
            padding: 20px;
            text-align: center;
            border-radius: 8px;
            background: #f7fafc;
            border: 2px solid #e2e8f0;
        }
        .metric-card.highlight {
            background: #ebf8ff;
            border-color: #4299e1;
        }
        .metric-card.success {
            background: #f0fff4;
            border-color: #48bb78;
        }
        .metric-value {
            font-size: 2.5em;
            font-weight: bold;
            color: #2d3748;
        }
        .metric-label {
            color: #718096;
            margin-top: 5px;
            font-size: 0.9em;
        }
        .chart-card {
            padding: 20px;
            background: #f7fafc;
            border-radius: 8px;
        }
        .chart-card h3 {
            margin-bottom: 15px;
            color: #4a5568;
        }
        .chart-bars { display: flex; flex-direction: column; gap: 10px; }
        .chart-bar-row {
            display: flex;
            align-items: center;
            gap: 10px;
        }
        .chart-label {
            min-width: 120px;
            font-size: 0.9em;
            color: #4a5568;
            text-transform: capitalize;
        }
        .chart-bar-container {
            flex: 1;
            background: #e2e8f0;
            height: 24px;
            border-radius: 4px;
            overflow: hidden;
        }
        .chart-bar {
            height: 100%;
            background: #4299e1;
            transition: width 0.3s ease;
        }
        .chart-bar.strategy-local_util { background: #48bb78; }
        .chart-bar.strategy-shared_package { background: #4299e1; }
        .chart-bar.strategy-mcp_server { background: #9f7aea; }
        .chart-bar.strategy-autonomous_agent { background: #ed8936; }
        .chart-bar.complexity-trivial { background: #48bb78; }
        .chart-bar.complexity-simple { background: #4299e1; }
        .chart-bar.complexity-moderate { background: #ed8936; }
        .chart-bar.complexity-complex { background: #f56565; }
        .chart-count {
            min-width: 30px;
            text-align: right;
            font-weight: bold;
            color: #4a5568;
        }
        .repo-card {
            padding: 15px;
            background: #f7fafc;
            border-radius: 8px;
            border: 2px solid #e2e8f0;
        }
        .repo-card.error {
            background: #fff5f5;
            border-color: #fc8181;
        }
        .repo-name {
            font-weight: bold;
            color: #2d3748;
            margin-bottom: 8px;
        }
        .repo-stats {
            display: flex;
            gap: 15px;
            font-size: 0.9em;
            color: #718096;
        }
        .repo-error {
            color: #e53e3e;
            font-size: 0.9em;
        }
        .duplicates-list, .suggestions-list {
            display: flex;
            flex-direction: column;
            gap: 15px;
        }
        .duplicate-card, .suggestion-card {
            border: 2px solid #e2e8f0;
            border-radius: 8px;
            padding: 20px;
            background: #fafafa;
        }
        .suggestion-card.breaking {
            border-color: #fc8181;
        }
        .duplicate-header, .suggestion-header {
            display: flex;
            align-items: center;
            gap: 15px;
            margin-bottom: 15px;
        }
        .duplicate-rank, .suggestion-rank {
            font-size: 1.5em;
            font-weight: bold;
            color: #a0aec0;
            min-width: 40px;
        }
        .duplicate-title, .suggestion-title {
            flex: 1;
        }
        .duplicate-pattern, .suggestion-strategy {
            display: inline-block;
            background: #edf2f7;
            padding: 4px 10px;
            border-radius: 4px;
            font-size: 0.85em;
            margin-left: 10px;
        }
        .suggestion-strategy.strategy-local_util { background: #c6f6d5; color: #22543d; }
        .suggestion-strategy.strategy-shared_package { background: #bee3f8; color: #2c5282; }
        .suggestion-strategy.strategy-mcp_server { background: #e9d8fd; color: #44337a; }
        .suggestion-strategy.strategy-autonomous_agent { background: #feebc8; color: #7c2d12; }
        .duplicate-impact, .suggestion-roi {
            padding: 6px 12px;
            border-radius: 6px;
            font-weight: bold;
            font-size: 0.9em;
        }
        .impact-high, .roi-high { background: #c6f6d5; color: #22543d; }
        .impact-medium, .roi-medium { background: #feebc8; color: #7c2d12; }
        .impact-low, .roi-low { background: #fed7d7; color: #742a2a; }
        .duplicate-stats, .suggestion-metrics {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
            margin-bottom: 15px;
        }
        .stat-badge, .metric-badge {
            background: #edf2f7;
            padding: 6px 12px;
            border-radius: 4px;
            font-size: 0.85em;
            color: #4a5568;
        }
        .metric-badge.complexity-trivial { background: #c6f6d5; color: #22543d; }
        .metric-badge.complexity-simple { background: #bee3f8; color: #2c5282; }
        .metric-badge.complexity-moderate { background: #feebc8; color: #7c2d12; }
        .metric-badge.complexity-complex { background: #fed7d7; color: #742a2a; }
        .metric-badge.risk-minimal { background: #c6f6d5; color: #22543d; }
        .metric-badge.risk-low { background: #bee3f8; color: #2c5282; }
        .metric-badge.risk-medium { background: #feebc8; color: #7c2d12; }
        .metric-badge.risk-high { background: #fed7d7; color: #742a2a; }
        .duplicate-files ul {
            margin-top: 10px;
            padding-left: 20px;
        }
        .duplicate-files li {
            margin: 5px 0;
            font-size: 0.9em;
        }
        .suggestion-rationale {
            margin-bottom: 10px;
            color: #4a5568;
        }
        .suggestion-target {
            margin-bottom: 10px;
            font-size: 0.9em;
        }
        .warning-box {
            background: #fff5f5;
            border: 1px solid #fc8181;
            border-radius: 6px;
            padding: 12px;
            margin-top: 12px;
            color: #742a2a;
        }
        code {
            background: #edf2f7;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }
        .empty-state {
            text-align: center;
            color: #a0aec0;
            padding: 40px;
            font-style: italic;
        }
        footer {
            text-align: center;
            color: #a0aec0;
            padding: 30px;
            font-size: 0.9em;
        }
    `;
  }

  /**
   * Get JavaScript
   * @private
   */
  static _getScripts() {
    return `
        // Add interactivity here if needed
        console.log('Duplicate Detection Report loaded');
    `;
  }

  /**
   * Get impact class
   * @private
   */
  static _getImpactClass(score) {
    if (score >= 70) return 'impact-high';
    if (score >= 40) return 'impact-medium';
    return 'impact-low';
  }

  /**
   * Get ROI class
   * @private
   */
  static _getROIClass(score) {
    if (score >= 80) return 'roi-high';
    if (score >= 50) return 'roi-medium';
    return 'roi-low';
  }

  /**
   * Escape HTML
   * @private
   */
  static _escapeHtml(text) {
    if (!text) return '';
    return String(text)
      .replace(/&/g, '&amp;')
      .replace(/</g, '&lt;')
      .replace(/>/g, '&gt;')
      .replace(/"/g, '&quot;')
      .replace(/'/g, '&#039;');
  }
}
</file>

<file path="pipeline-core/reports/json-report-generator.js">
/**
 * JSON Report Generator
 *
 * Generates structured JSON exports of duplicate detection scan results.
 * Suitable for programmatic consumption, API integration, and data analysis.
 */

import fs from 'fs/promises';
import path from 'path';

/**
 * Generate JSON reports from scan results
 */
export class JSONReportGenerator {
  /**
   * Generate a complete JSON report
   *
   * @param {Object} scanResult - Scan result data
   * @param {Object} options - Generation options
   * @returns {Object} - JSON report object
   */
  static generateReport(scanResult, options = {}) {
    const includeSourceCode = options.includeSourceCode !== false;
    const includeCodeBlocks = options.includeCodeBlocks !== false;
    const maxDuplicates = options.maxDuplicates || null; // null = all
    const maxSuggestions = options.maxSuggestions || null; // null = all
    const isInterProject = scanResult.scan_type === 'inter-project';

    const report = {
      report_version: '1.0.0',
      generated_at: new Date().toISOString(),
      scan_type: isInterProject ? 'inter-project' : 'intra-project',
      metadata: this._generateMetadata(scanResult, isInterProject),
      metrics: scanResult.metrics || {},
      summary: this._generateSummary(scanResult, isInterProject),
      duplicate_groups: this._formatDuplicateGroups(
        scanResult,
        isInterProject,
        maxDuplicates,
        includeSourceCode
      ),
      suggestions: this._formatSuggestions(
        scanResult,
        isInterProject,
        maxSuggestions
      )
    };

    // Include repository information for inter-project scans
    if (isInterProject) {
      report.scanned_repositories = scanResult.scanned_repositories || [];
      report.repository_scans = this._formatRepositoryScans(
        scanResult,
        includeCodeBlocks,
        includeSourceCode
      );
    } else {
      report.repository_info = scanResult.repository_info || {};
    }

    // Optionally include code blocks
    if (includeCodeBlocks) {
      report.code_blocks = this._formatCodeBlocks(
        scanResult,
        includeSourceCode
      );
    }

    return report;
  }

  /**
   * Generate a concise summary (minimal data)
   *
   * @param {Object} scanResult - Scan result data
   * @returns {Object} - Concise JSON summary
   */
  static generateSummary(scanResult) {
    const isInterProject = scanResult.scan_type === 'inter-project';
    const metrics = scanResult.metrics || {};

    return {
      scan_type: isInterProject ? 'inter-project' : 'intra-project',
      generated_at: new Date().toISOString(),
      summary: this._generateSummary(scanResult, isInterProject),
      metrics: {
        ...(isInterProject ? {
          repositories_scanned: metrics.total_repositories_scanned || 0,
          total_code_blocks: metrics.total_code_blocks || 0,
          cross_repo_duplicates: metrics.total_cross_repository_groups || 0,
          cross_repo_duplicated_lines: metrics.cross_repository_duplicated_lines || 0,
          total_suggestions: metrics.total_suggestions || 0,
          shared_package_candidates: metrics.shared_package_candidates || 0,
          mcp_server_candidates: metrics.mcp_server_candidates || 0
        } : {
          code_blocks: metrics.total_code_blocks || 0,
          duplicate_groups: metrics.total_duplicate_groups || 0,
          duplicated_lines: metrics.total_duplicated_lines || 0,
          potential_loc_reduction: metrics.potential_loc_reduction || 0,
          total_suggestions: metrics.total_suggestions || 0,
          quick_wins: metrics.quick_wins || 0
        })
      }
    };
  }

  /**
   * Save JSON report to file
   *
   * @param {Object} scanResult - Scan result data
   * @param {string} outputPath - Output file path
   * @param {Object} options - Generation options
   */
  static async saveReport(scanResult, outputPath, options = {}) {
    const report = this.generateReport(scanResult, options);
    await fs.mkdir(path.dirname(outputPath), { recursive: true });
    const prettyPrint = options.prettyPrint !== false;
    const json = prettyPrint ? JSON.stringify(report, null, 2) : JSON.stringify(report);
    await fs.writeFile(outputPath, json);
    return outputPath;
  }

  /**
   * Save concise summary to file
   *
   * @param {Object} scanResult - Scan result data
   * @param {string} outputPath - Output file path
   */
  static async saveSummary(scanResult, outputPath) {
    const summary = this.generateSummary(scanResult);
    await fs.mkdir(path.dirname(outputPath), { recursive: true });
    await fs.writeFile(outputPath, JSON.stringify(summary, null, 2));
    return outputPath;
  }

  // Private helper methods

  /**
   * Generate metadata section
   *
   * @private
   */
  static _generateMetadata(scanResult, isInterProject) {
    const metadata = scanResult.scan_metadata || {};
    const repoInfo = scanResult.repository_info || {};

    return {
      ...(isInterProject ? {
        repository_count: metadata.repository_count || 0,
        repositories: (scanResult.scanned_repositories || []).map(r => r.name)
      } : {
        repository_name: repoInfo.name || 'Unknown',
        repository_path: repoInfo.path || 'Unknown',
        total_files: repoInfo.total_files || 0,
        languages: repoInfo.languages || []
      }),
      scanned_at: metadata.scanned_at || new Date().toISOString(),
      duration_seconds: metadata.duration_seconds || 0,
      scanner_version: '2.0.0'
    };
  }

  /**
   * Generate summary section
   *
   * @private
   */
  static _generateSummary(scanResult, isInterProject) {
    const metrics = scanResult.metrics || {};
    const duplicateGroups = isInterProject
      ? (scanResult.cross_repository_duplicates || [])
      : (scanResult.duplicate_groups || []);
    const suggestions = isInterProject
      ? (scanResult.cross_repository_suggestions || [])
      : (scanResult.suggestions || []);

    const summary = {
      total_duplicate_groups: duplicateGroups.length,
      total_suggestions: suggestions.length
    };

    if (isInterProject) {
      summary.repositories_scanned = metrics.total_repositories_scanned || 0;
      summary.cross_repo_duplicates = duplicateGroups.length;
      summary.top_impact_score = duplicateGroups.length > 0
        ? Math.max(...duplicateGroups.map(g => g.impact_score))
        : 0;
    } else {
      summary.code_blocks_detected = metrics.total_code_blocks || 0;
      summary.exact_duplicates = metrics.exact_duplicates || 0;
      summary.structural_duplicates = metrics.structural_duplicates || 0;
      summary.duplicated_lines = metrics.total_duplicated_lines || 0;
      summary.potential_loc_reduction = metrics.potential_loc_reduction || 0;
      summary.quick_wins = metrics.quick_wins || 0;
    }

    // Strategy distribution
    summary.strategy_distribution = this._calculateStrategyDistribution(suggestions);

    // Complexity distribution
    summary.complexity_distribution = this._calculateComplexityDistribution(suggestions);

    // ROI statistics
    if (suggestions.length > 0) {
      const roiScores = suggestions.map(s => s.roi_score);
      summary.roi_statistics = {
        average: roiScores.reduce((a, b) => a + b, 0) / roiScores.length,
        min: Math.min(...roiScores),
        max: Math.max(...roiScores)
      };
    }

    return summary;
  }

  /**
   * Format duplicate groups
   *
   * @private
   */
  static _formatDuplicateGroups(scanResult, isInterProject, maxGroups, includeSourceCode) {
    const groups = isInterProject
      ? (scanResult.cross_repository_duplicates || [])
      : (scanResult.duplicate_groups || []);

    let formattedGroups = groups
      .sort((a, b) => b.impact_score - a.impact_score)
      .map(group => ({
        group_id: group.group_id,
        pattern_id: group.pattern_id,
        category: group.category,
        language: group.language,
        occurrence_count: group.occurrence_count,
        total_lines: group.total_lines,
        impact_score: group.impact_score,
        similarity_score: group.similarity_score,
        similarity_method: group.similarity_method,
        affected_files: group.affected_files || [],
        ...(isInterProject ? {
          repository_count: group.repository_count,
          affected_repositories: group.affected_repositories || []
        } : {}),
        ...(includeSourceCode && group.member_blocks && group.member_blocks.length > 0 ? {
          example_code: group.member_blocks[0].source_code
        } : {})
      }));

    if (maxGroups && maxGroups > 0) {
      formattedGroups = formattedGroups.slice(0, maxGroups);
    }

    return formattedGroups;
  }

  /**
   * Format suggestions
   *
   * @private
   */
  static _formatSuggestions(scanResult, isInterProject, maxSuggestions) {
    const suggestions = isInterProject
      ? (scanResult.cross_repository_suggestions || [])
      : (scanResult.suggestions || []);

    let formattedSuggestions = suggestions
      .sort((a, b) => b.roi_score - a.roi_score)
      .map(suggestion => ({
        suggestion_id: suggestion.suggestion_id,
        duplicate_group_id: suggestion.duplicate_group_id,
        strategy: suggestion.strategy,
        target_location: suggestion.target_location,
        impact_score: suggestion.impact_score,
        roi_score: suggestion.roi_score,
        complexity: suggestion.complexity,
        migration_risk: suggestion.migration_risk,
        estimated_effort_hours: suggestion.estimated_effort_hours,
        breaking_changes: suggestion.breaking_changes || false,
        confidence: suggestion.confidence,
        strategy_rationale: suggestion.strategy_rationale,
        ...(isInterProject ? {
          affected_repositories: suggestion.affected_repositories || [],
          affected_repositories_count: suggestion.affected_repositories_count || 0
        } : {
          affected_files_count: suggestion.affected_files_count || 0
        }),
        migration_steps: (suggestion.migration_steps || []).map(step => ({
          step_number: step.step_number,
          description: step.description,
          automated: step.automated,
          estimated_time: step.estimated_time
        }))
      }));

    if (maxSuggestions && maxSuggestions > 0) {
      formattedSuggestions = formattedSuggestions.slice(0, maxSuggestions);
    }

    return formattedSuggestions;
  }

  /**
   * Format repository scans (inter-project only)
   *
   * @private
   */
  static _formatRepositoryScans(scanResult, includeCodeBlocks, includeSourceCode) {
    const repositoryScans = scanResult.repository_scans || [];

    return repositoryScans.map(repoScan => ({
      repository_name: repoScan.repository_name,
      repository_path: repoScan.repository_path,
      code_blocks_count: repoScan.code_blocks?.length || 0,
      duplicate_groups_count: repoScan.duplicate_groups?.length || 0,
      suggestions_count: repoScan.suggestions?.length || 0,
      error: repoScan.error || null,
      ...(includeCodeBlocks && !includeSourceCode ? {
        code_blocks: (repoScan.code_blocks || []).map(b => ({
          block_id: b.block_id,
          pattern_id: b.pattern_id,
          category: b.category,
          line_count: b.line_count,
          file_path: b.location?.file_path || b.relative_path
        }))
      } : {}),
      ...(includeCodeBlocks && includeSourceCode ? {
        code_blocks: repoScan.code_blocks || []
      } : {})
    }));
  }

  /**
   * Format code blocks
   *
   * @private
   */
  static _formatCodeBlocks(scanResult, includeSourceCode) {
    const codeBlocks = scanResult.code_blocks || [];

    return codeBlocks.map(block => ({
      block_id: block.block_id,
      pattern_id: block.pattern_id,
      category: block.category,
      language: block.language,
      line_count: block.line_count,
      location: {
        file_path: block.location?.file_path || block.relative_path,
        line_start: block.location?.line_start,
        line_end: block.location?.line_end
      },
      content_hash: block.content_hash,
      semantic_tags: block.semantic_tags || [],
      ...(includeSourceCode ? {
        source_code: block.source_code
      } : {})
    }));
  }

  /**
   * Calculate strategy distribution
   *
   * @private
   */
  static _calculateStrategyDistribution(suggestions) {
    const distribution = {
      local_util: 0,
      shared_package: 0,
      mcp_server: 0,
      autonomous_agent: 0
    };

    suggestions.forEach(s => {
      if (distribution.hasOwnProperty(s.strategy)) {
        distribution[s.strategy]++;
      }
    });

    return distribution;
  }

  /**
   * Calculate complexity distribution
   *
   * @private
   */
  static _calculateComplexityDistribution(suggestions) {
    const distribution = {
      trivial: 0,
      simple: 0,
      moderate: 0,
      complex: 0
    };

    suggestions.forEach(s => {
      if (distribution.hasOwnProperty(s.complexity)) {
        distribution[s.complexity]++;
      }
    });

    return distribution;
  }
}
</file>

<file path="pipeline-core/reports/markdown-report-generator.js">
/**
 * Markdown Report Generator
 *
 * Generates concise Markdown summaries of duplicate detection scan results.
 * Suitable for terminal viewing, GitHub README integration, and quick reviews.
 */

import fs from 'fs/promises';
import path from 'path';

/**
 * Generate Markdown summary reports from scan results
 */
export class MarkdownReportGenerator {
  /**
   * Generate a complete Markdown report
   *
   * @param {Object} scanResult - Scan result data
   * @param {Object} options - Generation options
   * @returns {string} - Markdown report content
   */
  static generateReport(scanResult, options = {}) {
    const includeDetails = options.includeDetails !== false;
    const maxDuplicates = options.maxDuplicates || 10;
    const maxSuggestions = options.maxSuggestions || 10;
    const isInterProject = scanResult.scan_type === 'inter-project';

    let markdown = '';

    // Header
    markdown += this._generateHeader(scanResult, isInterProject);
    markdown += '\n\n';

    // Metrics
    markdown += this._generateMetrics(scanResult, isInterProject);
    markdown += '\n\n';

    // Repository Information (inter-project only)
    if (isInterProject) {
      markdown += this._generateRepositoryInfo(scanResult);
      markdown += '\n\n';
    }

    // Top Duplicate Groups
    markdown += this._generateDuplicateGroups(scanResult, isInterProject, maxDuplicates, includeDetails);
    markdown += '\n\n';

    // Top Suggestions
    markdown += this._generateSuggestions(scanResult, isInterProject, maxSuggestions, includeDetails);
    markdown += '\n\n';

    // Footer
    markdown += this._generateFooter(scanResult);

    return markdown;
  }

  /**
   * Generate a concise summary (for quick overview)
   *
   * @param {Object} scanResult - Scan result data
   * @returns {string} - Concise Markdown summary
   */
  static generateSummary(scanResult) {
    const isInterProject = scanResult.scan_type === 'inter-project';
    const metrics = scanResult.metrics || {};

    let summary = '# Duplicate Detection Summary\n\n';

    if (isInterProject) {
      summary += `**Scanned:** ${metrics.total_repositories_scanned || 0} repositories\n`;
      summary += `**Code Blocks:** ${metrics.total_code_blocks || 0}\n`;
      summary += `**Cross-Repo Duplicates:** ${metrics.total_cross_repository_groups || 0}\n`;
      summary += `**Suggestions:** ${metrics.total_suggestions || 0}\n`;
      summary += `**Shared Package Candidates:** ${metrics.shared_package_candidates || 0}\n`;
      summary += `**MCP Server Candidates:** ${metrics.mcp_server_candidates || 0}\n`;
    } else {
      summary += `**Code Blocks:** ${metrics.total_code_blocks || 0}\n`;
      summary += `**Duplicate Groups:** ${metrics.total_duplicate_groups || 0}\n`;
      summary += `**Duplicated Lines:** ${metrics.total_duplicated_lines || 0}\n`;
      summary += `**Suggestions:** ${metrics.total_suggestions || 0}\n`;
      summary += `**Quick Wins:** ${metrics.quick_wins || 0}\n`;
    }

    return summary;
  }

  /**
   * Save Markdown report to file
   *
   * @param {Object} scanResult - Scan result data
   * @param {string} outputPath - Output file path
   * @param {Object} options - Generation options
   */
  static async saveReport(scanResult, outputPath, options = {}) {
    const markdown = this.generateReport(scanResult, options);
    await fs.mkdir(path.dirname(outputPath), { recursive: true });
    await fs.writeFile(outputPath, markdown);
    return outputPath;
  }

  /**
   * Save concise summary to file
   *
   * @param {Object} scanResult - Scan result data
   * @param {string} outputPath - Output file path
   */
  static async saveSummary(scanResult, outputPath) {
    const markdown = this.generateSummary(scanResult);
    await fs.mkdir(path.dirname(outputPath), { recursive: true });
    await fs.writeFile(outputPath, markdown);
    return outputPath;
  }

  // Private helper methods

  /**
   * Generate report header
   *
   * @private
   */
  static _generateHeader(scanResult, isInterProject) {
    const metadata = scanResult.scan_metadata || {};
    const repoInfo = scanResult.repository_info || {};
    const scanType = isInterProject ? 'Inter-Project' : 'Intra-Project';

    let header = `# ${scanType} Duplicate Detection Report\n\n`;

    if (isInterProject) {
      header += `**Repositories:** ${metadata.repository_count || 0}\n`;
    } else {
      header += `**Repository:** ${repoInfo.name || 'Unknown'}\n`;
      header += `**Path:** \`${repoInfo.path || 'Unknown'}\`\n`;
    }

    header += `**Scanned:** ${new Date(metadata.scanned_at || Date.now()).toLocaleString()}\n`;
    header += `**Duration:** ${metadata.duration_seconds?.toFixed(2) || 0}s\n`;

    return header;
  }

  /**
   * Generate metrics table
   *
   * @private
   */
  static _generateMetrics(scanResult, isInterProject) {
    const metrics = scanResult.metrics || {};

    let markdown = '## Metrics\n\n';

    if (isInterProject) {
      markdown += '| Metric | Value |\n';
      markdown += '|--------|-------|\n';
      markdown += `| Repositories Scanned | ${metrics.total_repositories_scanned || 0} |\n`;
      markdown += `| Total Code Blocks | ${metrics.total_code_blocks || 0} |\n`;
      markdown += `| Intra-Project Groups | ${metrics.total_intra_project_groups || 0} |\n`;
      markdown += `| Cross-Repo Groups | ${metrics.total_cross_repository_groups || 0} |\n`;
      markdown += `| Cross-Repo Occurrences | ${metrics.cross_repository_occurrences || 0} |\n`;
      markdown += `| Cross-Repo Duplicated Lines | ${metrics.cross_repository_duplicated_lines || 0} |\n`;
      markdown += `| Total Suggestions | ${metrics.total_suggestions || 0} |\n`;
      markdown += `| Shared Package Candidates | ${metrics.shared_package_candidates || 0} |\n`;
      markdown += `| MCP Server Candidates | ${metrics.mcp_server_candidates || 0} |\n`;
      markdown += `| Avg Repos per Duplicate | ${metrics.average_repositories_per_duplicate || 0} |\n`;
    } else {
      markdown += '| Metric | Value |\n';
      markdown += '|--------|-------|\n';
      markdown += `| Total Code Blocks | ${metrics.total_code_blocks || 0} |\n`;
      markdown += `| Duplicate Groups | ${metrics.total_duplicate_groups || 0} |\n`;
      markdown += `| Exact Duplicates | ${metrics.exact_duplicates || 0} |\n`;
      markdown += `| Total Duplicated Lines | ${metrics.total_duplicated_lines || 0} |\n`;
      markdown += `| Potential LOC Reduction | ${metrics.potential_loc_reduction || 0} |\n`;
      markdown += `| Duplication Percentage | ${metrics.duplication_percentage?.toFixed(2) || 0}% |\n`;
      markdown += `| Total Suggestions | ${metrics.total_suggestions || 0} |\n`;
      markdown += `| Quick Wins | ${metrics.quick_wins || 0} |\n`;
      markdown += `| High Priority Suggestions | ${metrics.high_priority_suggestions || 0} |\n`;
    }

    return markdown;
  }

  /**
   * Generate repository information (inter-project only)
   *
   * @private
   */
  static _generateRepositoryInfo(scanResult) {
    const repos = scanResult.scanned_repositories || [];

    let markdown = '## Scanned Repositories\n\n';
    markdown += '| Repository | Code Blocks | Duplicate Groups | Status |\n';
    markdown += '|------------|-------------|------------------|--------|\n';

    for (const repo of repos) {
      const status = repo.error ? `‚ùå ${repo.error}` : '‚úÖ Success';
      markdown += `| ${repo.name} | ${repo.code_blocks || 0} | ${repo.duplicate_groups || 0} | ${status} |\n`;
    }

    return markdown;
  }

  /**
   * Generate duplicate groups section
   *
   * @private
   */
  static _generateDuplicateGroups(scanResult, isInterProject, maxGroups, includeDetails) {
    const groups = isInterProject
      ? (scanResult.cross_repository_duplicates || [])
      : (scanResult.duplicate_groups || []);

    let markdown = isInterProject
      ? '## Top Cross-Repository Duplicates\n\n'
      : '## Top Duplicate Groups\n\n';

    if (groups.length === 0) {
      markdown += '*No duplicates detected.*\n';
      return markdown;
    }

    // Sort by impact score descending
    const topGroups = [...groups]
      .sort((a, b) => b.impact_score - a.impact_score)
      .slice(0, maxGroups);

    for (const group of topGroups) {
      markdown += `### ${group.group_id}\n\n`;
      markdown += `- **Pattern:** ${group.pattern_id}\n`;
      markdown += `- **Category:** ${group.category}\n`;
      markdown += `- **Language:** ${group.language}\n`;
      markdown += `- **Occurrences:** ${group.occurrence_count}\n`;

      if (isInterProject) {
        markdown += `- **Repositories:** ${group.repository_count} (${group.affected_repositories?.join(', ') || 'N/A'})\n`;
      }

      markdown += `- **Total Lines:** ${group.total_lines}\n`;
      markdown += `- **Impact Score:** ${this._formatScore(group.impact_score)}\n`;
      markdown += `- **Similarity:** ${(group.similarity_score * 100).toFixed(0)}% (${group.similarity_method})\n`;

      if (includeDetails) {
        markdown += `- **Affected Files:**\n`;
        const files = group.affected_files || [];
        const maxFiles = 5;
        for (let i = 0; i < Math.min(files.length, maxFiles); i++) {
          markdown += `  - \`${files[i]}\`\n`;
        }
        if (files.length > maxFiles) {
          markdown += `  - *... and ${files.length - maxFiles} more*\n`;
        }
      }

      markdown += '\n';
    }

    return markdown;
  }

  /**
   * Generate suggestions section
   *
   * @private
   */
  static _generateSuggestions(scanResult, isInterProject, maxSuggestions, includeDetails) {
    const suggestions = isInterProject
      ? (scanResult.cross_repository_suggestions || [])
      : (scanResult.suggestions || []);

    let markdown = isInterProject
      ? '## Top Cross-Repository Suggestions\n\n'
      : '## Top Consolidation Suggestions\n\n';

    if (suggestions.length === 0) {
      markdown += '*No suggestions generated.*\n';
      return markdown;
    }

    // Sort by ROI score descending
    const topSuggestions = [...suggestions]
      .sort((a, b) => b.roi_score - a.roi_score)
      .slice(0, maxSuggestions);

    for (const suggestion of topSuggestions) {
      markdown += `### ${suggestion.suggestion_id}\n\n`;
      markdown += `- **Strategy:** ${this._formatStrategy(suggestion.strategy)}\n`;
      markdown += `- **Target:** \`${suggestion.target_location}\`\n`;
      markdown += `- **Impact Score:** ${this._formatScore(suggestion.impact_score)}\n`;
      markdown += `- **ROI Score:** ${this._formatScore(suggestion.roi_score)}\n`;
      markdown += `- **Complexity:** ${this._formatComplexity(suggestion.complexity)}\n`;
      markdown += `- **Risk:** ${this._formatRisk(suggestion.migration_risk)}\n`;

      if (isInterProject) {
        const repos = suggestion.affected_repositories || [];
        markdown += `- **Repositories:** ${repos.join(', ')}\n`;
      } else {
        markdown += `- **Affected Files:** ${suggestion.affected_files_count || 0}\n`;
      }

      if (suggestion.breaking_changes) {
        markdown += `- **‚ö†Ô∏è Breaking Changes:** Yes\n`;
      }

      if (suggestion.estimated_effort_hours) {
        markdown += `- **Estimated Effort:** ${suggestion.estimated_effort_hours}h\n`;
      }

      if (includeDetails && suggestion.strategy_rationale) {
        markdown += `\n**Rationale:** ${suggestion.strategy_rationale}\n`;
      }

      if (includeDetails && suggestion.migration_steps && suggestion.migration_steps.length > 0) {
        markdown += '\n**Migration Steps:**\n';
        for (const step of suggestion.migration_steps.slice(0, 5)) {
          const automated = step.automated ? 'ü§ñ' : 'üë§';
          markdown += `${step.step_number}. ${automated} ${step.description} *(${step.estimated_time})*\n`;
        }
        if (suggestion.migration_steps.length > 5) {
          markdown += `*... and ${suggestion.migration_steps.length - 5} more steps*\n`;
        }
      }

      markdown += '\n';
    }

    return markdown;
  }

  /**
   * Generate footer
   *
   * @private
   */
  static _generateFooter(scanResult) {
    const metadata = scanResult.scan_metadata || {};

    let footer = '---\n\n';
    footer += '*Report generated by Duplicate Detection Pipeline*\n\n';

    if (metadata.total_code_blocks) {
      footer += `*Analyzed ${metadata.total_code_blocks} code blocks*\n`;
    }

    return footer;
  }

  /**
   * Format score with emoji indicator
   *
   * @private
   */
  static _formatScore(score) {
    if (score >= 75) {
      return `üî¥ ${score.toFixed(1)}/100 (High)`;
    } else if (score >= 50) {
      return `üü° ${score.toFixed(1)}/100 (Medium)`;
    } else {
      return `üü¢ ${score.toFixed(1)}/100 (Low)`;
    }
  }

  /**
   * Format strategy with emoji
   *
   * @private
   */
  static _formatStrategy(strategy) {
    const strategyMap = {
      'local_util': 'üìÅ Local Utility',
      'shared_package': 'üì¶ Shared Package',
      'mcp_server': 'üîå MCP Server',
      'autonomous_agent': 'ü§ñ Autonomous Agent'
    };
    return strategyMap[strategy] || strategy;
  }

  /**
   * Format complexity with emoji
   *
   * @private
   */
  static _formatComplexity(complexity) {
    const complexityMap = {
      'trivial': 'üü¢ Trivial',
      'simple': 'üü° Simple',
      'moderate': 'üü† Moderate',
      'complex': 'üî¥ Complex'
    };
    return complexityMap[complexity] || complexity;
  }

  /**
   * Format risk with emoji
   *
   * @private
   */
  static _formatRisk(risk) {
    const riskMap = {
      'minimal': 'üü¢ Minimal',
      'low': 'üü° Low',
      'medium': 'üü† Medium',
      'high': 'üî¥ High'
    };
    return riskMap[risk] || risk;
  }
}
</file>

<file path="pipeline-core/reports/report-coordinator.js">
/**
 * Report Coordinator
 *
 * Unified interface for generating duplicate detection reports in multiple formats.
 * Coordinates HTML, Markdown, and JSON report generation.
 */

import { HTMLReportGenerator } from './html-report-generator.js';
import { MarkdownReportGenerator } from './markdown-report-generator.js';
import { JSONReportGenerator } from './json-report-generator.js';
import { createComponentLogger } from '../../utils/logger.js';
import path from 'path';
import fs from 'fs/promises';

const logger = createComponentLogger('ReportCoordinator');

/**
 * Coordinates report generation across multiple formats
 */
export class ReportCoordinator {
  constructor(outputDir = null) {
    this.outputDir = outputDir || path.join(process.cwd(), 'output', 'reports');
  }

  /**
   * Generate all report formats for a scan
   *
   * @param {Object} scanResult - Scan result data
   * @param {Object} options - Report generation options
   * @returns {Promise<Object>} - Paths to generated reports
   */
  async generateAllReports(scanResult, options = {}) {
    const startTime = Date.now();
    const isInterProject = scanResult.scan_type === 'inter-project';
    const scanType = isInterProject ? 'inter-project' : 'intra-project';

    // Generate filename base from repository or timestamp
    const baseFilename = this._generateBaseFilename(scanResult, isInterProject);

    logger.info({
      scanType,
      baseFilename
    }, 'Generating all report formats');

    try {
      // Ensure output directory exists
      await fs.mkdir(this.outputDir, { recursive: true });

      // Generate all formats in parallel
      const [htmlPath, markdownPath, jsonPath, summaryPath] = await Promise.all([
        this.generateHTMLReport(scanResult, baseFilename, options),
        this.generateMarkdownReport(scanResult, baseFilename, options),
        this.generateJSONReport(scanResult, baseFilename, options),
        this.generateJSONSummary(scanResult, baseFilename)
      ]);

      const duration = (Date.now() - startTime) / 1000;

      const result = {
        html: htmlPath,
        markdown: markdownPath,
        json: jsonPath,
        summary: summaryPath,
        output_dir: this.outputDir,
        duration_seconds: duration
      };

      logger.info({
        ...result,
        duration
      }, 'All reports generated successfully');

      return result;
    } catch (error) {
      logger.error({ error }, 'Failed to generate reports');
      throw error;
    }
  }

  /**
   * Generate HTML report
   *
   * @param {Object} scanResult - Scan result data
   * @param {string} baseFilename - Base filename (without extension)
   * @param {Object} options - Report options
   * @returns {Promise<string>} - Path to generated report
   */
  async generateHTMLReport(scanResult, baseFilename = null, options = {}) {
    const filename = baseFilename || this._generateBaseFilename(scanResult);
    const outputPath = path.join(this.outputDir, `${filename}.html`);

    logger.info({ outputPath }, 'Generating HTML report');

    const htmlOptions = {
      title: options.title || this._generateTitle(scanResult),
      ...options.html
    };

    await HTMLReportGenerator.saveReport(scanResult, outputPath, htmlOptions);

    logger.info({ outputPath }, 'HTML report generated');
    return outputPath;
  }

  /**
   * Generate Markdown report
   *
   * @param {Object} scanResult - Scan result data
   * @param {string} baseFilename - Base filename (without extension)
   * @param {Object} options - Report options
   * @returns {Promise<string>} - Path to generated report
   */
  async generateMarkdownReport(scanResult, baseFilename = null, options = {}) {
    const filename = baseFilename || this._generateBaseFilename(scanResult);
    const outputPath = path.join(this.outputDir, `${filename}.md`);

    logger.info({ outputPath }, 'Generating Markdown report');

    const markdownOptions = {
      includeDetails: options.includeDetails !== false,
      maxDuplicates: options.maxDuplicates || 20,
      maxSuggestions: options.maxSuggestions || 20,
      ...options.markdown
    };

    await MarkdownReportGenerator.saveReport(scanResult, outputPath, markdownOptions);

    logger.info({ outputPath }, 'Markdown report generated');
    return outputPath;
  }

  /**
   * Generate JSON report
   *
   * @param {Object} scanResult - Scan result data
   * @param {string} baseFilename - Base filename (without extension)
   * @param {Object} options - Report options
   * @returns {Promise<string>} - Path to generated report
   */
  async generateJSONReport(scanResult, baseFilename = null, options = {}) {
    const filename = baseFilename || this._generateBaseFilename(scanResult);
    const outputPath = path.join(this.outputDir, `${filename}.json`);

    logger.info({ outputPath }, 'Generating JSON report');

    const jsonOptions = {
      includeSourceCode: options.includeSourceCode !== false,
      includeCodeBlocks: options.includeCodeBlocks !== false,
      prettyPrint: options.prettyPrint !== false,
      maxDuplicates: options.maxDuplicates || null,
      maxSuggestions: options.maxSuggestions || null,
      ...options.json
    };

    await JSONReportGenerator.saveReport(scanResult, outputPath, jsonOptions);

    logger.info({ outputPath }, 'JSON report generated');
    return outputPath;
  }

  /**
   * Generate JSON summary (concise)
   *
   * @param {Object} scanResult - Scan result data
   * @param {string} baseFilename - Base filename (without extension)
   * @returns {Promise<string>} - Path to generated summary
   */
  async generateJSONSummary(scanResult, baseFilename = null) {
    const filename = baseFilename || this._generateBaseFilename(scanResult);
    const outputPath = path.join(this.outputDir, `${filename}-summary.json`);

    logger.info({ outputPath }, 'Generating JSON summary');

    await JSONReportGenerator.saveSummary(scanResult, outputPath);

    logger.info({ outputPath }, 'JSON summary generated');
    return outputPath;
  }

  /**
   * Generate Markdown summary (concise)
   *
   * @param {Object} scanResult - Scan result data
   * @param {string} baseFilename - Base filename (without extension)
   * @returns {Promise<string>} - Path to generated summary
   */
  async generateMarkdownSummary(scanResult, baseFilename = null) {
    const filename = baseFilename || this._generateBaseFilename(scanResult);
    const outputPath = path.join(this.outputDir, `${filename}-summary.md`);

    logger.info({ outputPath }, 'Generating Markdown summary');

    await MarkdownReportGenerator.saveSummary(scanResult, outputPath);

    logger.info({ outputPath }, 'Markdown summary generated');
    return outputPath;
  }

  /**
   * Generate base filename from scan result
   *
   * @private
   */
  _generateBaseFilename(scanResult, isInterProject = null) {
    if (isInterProject === null) {
      isInterProject = scanResult.scan_type === 'inter-project';
    }

    const timestamp = new Date().toISOString().replace(/[:.]/g, '-').split('T')[0];

    if (isInterProject) {
      const repoCount = scanResult.scan_metadata?.repository_count || 0;
      return `inter-project-scan-${repoCount}repos-${timestamp}`;
    } else {
      const repoInfo = scanResult.repository_info || {};
      const repoName = (repoInfo.name || 'unknown')
        .toLowerCase()
        .replace(/[^a-z0-9-]/g, '-');
      return `scan-${repoName}-${timestamp}`;
    }
  }

  /**
   * Generate report title
   *
   * @private
   */
  _generateTitle(scanResult) {
    const isInterProject = scanResult.scan_type === 'inter-project';

    if (isInterProject) {
      const repoCount = scanResult.scan_metadata?.repository_count || 0;
      return `Inter-Project Duplicate Detection Report (${repoCount} Repositories)`;
    } else {
      const repoInfo = scanResult.repository_info || {};
      return `Duplicate Detection Report: ${repoInfo.name || 'Unknown'}`;
    }
  }

  /**
   * Generate a quick summary string for terminal output
   *
   * @param {Object} scanResult - Scan result data
   * @returns {string} - Summary string
   */
  static generateQuickSummary(scanResult) {
    const isInterProject = scanResult.scan_type === 'inter-project';
    const metrics = scanResult.metrics || {};

    let summary = '\n';
    summary += '‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n';
    summary += `  ${isInterProject ? 'INTER-PROJECT' : 'INTRA-PROJECT'} SCAN SUMMARY\n`;
    summary += '‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n';

    if (isInterProject) {
      summary += `üìö Repositories Scanned:    ${metrics.total_repositories_scanned || 0}\n`;
      summary += `üì¶ Total Code Blocks:       ${metrics.total_code_blocks || 0}\n`;
      summary += `üîó Cross-Repo Duplicates:   ${metrics.total_cross_repository_groups || 0}\n`;
      summary += `üìè Duplicated Lines:        ${metrics.cross_repository_duplicated_lines || 0}\n`;
      summary += `üí° Total Suggestions:       ${metrics.total_suggestions || 0}\n`;
      summary += `  ‚îú‚îÄ Shared Package:        ${metrics.shared_package_candidates || 0}\n`;
      summary += `  ‚îî‚îÄ MCP Server:            ${metrics.mcp_server_candidates || 0}\n`;
    } else {
      summary += `üì¶ Code Blocks:             ${metrics.total_code_blocks || 0}\n`;
      summary += `üîÑ Duplicate Groups:        ${metrics.total_duplicate_groups || 0}\n`;
      summary += `  ‚îú‚îÄ Exact:                 ${metrics.exact_duplicates || 0}\n`;
      summary += `  ‚îî‚îÄ Structural:            ${metrics.structural_duplicates || 0}\n`;
      summary += `üìè Duplicated Lines:        ${metrics.total_duplicated_lines || 0}\n`;
      summary += `üìâ Potential LOC Reduction: ${metrics.potential_loc_reduction || 0}\n`;
      summary += `üí° Total Suggestions:       ${metrics.total_suggestions || 0}\n`;
      summary += `‚ö° Quick Wins:              ${metrics.quick_wins || 0}\n`;
    }

    summary += '\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n';

    return summary;
  }

  /**
   * Print quick summary to console
   *
   * @param {Object} scanResult - Scan result data
   */
  static printQuickSummary(scanResult) {
    console.log(ReportCoordinator.generateQuickSummary(scanResult));
  }
}
</file>

<file path="pipeline-core/scanners/ast-grep-detector.js">
// @ts-check
/** @typedef {import('../errors/error-types').ProcessError} ProcessError */
/** @typedef {import('../errors/error-types').NodeError} NodeError */

import { spawn } from 'child_process';
import { createComponentLogger } from '../../utils/logger.js';
import fs from 'fs/promises';
import path from 'path';

const logger = createComponentLogger('AstGrepDetector');

/**
 * AST-Grep Pattern Detector
 *
 * Executes ast-grep scans to detect code patterns using AST matching.
 */
export class AstGrepPatternDetector {
  constructor(options = {}) {
    this.rulesDirectory = options.rulesDirectory || path.join(process.cwd(), '.ast-grep/rules');
    this.configPath = options.configPath || path.join(process.cwd(), '.ast-grep/sgconfig.yml');
  }

  /**
   * Detect patterns in repository
   *
   * @param {string} repoPath - Absolute path to repository
   * @param {object} detectConfig - Detection configuration
   * @returns {Promise<object>}
   */
  async detectPatterns(repoPath, detectConfig = {}) {
    const startTime = Date.now();

    logger.info({ repoPath }, 'Starting pattern detection');

    try {
      // Load rules
      const rules = await this.loadRules(this.rulesDirectory);

      logger.info({ rulesCount: rules.length }, 'Loaded ast-grep rules');

      // Run ast-grep scan
      const matches = await this.runAstGrepScan(repoPath, detectConfig);

      // Normalize matches
      const normalized = matches.map(m => this.normalizeMatch(m, repoPath));

      const duration = (Date.now() - startTime) / 1000;

      const statistics = {
        total_matches: normalized.length,
        rules_applied: rules.length,
        files_scanned: new Set(normalized.map(m => m.file_path)).size,
        scan_duration_ms: duration * 1000
      };

      logger.info({
        repoPath,
        matches: statistics.total_matches,
        files: statistics.files_scanned,
        duration
      }, 'Pattern detection completed');

      return {
        matches: normalized,
        statistics: statistics
      };

    } catch (error) {
      logger.error({ repoPath, error }, 'Pattern detection failed');
      throw new PatternDetectionError(`Failed to detect patterns: ${error.message}`, {
        cause: error
      });
    }
  }

  /**
   * Run ast-grep scan command
   */
  async runAstGrepScan(repoPath, config) {
    return new Promise((resolve, reject) => {
      const args = [
        'scan',
        '--json',
        '--config', this.configPath
      ];

      // Note: ast-grep doesn't support --lang flag
      // Language filtering is done via rules and file extensions

      logger.debug({ args, cwd: repoPath }, 'Running ast-grep command');

      const proc = spawn('sg', args, {
        cwd: repoPath
        // Note: spawn() doesn't support maxBuffer (only exec() does)
        // Large output is handled by accumulating stdout chunks
      });

      let stdout = '';
      let stderr = '';

      if (proc.stdout) {
        proc.stdout.on('data', (data) => {
          stdout += data.toString();
        });
      }

      if (proc.stderr) {
        proc.stderr.on('data', (data) => {
          stderr += data.toString();
        });
      }

      proc.on('close', (code) => {
        if (code === 0 || code === null) {
          try {
            // Parse JSON output - ast-grep outputs a single JSON array
            const trimmed = stdout.trim();
            const matches = trimmed ? JSON.parse(trimmed) : [];

            resolve(matches);
          } catch (error) {
            logger.warn({ error: error.message, stderr }, 'Failed to parse ast-grep output, returning empty results');
            resolve([]);
          }
        } else {
          const error = /** @type {ProcessError} */ (new Error(`ast-grep exited with code ${code}`));
          error.code = code;
          error.stdout = stdout;
          error.stderr = stderr;
          reject(error);
        }
      });

      proc.on('error', (error) => {
        const nodeError = /** @type {NodeError} */ (error);
        if (nodeError.code === 'ENOENT') {
          reject(new Error('ast-grep (sg) command not found. Please install: npm install -g @ast-grep/cli'));
        } else {
          reject(error);
        }
      });
    });
  }

  /**
   * Normalize ast-grep match to standard format
   * Includes file context to capture function declarations
   */
  normalizeMatch(match, repoPath) {
    const filePath = match.file || match.path;
    const lineStart = match.range?.start?.line || match.line;
    // Use 'lines' for full context (includes operators like !==, ===), fallback to 'text' for match pattern
    const matchedText = match.lines || match.text || match.matched;

    return {
      rule_id: match.ruleId || match.rule_id,
      file_path: filePath,
      line_start: lineStart,
      line_end: match.range?.end?.line || (match.line + (match.lines?.length || 1) - 1),
      column_start: match.range?.start?.column || match.column,
      column_end: match.range?.end?.column,
      matched_text: matchedText,
      message: match.message,
      severity: match.severity || 'info',
      ast_node: match.metaVars || {},
      meta_variables: match.metaVars || {},
      _repoPath: repoPath  // Store for later context enrichment
    };
  }

  /**
   * Load all ast-grep rules from directory
   */
  async loadRules(rulesDir) {
    const rules = [];

    async function walkRules(dir) {
      const entries = await fs.readdir(dir, { withFileTypes: true });

      for (const entry of entries) {
        const fullPath = path.join(dir, entry.name);

        if (entry.isDirectory()) {
          await walkRules(fullPath);
        } else if (entry.name.endsWith('.yml') || entry.name.endsWith('.yaml')) {
          rules.push({
            path: fullPath,
            name: entry.name.replace(/\.(yml|yaml)$/, '')
          });
        }
      }
    }

    try {
      await walkRules(rulesDir);
    } catch (error) {
      logger.warn({ rulesDir, error }, 'Failed to load rules directory');
    }

    return rules;
  }

  /**
   * Detect patterns in a single file
   */
  async detectInFile(filePath, rules = []) {
    // For now, delegate to full scan
    // Can be optimized later to scan single file
    const dir = path.dirname(filePath);
    const result = await this.detectPatterns(dir, {
      include_patterns: [path.basename(filePath)]
    });

    return result.matches.filter(m => m.file_path === filePath);
  }
}

/**
 * Custom error class for pattern detection errors
 */
export class PatternDetectionError extends Error {
  constructor(message, options) {
    super(message, options);
    this.name = 'PatternDetectionError';
  }
}
</file>

<file path="pipeline-core/scanners/codebase-health-scanner.js">
#!/usr/bin/env node
/**
 * Codebase Health Scanner CLI
 *
 * Command-line interface for running codebase health scans:
 * - Timeout pattern detection
 * - Root directory analysis
 * - Import dependency analysis
 *
 * Based on AlephAuto debugging session: Nov 18, 2025
 *
 * Usage:
 *   node codebase-health-scanner.js /path/to/repo --scan timeout
 *   node codebase-health-scanner.js /path/to/repo --scan root
 *   node codebase-health-scanner.js /path/to/repo --scan all
 *   node codebase-health-scanner.js /path/to/repo --scan all --output report.md
 *
 * @module lib/scanners/codebase-health-scanner
 */

import { TimeoutPatternDetector } from './timeout-pattern-detector.js';
import { RootDirectoryAnalyzer } from './root-directory-analyzer.js';
import * as fs from 'fs/promises';
import * as path from 'path';

/**
 * Simple logger
 */
const logger = {
  info: (...args) => console.log('[INFO]', ...args),
  warn: (...args) => console.warn('[WARN]', ...args),
  error: (...args) => console.error('[ERROR]', ...args)
};

/**
 * Main CLI function
 */
async function main() {
  const args = process.argv.slice(2);

  // Parse arguments
  const repoPath = args[0];
  const scanType = args.includes('--scan')
    ? args[args.indexOf('--scan') + 1]
    : 'all';
  const outputFile = args.includes('--output')
    ? args[args.indexOf('--output') + 1]
    : null;
  const jsonOutput = args.includes('--json');

  // Validate repo path
  if (!repoPath) {
    console.error('Usage: node codebase-health-scanner.js <repo-path> [options]');
    console.error('');
    console.error('Options:');
    console.error('  --scan <type>       Scan type: timeout, root, all (default: all)');
    console.error('  --output <file>     Output file path (markdown report)');
    console.error('  --json              Output JSON instead of markdown');
    console.error('');
    console.error('Examples:');
    console.error('  node codebase-health-scanner.js ~/code/myproject --scan timeout');
    console.error('  node codebase-health-scanner.js ~/code/myproject --scan all --output report.md');
    process.exit(1);
  }

  const resolvedPath = path.resolve(repoPath);

  logger.info(`Starting codebase health scan: ${resolvedPath}`);
  logger.info(`Scan type: ${scanType}`);

  const results = {
    repository: resolvedPath,
    timestamp: new Date().toISOString(),
    scans: {}
  };

  try {
    // Run timeout pattern detection
    if (scanType === 'timeout' || scanType === 'all') {
      logger.info('Running timeout pattern detection...');
      const detector = new TimeoutPatternDetector({ logger });
      const findings = await detector.scan(resolvedPath);
      results.scans.timeout = findings;

      logger.info(
        `Timeout scan complete: ${findings.statistics.total_issues} issues in ${findings.statistics.affected_files} files`
      );
    }

    // Run root directory analysis
    if (scanType === 'root' || scanType === 'all') {
      logger.info('Running root directory analysis...');
      const analyzer = new RootDirectoryAnalyzer({ logger });
      const analysis = await analyzer.analyze(resolvedPath);
      results.scans.root = analysis;

      logger.info(
        `Root analysis complete: ${analysis.statistics.total_root_files} files, ${analysis.statistics.reduction_potential} can be moved`
      );
    }

    // Generate output
    if (jsonOutput) {
      const output = JSON.stringify(results, null, 2);
      if (outputFile) {
        await fs.writeFile(outputFile, output);
        logger.info(`JSON report saved to: ${outputFile}`);
      } else {
        console.log(output);
      }
    } else {
      const report = generateMarkdownReport(results);
      if (outputFile) {
        await fs.writeFile(outputFile, report);
        logger.info(`Markdown report saved to: ${outputFile}`);
      } else {
        console.log('\n' + report);
      }
    }

    logger.info('Scan complete!');
    process.exit(0);

  } catch (error) {
    logger.error(`Scan failed: ${error.message}`);
    console.error(error.stack);
    process.exit(1);
  }
}

/**
 * Generate markdown report from results
 */
function generateMarkdownReport(results) {
  const lines = [
    '# Codebase Health Report',
    '',
    `**Repository:** ${results.repository}`,
    `**Generated:** ${results.timestamp}`,
    '',
    '---',
    ''
  ];

  // Timeout scan report
  if (results.scans.timeout) {
    const timeout = results.scans.timeout;
    const detector = new TimeoutPatternDetector({ logger });

    lines.push('## Timeout Pattern Detection');
    lines.push('');
    lines.push(detector.generateReport(timeout));
    lines.push('');
  }

  // Root directory analysis report
  if (results.scans.root) {
    const root = results.scans.root;
    const analyzer = new RootDirectoryAnalyzer({ logger });

    lines.push('## Root Directory Analysis');
    lines.push('');
    lines.push(analyzer.generateReport(root));
    lines.push('');
  }

  // Summary
  lines.push('---', '', '## Summary', '');

  if (results.scans.timeout) {
    const stats = results.scans.timeout.statistics;
    lines.push(
      `- **Timeout Issues:** ${stats.total_issues} issues in ${stats.affected_files} files`,
      `  - High: ${stats.severity_breakdown.high || 0}`,
      `  - Medium: ${stats.severity_breakdown.medium || 0}`,
      `  - Low: ${stats.severity_breakdown.low || 0}`,
      ''
    );
  }

  if (results.scans.root) {
    const stats = results.scans.root.statistics;
    lines.push(
      `- **Root Directory:** ${stats.total_root_files} files (${stats.reduction_percentage}% reduction possible)`,
      `  - Can move: ${stats.reduction_potential} files`,
      `  - Final count: ${stats.final_root_files} files`,
      ''
    );
  }

  lines.push(
    '---',
    '',
    '## Next Steps',
    '',
    '1. Review high severity timeout issues first',
    '2. Plan root directory cleanup in phases',
    '3. Create git branch for changes: `git checkout -b health/cleanup`',
    '4. Implement fixes incrementally',
    '5. Test after each phase',
    ''
  );

  return lines.join('\n');
}

/**
 * Export for programmatic use
 */
export async function runHealthScan(repoPath, options = {}) {
  const results = {
    repository: path.resolve(repoPath),
    timestamp: new Date().toISOString(),
    scans: {}
  };

  const loggerInstance = options.logger || logger;

  if (options.scanTimeout !== false) {
    const detector = new TimeoutPatternDetector({ logger: loggerInstance });
    results.scans.timeout = await detector.scan(repoPath);
  }

  if (options.scanRoot !== false) {
    const analyzer = new RootDirectoryAnalyzer({ logger: loggerInstance });
    results.scans.root = await analyzer.analyze(repoPath);
  }

  return results;
}

// Run CLI if executed directly
// @ts-ignore - import.meta is available in ESM
if (import.meta.url === `file://${process.argv[1]}`) {
  main().catch(error => {
    console.error('Fatal error:', error);
    process.exit(1);
  });
}
</file>

<file path="pipeline-core/scanners/repository-scanner.js">
import { RepomixWorker } from '../../workers/repomix-worker.js';
import { createComponentLogger } from '../../utils/logger.js';
import { config } from '../../core/config.js';
import fs from 'fs/promises';
import path from 'path';
import { execSync } from 'child_process';

const logger = createComponentLogger('RepositoryScanner');

/**
 * Repository Scanner - Discovers files and extracts metadata
 *
 * Uses repomix for context aggregation:
 * - File discovery and metadata
 * - Git change tracking
 * - Token counting
 * - Directory structure
 */
export class RepositoryScanner {
  constructor(options = {}) {
    this.repomixWorker = new RepomixWorker({
      outputBaseDir: options.outputBaseDir || config.outputBaseDir,
      codeBaseDir: options.codeBaseDir || config.codeBaseDir,
      maxConcurrent: options.maxConcurrent || config.maxConcurrent
    });
  }

  /**
   * Scan repository and extract metadata
   *
   * @param {string} repoPath - Absolute path to repository
   * @param {object} scanConfig - Scan configuration
   * @returns {Promise<object>}
   */
  async scanRepository(repoPath, scanConfig = {}) {
    const startTime = Date.now();

    logger.info({ repoPath }, 'Starting repository scan');

    try {
      // Validate repository path
      await this.validateRepository(repoPath);

      // Get repository info
      const repoInfo = await this.getRepositoryInfo(repoPath);

      // Run repomix scan (optional)
      /** @type {{ totalFiles: number, totalLines: number, languages: string[] }} */
      let metadata = { totalFiles: 0, totalLines: 0, languages: [] };
      let repomixOutput = null;

      try {
        const repomixResult = await this.runRepomixScan(repoPath);
        metadata = await this.parseRepomixOutput(repomixResult.outputFile);
        repomixOutput = repomixResult.outputFile;
      } catch (error) {
        logger.warn({ error: error.message }, 'Repomix scan failed, using basic file discovery');
        // Fall back to basic file discovery
        const files = await this.listFiles(repoPath);
        metadata = {
          totalFiles: files.length,
          totalLines: 0, // Unknown without repomix
          languages: this.detectLanguages(files)
        };
      }

      // Get file metadata
      const fileMetadata = await this.getFileMetadata(repoPath, scanConfig);

      const duration = (Date.now() - startTime) / 1000;

      logger.info({
        repoPath,
        totalFiles: metadata.totalFiles,
        duration
      }, 'Repository scan completed');

      return {
        repository_info: {
          ...repoInfo,
          total_files: metadata.totalFiles,
          total_lines: metadata.totalLines,
          languages: metadata.languages
        },
        file_metadata: fileMetadata,
        scan_metadata: {
          duration_seconds: duration,
          repomix_output: repomixOutput,
          timestamp: new Date().toISOString(),
          used_repomix: repomixOutput !== null
        }
      };

    } catch (error) {
      logger.error({ repoPath, error }, 'Repository scan failed');
      throw new RepositoryScanError(`Failed to scan repository: ${error.message}`, {
        cause: error
      });
    }
  }

  /**
   * Validate that path is a valid repository
   */
  async validateRepository(repoPath) {
    try {
      const stats = await fs.stat(repoPath);
      if (!stats.isDirectory()) {
        throw new Error('Path is not a directory');
      }

      // Check if it's a git repository (optional, not required)
      try {
        await fs.access(path.join(repoPath, '.git'));
      } catch {
        logger.warn({ repoPath }, 'Not a git repository, proceeding anyway');
      }

    } catch (error) {
      throw new Error(`Invalid repository path: ${error.message}`);
    }
  }

  /**
   * Get repository information
   */
  async getRepositoryInfo(repoPath) {
    const name = path.basename(repoPath);

    const info = {
      path: repoPath,
      name: name,
      git_remote: null,
      git_branch: null,
      git_commit: null
    };

    // Try to get git info
    try {
      const gitDir = path.join(repoPath, '.git');
      await fs.access(gitDir);

      // Get remote
      try {
        info.git_remote = execSync('git config --get remote.origin.url', {
          cwd: repoPath,
          encoding: 'utf-8'
        }).trim();
      } catch {
        logger.debug({ repoPath }, 'No git remote found');
      }

      // Get branch
      try {
        info.git_branch = execSync('git rev-parse --abbrev-ref HEAD', {
          cwd: repoPath,
          encoding: 'utf-8'
        }).trim();
      } catch {
        logger.debug({ repoPath }, 'Could not determine git branch');
      }

      // Get commit
      try {
        info.git_commit = execSync('git rev-parse HEAD', {
          cwd: repoPath,
          encoding: 'utf-8'
        }).trim();
      } catch {
        logger.debug({ repoPath }, 'Could not determine git commit');
      }

    } catch {
      logger.debug({ repoPath }, 'Not a git repository');
    }

    return info;
  }

  /**
   * Run repomix scan
   */
  async runRepomixScan(repoPath) {
    const relativePath = path.relative(config.codeBaseDir, repoPath) || path.basename(repoPath);

    const job = this.repomixWorker.createRepomixJob(repoPath, relativePath);

    return await this.repomixWorker.runJobHandler(job);
  }

  /**
   * Parse repomix output to extract metadata
   * @returns {Promise<{totalFiles: number, totalLines: number, languages: string[]}>}
   */
  async parseRepomixOutput(outputFile) {
    try {
      const content = await fs.readFile(outputFile, 'utf-8');

      // Parse XML format (default repomix output)
      const metadata = {
        totalFiles: 0,
        totalLines: 0,
        languages: new Set()
      };

      // Count files in directory_structure section
      const dirStructMatch = content.match(/<directory_structure>([\s\S]*?)<\/directory_structure>/);
      if (dirStructMatch) {
        const structure = dirStructMatch[1];
        // Count file entries (lines without trailing /)
        metadata.totalFiles = (structure.match(/\n\s+[^\s/]+\.[^\s/]+$/gm) || []).length;
      }

      // Count file entries
      const fileMatches = content.matchAll(/<file path="([^"]+)">/g);
      for (const match of fileMatches) {
        metadata.totalFiles++;

        // Extract language from file extension
        const ext = path.extname(match[1]).toLowerCase();
        const langMap = {
          '.js': 'javascript',
          '.ts': 'typescript',
          '.jsx': 'javascript',
          '.tsx': 'typescript',
          '.py': 'python',
          '.go': 'go',
          '.rs': 'rust',
          '.java': 'java',
          '.rb': 'ruby',
          '.php': 'php'
        };
        if (langMap[ext]) {
          metadata.languages.add(langMap[ext]);
        }
      }

      // Estimate total lines (rough count)
      const linesMatch = content.match(/\n/g);
      metadata.totalLines = linesMatch ? linesMatch.length : 0;

      // Convert Set to Array
      /** @type {{totalFiles: number, totalLines: number, languages: string[]}} */
      const result = {
        ...metadata,
        languages: Array.from(metadata.languages)
      };

      return result;

    } catch (error) {
      logger.warn({ outputFile, error }, 'Failed to parse repomix output');
      return {
        totalFiles: 0,
        totalLines: 0,
        languages: []
      };
    }
  }

  /**
   * Get detailed file metadata
   */
  async getFileMetadata(repoPath, scanConfig) {
    // For now, return empty array
    // Will be populated by ast-grep scan
    return [];
  }

  /**
   * List all files in repository
   */
  async listFiles(repoPath, filter = {}) {
    const files = [];

    async function walk(dir) {
      const entries = await fs.readdir(dir, { withFileTypes: true });

      for (const entry of entries) {
        const fullPath = path.join(dir, entry.name);

        // Skip common ignored directories
        if (entry.isDirectory()) {
          if (['node_modules', '.git', 'dist', 'build', '.next'].includes(entry.name)) {
            continue;
          }
          await walk(fullPath);
        } else {
          // Apply language filter if provided
          if (filter.languages) {
            const ext = path.extname(entry.name).toLowerCase();
            const langMap = {
              '.js': 'javascript',
              '.ts': 'typescript',
              '.jsx': 'javascript',
              '.tsx': 'typescript',
              '.py': 'python'
            };
            if (filter.languages.includes(langMap[ext])) {
              files.push(fullPath);
            }
          } else {
            files.push(fullPath);
          }
        }
      }
    }

    await walk(repoPath);
    return files;
  }

  /**
   * Detect languages from file list
   */
  detectLanguages(files) {
    const langMap = {
      '.js': 'javascript',
      '.ts': 'typescript',
      '.jsx': 'javascript',
      '.tsx': 'typescript',
      '.py': 'python',
      '.go': 'go',
      '.rs': 'rust',
      '.java': 'java',
      '.rb': 'ruby',
      '.php': 'php'
    };

    const languages = new Set();
    for (const file of files) {
      const ext = path.extname(file).toLowerCase();
      if (langMap[ext]) {
        languages.add(langMap[ext]);
      }
    }

    return Array.from(languages);
  }
}

/**
 * Custom error class for repository scanning errors
 */
export class RepositoryScanError extends Error {
  constructor(message, options) {
    super(message, options);
    this.name = 'RepositoryScanError';
  }
}
</file>

<file path="pipeline-core/scanners/root-directory-analyzer.js">
/**
 * Root Directory Analyzer for AlephAuto
 *
 * Analyzes project root directories for organization issues:
 * - Too many files in root
 * - Misplaced configuration files
 * - Database files in root
 * - Scripts that should be in scripts/
 * - Library code in root
 *
 * Generates cleanup recommendations with zero import breakage.
 *
 * Based on debugging session: AnalyticsBot root directory cleanup
 *
 * @module lib/scanners/root-directory-analyzer
 */

import * as fs from 'fs/promises';
import * as path from 'path';
import { spawn } from 'child_process';

/**
 * Root Directory Analyzer
 *
 * Scans project root directories and identifies cleanup opportunities.
 */
export class RootDirectoryAnalyzer {
  constructor(options = {}) {
    this.logger = options.logger || console;
    this.maxRootFiles = options.maxRootFiles || 20;
    this.thresholds = {
      pythonFiles: 3,
      shellScripts: 3,
      jsFiles: 3,
      configFiles: 5,
      dataFiles: 0 // No data files in root
    };
  }

  /**
   * Analyze repository root directory
   *
   * @param {string} repoPath - Absolute path to repository
   * @param {object} options - Analysis options
   * @returns {Promise<object>} Analysis results with recommendations
   */
  async analyze(repoPath, options = {}) {
    const startTime = Date.now();

    this.logger.info(`[RootDirectoryAnalyzer] Starting analysis: ${repoPath}`);

    try {
      // Get all files in root
      const rootFiles = await this.getRootFiles(repoPath);

      // Categorize files
      const categorized = this.categorizeFiles(rootFiles);

      // Analyze import dependencies
      const dependencies = await this.analyzeImportDependencies(repoPath, categorized);

      // Generate recommendations
      const recommendations = this.generateRecommendations(categorized, dependencies);

      // Calculate statistics
      const statistics = {
        total_root_files: rootFiles.length,
        reduction_potential: recommendations.reduce((sum, r) => sum + r.files.length, 0),
        final_root_files: rootFiles.length - recommendations.reduce((sum, r) => sum + r.files.length, 0),
        reduction_percentage: Math.round(
          (recommendations.reduce((sum, r) => sum + r.files.length, 0) / rootFiles.length) * 100
        ),
        scan_duration_ms: Date.now() - startTime
      };

      const result = {
        root_files: rootFiles,
        categorized,
        dependencies,
        recommendations,
        statistics
      };

      this.logger.info(
        `[RootDirectoryAnalyzer] Analysis complete: ${rootFiles.length} files, ${statistics.reduction_potential} can be moved (${statistics.reduction_percentage}% reduction)`
      );

      return result;
    } catch (error) {
      this.logger.error(`[RootDirectoryAnalyzer] Analysis failed: ${error.message}`);
      throw error;
    }
  }

  /**
   * Get all files in root directory (depth 1)
   */
  async getRootFiles(repoPath) {
    const entries = await fs.readdir(repoPath, { withFileTypes: true });

    return entries
      .filter(entry => entry.isFile() && !entry.name.startsWith('.'))
      .map(entry => ({
        name: entry.name,
        path: path.join(repoPath, entry.name),
        extension: path.extname(entry.name)
      }));
  }

  /**
   * Categorize files by type
   */
  categorizeFiles(files) {
    const categories = {
      python: [],
      shell: [],
      javascript: [],
      typescript: [],
      config: [],
      data: [],
      documentation: [],
      packageManager: [],
      deployment: [],
      other: []
    };

    const extensionMap = {
      '.py': 'python',
      '.sh': 'shell',
      '.bash': 'shell',
      '.js': 'javascript',
      '.mjs': 'javascript',
      '.cjs': 'javascript',
      '.ts': 'typescript',
      '.tsx': 'typescript',
      '.json': 'config',
      '.yml': 'config',
      '.yaml': 'config',
      '.toml': 'config',
      '.ini': 'config',
      '.db': 'data',
      '.sqlite': 'data',
      '.sqlite3': 'data',
      '.md': 'documentation',
      '.txt': 'documentation'
    };

    const packageFiles = [
      'package.json',
      'package-lock.json',
      'yarn.lock',
      'pnpm-lock.yaml',
      'requirements.txt',
      'Pipfile',
      'Pipfile.lock',
      'pyproject.toml',
      'poetry.lock',
      'Cargo.toml',
      'Cargo.lock',
      'go.mod',
      'go.sum'
    ];

    const deploymentFiles = [
      'Dockerfile',
      'docker-compose.yml',
      'docker-compose.yaml',
      'render.yaml',
      'vercel.json',
      'netlify.toml',
      'Procfile',
      'doppler.yaml',
      '.env.example'
    ];

    for (const file of files) {
      // Special cases
      if (packageFiles.includes(file.name)) {
        categories.packageManager.push(file);
      } else if (deploymentFiles.includes(file.name)) {
        categories.deployment.push(file);
      } else {
        // Categorize by extension
        const category = extensionMap[file.extension] || 'other';
        categories[category].push(file);
      }
    }

    return categories;
  }

  /**
   * Analyze import dependencies for Python files
   */
  async analyzeImportDependencies(repoPath, categorized) {
    const dependencies = {
      python: {},
      javascript: {}
    };

    // Analyze Python imports
    for (const file of categorized.python) {
      const imports = await this.analyzePythonImports(file.path, repoPath);
      dependencies.python[file.name] = {
        imports_from_root: imports.fromRoot,
        imported_by: [],
        can_move: imports.fromRoot.length === 0
      };
    }

    // Build reverse dependency graph (who imports this file)
    for (const [fileName, data] of Object.entries(dependencies.python)) {
      for (const imported of data.imports_from_root) {
        const importedFile = imported.replace(/^from /, '').replace(/ import.*/, '') + '.py';
        if (dependencies.python[importedFile]) {
          dependencies.python[importedFile].imported_by.push(fileName);
        }
      }
    }

    return dependencies;
  }

  /**
   * Analyze Python imports in a file
   */
  async analyzePythonImports(filePath, repoPath) {
    try {
      const content = await fs.readFile(filePath, 'utf-8');
      const lines = content.split('\n');

      const imports = {
        fromRoot: [],
        fromPackages: [],
        relative: []
      };

      for (const line of lines) {
        const trimmed = line.trim();

        // Match: from module import ...
        const fromMatch = trimmed.match(/^from\s+([^\s]+)\s+import/);
        if (fromMatch) {
          const module = fromMatch[1];

          if (module.startsWith('.')) {
            imports.relative.push(trimmed);
          } else if (!module.includes('.') && this.isLocalModule(module, repoPath)) {
            imports.fromRoot.push(trimmed);
          } else {
            imports.fromPackages.push(trimmed);
          }
        }

        // Match: import module
        const importMatch = trimmed.match(/^import\s+([^\s]+)/);
        if (importMatch) {
          const module = importMatch[1];

          if (this.isLocalModule(module, repoPath)) {
            imports.fromRoot.push(trimmed);
          } else {
            imports.fromPackages.push(trimmed);
          }
        }
      }

      return imports;
    } catch {
      return { fromRoot: [], fromPackages: [], relative: [] };
    }
  }

  /**
   * Check if module is local (exists as .py file in root)
   */
  isLocalModule(moduleName, repoPath) {
    const filePath = path.join(repoPath, `${moduleName}.py`);
    try {
      fs.access(filePath);
      return true;
    } catch {
      return false;
    }
  }

  /**
   * Generate cleanup recommendations
   */
  generateRecommendations(categorized, dependencies) {
    const recommendations = [];

    // Recommendation 1: Move Python library files to lib/
    const pythonLibFiles = categorized.python.filter(f => {
      const baseName = f.name.replace('.py', '');
      const dep = dependencies.python[f.name];

      // Identify as library if:
      // - Has "lib", "utils", "helper", "middleware", "config" in name
      // - Is imported by other root files
      // - Is not a main entry point (server, app, main)
      const isLibrary =
        /(lib|util|helper|middleware|config|auth|rate|sentry)/.test(baseName) ||
        (dep && dep.imported_by.length > 0);

      const isEntryPoint = /(server|app|main|run|start)/.test(baseName);

      return isLibrary && !isEntryPoint;
    });

    if (pythonLibFiles.length > 0) {
      recommendations.push({
        id: 'move_python_lib',
        title: 'Move Python Library Files to lib/',
        description: 'Move shared Python modules to lib/ directory for better organization',
        files: pythonLibFiles,
        target_directory: 'lib',
        impact: 'medium',
        requires_import_updates: true,
        import_changes: this.calculateImportChanges(pythonLibFiles, 'lib', dependencies),
        commands: this.generateMoveCommands(pythonLibFiles, 'lib')
      });
    }

    // Recommendation 2: Move scripts to scripts/
    const scriptFiles = [
      ...categorized.python.filter(f => {
        const baseName = f.name.replace('.py', '');
        return /(migrate|configure|inject|setup|test|script)/.test(baseName);
      }),
      ...categorized.shell
    ];

    if (scriptFiles.length > 0) {
      recommendations.push({
        id: 'move_scripts',
        title: 'Move Scripts to scripts/',
        description: 'Move utility scripts and shell scripts to scripts/ directory',
        files: scriptFiles,
        target_directory: 'scripts',
        impact: 'low',
        requires_import_updates: false,
        commands: this.generateMoveCommands(scriptFiles, 'scripts')
      });
    }

    // Recommendation 3: Move data files to data/
    if (categorized.data.length > 0) {
      recommendations.push({
        id: 'move_data',
        title: 'Move Data Files to data/',
        description: 'Move database and data files to data/ directory',
        files: categorized.data,
        target_directory: 'data',
        impact: 'medium',
        requires_import_updates: true,
        path_updates_required: 'Update hardcoded paths to data files',
        commands: this.generateMoveCommands(categorized.data, 'data')
      });
    }

    // Recommendation 4: Move config files to config/
    const movableConfigs = categorized.config.filter(f =>
      !['package.json', 'package-lock.json', 'tsconfig.json'].includes(f.name)
    );

    if (movableConfigs.length > this.thresholds.configFiles) {
      recommendations.push({
        id: 'move_configs',
        title: 'Move Configuration Files to config/',
        description: 'Move project-specific config files to config/ directory',
        files: movableConfigs,
        target_directory: 'config',
        impact: 'medium',
        requires_import_updates: true,
        path_updates_required: 'Update config file path references in code',
        commands: this.generateMoveCommands(movableConfigs, 'config')
      });
    }

    // Recommendation 5: Remove temporary/generated files
    const tempFiles = categorized.other.filter(f =>
      f.name.includes('repomix-output') ||
      f.name.endsWith('.log') ||
      f.name.endsWith('.tmp')
    );

    if (tempFiles.length > 0) {
      recommendations.push({
        id: 'remove_temp',
        title: 'Remove or Move Temporary Files',
        description: 'Remove generated files or add to .gitignore',
        files: tempFiles,
        target_directory: 'docs (or delete)',
        impact: 'low',
        requires_import_updates: false,
        commands: tempFiles.map(f => `mv ${f.name} docs/ # or add to .gitignore`)
      });
    }

    return recommendations;
  }

  /**
   * Calculate import changes needed for moving files
   */
  calculateImportChanges(files, targetDir, dependencies) {
    const changes = [];

    for (const file of files) {
      const dep = dependencies.python[file.name];
      if (!dep) continue;

      // Files that import this module need updating
      for (const importer of dep.imported_by) {
        const baseName = file.name.replace('.py', '');
        changes.push({
          file: importer,
          old_import: `from ${baseName} import`,
          new_import: `from ${targetDir}.${baseName} import`
        });
      }

      // This module's imports from other root files need updating
      for (const imported of dep.imports_from_root) {
        const match = imported.match(/from\s+([^\s]+)/);
        if (match) {
          const module = match[1];
          changes.push({
            file: file.name,
            old_import: `from ${module}`,
            new_import: `from ${targetDir}.${module}`
          });
        }
      }
    }

    return changes;
  }

  /**
   * Generate move commands
   */
  generateMoveCommands(files, targetDir) {
    return files.map(f => `git mv ${f.name} ${targetDir}/`);
  }

  /**
   * Generate markdown report
   */
  generateReport(analysis) {
    const lines = [
      '# Root Directory Cleanup Analysis',
      '',
      `**Generated:** ${new Date().toISOString()}`,
      `**Total Root Files:** ${analysis.statistics.total_root_files}`,
      `**Reduction Potential:** ${analysis.statistics.reduction_potential} files (${analysis.statistics.reduction_percentage}%)`,
      `**Final Root Files:** ${analysis.statistics.final_root_files}`,
      '',
      '## Current State',
      '',
      '### Files by Category',
      ''
    ];

    // Category breakdown
    Object.entries(analysis.categorized).forEach(([category, files]) => {
      if (files.length > 0) {
        lines.push(`**${category}:** ${files.length} files`);
        files.forEach(f => lines.push(`  - ${f.name}`));
        lines.push('');
      }
    });

    lines.push('## Recommendations', '');

    // Recommendations
    analysis.recommendations.forEach((rec, i) => {
      lines.push(
        `### ${i + 1}. ${rec.title}`,
        '',
        `**Files to Move:** ${rec.files.length}`,
        `**Target:** \`${rec.target_directory}/\``,
        `**Impact:** ${rec.impact}`,
        `**Requires Import Updates:** ${rec.requires_import_updates ? 'Yes' : 'No'}`,
        ''
      );

      if (rec.import_changes && rec.import_changes.length > 0) {
        lines.push('**Import Changes Required:**', '');
        rec.import_changes.slice(0, 5).forEach(change => {
          lines.push(`- \`${change.file}\`: \`${change.old_import}\` ‚Üí \`${change.new_import}\``);
        });
        if (rec.import_changes.length > 5) {
          lines.push(`- ... and ${rec.import_changes.length - 5} more`);
        }
        lines.push('');
      }

      lines.push('**Commands:**', '```bash');
      lines.push(`mkdir -p ${rec.target_directory}`);
      rec.commands.slice(0, 10).forEach(cmd => lines.push(cmd));
      if (rec.commands.length > 10) {
        lines.push(`# ... and ${rec.commands.length - 10} more files`);
      }
      lines.push('```', '');
    });

    lines.push(
      '## Implementation Steps',
      '',
      '1. Create backup branch: `git checkout -b cleanup/root-directory`',
      '2. Run tests to establish baseline',
      '3. Execute recommendations in order',
      '4. Update imports after each phase',
      '5. Test after each phase',
      '6. Commit incrementally',
      ''
    );

    return lines.join('\n');
  }
}

/**
 * Export analyzer instance creator
 */
export function createRootDirectoryAnalyzer(options) {
  return new RootDirectoryAnalyzer(options);
}
</file>

<file path="pipeline-core/scanners/timeout_detector.py">
#!/usr/bin/env python3
"""
Timeout Pattern Detector (Python Version)

Simpler Python implementation for detecting timeout anti-patterns.
Useful for Python-heavy projects or when ast-grep is not available.

Based on AlephAuto debugging session: Nov 18, 2025

Usage:
    python3 timeout_detector.py /path/to/repo
    python3 timeout_detector.py /path/to/repo --output report.md

Author: Derived from AnalyticsBot dashboard debugging session
"""

import os
import sys
import re
import json
from pathlib import Path
from typing import List, Dict, Any
from dataclasses import dataclass, asdict


@dataclass
class Finding:
    """Represents a single finding"""
    file_path: str
    line_number: int
    severity: str
    category: str
    message: str
    code_snippet: str
    recommendation: str


class TimeoutDetector:
    """Detects timeout and infinite loading patterns in codebases"""

    def __init__(self, logger=None):
        self.logger = logger or print
        self.findings: List[Finding] = []

    def scan_directory(self, repo_path: str) -> Dict[str, Any]:
        """Scan a directory for timeout patterns"""
        repo_path = Path(repo_path)

        self.logger(f"Scanning: {repo_path}")

        # Find all TypeScript/JavaScript/Python files
        extensions = ['.ts', '.tsx', '.js', '.jsx', '.py']
        files = []

        for ext in extensions:
            files.extend(repo_path.rglob(f'*{ext}'))

        # Skip node_modules and other common excludes
        files = [
            f for f in files
            if 'node_modules' not in str(f)
            and '.git' not in str(f)
            and 'dist' not in str(f)
            and 'build' not in str(f)
        ]

        self.logger(f"Found {len(files)} files to scan")

        # Scan each file
        for file_path in files:
            self._scan_file(file_path)

        # Generate statistics
        stats = self._calculate_statistics()

        return {
            'findings': [asdict(f) for f in self.findings],
            'statistics': stats
        }

    def _scan_file(self, file_path: Path):
        """Scan a single file for patterns"""
        try:
            content = file_path.read_text(encoding='utf-8')
            lines = content.split('\n')

            for i, line in enumerate(lines, start=1):
                # Pattern 1: Promise.race without timeout
                if 'Promise.race' in line and 'timeout' not in line.lower():
                    self.findings.append(Finding(
                        file_path=str(file_path),
                        line_number=i,
                        severity='high',
                        category='promise_race_no_timeout',
                        message='Promise.race() without timeout wrapper',
                        code_snippet=line.strip(),
                        recommendation='Wrap with withTimeout() or add setTimeout rejection'
                    ))

                # Pattern 2: setLoading(true) without finally
                if 'setLoading(true)' in line or 'setLoading( true )' in line:
                    # Check if there's a finally block in the next 20 lines
                    has_finally = any(
                        'finally' in lines[j]
                        for j in range(i, min(i + 20, len(lines)))
                    )

                    if not has_finally:
                        self.findings.append(Finding(
                            file_path=str(file_path),
                            line_number=i,
                            severity='medium',
                            category='loading_without_finally',
                            message='setLoading(true) without finally block',
                            code_snippet=line.strip(),
                            recommendation='Add finally block with setLoading(false)'
                        ))

                # Pattern 3: async function without try-catch
                if re.match(r'^\s*async\s+(function|\w+)\s*\(', line):
                    # Look ahead for try-catch
                    has_try_catch = any(
                        re.search(r'\btry\b', lines[j]) and re.search(r'\bcatch\b', lines[j + 10] if j + 10 < len(lines) else '')
                        for j in range(i, min(i + 50, len(lines)))
                    )

                    if not has_try_catch:
                        self.findings.append(Finding(
                            file_path=str(file_path),
                            line_number=i,
                            severity='low',
                            category='async_no_error_handling',
                            message='Async function without try-catch',
                            code_snippet=line.strip(),
                            recommendation='Add try-catch block for error handling'
                        ))

                # Pattern 4: setTimeout without clearTimeout
                if 'setTimeout' in line and 'clearTimeout' not in content:
                    # Check if it's part of a timeout pattern
                    if 'reject' in line or 'timeout' in line.lower():
                        # This is likely a timeout pattern, check for cleanup
                        has_cleanup = 'clearTimeout' in content or 'return () =>' in content

                        if not has_cleanup:
                            self.findings.append(Finding(
                                file_path=str(file_path),
                                line_number=i,
                                severity='low',
                                category='settimeout_no_cleanup',
                                message='setTimeout without cleanup in useEffect/cleanup',
                                code_snippet=line.strip(),
                                recommendation='Add cleanup function: return () => clearTimeout(timeoutId)'
                            ))

        except Exception as e:
            self.logger(f"Error scanning {file_path}: {e}")

    def _calculate_statistics(self) -> Dict[str, Any]:
        """Calculate statistics from findings"""
        severity_breakdown = {}
        category_breakdown = {}
        files_affected = set()

        for finding in self.findings:
            # Severity
            severity_breakdown[finding.severity] = severity_breakdown.get(finding.severity, 0) + 1

            # Category
            category_breakdown[finding.category] = category_breakdown.get(finding.category, 0) + 1

            # Files
            files_affected.add(finding.file_path)

        return {
            'total_findings': len(self.findings),
            'files_affected': len(files_affected),
            'severity_breakdown': severity_breakdown,
            'category_breakdown': category_breakdown
        }

    def generate_report(self, results: Dict[str, Any]) -> str:
        """Generate markdown report"""
        lines = [
            '# Timeout Pattern Detection Report',
            '',
            '## Statistics',
            '',
            f"- **Total Findings:** {results['statistics']['total_findings']}",
            f"- **Files Affected:** {results['statistics']['files_affected']}",
            '',
            '### Severity Breakdown',
            ''
        ]

        for severity, count in results['statistics']['severity_breakdown'].items():
            emoji = {'high': 'üî¥', 'medium': 'üü°', 'low': 'üü¢'}.get(severity, '‚ö™')
            lines.append(f"- {emoji} **{severity.upper()}:** {count}")

        lines.extend(['', '### Category Breakdown', ''])

        for category, count in results['statistics']['category_breakdown'].items():
            lines.append(f"- **{category}:** {count}")

        lines.extend(['', '## Findings', ''])

        # Group by severity
        for severity in ['high', 'medium', 'low']:
            severity_findings = [
                f for f in self.findings
                if f.severity == severity
            ]

            if severity_findings:
                lines.extend([f'### {severity.upper()} Severity ({len(severity_findings)})', ''])

                for finding in severity_findings[:10]:  # Limit to 10 per severity
                    lines.extend([
                        f"**{finding.file_path}:{finding.line_number}**",
                        f"- Category: {finding.category}",
                        f"- Message: {finding.message}",
                        f"- Code: `{finding.code_snippet}`",
                        f"- Recommendation: {finding.recommendation}",
                        ''
                    ])

                if len(severity_findings) > 10:
                    lines.append(f"*... and {len(severity_findings) - 10} more*\n")

        return '\n'.join(lines)


def main():
    """Main CLI entry point"""
    if len(sys.argv) < 2:
        print("Usage: python3 timeout_detector.py <repo-path> [--output <file>]")
        print("")
        print("Examples:")
        print("  python3 timeout_detector.py ~/code/myproject")
        print("  python3 timeout_detector.py ~/code/myproject --output report.md")
        sys.exit(1)

    repo_path = sys.argv[1]
    output_file = None

    if '--output' in sys.argv:
        output_index = sys.argv.index('--output')
        if output_index + 1 < len(sys.argv):
            output_file = sys.argv[output_index + 1]

    # Run scan
    detector = TimeoutDetector()
    results = detector.scan_directory(repo_path)

    # Generate report
    if '--json' in sys.argv:
        output = json.dumps(results, indent=2)
    else:
        output = detector.generate_report(results)

    # Output
    if output_file:
        Path(output_file).write_text(output)
        print(f"Report saved to: {output_file}")
    else:
        print(output)

    # Exit code based on high severity findings
    high_severity_count = results['statistics']['severity_breakdown'].get('high', 0)
    sys.exit(1 if high_severity_count > 0 else 0)


if __name__ == '__main__':
    main()
</file>

<file path="pipeline-core/scanners/timeout-pattern-detector.js">
/**
 * Timeout Pattern Detector for AlephAuto
 *
 * Detects common infinite loading patterns in React/TypeScript applications:
 * - Missing timeout wrappers on Promise.race() calls
 * - Loading states that never reset
 * - Async operations without error handling
 * - Missing finally blocks in loading logic
 *
 * Based on debugging session: AnalyticsBot dashboard infinite loading issue
 *
 * @module lib/scanners/timeout-pattern-detector
 */

// @ts-check
/** @typedef {import('../errors/error-types').NodeError} NodeError */

import { spawn } from 'child_process';
import * as fs from 'fs/promises';
import * as path from 'path';

/**
 * Timeout Pattern Detector
 *
 * Scans codebases for common timeout and infinite loading anti-patterns.
 */
export class TimeoutPatternDetector {
  constructor(options = {}) {
    this.logger = options.logger || console;
    this.astGrepBinary = options.astGrepBinary || 'sg';
  }

  /**
   * Scan repository for timeout anti-patterns
   *
   * @param {string} repoPath - Absolute path to repository
   * @param {object} options - Scan options
   * @returns {Promise<object>} Scan results with findings
   */
  async scan(repoPath, options = {}) {
    const startTime = Date.now();

    this.logger.info(`[TimeoutPatternDetector] Starting scan: ${repoPath}`);

    try {
      const findings = {
        promiseRaceWithoutTimeout: [],
        loadingWithoutFinally: [],
        asyncWithoutErrorHandling: [],
        missingTimeoutConstants: [],
        setLoadingWithoutReset: [],
        statistics: {}
      };

      // Run parallel pattern searches
      const [
        promiseRaceIssues,
        loadingIssues,
        asyncIssues,
        constantIssues,
        resetIssues
      ] = await Promise.all([
        this.findPromiseRaceWithoutTimeout(repoPath),
        this.findLoadingWithoutFinally(repoPath),
        this.findAsyncWithoutErrorHandling(repoPath),
        this.findMissingTimeoutConstants(repoPath),
        this.findSetLoadingWithoutReset(repoPath)
      ]);

      findings.promiseRaceWithoutTimeout = promiseRaceIssues;
      findings.loadingWithoutFinally = loadingIssues;
      findings.asyncWithoutErrorHandling = asyncIssues;
      findings.missingTimeoutConstants = constantIssues;
      findings.setLoadingWithoutReset = resetIssues;

      // Calculate statistics
      const totalIssues = Object.values(findings)
        .filter(Array.isArray)
        .reduce((sum, arr) => sum + arr.length, 0);

      const affectedFiles = new Set(
        Object.values(findings)
          .filter(Array.isArray)
          .flat()
          .map(f => f.file_path)
      ).size;

      findings.statistics = {
        total_issues: totalIssues,
        affected_files: affectedFiles,
        scan_duration_ms: Date.now() - startTime,
        severity_breakdown: this.calculateSeverityBreakdown(findings)
      };

      this.logger.info(
        `[TimeoutPatternDetector] Scan complete: ${totalIssues} issues in ${affectedFiles} files (${findings.statistics.scan_duration_ms}ms)`
      );

      return findings;
    } catch (error) {
      this.logger.error(`[TimeoutPatternDetector] Scan failed: ${error.message}`);
      throw error;
    }
  }

  /**
   * Find Promise.race() calls without proper timeout wrappers
   */
  async findPromiseRaceWithoutTimeout(repoPath) {
    const pattern = 'Promise.race([$$$PROMISES])';
    const matches = await this.searchPattern(repoPath, pattern, {
      language: 'typescript',
      extensions: ['.ts', '.tsx', '.js', '.jsx']
    });

    // Filter out patterns that already have timeout logic
    return matches
      .filter(match => {
        const context = match.matched_text.toLowerCase();
        return !(
          context.includes('timeout') ||
          context.includes('settimeout') ||
          context.includes('withtimeout')
        );
      })
      .map(match => ({
        ...match,
        severity: 'high',
        category: 'promise_race_no_timeout',
        message: 'Promise.race() without timeout wrapper - may hang indefinitely',
        recommendation: 'Wrap with withTimeout() utility or add setTimeout() rejection promise',
        fix_example: `
// Before:
const result = await Promise.race([fetchData(), otherPromise()]);

// After:
import { withTimeout, TIMEOUT } from '../utils/timeout';
const result = await withTimeout(
  Promise.race([fetchData(), otherPromise()]),
  TIMEOUT.NORMAL,
  'Operation timeout'
);`
      }));
  }

  /**
   * Find loading state setters without finally blocks
   */
  async findLoadingWithoutFinally(repoPath) {
    const pattern = 'setLoading(true)';
    const matches = await this.searchPattern(repoPath, pattern, {
      language: 'typescript',
      extensions: ['.ts', '.tsx', '.js', '.jsx']
    });

    return matches.map(match => ({
      ...match,
      severity: 'medium',
      category: 'loading_without_finally',
      message: 'setLoading(true) should be paired with finally block to ensure reset',
      recommendation: 'Add finally block with setLoading(false)',
      fix_example: `
// Pattern:
try {
  setLoading(true);
  await fetchData();
} catch (error) {
  handleError(error);
} finally {
  setLoading(false); // Always reset loading state
}`
    }));
  }

  /**
   * Find async operations without error handling
   */
  async findAsyncWithoutErrorHandling(repoPath) {
    const pattern = 'async $FUNC($$$PARAMS) { $$$BODY }';
    const matches = await this.searchPattern(repoPath, pattern, {
      language: 'typescript',
      extensions: ['.ts', '.tsx', '.js', '.jsx']
    });

    // Filter for functions without try-catch
    return matches
      .filter(match => {
        const body = match.matched_text;
        return !(
          body.includes('try') &&
          body.includes('catch')
        );
      })
      .map(match => ({
        ...match,
        severity: 'medium',
        category: 'async_no_error_handling',
        message: 'Async function without try-catch - errors may be unhandled',
        recommendation: 'Add try-catch block or propagate errors explicitly'
      }));
  }

  /**
   * Find missing timeout constants
   */
  async findMissingTimeoutConstants(repoPath) {
    // Check if timeout utility exists
    const timeoutUtilPath = path.join(repoPath, 'src/utils/timeout.ts');

    try {
      await fs.access(timeoutUtilPath);
      return []; // Timeout utility exists
    } catch {
      return [{
        file_path: 'src/utils/timeout.ts',
        line_start: 0,
        severity: 'info',
        category: 'missing_timeout_utility',
        message: 'No centralized timeout utility found',
        recommendation: 'Create src/utils/timeout.ts with reusable timeout functions',
        fix_example: `
// src/utils/timeout.ts
export function withTimeout<T>(
  promise: Promise<T>,
  timeoutMs: number,
  errorMsg: string
): Promise<T> {
  const timeout = new Promise<never>((_, reject) => {
    setTimeout(() => reject(new Error(errorMsg)), timeoutMs);
  });
  return Promise.race([promise, timeout]);
}

export const TIMEOUT = {
  FAST: 2000,
  NORMAL: 5000,
  SLOW: 10000,
  VERY_SLOW: 15000,
} as const;`
      }];
    }
  }

  /**
   * Find setLoading without corresponding reset in useEffect
   */
  async findSetLoadingWithoutReset(repoPath) {
    const pattern = 'setLoading($VAL)';
    const matches = await this.searchPattern(repoPath, pattern, {
      language: 'typescript',
      extensions: ['.ts', '.tsx']
    });

    const findings = [];

    // Group by file and check for safety net pattern
    const fileGroups = this.groupByFile(matches);

    for (const [filePath, fileMatches] of Object.entries(fileGroups)) {
      const fileContent = await this.readFile(path.join(repoPath, filePath));

      // Check if file has safety net timeout pattern
      const hasSafetyNet =
        fileContent.includes('useEffect') &&
        fileContent.includes('setTimeout') &&
        fileContent.includes('setLoading(false)') &&
        fileContent.includes('return () => clearTimeout');

      if (!hasSafetyNet && fileMatches.some(m => m.matched_text.includes('true'))) {
        findings.push({
          file_path: filePath,
          line_start: fileMatches[0].line_start,
          severity: 'low',
          category: 'missing_loading_safety_net',
          message: 'Consider adding maximum loading timeout safety net',
          recommendation: 'Add useEffect with timeout to force loading=false after max wait time',
          fix_example: `
// Add safety net in useDashboardData or similar hooks:
useEffect(() => {
  if (!loading) return;

  const maxLoadingTimeout = setTimeout(() => {
    console.error('MAXIMUM LOADING TIMEOUT REACHED');
    setLoading(false);
    setError(new Error('Load timeout - please refresh'));
  }, TIMEOUT.VERY_SLOW);

  return () => clearTimeout(maxLoadingTimeout);
}, [loading]);`
        });
      }
    }

    return findings;
  }

  /**
   * Search for pattern using ast-grep
   */
  async searchPattern(repoPath, pattern, options = {}) {
    return new Promise((resolve, reject) => {
      const args = [
        'run',
        '--pattern', pattern,
        '--json'
      ];

      if (options.language) {
        args.push('--lang', options.language);
      }

      const proc = spawn(this.astGrepBinary, args, {
        cwd: repoPath,
        timeout: 30000 // 30 second timeout
      });

      let stdout = '';
      let stderr = '';

      proc.stdout.on('data', (data) => {
        stdout += data.toString();
      });

      proc.stderr.on('data', (data) => {
        stderr += data.toString();
      });

      proc.on('close', (code) => {
        if (code === 0 || code === null) {
          try {
            const results = stdout.trim() ? JSON.parse(stdout) : [];
            const normalized = results.map(r => ({
              file_path: r.file,
              line_start: r.range?.start?.line || 0,
              line_end: r.range?.end?.line || 0,
              matched_text: r.text || r.lines || '',
              meta_variables: r.metaVars || {}
            }));
            resolve(normalized);
          } catch (error) {
            this.logger.warn(`Failed to parse ast-grep output: ${error.message}`);
            resolve([]);
          }
        } else {
          reject(new Error(`ast-grep exited with code ${code}: ${stderr}`));
        }
      });

      proc.on('error', (error) => {
        const nodeError = /** @type {NodeError} */ (error);
        if (nodeError.code === 'ENOENT') {
          reject(new Error('ast-grep (sg) not found. Install: npm install -g @ast-grep/cli'));
        } else {
          reject(error);
        }
      });
    });
  }

  /**
   * Read file contents
   */
  async readFile(filePath) {
    try {
      return await fs.readFile(filePath, 'utf-8');
    } catch {
      return '';
    }
  }

  /**
   * Group matches by file
   */
  groupByFile(matches) {
    const groups = {};
    for (const match of matches) {
      if (!groups[match.file_path]) {
        groups[match.file_path] = [];
      }
      groups[match.file_path].push(match);
    }
    return groups;
  }

  /**
   * Calculate severity breakdown
   */
  calculateSeverityBreakdown(findings) {
    const breakdown = { high: 0, medium: 0, low: 0, info: 0 };

    Object.values(findings)
      .filter(Array.isArray)
      .flat()
      .forEach(finding => {
        const severity = finding.severity || 'info';
        breakdown[severity] = (breakdown[severity] || 0) + 1;
      });

    return breakdown;
  }

  /**
   * Generate report
   */
  generateReport(findings) {
    const lines = [
      '# Timeout Pattern Detection Report',
      '',
      `**Generated:** ${new Date().toISOString()}`,
      `**Total Issues:** ${findings.statistics.total_issues}`,
      `**Affected Files:** ${findings.statistics.affected_files}`,
      `**Scan Duration:** ${findings.statistics.scan_duration_ms}ms`,
      '',
      '## Severity Breakdown',
      ''
    ];

    Object.entries(findings.statistics.severity_breakdown).forEach(([severity, count]) => {
      if (count > 0) {
        const emoji = { high: 'üî¥', medium: 'üü°', low: 'üü¢', info: 'üîµ' }[severity];
        lines.push(`- ${emoji} **${severity.toUpperCase()}:** ${count}`);
      }
    });

    lines.push('', '## Findings by Category', '');

    const categories = [
      { key: 'promiseRaceWithoutTimeout', title: 'Promise.race() Without Timeout' },
      { key: 'loadingWithoutFinally', title: 'Loading State Without Finally Block' },
      { key: 'asyncWithoutErrorHandling', title: 'Async Without Error Handling' },
      { key: 'missingTimeoutConstants', title: 'Missing Timeout Utilities' },
      { key: 'setLoadingWithoutReset', title: 'Loading Without Safety Net' }
    ];

    categories.forEach(({ key, title }) => {
      const items = findings[key] || [];
      if (items.length > 0) {
        lines.push(`### ${title} (${items.length})`, '');
        items.forEach((item, i) => {
          lines.push(
            `**${i + 1}. ${item.file_path}:${item.line_start}**`,
            `- **Severity:** ${item.severity}`,
            `- **Message:** ${item.message}`,
            `- **Recommendation:** ${item.recommendation}`,
            ''
          );

          if (item.fix_example) {
            lines.push('```typescript', item.fix_example.trim(), '```', '');
          }
        });
      }
    });

    return lines.join('\n');
  }
}

/**
 * Export scanner instance creator
 */
export function createTimeoutPatternDetector(options) {
  return new TimeoutPatternDetector(options);
}
</file>

<file path="pipeline-core/similarity/__init__.py">
"""
Similarity calculation modules for duplicate detection
"""

from .structural import calculate_structural_similarity, normalize_code
from .grouping import group_by_similarity

__all__ = [
    'calculate_structural_similarity',
    'normalize_code',
    'group_by_similarity',
]
</file>

<file path="pipeline-core/similarity/config.py">
"""
Similarity Algorithm Configuration

Centralized configuration for all similarity thresholds and parameters.
Allows tuning without modifying code.
"""

import os


class SimilarityConfig:
    """Configuration for multi-layer similarity algorithm."""

    # Layer 0: Complexity filtering
    MIN_LINE_COUNT = int(os.getenv('MIN_LINE_COUNT', '1'))
    MIN_UNIQUE_TOKENS = int(os.getenv('MIN_UNIQUE_TOKENS', '3'))

    # Layer 2: Structural similarity
    STRUCTURAL_THRESHOLD = float(os.getenv('STRUCTURAL_THRESHOLD', '0.90'))

    # Penalties
    OPPOSITE_LOGIC_PENALTY = float(os.getenv('OPPOSITE_LOGIC_PENALTY', '0.8'))
    STATUS_CODE_PENALTY = float(os.getenv('STATUS_CODE_PENALTY', '0.7'))
    SEMANTIC_METHOD_PENALTY = float(os.getenv('SEMANTIC_METHOD_PENALTY', '0.85'))

    # Method chain validation
    CHAIN_WEIGHT_LEVENSHTEIN = float(os.getenv('CHAIN_WEIGHT_LEVENSHTEIN', '0.7'))
    CHAIN_WEIGHT_CHAIN = float(os.getenv('CHAIN_WEIGHT_CHAIN', '0.3'))

    # Layer 3: Semantic validation
    MIN_COMPLEXITY_RATIO = float(os.getenv('MIN_COMPLEXITY_RATIO', '0.5'))

    # Layer 4: Quality filtering
    MIN_GROUP_QUALITY = float(os.getenv('MIN_GROUP_QUALITY', '0.70'))

    # Quality score weights
    QUALITY_WEIGHT_SIMILARITY = float(os.getenv('QUALITY_WEIGHT_SIMILARITY', '0.4'))
    QUALITY_WEIGHT_SIZE = float(os.getenv('QUALITY_WEIGHT_SIZE', '0.2'))
    QUALITY_WEIGHT_COMPLEXITY = float(os.getenv('QUALITY_WEIGHT_COMPLEXITY', '0.2'))
    QUALITY_WEIGHT_SEMANTIC = float(os.getenv('QUALITY_WEIGHT_SEMANTIC', '0.2'))

    @classmethod
    def to_dict(cls) -> dict:
        """Export configuration as dictionary."""
        return {
            'complexity': {
                'min_line_count': cls.MIN_LINE_COUNT,
                'min_unique_tokens': cls.MIN_UNIQUE_TOKENS,
            },
            'structural': {
                'threshold': cls.STRUCTURAL_THRESHOLD,
                'opposite_logic_penalty': cls.OPPOSITE_LOGIC_PENALTY,
                'status_code_penalty': cls.STATUS_CODE_PENALTY,
                'semantic_method_penalty': cls.SEMANTIC_METHOD_PENALTY,
            },
            'chain_validation': {
                'levenshtein_weight': cls.CHAIN_WEIGHT_LEVENSHTEIN,
                'chain_weight': cls.CHAIN_WEIGHT_CHAIN,
            },
            'semantic': {
                'min_complexity_ratio': cls.MIN_COMPLEXITY_RATIO,
            },
            'quality': {
                'min_group_quality': cls.MIN_GROUP_QUALITY,
                'weights': {
                    'similarity': cls.QUALITY_WEIGHT_SIMILARITY,
                    'size': cls.QUALITY_WEIGHT_SIZE,
                    'complexity': cls.QUALITY_WEIGHT_COMPLEXITY,
                    'semantic': cls.QUALITY_WEIGHT_SEMANTIC,
                },
            },
        }

    @classmethod
    def print_config(cls):
        """Print current configuration."""
        import json
        print("=== Similarity Algorithm Configuration ===")
        print(json.dumps(cls.to_dict(), indent=2))
        print("=" * 42)


# Global config instance
config = SimilarityConfig()
</file>

<file path="pipeline-core/similarity/grouping.py">
"""
Multi-Layer Similarity Grouping

Priority 2: Implement multi-layer grouping algorithm from Phase 1 design.

Combines:
- Layer 1: Exact matching (hash-based)
- Layer 2: Structural similarity (AST-based)
- Layer 3: Semantic equivalence (category + tags) [TODO]
"""

from typing import List, Dict, Set
from collections import defaultdict
from pathlib import Path
import sys
import os

# Debug mode - set PIPELINE_DEBUG=1 to enable verbose output
DEBUG = os.environ.get('PIPELINE_DEBUG', '').lower() in ('1', 'true', 'yes')

# Import models from correct path (relative to pipeline-core)
sys.path.insert(0, str(Path(__file__).parent.parent / 'models'))
sys.path.insert(0, str(Path(__file__).parent.parent))

from code_block import CodeBlock
from duplicate_group import DuplicateGroup

from .structural import (
    calculate_structural_similarity,
    calculate_ast_hash,
    extract_logical_operators,
    extract_http_status_codes,
    extract_semantic_methods,
    extract_method_chain
)
from .semantic import are_semantically_compatible, validate_duplicate_group

# Minimum complexity threshold for duplicate detection
# IMPORTANT: Keep this low to avoid filtering out genuine duplicates
MIN_COMPLEXITY_THRESHOLD = {
    'min_line_count': 1,  # At least 1 line of code (very permissive)
    'min_unique_tokens': 3,  # At least 3 meaningful tokens (very permissive)
}

# Minimum quality threshold for duplicate groups
MIN_GROUP_QUALITY = float(os.getenv('MIN_GROUP_QUALITY', '0.70'))  # Groups must score at least 70% quality


def calculate_code_complexity(source_code: str) -> dict:
    """
    Calculate basic complexity metrics for code block.

    Returns:
        {
            'line_count': int,
            'unique_tokens': int,
            'has_control_flow': bool
        }
    """
    import re

    lines = [line.strip() for line in source_code.split('\n') if line.strip()]
    line_count = len(lines)

    # Count unique tokens (simple tokenization)
    tokens = re.findall(r'\b\w+\b', source_code)
    unique_tokens = len(set(tokens))

    # Check for control flow
    control_flow_keywords = ['if', 'else', 'for', 'while', 'switch', 'case', 'try', 'catch']
    has_control_flow = any(keyword in source_code for keyword in control_flow_keywords)

    return {
        'line_count': line_count,
        'unique_tokens': unique_tokens,
        'has_control_flow': has_control_flow
    }


def is_complex_enough(block: 'CodeBlock') -> bool:
    """
    Check if block meets minimum complexity threshold.

    Trivial code (e.g., `return user.name;`) should not be grouped
    unless there are many occurrences.
    """
    complexity = calculate_code_complexity(block.source_code)

    # Must meet minimum thresholds
    if complexity['line_count'] < MIN_COMPLEXITY_THRESHOLD['min_line_count']:
        return False

    if complexity['unique_tokens'] < MIN_COMPLEXITY_THRESHOLD['min_unique_tokens']:
        # Exception: If has control flow, allow lower token count
        if not complexity['has_control_flow']:
            return False

    return True


def calculate_group_quality_score(group_blocks: List['CodeBlock'], similarity_score: float) -> float:
    """
    Calculate quality score for a duplicate group.

    Factors:
    - Average similarity score (40%)
    - Group size (20%)
    - Code complexity (20%)
    - Semantic consistency (20%)

    Returns:
        Quality score 0.0-1.0
    """
    if not group_blocks or len(group_blocks) < 2:
        return 0.0

    # Factor 1: Similarity score (40% weight)
    similarity_factor = similarity_score * 0.4

    # Factor 2: Group size (20% weight)
    # Larger groups more likely to be genuine duplicates
    # 2 members = 0.5, 3 members = 0.75, 4+ members = 1.0
    size_factor = min(len(group_blocks) / 4.0, 1.0) * 0.2

    # Factor 3: Code complexity (20% weight)
    # More complex code = higher confidence in duplicate
    avg_line_count = sum(b.line_count for b in group_blocks) / len(group_blocks)
    complexity_factor = min(avg_line_count / 10.0, 1.0) * 0.2

    # Factor 4: Semantic consistency (20% weight)
    # All blocks same category = 1.0, mixed = lower
    categories = set(b.category for b in group_blocks)
    pattern_ids = set(b.pattern_id for b in group_blocks)

    semantic_factor = 0.0
    if len(categories) == 1 and len(pattern_ids) == 1:
        semantic_factor = 1.0 * 0.2  # Perfect consistency
    elif len(categories) == 1:
        semantic_factor = 0.7 * 0.2  # Same category, different patterns
    elif len(pattern_ids) == 1:
        semantic_factor = 0.5 * 0.2  # Same pattern, different categories
    else:
        semantic_factor = 0.3 * 0.2  # Mixed

    total_quality = similarity_factor + size_factor + complexity_factor + semantic_factor

    return total_quality


def validate_exact_group_semantics(group_blocks: List['CodeBlock']) -> tuple:
    """
    Validate that exact hash matches don't have semantic differences.

    This prevents Layer 1 from bypassing semantic validation that exists in Layer 2.
    Checks for:
    - Method chain differences (e.g., .reverse())
    - HTTP status code differences (201 vs 200)
    - Opposite logical operators (!== vs ===)
    - Semantic method opposites (Math.max vs Math.min)

    Returns:
        (is_valid, reason) - False if semantic differences detected
    """
    if len(group_blocks) < 2:
        return True, "single_block"

    # Extract function names for logging
    func_names = []
    for block in group_blocks:
        for tag in block.tags:
            if tag.startswith('function:'):
                func_names.append(tag[9:])
                break

    # Check all pairs for semantic differences
    for i in range(len(group_blocks)):
        for j in range(i + 1, len(group_blocks)):
            code1 = group_blocks[i].source_code
            code2 = group_blocks[j].source_code

            # Check 1: Method chain differences
            chain1 = extract_method_chain(code1)
            chain2 = extract_method_chain(code2)
            if chain1 != chain2:
                print(f"DEBUG: Layer 1 REJECTED - Method chain mismatch: {func_names}", file=sys.stderr)
                print(f"       {chain1} vs {chain2}", file=sys.stderr)
                return False, f"method_chain_mismatch: {chain1} vs {chain2}"

            # Check 2: HTTP status code differences
            status1 = extract_http_status_codes(code1)
            status2 = extract_http_status_codes(code2)
            if status1 and status2 and status1 != status2:
                print(f"DEBUG: Layer 1 REJECTED - Status code mismatch: {func_names}", file=sys.stderr)
                print(f"       {status1} vs {status2}", file=sys.stderr)
                return False, f"status_code_mismatch: {status1} vs {status2}"

            # Check 3: Logical operator opposites
            ops1 = extract_logical_operators(code1)
            ops2 = extract_logical_operators(code2)
            opposite_pairs = [
                ({'==='}, {'!=='}),
                ({'=='}, {'!='}),
            ]
            for pair1, pair2 in opposite_pairs:
                if (pair1.issubset(ops1) and pair2.issubset(ops2)) or \
                   (pair2.issubset(ops1) and pair1.issubset(ops2)):
                    print(f"DEBUG: Layer 1 REJECTED - Opposite logic: {func_names}", file=sys.stderr)
                    print(f"       {ops1} vs {ops2}", file=sys.stderr)
                    return False, f"opposite_logic: {ops1} vs {ops2}"

            # Check 4: Semantic method opposites (Math.max vs Math.min)
            methods1 = extract_semantic_methods(code1)
            methods2 = extract_semantic_methods(code2)
            if methods1 and methods2 and methods1 != methods2:
                print(f"DEBUG: Layer 1 REJECTED - Semantic method mismatch: {func_names}", file=sys.stderr)
                print(f"       {methods1} vs {methods2}", file=sys.stderr)
                return False, f"semantic_method_mismatch: {methods1} vs {methods2}"

    return True, "semantically_compatible"


def group_by_similarity(
    blocks: List['CodeBlock'],
    similarity_threshold: float = 0.90
) -> List['DuplicateGroup']:
    """
    Group code blocks using multi-layer similarity algorithm with complexity filtering.

    Implements Layer 0 (complexity), Layer 1 (exact), Layer 2 (structural), and Layer 3 (semantic).

    Algorithm:
    0. Layer 0: Filter trivial blocks (below complexity threshold)
    1. Layer 1: Group by exact content hash (O(n))
    2. Layer 2: Group remaining by structural similarity (O(n*k))
    3. Layer 3: Semantic validation (pattern, category, tags)

    Returns:
        List of DuplicateGroup objects with similarity scores
    """
    # Layer 0: Filter out trivial blocks before grouping
    complex_blocks = [b for b in blocks if is_complex_enough(b)]
    trivial_count = len(blocks) - len(complex_blocks)

    if trivial_count > 0:
        print(f"Layer 0: Filtered {trivial_count} trivial blocks (below complexity threshold)", file=sys.stderr)

    groups = []
    grouped_block_ids = set()

    # Layer 1: Exact matching (hash-based)
    print(f"Layer 1: Grouping by exact content hash...", file=sys.stderr)
    exact_groups = _group_by_exact_hash(complex_blocks)

    for hash_val, group_blocks in exact_groups.items():
        if len(group_blocks) >= 2:
            # Extract function names for debug logging
            func_names = []
            for block in group_blocks:
                for tag in block.tags:
                    if tag.startswith('function:'):
                        func_names.append(tag[9:])
                        break

            print(f"DEBUG: Layer 1 exact group candidate: {func_names} (hash={hash_val[:8]})", file=sys.stderr)

            # ‚úÖ NEW: Validate semantic compatibility BEFORE accepting group
            is_valid, reason = validate_exact_group_semantics(group_blocks)
            if not is_valid:
                print(f"DEBUG: Layer 1 group REJECTED (semantic): {func_names} - {reason}", file=sys.stderr)
                continue  # Skip this group - semantic mismatch detected

            # Check group quality
            quality_score = calculate_group_quality_score(group_blocks, 1.0)

            if quality_score >= MIN_GROUP_QUALITY:
                group = _create_duplicate_group(
                    group_blocks,
                    similarity_score=1.0,
                    similarity_method='exact_match'  # Must match pydantic enum
                )
                groups.append(group)
                print(f"DEBUG: Layer 1 group ACCEPTED: {func_names} (quality={quality_score:.2f})", file=sys.stderr)

                # Mark these blocks as grouped
                for block in group_blocks:
                    grouped_block_ids.add(block.block_id)
            else:
                print(f"DEBUG: Layer 1 group REJECTED (quality): {func_names} (quality={quality_score:.2f})", file=sys.stderr)

    print(f"Layer 1: Found {len(groups)} exact duplicate groups", file=sys.stderr)

    # Layer 2: Structural similarity (for ungrouped blocks)
    ungrouped_blocks = [b for b in complex_blocks if b.block_id not in grouped_block_ids]
    print(f"Layer 2: Checking {len(ungrouped_blocks)} remaining blocks for structural similarity...", file=sys.stderr)

    structural_groups = _group_by_structural_similarity(
        ungrouped_blocks,
        similarity_threshold
    )

    for group_blocks, similarity_score in structural_groups:
        if len(group_blocks) >= 2:
            # Check group quality
            quality_score = calculate_group_quality_score(group_blocks, similarity_score)

            if quality_score >= MIN_GROUP_QUALITY:
                group = _create_duplicate_group(
                    group_blocks,
                    similarity_score=similarity_score,
                    similarity_method='structural'
                )
                groups.append(group)
            else:
                print(f"Warning: Structural group rejected (quality={quality_score:.2f} < {MIN_GROUP_QUALITY}): {[b.block_id for b in group_blocks]}", file=sys.stderr)

            # Mark these blocks as grouped
            for block in group_blocks:
                grouped_block_ids.add(block.block_id)

    print(f"Layer 2: Found {len(structural_groups)} structural duplicate groups", file=sys.stderr)

    # TODO: Layer 3 - Semantic similarity
    # Group remaining blocks by category + semantic tags

    print(f"Total: {len(groups)} duplicate groups found", file=sys.stderr)
    return groups


def _group_by_exact_hash(blocks: List['CodeBlock']) -> Dict[str, List['CodeBlock']]:
    """Group blocks by exact content hash."""
    hash_groups = defaultdict(list)

    for i, block in enumerate(blocks):
        hash_val = block.content_hash
        hash_groups[hash_val].append(block)

        # Debug: Show first 20 blocks and their hashes
        if i < 20:
            func_name = None
            for tag in block.tags:
                if tag.startswith('function:'):
                    func_name = tag[9:]
                    break
            print(f"Warning: DEBUG hash block {i}: {func_name or 'unknown'} at {block.location.file_path}:{block.location.line_start}, hash={hash_val[:8]}, code_len={len(block.source_code)}", file=sys.stderr)

    return hash_groups


def _group_by_structural_similarity(
    blocks: List['CodeBlock'],
    threshold: float
) -> List[tuple[List['CodeBlock'], float]]:
    """
    Group blocks by structural similarity using clustering.

    Returns:
        List of (group_blocks, similarity_score) tuples
    """
    if not blocks:
        return []

    # Build similarity matrix
    n = len(blocks)
    groups = []
    used = set()

    # For each block, find all structurally similar blocks
    for i, block1 in enumerate(blocks):
        if i in used:
            continue

        # Start a new group with this block
        group = [block1]
        similarities = []

        # Compare with remaining blocks
        for j in range(i + 1, n):
            if j in used:
                continue

            block2 = blocks[j]

            # Pre-check semantic compatibility
            if not are_semantically_compatible(block1, block2):
                continue  # Skip incompatible blocks

            # Calculate structural similarity
            similarity, method = calculate_structural_similarity(
                block1.source_code,
                block2.source_code,
                threshold
            )

            if similarity >= threshold:
                group.append(block2)
                similarities.append(similarity)
                used.add(j)

        # If we found similar blocks, validate and create a group
        if len(group) >= 2:
            # Validate complete group
            if validate_duplicate_group(group):
                used.add(i)
                # Average similarity score for the group
                avg_similarity = sum(similarities) / len(similarities) if similarities else 1.0
                groups.append((group, avg_similarity))
            else:
                # Group failed semantic validation
                print(f"Warning: Group rejected by semantic validation: {[b.block_id for b in group]}", file=sys.stderr)

    return groups


def _create_duplicate_group(
    blocks: List['CodeBlock'],
    similarity_score: float,
    similarity_method: str
) -> 'DuplicateGroup':
    """Create a DuplicateGroup from a list of similar blocks."""

    return DuplicateGroup(
        group_id=f"dg_{blocks[0].content_hash[:12]}",
        pattern_id=blocks[0].pattern_id,
        member_block_ids=[b.block_id for b in blocks],
        similarity_score=similarity_score,
        similarity_method=similarity_method,
        category=blocks[0].category,
        language=blocks[0].language,
        occurrence_count=len(blocks),
        total_lines=sum(b.line_count for b in blocks),
        affected_files=list(set(b.location.file_path for b in blocks)),
        affected_repositories=list(set(b.repository_path for b in blocks))
    )
</file>

<file path="pipeline-core/similarity/semantic.py">
"""
Semantic Similarity Validation

Layer 3 of the multi-layer similarity algorithm.
Validates that structurally similar code blocks are also semantically equivalent.
"""

from typing import List, Optional
from pathlib import Path
import sys

# Import models from correct path (relative to pipeline-core)
sys.path.insert(0, str(Path(__file__).parent.parent / 'models'))
sys.path.insert(0, str(Path(__file__).parent.parent))

from code_block import CodeBlock, SemanticCategory


def are_semantically_compatible(block1: 'CodeBlock', block2: 'CodeBlock') -> bool:
    """
    Check if two code blocks are semantically compatible for grouping.

    Returns True if blocks can be considered semantic duplicates.

    Validation checks:
    1. Same pattern_id (ast-grep rule)
    2. Same category (semantic categorization)
    3. Compatible tags (if present)
    4. Similar complexity (line count within 50%)
    """

    # Check 1: Must match same ast-grep pattern
    if block1.pattern_id != block2.pattern_id:
        return False

    # Check 2: Must be same semantic category
    if block1.category != block2.category:
        return False

    # Check 3: Tag compatibility
    # If both have function tags, they must be different functions
    # (same function in same file = already deduplicated)
    tags1 = set(block1.tags)
    tags2 = set(block2.tags)

    func1 = _extract_function_tag(tags1)
    func2 = _extract_function_tag(tags2)

    if func1 and func2:
        # Both are named functions
        if func1 == func2 and block1.location.file_path == block2.location.file_path:
            # Same function in same file ‚Üí already deduplicated, should not group
            return False

    # Check 4: Complexity similarity
    # Blocks should have similar size (within 50% difference)
    line_ratio = min(block1.line_count, block2.line_count) / max(block1.line_count, block2.line_count)
    if line_ratio < 0.5:
        # One block is more than 2x the size of the other
        return False

    return True


def calculate_tag_overlap(block1: 'CodeBlock', block2: 'CodeBlock') -> float:
    """
    Calculate semantic tag overlap between two blocks.

    Returns:
        Overlap ratio 0.0-1.0
    """
    tags1 = set(block1.tags)
    tags2 = set(block2.tags)

    if not tags1 and not tags2:
        return 1.0  # No tags on either

    if not tags1 or not tags2:
        return 0.5  # One has tags, other doesn't

    # Calculate Jaccard similarity
    intersection = tags1 & tags2
    union = tags1 | tags2

    return len(intersection) / len(union) if union else 0.0


def _extract_function_tag(tags: set) -> Optional[str]:
    """Extract function name from tag set."""
    for tag in tags:
        if tag.startswith('function:'):
            return tag[9:]  # Remove 'function:' prefix
    return None


def validate_duplicate_group(blocks: List['CodeBlock']) -> bool:
    """
    Validate that a group of blocks are truly semantic duplicates.

    All blocks must:
    - Match same pattern
    - Have same category
    - Be semantically compatible pairwise
    """
    if len(blocks) < 2:
        return False

    # All blocks must have same pattern_id
    pattern_ids = set(b.pattern_id for b in blocks)
    if len(pattern_ids) > 1:
        return False

    # All blocks must have same category
    categories = set(b.category for b in blocks)
    if len(categories) > 1:
        return False

    # Pairwise semantic compatibility
    for i, block1 in enumerate(blocks):
        for block2 in blocks[i+1:]:
            if not are_semantically_compatible(block1, block2):
                return False

    return True
</file>

<file path="pipeline-core/similarity/structural.py">
"""
Structural Similarity Calculation

Priority 2: Implement Structural Similarity
Compares code based on AST structure, ignoring variable names and minor differences.
"""

import re
import sys
import hashlib
from typing import Tuple, Set
from difflib import SequenceMatcher
from dataclasses import dataclass, field


@dataclass
class SemanticFeatures:
    """
    Semantic features extracted from original code before normalization.

    These features preserve semantic information that would be lost during
    normalization (e.g., HTTP status codes, logical operators, method names).
    """
    http_status_codes: Set[int] = field(default_factory=set)
    logical_operators: Set[str] = field(default_factory=set)
    semantic_methods: Set[str] = field(default_factory=set)


def extract_semantic_features(source_code: str) -> SemanticFeatures:
    """
    Extract all semantic features from ORIGINAL code before normalization.

    This function MUST be called before normalize_code() to preserve semantic
    information that would otherwise be stripped away.

    Args:
        source_code: Raw, unnormalized source code

    Returns:
        SemanticFeatures containing all detected semantic markers

    Example:
        code = "res.status(201).json({ data: user });"
        features = extract_semantic_features(code)
        # features.http_status_codes = {201}
    """
    features = SemanticFeatures()

    if not source_code:
        return features

    # Extract HTTP status codes (e.g., .status(200), .status(404))
    status_pattern = r'\.status\((\d{3})\)'
    for match in re.finditer(status_pattern, source_code):
        status_code = int(match.group(1))
        features.http_status_codes.add(status_code)

    # Extract logical operators (===, !==, ==, !=, !, &&, ||)
    # Order matters: match longer operators first to avoid partial matches
    operator_patterns = [
        (r'!==', '!=='),   # Strict inequality
        (r'===', '==='),   # Strict equality
        (r'!=', '!='),     # Loose inequality
        (r'==', '=='),     # Loose equality
        (r'!\s*[^=]', '!'), # Logical NOT (followed by non-=)
        (r'&&', '&&'),     # Logical AND
        (r'\|\|', '||'),   # Logical OR
    ]

    for pattern, operator_name in operator_patterns:
        if re.search(pattern, source_code):
            features.logical_operators.add(operator_name)

    # Extract semantic methods (Math.max, Math.min, console.log, etc.)
    semantic_patterns = {
        'Math.max': r'Math\.max\s*\(',
        'Math.min': r'Math\.min\s*\(',
        'Math.floor': r'Math\.floor\s*\(',
        'Math.ceil': r'Math\.ceil\s*\(',
        'Math.round': r'Math\.round\s*\(',
        'console.log': r'console\.log\s*\(',
        'console.error': r'console\.error\s*\(',
        'console.warn': r'console\.warn\s*\(',
        '.reverse': r'\.reverse\s*\(',
        '.toUpperCase': r'\.toUpperCase\s*\(',
        '.toLowerCase': r'\.toLowerCase\s*\(',
    }

    for method_name, pattern in semantic_patterns.items():
        if re.search(pattern, source_code):
            features.semantic_methods.add(method_name)

    return features


def normalize_code(source_code: str) -> str:
    """
    Normalize code by removing variable-specific information.

    This allows structural comparison by focusing on code structure
    rather than specific names or values.
    """
    if not source_code:
        return ""

    # Remove comments
    normalized = re.sub(r'//.*?$', '', source_code, flags=re.MULTILINE)  # Single-line comments
    normalized = re.sub(r'/\*.*?\*/', '', normalized, flags=re.DOTALL)   # Multi-line comments

    # Normalize whitespace (collapse to single spaces)
    normalized = re.sub(r'\s+', ' ', normalized)

    # Normalize string literals (replace with placeholder)
    normalized = re.sub(r"'[^']*'", "'STR'", normalized)
    normalized = re.sub(r'"[^"]*"', '"STR"', normalized)
    normalized = re.sub(r'`[^`]*`', '`STR`', normalized)

    # Normalize numbers (replace with placeholder)
    normalized = re.sub(r'\b\d+\b', 'NUM', normalized)

    # Semantic operator and method whitelist
    # These methods have semantic meaning and should be preserved during normalization
    SEMANTIC_METHODS = {
        # Array functional methods (already preserved)
        'map', 'filter', 'reduce', 'forEach', 'find', 'some', 'every',
        'slice', 'splice', 'push', 'pop', 'shift', 'unshift',
        'join', 'split', 'includes', 'indexOf',

        # Object methods (already preserved)
        'get', 'set', 'has', 'delete',
        'keys', 'values', 'entries',

        # Async patterns (already preserved)
        'then', 'catch', 'finally', 'async', 'await',

        # Array transformations (already preserved)
        'reverse', 'sort', 'concat',

        # NEW: Math operations (opposite semantics)
        'max', 'min', 'abs', 'floor', 'ceil', 'round',

        # NEW: String operations (semantic meaning)
        'trim', 'toLowerCase', 'toUpperCase', 'replace',

        # NEW: HTTP/API methods (semantic differences)
        'status', 'json', 'send', 'redirect',

        # NEW: Properties with semantic value
        'length', 'name', 'value', 'id', 'type'
    }

    # Preserve important objects
    SEMANTIC_OBJECTS = {
        'Math', 'Object', 'Array', 'String', 'Number', 'Boolean',
        'console', 'process', 'JSON', 'Date', 'Promise'
    }

    # Build a pattern that matches identifiers but NOT important methods/objects
    # First, mark important objects for preservation (Math, Object, etc.)
    for obj in SEMANTIC_OBJECTS:
        normalized = re.sub(rf'\b{obj}\b', f'__PRESERVE_OBJ_{obj.upper()}__', normalized)

    # Second, mark important methods so they're not replaced
    for method in SEMANTIC_METHODS:
        normalized = re.sub(rf'\b{method}\b', f'__PRESERVE_{method.upper()}__', normalized)

    # Now normalize other identifiers
    normalized = re.sub(r'\b[a-z][a-zA-Z0-9_]*\b', 'var', normalized)
    normalized = re.sub(r'\b[A-Z][A-Z0-9_]*\b', 'CONST', normalized)

    # Restore preserved objects
    for obj in SEMANTIC_OBJECTS:
        normalized = normalized.replace(f'__PRESERVE_OBJ_{obj.upper()}__', obj)

    # Restore preserved methods in lowercase
    for method in SEMANTIC_METHODS:
        normalized = normalized.replace(f'__PRESERVE_{method.upper()}__', method)

    # Remove extra spaces around operators and punctuation
    normalized = re.sub(r'\s*([(){}[\];,.])\s*', r'\1', normalized)
    normalized = re.sub(r'\s*(=>|===?|!==?|[+\-*/%<>=&|])\s*', r' \1 ', normalized)

    # Collapse multiple spaces
    normalized = re.sub(r'\s+', ' ', normalized)

    # Trim
    normalized = normalized.strip()

    return normalized


def calculate_ast_hash(source_code: str) -> str:
    """
    Calculate a hash based on normalized code structure.

    This creates a structural fingerprint that's the same for
    code with the same structure but different variable names.
    """
    normalized = normalize_code(source_code)
    return hashlib.sha256(normalized.encode()).hexdigest()


def calculate_levenshtein_similarity(str1: str, str2: str) -> float:
    """
    Calculate similarity using Levenshtein distance via SequenceMatcher.

    Returns a similarity ratio between 0.0 and 1.0.
    """
    if not str1 or not str2:
        return 0.0

    return SequenceMatcher(None, str1, str2).ratio()


def extract_logical_operators(source_code: str) -> set:
    """
    Extract logical operators from source code for semantic comparison.

    Returns:
        Set of logical operators found (e.g., {'===', '!=='})
    """
    operators = set()

    # Find all logical operators
    # Check compound operators first (order matters)
    if '!==' in source_code:
        operators.add('!==')
    if '===' in source_code:
        operators.add('===')
    if '!=' in source_code and '!==' not in source_code:
        operators.add('!=')
    if '==' in source_code and '===' not in source_code:
        operators.add('==')

    # Match standalone ! (not part of !== or !=)
    if re.search(r'(?<![!=])!(?![=])', source_code):
        operators.add('!')

    return operators


def extract_http_status_codes(source_code: str) -> set:
    """
    Extract HTTP status codes from response patterns.

    Returns:
        Set of status codes (e.g., {200, 201, 404})
    """
    status_codes = set()

    # Pattern: res.status(200), response.status(404), etc.
    pattern = r'(?:res|response)\.status\((\d{3})\)'
    matches = re.finditer(pattern, source_code)

    for match in matches:
        status_codes.add(int(match.group(1)))

    return status_codes


def extract_semantic_methods(source_code: str) -> set:
    """
    Extract critical semantic methods that indicate opposite or different behavior.

    Returns:
        Set of semantic methods found (e.g., {'Math.max', 'Math.min'})
    """
    methods = set()

    # Critical Math methods with opposite semantics
    math_opposites = ['max', 'min', 'floor', 'ceil']
    for method in math_opposites:
        if f'Math.{method}' in source_code:
            methods.add(f'Math.{method}')

    return methods


def extract_method_chain(source_code: str) -> list:
    """
    Extract method chain structure from code.

    Returns:
        List of methods in chain order (e.g., ['filter', 'map', 'reverse'])
    """
    # Pattern: .method1().method2().method3()
    # Handles both arr.filter().map() and arr.filter(fn).map(fn)

    chains = []

    # Find chained method calls
    # Pattern: .method_name( ... ).method_name( ... )
    pattern = r'\.([a-zA-Z_][a-zA-Z0-9_]*)\s*\('
    matches = list(re.finditer(pattern, source_code))

    if not matches:
        return []

    # Build chains by tracking consecutive method calls
    current_chain = []
    last_pos = -1

    for i, match in enumerate(matches):
        method_name = match.group(1)

        # Check if this is part of a chain (close to previous match)
        # Allow up to 100 characters between methods for complex arguments
        if current_chain and match.start() - last_pos > 100:
            # Too far apart, start a new chain
            if len(current_chain) > 1:  # Only save actual chains (2+ methods)
                chains.append(current_chain)
            current_chain = [method_name]
        else:
            current_chain.append(method_name)

        # Find end of this method call (rough approximation)
        # Look for closing paren or next method
        if i < len(matches) - 1:
            last_pos = matches[i + 1].start()
        else:
            last_pos = len(source_code)

    # Add the last chain if it has multiple methods
    if len(current_chain) > 1:
        chains.append(current_chain)

    # Return longest chain (most significant)
    return max(chains, key=len) if chains else []


def compare_method_chains(code1: str, code2: str) -> float:
    """
    Compare method chain structure between two code blocks.

    Returns:
        Similarity score 0.0-1.0 based on chain overlap
    """
    chain1 = extract_method_chain(code1)
    chain2 = extract_method_chain(code2)

    if not chain1 and not chain2:
        return 1.0  # No chains in either

    if not chain1 or not chain2:
        return 0.5  # One has chain, other doesn't

    # Exact match
    if chain1 == chain2:
        return 1.0

    # Check if one is a subset of the other (additional operation)
    if len(chain1) != len(chain2):
        # Different length chains ‚Üí likely different behavior
        # e.g., [filter, map] vs [filter, map, reverse]

        # Check if shorter is prefix of longer
        shorter = chain1 if len(chain1) < len(chain2) else chain2
        longer = chain1 if len(chain1) > len(chain2) else chain2

        if longer[:len(shorter)] == shorter:
            # One is extension of other ‚Üí partial match
            # Similarity based on overlap ratio
            return len(shorter) / len(longer)
        else:
            # Different chains entirely
            return 0.0

    # Same length but different methods ‚Üí check overlap
    overlap = sum(1 for m1, m2 in zip(chain1, chain2) if m1 == m2)
    return overlap / len(chain1)


def calculate_semantic_penalty(features1: SemanticFeatures, features2: SemanticFeatures) -> float:
    """
    Calculate combined semantic penalty based on extracted features.

    Penalties are multiplicative - each mismatch reduces similarity:
    - HTTP status codes: 0.70x (30% penalty)
    - Logical operators: 0.80x (20% penalty)
    - Semantic methods: 0.75x (25% penalty)

    Args:
        features1: Semantic features from first code block
        features2: Semantic features from second code block

    Returns:
        Penalty multiplier (0.0-1.0). Returns 1.0 if no penalties apply.

    Example:
        Different status codes (200 vs 404) + different operators (=== vs !==)
        = 0.70 * 0.80 = 0.56x final similarity
    """
    penalty = 1.0

    # Penalty 1: HTTP Status Code Mismatch (30% penalty)
    if features1.http_status_codes and features2.http_status_codes:
        if features1.http_status_codes != features2.http_status_codes:
            # Different status codes indicate different semantic intent
            # (e.g., 200 OK vs 201 Created vs 404 Not Found)
            penalty *= 0.70
            print(f"Warning: DEBUG: HTTP status code penalty: {features1.http_status_codes} vs {features2.http_status_codes}, penalty={penalty:.2f}", file=sys.stderr)

    # Penalty 2: Logical Operator Mismatch (20% penalty)
    if features1.logical_operators and features2.logical_operators:
        if features1.logical_operators != features2.logical_operators:
            # Different logical operators indicate different boolean logic
            # (e.g., === vs !==, && vs ||)
            penalty *= 0.80
            print(f"Warning: DEBUG: Logical operator penalty: {features1.logical_operators} vs {features2.logical_operators}, penalty={penalty:.2f}", file=sys.stderr)

    # Penalty 3: Semantic Method Mismatch (25% penalty)
    if features1.semantic_methods and features2.semantic_methods:
        if features1.semantic_methods != features2.semantic_methods:
            # Different semantic methods indicate different operations
            # (e.g., Math.max vs Math.min, toUpperCase vs toLowerCase)
            penalty *= 0.75
            print(f"Warning: DEBUG: Semantic method penalty: {features1.semantic_methods} vs {features2.semantic_methods}, penalty={penalty:.2f}", file=sys.stderr)

    return penalty


def calculate_structural_similarity(code1: str, code2: str, threshold: float = 0.90) -> Tuple[float, str]:
    """
    Calculate structural similarity between two code blocks using unified penalty system.

    Priority 2: Structural Similarity (Layer 2 of 3-layer algorithm)

    Returns:
        (similarity_score, method)
        - similarity_score: 0.0 to 1.0
        - method: 'exact', 'structural', or 'different'

    Algorithm (NEW TWO-PHASE FLOW):
    1. Exact match: Compare hashes ‚Üí 1.0 similarity
    2. PHASE 1: Extract semantic features from ORIGINAL code (BEFORE normalization)
    3. PHASE 2: Normalize code and calculate base structural similarity
    4. PHASE 3: Apply unified semantic penalties using original features
    5. Return final similarity score and method
    """
    if not code1 or not code2:
        return 0.0, 'different'

    # Layer 1: Exact content match (fastest)
    hash1 = hashlib.sha256(code1.encode()).hexdigest()
    hash2 = hashlib.sha256(code2.encode()).hexdigest()

    if hash1 == hash2:
        return 1.0, 'exact'

    # ‚úÖ PHASE 1: Extract semantic features from ORIGINAL code
    # This MUST happen BEFORE normalization to preserve semantic information
    features1 = extract_semantic_features(code1)
    features2 = extract_semantic_features(code2)

    # ‚úÖ PHASE 2: Normalize code for structural comparison
    normalized1 = normalize_code(code1)
    normalized2 = normalize_code(code2)

    # Check if normalized versions are identical (structural duplicate)
    if normalized1 == normalized2:
        base_similarity = 0.95  # Slightly less than exact match
    else:
        # Calculate similarity ratio using Levenshtein
        base_similarity = calculate_levenshtein_similarity(normalized1, normalized2)

        # Layer 2.5: Method chain validation
        chain_similarity = compare_method_chains(code1, code2)

        if chain_similarity < 1.0:
            # Different chain structure ‚Üí penalize similarity
            # Weight: 70% Levenshtein + 30% chain similarity
            base_similarity = (base_similarity * 0.7) + (chain_similarity * 0.3)

    # ‚úÖ PHASE 3: Apply unified semantic penalties using ORIGINAL features
    penalty = calculate_semantic_penalty(features1, features2)
    final_similarity = base_similarity * penalty

    # Determine method based on final similarity score
    if final_similarity >= threshold:
        return final_similarity, 'structural'
    else:
        return final_similarity, 'different'


def are_structurally_similar(code1: str, code2: str, threshold: float = 0.90) -> bool:
    """
    Check if two code blocks are structurally similar.

    Returns True if similarity score >= threshold.
    """
    score, _ = calculate_structural_similarity(code1, code2, threshold)
    return score >= threshold
</file>

<file path="pipeline-core/types/scan-orchestrator-types.js">
/**
 * Zod Schemas for ScanOrchestrator
 *
 * Following TypeScript Type Validator skill best practices:
 * - Single source of truth: Derive TypeScript types from Zod schemas
 * - Runtime validation for external inputs
 * - Clear error messages
 * - Strict validation with .strict()
 */
import { z } from 'zod';
// ============================================================================
// Zod Schemas for Runtime Validation
// ============================================================================
/**
 * Repository information schema
 */
export const RepositoryInfoSchema = z.object({
    path: z.string()
        .min(1, 'Repository path must not be empty'),
    name: z.string()
        .min(1, 'Repository name must not be empty'),
    gitRemote: z.string().url().optional(),
    gitBranch: z.string().optional(),
    gitCommit: z.string().optional(),
    totalFiles: z.number()
        .int('Total files must be an integer')
        .nonnegative('Total files must be non-negative'),
    totalLines: z.number()
        .int('Total lines must be an integer')
        .nonnegative('Total lines must be non-negative'),
    languages: z.array(z.string())
}).strict();
/**
 * Pattern match schema from ast-grep
 */
export const PatternMatchSchema = z.object({
    file_path: z.string()
        .min(1, 'File path must not be empty'),
    rule_id: z.string()
        .min(1, 'Rule ID must not be empty'),
    matched_text: z.string(),
    line_start: z.number()
        .int('Line start must be an integer')
        .positive('Line start must be positive'),
    line_end: z.number()
        .int('Line end must be an integer')
        .positive('Line end must be positive'),
    column_start: z.number().int().nonnegative().optional(),
    column_end: z.number().int().nonnegative().optional(),
    severity: z.string().optional(),
    confidence: z.number().min(0).max(1).optional()
}).strict();
/**
 * Scan configuration schema
 */
export const ScanConfigSchema = z.object({
    scan_config: z.object({
        includeTests: z.boolean().optional(),
        maxDepth: z.number()
            .int('Max depth must be an integer')
            .positive('Max depth must be positive')
            .optional(),
        excludePaths: z.array(z.string()).optional()
    }).passthrough().optional(), // Allow additional properties
    pattern_config: z.object({
        rulesDirectory: z.string().optional(),
        configPath: z.string().optional()
    }).passthrough().optional(), // Allow additional properties
    generateReports: z.boolean().optional()
}).passthrough(); // Allow additional top-level properties for extensibility
/**
 * Python pipeline input schema
 */
export const PythonPipelineInputSchema = z.object({
    repository_info: RepositoryInfoSchema,
    pattern_matches: z.array(PatternMatchSchema),
    scan_config: ScanConfigSchema
}).strict();
/**
 * Code block schema
 */
export const CodeBlockSchema = z.object({
    block_id: z.string()
        .min(1, 'Block ID must not be empty'),
    file_path: z.string()
        .min(1, 'File path must not be empty'),
    line_start: z.number()
        .int('Line start must be an integer')
        .positive('Line start must be positive'),
    line_end: z.number()
        .int('Line end must be an integer')
        .positive('Line end must be positive'),
    source_code: z.string(),
    language: z.string()
        .min(1, 'Language must not be empty'),
    semantic_category: z.string().optional(),
    tags: z.array(z.string()),
    complexity_metrics: z.object({
        cyclomatic: z.number().int().nonnegative(),
        cognitive: z.number().int().nonnegative(),
        lines: z.number().int().nonnegative()
    }).optional(),
    similarity_hash: z.string().optional()
}).strict();
/**
 * Duplicate group schema
 */
export const DuplicateGroupSchema = z.object({
    group_id: z.string()
        .min(1, 'Group ID must not be empty'),
    block_ids: z.array(z.string())
        .min(2, 'Duplicate group must have at least 2 blocks'),
    similarity_score: z.number()
        .min(0, 'Similarity score must be between 0 and 1')
        .max(1, 'Similarity score must be between 0 and 1'),
    group_type: z.enum(['exact', 'structural', 'semantic']),
    impact_score: z.number()
        .min(0, 'Impact score must be non-negative')
        .optional()
}).strict();
/**
 * Consolidation suggestion schema
 */
export const ConsolidationSuggestionSchema = z.object({
    suggestion_id: z.string()
        .min(1, 'Suggestion ID must not be empty'),
    group_id: z.string()
        .min(1, 'Group ID must not be empty'),
    target_location: z.string()
        .min(1, 'Target location must not be empty'),
    affected_files: z.array(z.string())
        .min(1, 'Must have at least one affected file'),
    estimated_loc_reduction: z.number()
        .int('LOC reduction must be an integer')
        .nonnegative('LOC reduction must be non-negative'),
    priority: z.enum(['high', 'medium', 'low']),
    reasoning: z.string().optional()
}).strict();
/**
 * Scan metrics schema
 */
export const ScanMetricsSchema = z.object({
    total_code_blocks: z.number()
        .int('Total code blocks must be an integer')
        .nonnegative('Total code blocks must be non-negative'),
    total_duplicate_groups: z.number()
        .int('Total duplicate groups must be an integer')
        .nonnegative('Total duplicate groups must be non-negative'),
    total_suggestions: z.number()
        .int('Total suggestions must be an integer')
        .nonnegative('Total suggestions must be non-negative'),
    potential_loc_reduction: z.number()
        .int('Potential LOC reduction must be an integer')
        .nonnegative('Potential LOC reduction must be non-negative'),
    duplication_percentage: z.number()
        .min(0, 'Duplication percentage must be between 0 and 100')
        .max(100, 'Duplication percentage must be between 0 and 100').optional(),
    total_cross_repository_groups: z.number()
        .int().nonnegative().optional()
}).strict();
/**
 * Python pipeline output schema
 */
export const PythonPipelineOutputSchema = z.object({
    code_blocks: z.array(CodeBlockSchema),
    duplicate_groups: z.array(DuplicateGroupSchema),
    suggestions: z.array(ConsolidationSuggestionSchema),
    metrics: ScanMetricsSchema
}).strict();
/**
 * Scan result schema
 */
export const ScanResultSchema = z.object({
    code_blocks: z.array(CodeBlockSchema),
    duplicate_groups: z.array(DuplicateGroupSchema),
    suggestions: z.array(ConsolidationSuggestionSchema),
    metrics: ScanMetricsSchema,
    scan_metadata: z.object({
        duration_seconds: z.number().nonnegative(),
        scanned_at: z.string(), // ISO date string
        repository_path: z.string()
    }).strict(),
    report_paths: z.object({
        html: z.string().optional(),
        markdown: z.string().optional(),
        json: z.string().optional()
    }).strict().optional()
}).strict();
/**
 * Report options schema
 */
export const ReportOptionsSchema = z.object({
    outputDir: z.string()
        .min(1, 'Output directory must not be empty'),
    title: z.string().optional(),
    includeSourceCode: z.boolean().optional(),
    includeDetails: z.boolean().optional(),
    includeCodeBlocks: z.boolean().optional()
}).passthrough(); // Allow additional options
/**
 * ScanOrchestrator options schema
 */
export const ScanOrchestratorOptionsSchema = z.object({
    scanner: z.any().optional(), // RepositoryScanner options
    detector: z.any().optional(), // AstGrepPatternDetector options
    pythonPath: z.string().optional(),
    extractorScript: z.string().optional(),
    autoGenerateReports: z.boolean().optional(),
    reportConfig: ReportOptionsSchema.optional()
}).passthrough();
</file>

<file path="pipeline-core/types/scan-orchestrator-types.ts">
/**
 * Zod Schemas for ScanOrchestrator
 *
 * Following TypeScript Type Validator skill best practices:
 * - Single source of truth: Derive TypeScript types from Zod schemas
 * - Runtime validation for external inputs
 * - Clear error messages
 * - Strict validation with .strict()
 */

import { z } from 'zod';

// ============================================================================
// Zod Schemas for Runtime Validation
// ============================================================================

/**
 * Repository information schema
 */
export const RepositoryInfoSchema = z.object({
  path: z.string()
    .min(1, 'Repository path must not be empty'),
  name: z.string()
    .min(1, 'Repository name must not be empty'),
  gitRemote: z.string().url().optional(),
  gitBranch: z.string().optional(),
  gitCommit: z.string().optional(),
  totalFiles: z.number()
    .int('Total files must be an integer')
    .nonnegative('Total files must be non-negative'),
  totalLines: z.number()
    .int('Total lines must be an integer')
    .nonnegative('Total lines must be non-negative'),
  languages: z.array(z.string())
}).strict();

/**
 * Pattern match schema from ast-grep
 */
export const PatternMatchSchema = z.object({
  file_path: z.string()
    .min(1, 'File path must not be empty'),
  rule_id: z.string()
    .min(1, 'Rule ID must not be empty'),
  matched_text: z.string(),
  line_start: z.number()
    .int('Line start must be an integer')
    .positive('Line start must be positive'),
  line_end: z.number()
    .int('Line end must be an integer')
    .positive('Line end must be positive'),
  column_start: z.number().int().nonnegative().optional(),
  column_end: z.number().int().nonnegative().optional(),
  severity: z.string().optional(),
  confidence: z.number().min(0).max(1).optional()
}).strict();

/**
 * Scan configuration schema
 */
export const ScanConfigSchema = z.object({
  scan_config: z.object({
    includeTests: z.boolean().optional(),
    maxDepth: z.number()
      .int('Max depth must be an integer')
      .positive('Max depth must be positive')
      .optional(),
    excludePaths: z.array(z.string()).optional()
  }).passthrough().optional(), // Allow additional properties
  pattern_config: z.object({
    rulesDirectory: z.string().optional(),
    configPath: z.string().optional()
  }).passthrough().optional(), // Allow additional properties
  generateReports: z.boolean().optional()
}).passthrough(); // Allow additional top-level properties for extensibility

/**
 * Python pipeline input schema
 */
export const PythonPipelineInputSchema = z.object({
  repository_info: RepositoryInfoSchema,
  pattern_matches: z.array(PatternMatchSchema),
  scan_config: ScanConfigSchema
}).strict();

/**
 * Code block schema
 */
export const CodeBlockSchema = z.object({
  block_id: z.string()
    .min(1, 'Block ID must not be empty'),
  file_path: z.string()
    .min(1, 'File path must not be empty'),
  line_start: z.number()
    .int('Line start must be an integer')
    .positive('Line start must be positive'),
  line_end: z.number()
    .int('Line end must be an integer')
    .positive('Line end must be positive'),
  source_code: z.string(),
  language: z.string()
    .min(1, 'Language must not be empty'),
  semantic_category: z.string().optional(),
  tags: z.array(z.string()),
  complexity_metrics: z.object({
    cyclomatic: z.number().int().nonnegative(),
    cognitive: z.number().int().nonnegative(),
    lines: z.number().int().nonnegative()
  }).optional(),
  similarity_hash: z.string().optional()
}).strict();

/**
 * Duplicate group schema
 */
export const DuplicateGroupSchema = z.object({
  group_id: z.string()
    .min(1, 'Group ID must not be empty'),
  block_ids: z.array(z.string())
    .min(2, 'Duplicate group must have at least 2 blocks'),
  similarity_score: z.number()
    .min(0, 'Similarity score must be between 0 and 1')
    .max(1, 'Similarity score must be between 0 and 1'),
  group_type: z.enum(['exact', 'structural', 'semantic']),
  impact_score: z.number()
    .min(0, 'Impact score must be non-negative')
    .optional()
}).strict();

/**
 * Consolidation suggestion schema
 */
export const ConsolidationSuggestionSchema = z.object({
  suggestion_id: z.string()
    .min(1, 'Suggestion ID must not be empty'),
  group_id: z.string()
    .min(1, 'Group ID must not be empty'),
  target_location: z.string()
    .min(1, 'Target location must not be empty'),
  affected_files: z.array(z.string())
    .min(1, 'Must have at least one affected file'),
  estimated_loc_reduction: z.number()
    .int('LOC reduction must be an integer')
    .nonnegative('LOC reduction must be non-negative'),
  priority: z.enum(['high', 'medium', 'low']),
  reasoning: z.string().optional()
}).strict();

/**
 * Scan metrics schema
 */
export const ScanMetricsSchema = z.object({
  total_code_blocks: z.number()
    .int('Total code blocks must be an integer')
    .nonnegative('Total code blocks must be non-negative'),
  total_duplicate_groups: z.number()
    .int('Total duplicate groups must be an integer')
    .nonnegative('Total duplicate groups must be non-negative'),
  total_suggestions: z.number()
    .int('Total suggestions must be an integer')
    .nonnegative('Total suggestions must be non-negative'),
  potential_loc_reduction: z.number()
    .int('Potential LOC reduction must be an integer')
    .nonnegative('Potential LOC reduction must be non-negative'),
  duplication_percentage: z.number()
    .min(0, 'Duplication percentage must be between 0 and 100')
    .max(100, 'Duplication percentage must be between 0 and 100').optional(),
  total_cross_repository_groups: z.number()
    .int().nonnegative().optional()
}).strict();

/**
 * Python pipeline output schema
 */
export const PythonPipelineOutputSchema = z.object({
  code_blocks: z.array(CodeBlockSchema),
  duplicate_groups: z.array(DuplicateGroupSchema),
  suggestions: z.array(ConsolidationSuggestionSchema),
  metrics: ScanMetricsSchema
}).strict();

/**
 * Scan result schema
 */
export const ScanResultSchema = z.object({
  code_blocks: z.array(CodeBlockSchema),
  duplicate_groups: z.array(DuplicateGroupSchema),
  suggestions: z.array(ConsolidationSuggestionSchema),
  metrics: ScanMetricsSchema,
  scan_metadata: z.object({
    duration_seconds: z.number().nonnegative(),
    scanned_at: z.string(), // ISO date string
    repository_path: z.string()
  }).strict(),
  report_paths: z.object({
    html: z.string().optional(),
    markdown: z.string().optional(),
    json: z.string().optional()
  }).strict().optional()
}).strict();

/**
 * Report options schema
 */
export const ReportOptionsSchema = z.object({
  outputDir: z.string()
    .min(1, 'Output directory must not be empty'),
  title: z.string().optional(),
  includeSourceCode: z.boolean().optional(),
  includeDetails: z.boolean().optional(),
  includeCodeBlocks: z.boolean().optional()
}).passthrough(); // Allow additional options

/**
 * ScanOrchestrator options schema
 */
export const ScanOrchestratorOptionsSchema = z.object({
  scanner: z.any().optional(), // RepositoryScanner options
  detector: z.any().optional(), // AstGrepPatternDetector options
  pythonPath: z.string().optional(),
  extractorScript: z.string().optional(),
  autoGenerateReports: z.boolean().optional(),
  reportConfig: ReportOptionsSchema.optional()
}).passthrough();

// ============================================================================
// Derive TypeScript Types from Zod Schemas (Single Source of Truth)
// ============================================================================

/**
 * Repository information type - derived from schema
 */
export type RepositoryInfo = z.infer<typeof RepositoryInfoSchema>;

/**
 * Pattern match type - derived from schema
 */
export type PatternMatch = z.infer<typeof PatternMatchSchema>;

/**
 * Scan configuration type - derived from schema
 */
export type ScanConfig = z.infer<typeof ScanConfigSchema>;

/**
 * Python pipeline input type - derived from schema
 */
export type PythonPipelineInput = z.infer<typeof PythonPipelineInputSchema>;

/**
 * Code block type - derived from schema
 */
export type CodeBlock = z.infer<typeof CodeBlockSchema>;

/**
 * Duplicate group type - derived from schema
 */
export type DuplicateGroup = z.infer<typeof DuplicateGroupSchema>;

/**
 * Consolidation suggestion type - derived from schema
 */
export type ConsolidationSuggestion = z.infer<typeof ConsolidationSuggestionSchema>;

/**
 * Scan metrics type - derived from schema
 */
export type ScanMetrics = z.infer<typeof ScanMetricsSchema>;

/**
 * Python pipeline output type - derived from schema
 */
export type PythonPipelineOutput = z.infer<typeof PythonPipelineOutputSchema>;

/**
 * Scan result type - derived from schema
 */
export type ScanResult = z.infer<typeof ScanResultSchema>;

/**
 * Report options type - derived from schema
 */
export type ReportOptions = z.infer<typeof ReportOptionsSchema>;

/**
 * ScanOrchestrator options type - derived from schema
 */
export type ScanOrchestratorOptions = z.infer<typeof ScanOrchestratorOptionsSchema>;
</file>

<file path="pipeline-core/utils/error-helpers.js">
/**
 * Error Helper Utilities
 *
 * Provides safe error message extraction and error object conversion
 * to prevent cascading TypeErrors when handling errors.
 *
 * @module lib/utils/error-helpers
 */

/**
 * Safely extracts an error message from any error type
 *
 * @param {Error|string|null|undefined|unknown} error - The error to extract message from
 * @param {string} fallback - Fallback message if error is null/undefined
 * @returns {string} The error message
 *
 * @example
 * safeErrorMessage(new Error('test')) // 'test'
 * safeErrorMessage('string error') // 'string error'
 * safeErrorMessage(null) // 'Unknown error'
 * safeErrorMessage(undefined, 'Custom fallback') // 'Custom fallback'
 */
export function safeErrorMessage(error, fallback = 'Unknown error') {
  // Handle null/undefined
  if (error == null) {
    return fallback;
  }

  // Handle Error objects (standard and custom)
  if (error instanceof Error) {
    return error.message || fallback;
  }

  // Handle error-like objects with message property
  if (typeof error === 'object' && 'message' in error && typeof error.message === 'string') {
    return error.message || fallback;
  }

  // Handle strings
  if (typeof error === 'string') {
    return error || fallback;
  }

  // Handle primitives and other objects
  try {
    const stringified = String(error);
    // Avoid unhelpful "[object Object]" messages
    if (stringified === '[object Object]') {
      return JSON.stringify(error);
    }
    return stringified;
  } catch {
    return fallback;
  }
}

/**
 * Safely extracts a stack trace from an error
 *
 * @param {Error|unknown} error - The error to extract stack from
 * @returns {string|undefined} The stack trace or undefined
 */
export function safeErrorStack(error) {
  if (error == null) {
    return undefined;
  }

  // Handle Error objects
  if (error instanceof Error && error.stack) {
    return error.stack;
  }

  // Handle error-like objects with stack property
  if (typeof error === 'object' && 'stack' in error && typeof error.stack === 'string') {
    return error.stack;
  }

  return undefined;
}

/**
 * Converts any error type to a structured error object
 *
 * @param {Error|string|null|undefined|unknown} error - The error to convert
 * @param {Object} [options={}] - Conversion options
 * @param {string} [options.fallbackMessage='Unknown error'] - Fallback message for null/undefined errors
 * @param {Object} [options.metadata={}] - Additional metadata to include
 * @returns {Object} Structured error object with message, stack, type, and metadata
 *
 * @example
 * toErrorObject(new Error('test'))
 * // { message: 'test', stack: '...', type: 'Error', isError: true }
 *
 * toErrorObject('string error', { metadata: { jobId: '123' } })
 * // { message: 'string error', type: 'string', isError: false, metadata: { jobId: '123' } }
 */
export function toErrorObject(error, options = {}) {
  const {
    fallbackMessage = 'Unknown error',
    metadata = {}
  } = options;

  const errorObj = {
    message: safeErrorMessage(error, fallbackMessage),
    stack: safeErrorStack(error),
    type: getErrorType(error),
    isError: error instanceof Error,
    metadata
  };

  // Include error name if available
  if (error instanceof Error && error.name) {
    errorObj.name = error.name;
  }

  // Include error code if available (for system errors, etc.)
  if (error && typeof error === 'object' && 'code' in error) {
    errorObj.code = error.code;
  }

  return errorObj;
}

/**
 * Gets the type of an error value
 *
 * @param {unknown} error - The error to get type of
 * @returns {string} The error type
 * @private
 */
function getErrorType(error) {
  if (error == null) {
    return error === null ? 'null' : 'undefined';
  }

  if (error instanceof Error) {
    return error.constructor.name || 'Error';
  }

  return typeof error;
}

/**
 * Checks if a value is an error-like object
 *
 * @param {unknown} value - The value to check
 * @returns {boolean} True if the value is error-like
 *
 * @example
 * isErrorLike(new Error('test')) // true
 * isErrorLike({ message: 'test', stack: '...' }) // true
 * isErrorLike('string error') // false
 * isErrorLike(null) // false
 */
export function isErrorLike(value) {
  if (value instanceof Error) {
    return true;
  }

  if (value && typeof value === 'object') {
    return 'message' in value || 'stack' in value;
  }

  return false;
}

/**
 * Safely serializes an error for logging or transmission
 *
 * @param {Error|unknown} error - The error to serialize
 * @param {boolean} includeStack - Whether to include stack trace (default: true)
 * @returns {Object} Serialized error object
 *
 * @example
 * serializeError(new Error('test'))
 * // { message: 'test', name: 'Error', stack: '...', type: 'Error' }
 */
export function serializeError(error, includeStack = true) {
  const serialized = {
    message: safeErrorMessage(error),
    type: getErrorType(error)
  };

  if (error instanceof Error) {
    serialized.name = error.name;
    if (includeStack && error.stack) {
      serialized.stack = error.stack;
    }
    if (error.cause) {
      serialized.cause = serializeError(error.cause, includeStack);
    }
  }

  // Include code for system errors
  if (error && typeof error === 'object' && 'code' in error) {
    serialized.code = error.code;
  }

  return serialized;
}

/**
 * Creates a formatted error message for display
 *
 * @param {Error|unknown} error - The error to format
 * @param {Object} [options={}] - Formatting options
 * @param {boolean} [options.includeType=false] - Include error type in message
 * @param {boolean} [options.includeCode=false] - Include error code if available
 * @returns {string} Formatted error message
 *
 * @example
 * formatErrorMessage(new Error('test'), { includeType: true })
 * // '[Error] test'
 *
 * formatErrorMessage({ code: 'ENOENT', message: 'File not found' }, { includeCode: true })
 * // '[ENOENT] File not found'
 */
export function formatErrorMessage(error, options = {}) {
  const {
    includeType = false,
    includeCode = false
  } = options;

  const message = safeErrorMessage(error);
  const parts = [];

  if (includeCode && error && typeof error === 'object' && 'code' in error) {
    parts.push(`[${error.code}]`);
  } else if (includeType) {
    parts.push(`[${getErrorType(error)}]`);
  }

  parts.push(message);

  return parts.join(' ');
}

/**
 * Combines multiple errors into a single error message
 *
 * @param {Array<Error|unknown>} errors - Array of errors to combine
 * @param {string} separator - Separator between error messages (default: '; ')
 * @returns {string} Combined error message
 *
 * @example
 * combineErrors([new Error('error 1'), new Error('error 2')])
 * // 'error 1; error 2'
 */
export function combineErrors(errors, separator = '; ') {
  if (!Array.isArray(errors) || errors.length === 0) {
    return 'Unknown error';
  }

  return errors
    .map(error => safeErrorMessage(error))
    .filter(msg => msg && msg !== 'Unknown error')
    .join(separator);
}
</file>

<file path="pipeline-core/doppler-health-monitor.js">
/**
 * Doppler Health Monitor
 *
 * Monitors Doppler cache age and alerts when secrets may be stale.
 *
 * Usage:
 *   import { DopplerHealthMonitor } from './sidequest/pipeline-core/doppler-health-monitor.js';
 *
 *   const monitor = new DopplerHealthMonitor();
 *   await monitor.startMonitoring(15); // Check every 15 minutes
 *
 *   // Or check on-demand:
 *   const health = await monitor.checkCacheHealth();
 */

import { createComponentLogger } from '../utils/logger.js';
import Sentry from '@sentry/node';
import fs from 'fs/promises';
import path from 'path';
import os from 'os';

const logger = createComponentLogger('DopplerHealthMonitor');

export class DopplerHealthMonitor {
  constructor(options = {}) {
    // Doppler fallback cache location
    this.cacheFile = options.cacheFile || path.join(os.homedir(), '.doppler', '.fallback.json');

    // Max cache age before triggering alerts (default: 24 hours)
    this.maxCacheAge = options.maxCacheAge || 24 * 60 * 60 * 1000;

    // Warning threshold (default: 12 hours)
    this.warningThreshold = options.warningThreshold || 12 * 60 * 60 * 1000;

    this.monitoringInterval = null;
  }

  /**
   * Check Doppler cache health
   *
   * @returns {Promise<Object>} Health status with cache age info
   */
  async checkCacheHealth() {
    try {
      const stats = await fs.stat(this.cacheFile);
      const cacheAge = Date.now() - stats.mtime.getTime();
      const cacheAgeHours = Math.floor(cacheAge / (60 * 60 * 1000));
      const cacheAgeMinutes = Math.floor((cacheAge % (60 * 60 * 1000)) / (60 * 1000));

      const status = {
        healthy: cacheAge <= this.maxCacheAge,
        cacheAgeMs: cacheAge,
        cacheAgeHours,
        cacheAgeMinutes,
        lastModified: stats.mtime.toISOString(),
        usingFallback: true, // If cache file exists and has age, we're using fallback
        severity: this.getSeverity(cacheAge)
      };

      if (cacheAge > this.maxCacheAge) {
        logger.error({
          cacheAgeHours,
          lastModified: stats.mtime
        }, 'Doppler cache is critically stale - secrets may be outdated');

        Sentry.captureMessage('Doppler cache critically stale', {
          level: 'error',
          extra: {
            cacheAgeHours,
            cacheAgeMinutes,
            lastModified: stats.mtime.toISOString(),
            threshold: this.maxCacheAge / (60 * 60 * 1000)
          },
          tags: {
            component: 'doppler-health-monitor',
            severity: 'critical'
          }
        });
      } else if (cacheAge > this.warningThreshold) {
        logger.warn({
          cacheAgeHours,
          lastModified: stats.mtime
        }, 'Doppler cache is aging - approaching staleness threshold');

        Sentry.captureMessage('Doppler cache aging', {
          level: 'warning',
          extra: {
            cacheAgeHours,
            cacheAgeMinutes,
            lastModified: stats.mtime.toISOString(),
            threshold: this.warningThreshold / (60 * 60 * 1000)
          },
          tags: {
            component: 'doppler-health-monitor',
            severity: 'warning'
          }
        });
      } else {
        logger.debug({
          cacheAgeHours,
          cacheAgeMinutes
        }, 'Doppler cache health check - within thresholds');
      }

      return status;
    } catch (error) {
      if (error.code === 'ENOENT') {
        // Cache file doesn't exist - probably using live Doppler
        logger.debug('No Doppler fallback cache found - likely using live API');

        return {
          healthy: true,
          cacheAgeMs: 0,
          cacheAgeHours: 0,
          cacheAgeMinutes: 0,
          lastModified: null,
          usingFallback: false,
          severity: 'healthy'
        };
      }

      logger.error({ error }, 'Failed to check Doppler cache health');

      Sentry.captureException(error, {
        tags: {
          component: 'doppler-health-monitor',
          operation: 'check-cache-health'
        }
      });

      return {
        healthy: false,
        error: error.message,
        errorCode: error.code,
        severity: 'error'
      };
    }
  }

  /**
   * Get severity level based on cache age
   *
   * @param {number} cacheAge - Age in milliseconds
   * @returns {string} Severity level
   */
  getSeverity(cacheAge) {
    if (cacheAge > this.maxCacheAge) {
      return 'critical';
    } else if (cacheAge > this.warningThreshold) {
      return 'warning';
    } else {
      return 'healthy';
    }
  }

  /**
   * Start periodic monitoring
   *
   * @param {number} intervalMinutes - Check interval in minutes (default: 15)
   * @returns {Promise<void>}
   */
  async startMonitoring(intervalMinutes = 15) {
    if (this.monitoringInterval) {
      logger.warn('Monitoring already started, stopping previous interval');
      this.stopMonitoring();
    }

    logger.info({
      intervalMinutes,
      maxCacheAgeHours: this.maxCacheAge / (60 * 60 * 1000),
      warningThresholdHours: this.warningThreshold / (60 * 60 * 1000)
    }, 'Starting Doppler health monitoring');

    // Initial check
    const initialHealth = await this.checkCacheHealth();
    logger.info({ initialHealth }, 'Initial Doppler health check complete');

    // Periodic checks
    this.monitoringInterval = setInterval(async () => {
      await this.checkCacheHealth();
    }, intervalMinutes * 60 * 1000);

    return initialHealth;
  }

  /**
   * Stop periodic monitoring
   */
  stopMonitoring() {
    if (this.monitoringInterval) {
      clearInterval(this.monitoringInterval);
      this.monitoringInterval = null;
      logger.info('Doppler health monitoring stopped');
    }
  }

  /**
   * Get cache file path (for testing/debugging)
   *
   * @returns {string} Cache file path
   */
  getCacheFilePath() {
    return this.cacheFile;
  }
}

export default DopplerHealthMonitor;
</file>

<file path="pipeline-core/inter-project-scanner.js">
/**
 * Inter-Project Scanner
 *
 * Scans multiple repositories to detect duplicate code patterns across projects.
 * Identifies candidates for shared abstractions (packages, MCP servers, etc.).
 */

import { ScanOrchestrator } from './scan-orchestrator.js';
import { createComponentLogger } from '../utils/logger.js';
import path from 'path';
import fs from 'fs/promises';

const logger = createComponentLogger('InterProjectScanner');

/**
 * Scanner for detecting duplicates across multiple repositories
 */
export class InterProjectScanner {
  constructor(config = {}) {
    this.orchestrator = new ScanOrchestrator(config.orchestrator || {});
    this.outputDir = config.outputDir || path.join(process.cwd(), 'output', 'inter-project-scans');
  }

  /**
   * Scan multiple repositories and detect cross-project duplicates
   *
   * @param {string[]} repoPaths - Array of repository paths to scan
   * @param {Object} scanConfig - Scan configuration
   * @returns {Promise<Object>} - Inter-project scan results
   */
  async scanRepositories(repoPaths, scanConfig = {}) {
    const startTime = Date.now();

    logger.info({
      repositoryCount: repoPaths.length,
      repositories: repoPaths
    }, 'Starting inter-project scan');

    try {
      // Stage 1: Scan each repository individually
      logger.info('Stage 1: Scanning individual repositories');
      const repositoryScans = [];

      for (const repoPath of repoPaths) {
        try {
          logger.info({ repoPath }, 'Scanning repository');
          const scanResult = await this.orchestrator.scanRepository(repoPath, scanConfig);

          repositoryScans.push({
            repository_path: repoPath,
            repository_name: path.basename(repoPath),
            scan_result: scanResult,
            code_blocks: scanResult.code_blocks || [],
            duplicate_groups: scanResult.duplicate_groups || [],
            suggestions: scanResult.suggestions || []
          });

          logger.info({
            repoPath,
            blocks: scanResult.code_blocks?.length || 0,
            groups: scanResult.duplicate_groups?.length || 0
          }, 'Repository scan completed');

        } catch (error) {
          logger.warn({ repoPath, error: error.message }, 'Failed to scan repository');
          repositoryScans.push({
            repository_path: repoPath,
            repository_name: path.basename(repoPath),
            error: error.message,
            code_blocks: [],
            duplicate_groups: [],
            suggestions: []
          });
        }
      }

      // Stage 2: Aggregate code blocks across all repositories
      logger.info('Stage 2: Aggregating code blocks across repositories');
      const allCodeBlocks = this._aggregateCodeBlocks(repositoryScans);

      logger.info({
        totalBlocks: allCodeBlocks.length,
        repositories: repositoryScans.length
      }, 'Code blocks aggregated');

      // Stage 3: Detect cross-repository duplicates
      logger.info('Stage 3: Detecting cross-repository duplicates');
      const crossRepoDuplicates = this._detectCrossRepoDuplicates(allCodeBlocks);

      logger.info({
        crossRepoGroups: crossRepoDuplicates.length
      }, 'Cross-repository duplicates detected');

      // Stage 4: Generate cross-repository consolidation suggestions
      logger.info('Stage 4: Generating cross-repository suggestions');
      const crossRepoSuggestions = this._generateCrossRepoSuggestions(
        crossRepoDuplicates,
        repositoryScans
      );

      logger.info({
        suggestions: crossRepoSuggestions.length
      }, 'Cross-repository suggestions generated');

      // Stage 5: Calculate inter-project metrics
      const metrics = this._calculateInterProjectMetrics(
        repositoryScans,
        crossRepoDuplicates,
        crossRepoSuggestions
      );

      const duration = (Date.now() - startTime) / 1000;

      const result = {
        scan_type: 'inter-project',
        scanned_repositories: repositoryScans.map(r => ({
          path: r.repository_path,
          name: r.repository_name,
          code_blocks: r.code_blocks.length,
          duplicate_groups: r.duplicate_groups.length,
          error: r.error
        })),
        cross_repository_duplicates: crossRepoDuplicates,
        cross_repository_suggestions: crossRepoSuggestions,
        repository_scans: repositoryScans,
        metrics,
        scan_metadata: {
          duration_seconds: duration,
          scanned_at: new Date().toISOString(),
          repository_count: repoPaths.length,
          total_code_blocks: allCodeBlocks.length
        }
      };

      logger.info({
        duration,
        repositories: repoPaths.length,
        crossRepoGroups: crossRepoDuplicates.length,
        suggestions: crossRepoSuggestions.length
      }, 'Inter-project scan completed');

      return result;

    } catch (error) {
      logger.error({ error }, 'Inter-project scan failed');
      throw error;
    }
  }

  /**
   * Aggregate code blocks from all repository scans
   *
   * @private
   */
  _aggregateCodeBlocks(repositoryScans) {
    const allBlocks = [];

    for (const repoScan of repositoryScans) {
      if (!repoScan.code_blocks) continue;

      for (const block of repoScan.code_blocks) {
        // Add repository context to each block
        allBlocks.push({
          ...block,
          source_repository: repoScan.repository_name,
          source_repository_path: repoScan.repository_path
        });
      }
    }

    return allBlocks;
  }

  /**
   * Detect duplicates that span multiple repositories
   *
   * @private
   */
  _detectCrossRepoDuplicates(allCodeBlocks) {
    // Group by content hash (exact matches across repos)
    const hashGroups = {};

    for (const block of allCodeBlocks) {
      const hash = block.content_hash;
      if (!hashGroups[hash]) {
        hashGroups[hash] = [];
      }
      hashGroups[hash].push(block);
    }

    // Filter for groups that span multiple repositories
    const crossRepoGroups = [];

    for (const [hash, blocks] of Object.entries(hashGroups)) {
      if (blocks.length < 2) continue;

      // Get unique repositories
      const repositories = new Set(blocks.map(b => b.source_repository));

      // Only include if it spans multiple repositories
      if (repositories.size >= 2) {
        const group = {
          group_id: `cross_${hash.substring(0, 12)}`,
          pattern_id: blocks[0].pattern_id,
          content_hash: hash,
          member_blocks: blocks,
          occurrence_count: blocks.length,
          repository_count: repositories.size,
          affected_repositories: Array.from(repositories),
          affected_files: blocks.map(b => `${b.source_repository}/${b.relative_path}`),
          category: blocks[0].category,
          language: blocks[0].language,
          total_lines: blocks.reduce((sum, b) => sum + b.line_count, 0),
          similarity_score: 1.0, // Exact matches
          similarity_method: 'exact_match'
        };

        // Calculate impact score
        group.impact_score = this._calculateCrossRepoImpactScore(group);

        crossRepoGroups.push(group);
      }
    }

    // Sort by impact score descending
    crossRepoGroups.sort((a, b) => b.impact_score - a.impact_score);

    return crossRepoGroups;
  }

  /**
   * Calculate impact score for cross-repository duplicate
   *
   * @private
   */
  _calculateCrossRepoImpactScore(group) {
    // Base score from occurrence count and repository count
    let score = 0;

    // More occurrences = higher impact
    score += Math.min(group.occurrence_count * 5, 40);

    // More repositories = much higher impact (shared code is valuable)
    score += group.repository_count * 15;

    // More lines = higher impact
    score += Math.min(group.total_lines / 2, 20);

    // Bonus for high-value categories
    const categoryBonuses = {
      'api_handler': 10,
      'auth_check': 10,
      'database_operation': 8,
      'validator': 8,
      'error_handler': 6
    };
    score += categoryBonuses[group.category] || 0;

    return Math.min(score, 100);
  }

  /**
   * Generate consolidation suggestions for cross-repository duplicates
   *
   * @private
   */
  _generateCrossRepoSuggestions(crossRepoDuplicates, repositoryScans) {
    const suggestions = [];

    for (const group of crossRepoDuplicates) {
      // Cross-repo duplicates should generally be shared packages or higher
      const strategy = this._determineCrossRepoStrategy(group);
      const rationale = this._generateCrossRepoRationale(group);
      const complexity = this._assessComplexity(group);
      const risk = this._assessRisk(group, strategy);

      const suggestion = {
        suggestion_id: `cs_${group.group_id}`,
        duplicate_group_id: group.group_id,
        strategy,
        strategy_rationale: rationale,
        target_location: this._suggestCrossRepoLocation(group, strategy),
        impact_score: group.impact_score,
        complexity,
        migration_risk: risk,
        affected_repositories: group.affected_repositories,
        affected_repositories_count: group.repository_count,
        affected_files_count: group.affected_files.length,
        breaking_changes: strategy !== 'shared_package',
        confidence: 0.9,
        roi_score: this._calculateCrossRepoROI(group, complexity, risk)
      };

      suggestions.push(suggestion);
    }

    // Sort by ROI descending
    suggestions.sort((a, b) => b.roi_score - a.roi_score);

    return suggestions;
  }

  /**
   * Determine consolidation strategy for cross-repo duplicate
   *
   * @private
   */
  _determineCrossRepoStrategy(group) {
    const repoCount = group.repository_count;
    const occurrences = group.occurrence_count;
    const category = group.category;

    // 2-3 repos: shared package
    if (repoCount <= 3 && occurrences <= 10) {
      return 'shared_package';
    }

    // Many repos or complex logic: MCP server
    if (repoCount >= 4 || category in ['api_handler', 'database_operation']) {
      return 'mcp_server';
    }

    // High complexity cross-language: autonomous agent
    if (occurrences >= 20) {
      return 'autonomous_agent';
    }

    return 'shared_package';
  }

  /**
   * Generate rationale for cross-repo suggestion
   *
   * @private
   */
  _generateCrossRepoRationale(group) {
    const repos = group.repository_count;
    const occurrences = group.occurrence_count;
    const category = group.category;

    return `Found ${occurrences} occurrences across ${repos} repositories. ` +
           `Category: ${category}. Strong candidate for shared abstraction to ` +
           `eliminate duplication and ensure consistency across projects.`;
  }

  /**
   * Assess complexity of cross-repo consolidation
   *
   * @private
   */
  _assessComplexity(group) {
    const repoCount = group.repository_count;

    if (repoCount === 2) return 'simple';
    if (repoCount === 3) return 'moderate';
    return 'complex';
  }

  /**
   * Assess risk of cross-repo consolidation
   *
   * @private
   */
  _assessRisk(group, strategy) {
    if (strategy === 'shared_package') return 'medium';
    if (strategy === 'mcp_server') return 'high';
    return 'high';
  }

  /**
   * Suggest target location for cross-repo consolidation
   *
   * @private
   */
  _suggestCrossRepoLocation(group, strategy) {
    const category = group.category;

    if (strategy === 'shared_package') {
      const categoryPaths = {
        'logger': '@shared/logging',
        'config_access': '@shared/config',
        'api_handler': '@shared/api-middleware',
        'auth_check': '@shared/auth',
        'database_operation': '@shared/database',
        'validator': '@shared/validation',
        'error_handler': '@shared/errors'
      };

      return categoryPaths[category] || `@shared/${category}`;
    }

    if (strategy === 'mcp_server') {
      return `mcp-servers/${group.pattern_id}-server`;
    }

    return `agents/${group.pattern_id}-agent`;
  }

  /**
   * Calculate ROI for cross-repo consolidation
   *
   * @private
   */
  _calculateCrossRepoROI(group, complexity, risk) {
    let roi = group.impact_score;

    // Cross-repo consolidation has high value
    roi *= 1.2;

    // Adjust for complexity
    const complexityMultipliers = {
      'simple': 1.1,
      'moderate': 1.0,
      'complex': 0.8
    };
    roi *= complexityMultipliers[complexity] || 1.0;

    // Adjust for risk
    const riskMultipliers = {
      'low': 1.1,
      'medium': 1.0,
      'high': 0.8
    };
    roi *= riskMultipliers[risk] || 1.0;

    return Math.min(roi, 100);
  }

  /**
   * Calculate inter-project metrics
   *
   * @private
   */
  _calculateInterProjectMetrics(repositoryScans, crossRepoDuplicates, suggestions) {
    const totalCodeBlocks = repositoryScans.reduce(
      (sum, r) => sum + r.code_blocks.length, 0
    );

    const totalIntraProjectGroups = repositoryScans.reduce(
      (sum, r) => sum + r.duplicate_groups.length, 0
    );

    const crossRepoOccurrences = crossRepoDuplicates.reduce(
      (sum, g) => sum + g.occurrence_count, 0
    );

    const crossRepoLines = crossRepoDuplicates.reduce(
      (sum, g) => sum + g.total_lines, 0
    );

    return {
      total_repositories_scanned: repositoryScans.length,
      total_code_blocks: totalCodeBlocks,
      total_intra_project_groups: totalIntraProjectGroups,
      total_cross_repository_groups: crossRepoDuplicates.length,
      cross_repository_occurrences: crossRepoOccurrences,
      cross_repository_duplicated_lines: crossRepoLines,
      total_suggestions: suggestions.length,
      shared_package_candidates: suggestions.filter(s => s.strategy === 'shared_package').length,
      mcp_server_candidates: suggestions.filter(s => s.strategy === 'mcp_server').length,
      average_repositories_per_duplicate: crossRepoDuplicates.length > 0
        ? (crossRepoDuplicates.reduce((sum, g) => sum + g.repository_count, 0) / crossRepoDuplicates.length).toFixed(1)
        : 0
    };
  }

  /**
   * Save inter-project scan results to file
   *
   * @param {Object} results - Scan results
   * @param {string} filename - Output filename
   */
  async saveResults(results, filename = 'inter-project-scan.json') {
    await fs.mkdir(this.outputDir, { recursive: true });
    const outputPath = path.join(this.outputDir, filename);
    await fs.writeFile(outputPath, JSON.stringify(results, null, 2));
    logger.info({ outputPath }, 'Scan results saved');
    return outputPath;
  }
}
</file>

<file path="pipeline-core/scan-orchestrator.d.ts">
/**
 * Scan Orchestrator - TypeScript version
 *
 * Coordinates the entire duplicate detection pipeline:
 * 1. Repository scanning (repomix)
 * 2. Pattern detection (ast-grep)
 * 3. Code block extraction (Python/pydantic)
 * 4. Semantic annotation (Python)
 * 5. Duplicate grouping (Python)
 * 6. Suggestion generation (Python)
 * 7. Report generation (Python)
 */
/**
 * Repository information from scanner
 */
export interface RepositoryInfo {
    path: string;
    name: string;
    gitRemote?: string;
    gitBranch?: string;
    gitCommit?: string;
    totalFiles: number;
    totalLines: number;
    languages: string[];
}
/**
 * Pattern match from ast-grep
 */
export interface PatternMatch {
    file_path: string;
    rule_id: string;
    matched_text: string;
    line_start: number;
    line_end: number;
    column_start?: number;
    column_end?: number;
    severity?: string;
    confidence?: number;
}
/**
 * Scan configuration options
 */
export interface ScanConfig {
    scan_config?: {
        includeTests?: boolean;
        maxDepth?: number;
        excludePaths?: string[];
        [key: string]: any;
    };
    pattern_config?: {
        rulesDirectory?: string;
        configPath?: string;
        [key: string]: any;
    };
    generateReports?: boolean;
    [key: string]: any;
}
/**
 * Python pipeline input data structure
 */
export interface PythonPipelineInput {
    repository_info: RepositoryInfo;
    pattern_matches: PatternMatch[];
    scan_config: ScanConfig;
}
/**
 * Code block from Python pipeline
 */
export interface CodeBlock {
    block_id: string;
    file_path: string;
    line_start: number;
    line_end: number;
    source_code: string;
    language: string;
    semantic_category?: string;
    tags: string[];
    complexity_metrics?: {
        cyclomatic: number;
        cognitive: number;
        halstead: Record<string, number>;
    };
}
/**
 * Duplicate group from Python pipeline
 */
export interface DuplicateGroup {
    group_id: string;
    block_ids: string[];
    similarity_score: number;
    group_type: 'exact' | 'structural' | 'semantic';
    total_lines: number;
    potential_reduction: number;
    impact_score?: number;
}
/**
 * Consolidation suggestion from Python pipeline
 */
export interface ConsolidationSuggestion {
    suggestion_id: string;
    group_id: string;
    suggestion_type: string;
    priority: 'critical' | 'high' | 'medium' | 'low';
    estimated_effort: 'minimal' | 'low' | 'medium' | 'high';
    potential_reduction: number;
    implementation_notes?: string;
    migration_steps?: Array<{
        order: number;
        description: string;
        code_snippet?: string;
    }>;
}
/**
 * Scan metrics from Python pipeline
 */
export interface ScanMetrics {
    total_code_blocks: number;
    code_blocks_by_category?: Record<string, number>;
    code_blocks_by_language?: Record<string, number>;
    total_duplicate_groups: number;
    exact_duplicates: number;
    structural_duplicates: number;
    semantic_duplicates: number;
    total_duplicated_lines: number;
    potential_loc_reduction: number;
    duplication_percentage: number;
    total_suggestions: number;
    quick_wins?: number;
    high_priority_suggestions?: number;
}
/**
 * Python pipeline output structure
 */
export interface PythonPipelineOutput {
    code_blocks: CodeBlock[];
    duplicate_groups: DuplicateGroup[];
    suggestions: ConsolidationSuggestion[];
    metrics: ScanMetrics;
    repository_info: RepositoryInfo;
    scan_type?: 'single-project' | 'inter-project';
    error?: string;
    warnings?: string[];
}
/**
 * Scan result with metadata
 */
export interface ScanResult extends PythonPipelineOutput {
    scan_metadata: {
        duration_seconds: number;
        scanned_at: string;
        repository_path: string;
    };
    report_paths?: ReportPaths;
}
/**
 * Report generation paths
 */
export interface ReportPaths {
    html?: string;
    markdown?: string;
    summary?: string;
    json?: string;
}
/**
 * Report generation options
 */
export interface ReportOptions {
    outputDir?: string;
    baseName?: string;
    title?: string;
    html?: boolean;
    markdown?: boolean;
    summary?: boolean;
    json?: boolean;
    includeDetails?: boolean;
    maxDuplicates?: number;
    maxSuggestions?: number;
}
/**
 * Scan orchestrator constructor options
 */
export interface ScanOrchestratorOptions {
    scanner?: Record<string, any>;
    detector?: Record<string, any>;
    pythonPath?: string;
    extractorScript?: string;
    reports?: ReportOptions;
    outputDir?: string;
    autoGenerateReports?: boolean;
    config?: Record<string, any>;
}
/**
 * Cross-repository duplicate group
 */
export interface CrossRepositoryDuplicate {
    group_id: string;
    pattern_id: string;
    content_hash: string;
    member_blocks: CodeBlock[];
    occurrence_count: number;
    repository_count: number;
    affected_repositories: string[];
    affected_files: string[];
    category: string;
    language: string;
    total_lines: number;
    similarity_score: number;
    similarity_method: string;
    impact_score: number;
}
/**
 * Cross-repository consolidation suggestion
 */
export interface CrossRepositorySuggestion {
    suggestion_id: string;
    group_id: string;
    suggestion_type: string;
    priority: 'critical' | 'high' | 'medium' | 'low';
    estimated_effort: 'minimal' | 'low' | 'medium' | 'high';
    potential_reduction: number;
    affected_repositories: string[];
    implementation_notes?: string;
    migration_steps?: Array<{
        order: number;
        description: string;
        code_snippet?: string;
    }>;
}
/**
 * Multi-repository scan result
 */
export interface MultiRepositoryScanResult {
    repositories: Array<ScanResult | {
        error: string;
        repository_path: string;
    }>;
    total_scanned: number;
    successful: number;
    failed: number;
    cross_repository_duplicates?: CrossRepositoryDuplicate[];
    cross_repository_suggestions?: CrossRepositorySuggestion[];
    scan_type?: 'single-project' | 'inter-project';
    metrics?: ScanMetrics & {
        total_cross_repository_groups?: number;
        cross_repository_occurrences?: number;
        cross_repository_duplicated_lines?: number;
    };
}
/**
 * Scan Orchestrator - Coordinates the duplicate detection pipeline
 */
export declare class ScanOrchestrator {
    private readonly repositoryScanner;
    private readonly patternDetector;
    private readonly pythonPath;
    private readonly extractorScript;
    private readonly reportConfig;
    private readonly outputDir;
    private readonly autoGenerateReports;
    private readonly config;
    constructor(options?: ScanOrchestratorOptions);
    /**
     * Scan a single repository for duplicates
     */
    scanRepository(repoPath: string, scanConfig?: ScanConfig): Promise<ScanResult>;
    /**
     * Run Python pipeline for extraction, grouping, and reporting
     */
    private runPythonPipeline;
    /**
     * Generate reports from scan results
     */
    generateReports(scanResult: ScanResult, options?: ReportOptions): Promise<ReportPaths>;
    /**
     * Scan multiple repositories (inter-project analysis)
     */
    scanMultipleRepositories(repoPaths: string[], scanConfig?: ScanConfig): Promise<MultiRepositoryScanResult>;
}
/**
 * Custom error class for scan orchestration errors
 */
export declare class ScanError extends Error {
    constructor(message: string, options?: {
        cause?: unknown;
    });
}
</file>

<file path="pipeline-core/scan-orchestrator.js">
/**
 * Scan Orchestrator - TypeScript version
 *
 * Coordinates the entire duplicate detection pipeline:
 * 1. Repository scanning (repomix)
 * 2. Pattern detection (ast-grep)
 * 3. Code block extraction (Python/pydantic)
 * 4. Semantic annotation (Python)
 * 5. Duplicate grouping (Python)
 * 6. Suggestion generation (Python)
 * 7. Report generation (Python)
 */
import { RepositoryScanner } from './scanners/repository-scanner.js';
import { AstGrepPatternDetector } from './scanners/ast-grep-detector.js';
import { HTMLReportGenerator } from './reports/html-report-generator.js';
import { MarkdownReportGenerator } from './reports/markdown-report-generator.js';
import { InterProjectScanner } from './inter-project-scanner.js';
import { createComponentLogger } from '../utils/logger.js';
import { spawn } from 'child_process';
import * as path from 'path';
import * as fs from 'fs/promises';
import * as Sentry from '@sentry/node';
const logger = createComponentLogger('ScanOrchestrator');
// ============================================================================
// Main ScanOrchestrator Class
// ============================================================================
/**
 * Scan Orchestrator - Coordinates the duplicate detection pipeline
 */
export class ScanOrchestrator {
    repositoryScanner;
    patternDetector;
    pythonPath;
    extractorScript;
    reportConfig;
    outputDir;
    autoGenerateReports;
    config;
    constructor(options = {}) {
        // JavaScript components
        this.repositoryScanner = new RepositoryScanner(options.scanner || {});
        this.patternDetector = new AstGrepPatternDetector(options.detector || {});
        // Python components (called via subprocess)
        // Use venv python by default if it exists, otherwise fall back to system python3
        const venvPython = path.join(process.cwd(), 'venv/bin/python3');
        this.pythonPath = options.pythonPath || venvPython;
        this.extractorScript = options.extractorScript || path.join(process.cwd(), 'sidequest/pipeline-core/extractors/extract_blocks.py');
        // Report generation configuration
        this.reportConfig = options.reports || {};
        this.outputDir = options.outputDir || path.join(process.cwd(), 'output', 'reports');
        this.autoGenerateReports = options.autoGenerateReports !== false; // Default: true
        // Configuration
        this.config = options.config || {};
    }
    /**
     * Scan a single repository for duplicates
     */
    async scanRepository(repoPath, scanConfig = {}) {
        const startTime = Date.now();
        // Validate repoPath
        if (!repoPath || typeof repoPath !== 'string') {
            const error = new ScanError(`Invalid repository path: ${repoPath === undefined ? 'undefined' : repoPath === null ? 'null' : typeof repoPath}. ` +
                `Expected a valid file system path string.`);
            logger.error({ repoPath, type: typeof repoPath }, 'Invalid repository path provided');
            Sentry.captureException(error, {
                tags: {
                    error_type: 'validation_error',
                    component: 'ScanOrchestrator'
                },
                extra: {
                    repoPath,
                    repoPathType: typeof repoPath,
                    scanConfig
                }
            });
            throw error;
        }
        logger.info({ repoPath }, 'Starting repository duplicate scan');
        try {
            // Stage 1: Repository scanning
            logger.info('Stage 1/7: Scanning repository with repomix');
            const repoScan = await this.repositoryScanner.scanRepository(repoPath, scanConfig.scan_config || {});
            // Stage 2: Pattern detection
            logger.info('Stage 2/7: Detecting patterns with ast-grep');
            const patterns = await this.patternDetector.detectPatterns(repoPath, scanConfig.pattern_config || {});
            // Stage 3-7: Python pipeline
            logger.info('Stage 3-7: Running Python extraction and analysis pipeline');
            const pythonResult = await this.runPythonPipeline({
                repository_info: repoScan.repository_info,
                pattern_matches: patterns.matches,
                scan_config: scanConfig
            });
            const duration = (Date.now() - startTime) / 1000;
            logger.info({
                repoPath,
                duration,
                blocks: pythonResult.metrics?.total_code_blocks || 0,
                groups: pythonResult.metrics?.total_duplicate_groups || 0,
                suggestions: pythonResult.metrics?.total_suggestions || 0
            }, 'Repository scan completed successfully');
            const scanResult = {
                ...pythonResult,
                scan_metadata: {
                    duration_seconds: duration,
                    scanned_at: new Date().toISOString(),
                    repository_path: repoPath
                }
            };
            // Auto-generate reports if enabled
            if (this.autoGenerateReports && scanConfig.generateReports !== false) {
                logger.info('Auto-generating reports');
                try {
                    const reportPaths = await this.generateReports(scanResult, this.reportConfig);
                    scanResult.report_paths = reportPaths;
                    logger.info({ reportPaths }, 'Reports auto-generated successfully');
                }
                catch (error) {
                    logger.warn({ error }, 'Report auto-generation failed, continuing');
                }
            }
            return scanResult;
        }
        catch (error) {
            logger.error({ repoPath, error }, 'Repository scan failed');
            throw new ScanError(`Scan failed for ${repoPath}: ${error.message}`, {
                cause: error
            });
        }
    }
    /**
     * Run Python pipeline for extraction, grouping, and reporting
     */
    async runPythonPipeline(data) {
        return new Promise((resolve, reject) => {
            logger.debug('Launching Python extraction pipeline');
            const proc = spawn(this.pythonPath, [this.extractorScript], {
                timeout: 600000, // 10 minute timeout
            });
            let stdout = '';
            let stderr = '';
            // Send input data via stdin
            const jsonData = JSON.stringify(data);
            logger.debug({
                patternMatchCount: data.pattern_matches?.length,
                repoPath: data.repository_info?.path
            }, 'Sending data to Python pipeline');
            proc.stdin?.write(jsonData);
            proc.stdin?.end();
            proc.stdout?.on('data', (data) => {
                stdout += data.toString();
            });
            proc.stderr?.on('data', (data) => {
                const stderrText = data.toString();
                stderr += stderrText;
                // Log warnings at warn level so they're visible
                if (stderrText.includes('Warning:')) {
                    logger.warn({ stderr: stderrText }, 'Python pipeline warning');
                }
                else {
                    logger.debug({ stderr: stderrText }, 'Python pipeline stderr');
                }
            });
            proc.on('close', (code) => {
                if (code === 0) {
                    try {
                        const result = JSON.parse(stdout);
                        resolve(result);
                    }
                    catch (error) {
                        logger.error({ stdout, stderr }, 'Failed to parse Python pipeline output');
                        reject(new Error(`Failed to parse Python output: ${error.message}`));
                    }
                }
                else {
                    logger.error({ code, stderr }, 'Python pipeline failed');
                    reject(new Error(`Python pipeline exited with code ${code}: ${stderr}`));
                }
            });
            proc.on('error', (error) => {
                if (error.code === 'ENOENT') {
                    reject(new Error(`Python not found at: ${this.pythonPath}`));
                }
                else {
                    reject(error);
                }
            });
        });
    }
    /**
     * Generate reports from scan results
     */
    async generateReports(scanResult, options = {}) {
        const repoInfo = scanResult.repository_info || {};
        const repoName = repoInfo.name || 'scan';
        const isInterProject = scanResult.scan_type === 'inter-project';
        const timestamp = new Date().toISOString().replace(/[:.]/g, '-').slice(0, -5);
        const baseName = options.baseName || `${repoName}-${timestamp}`;
        const outputDir = options.outputDir || this.outputDir;
        await fs.mkdir(outputDir, { recursive: true });
        const reportPaths = {};
        try {
            // Generate HTML report
            if (options.html !== false) {
                logger.info({ outputDir }, 'Generating HTML report');
                const htmlPath = path.join(outputDir, `${baseName}.html`);
                await HTMLReportGenerator.saveReport(scanResult, htmlPath, {
                    title: options.title || (isInterProject
                        ? 'Inter-Project Duplicate Detection Report'
                        : `Duplicate Detection Report - ${repoName}`)
                });
                reportPaths.html = htmlPath;
                logger.info({ path: htmlPath }, 'HTML report generated');
            }
            // Generate Markdown report
            if (options.markdown !== false) {
                logger.info({ outputDir }, 'Generating Markdown report');
                const markdownPath = path.join(outputDir, `${baseName}.md`);
                await MarkdownReportGenerator.saveReport(scanResult, markdownPath, {
                    includeDetails: options.includeDetails !== false,
                    maxDuplicates: options.maxDuplicates || 10,
                    maxSuggestions: options.maxSuggestions || 10
                });
                reportPaths.markdown = markdownPath;
                logger.info({ path: markdownPath }, 'Markdown report generated');
            }
            // Generate concise summary
            if (options.summary !== false) {
                logger.info({ outputDir }, 'Generating summary');
                const summaryPath = path.join(outputDir, `${baseName}-summary.md`);
                await MarkdownReportGenerator.saveSummary(scanResult, summaryPath);
                reportPaths.summary = summaryPath;
                logger.info({ path: summaryPath }, 'Summary generated');
            }
            return reportPaths;
        }
        catch (error) {
            logger.error({ error }, 'Report generation failed');
            throw new ScanError(`Report generation failed: ${error.message}`, {
                cause: error
            });
        }
    }
    /**
     * Scan multiple repositories (inter-project analysis)
     */
    async scanMultipleRepositories(repoPaths, scanConfig = {}) {
        logger.info({ count: repoPaths.length }, 'Starting multi-repository scan');
        // Use InterProjectScanner for cross-repository analysis
        const interProjectScanner = new InterProjectScanner({
            orchestrator: {
                scanner: {},
                detector: {},
                pythonPath: this.pythonPath,
                extractorScript: this.extractorScript,
                outputDir: this.outputDir,
                autoGenerateReports: this.autoGenerateReports,
                reports: this.reportConfig
            },
            outputDir: this.outputDir
        });
        try {
            // Delegate to InterProjectScanner for full cross-repository analysis
            const interProjectResult = await interProjectScanner.scanRepositories(repoPaths, scanConfig);
            // Transform InterProjectScanner result to MultiRepositoryScanResult
            const results = [];
            for (const repoScan of interProjectResult.repository_scans) {
                if (repoScan.error) {
                    results.push({
                        error: repoScan.error,
                        repository_path: repoScan.repository_path
                    });
                }
                else if (repoScan.scan_result) {
                    results.push(repoScan.scan_result);
                }
            }
            return {
                repositories: results,
                total_scanned: repoPaths.length,
                successful: results.filter(r => !('error' in r)).length,
                failed: results.filter(r => 'error' in r).length,
                cross_repository_duplicates: interProjectResult.cross_repository_duplicates,
                cross_repository_suggestions: interProjectResult.cross_repository_suggestions,
                scan_type: 'inter-project',
                metrics: interProjectResult.metrics
            };
        }
        catch (error) {
            logger.error({ error }, 'Multi-repository scan failed');
            // Fallback: scan each repository individually without cross-repo analysis
            logger.warn('Falling back to individual repository scans without cross-repository analysis');
            const results = [];
            for (const repoPath of repoPaths) {
                try {
                    const result = await this.scanRepository(repoPath, scanConfig);
                    results.push(result);
                }
                catch (scanError) {
                    logger.warn({ repoPath, error: scanError }, 'Repository scan failed, continuing');
                    results.push({
                        error: scanError.message,
                        repository_path: repoPath
                    });
                }
            }
            return {
                repositories: results,
                total_scanned: repoPaths.length,
                successful: results.filter(r => !('error' in r)).length,
                failed: results.filter(r => 'error' in r).length,
                scan_type: 'single-project'
            };
        }
    }
}
// ============================================================================
// Error Class
// ============================================================================
/**
 * Custom error class for scan orchestration errors
 */
export class ScanError extends Error {
    constructor(message, options) {
        super(message);
        if (options?.cause) {
            this.cause = options.cause;
        }
        this.name = 'ScanError';
    }
}
</file>

<file path="pipeline-core/scan-orchestrator.ts">
/**
 * Scan Orchestrator - TypeScript version
 *
 * Coordinates the entire duplicate detection pipeline:
 * 1. Repository scanning (repomix)
 * 2. Pattern detection (ast-grep)
 * 3. Code block extraction (Python/pydantic)
 * 4. Semantic annotation (Python)
 * 5. Duplicate grouping (Python)
 * 6. Suggestion generation (Python)
 * 7. Report generation (Python)
 */

import { RepositoryScanner } from './scanners/repository-scanner.js';
import { AstGrepPatternDetector } from './scanners/ast-grep-detector.js';
import { HTMLReportGenerator } from './reports/html-report-generator.js';
import { MarkdownReportGenerator } from './reports/markdown-report-generator.js';
import { InterProjectScanner } from './inter-project-scanner.js';
import { createComponentLogger } from '../utils/logger.js';
import { spawn, ChildProcess } from 'child_process';
import * as path from 'path';
import * as fs from 'fs/promises';
import * as Sentry from '@sentry/node';

const logger = createComponentLogger('ScanOrchestrator');

// ============================================================================
// Type Definitions
// ============================================================================

/**
 * Repository information from scanner
 */
export interface RepositoryInfo {
  path: string;
  name: string;
  gitRemote?: string;
  gitBranch?: string;
  gitCommit?: string;
  totalFiles: number;
  totalLines: number;
  languages: string[];
}

/**
 * Pattern match from ast-grep
 */
export interface PatternMatch {
  file_path: string;
  rule_id: string;
  matched_text: string;
  line_start: number;
  line_end: number;
  column_start?: number;
  column_end?: number;
  severity?: string;
  confidence?: number;
}

/**
 * Scan configuration options
 */
export interface ScanConfig {
  scan_config?: {
    includeTests?: boolean;
    maxDepth?: number;
    excludePaths?: string[];
    [key: string]: any;
  };
  pattern_config?: {
    rulesDirectory?: string;
    configPath?: string;
    [key: string]: any;
  };
  generateReports?: boolean;
  [key: string]: any;
}

/**
 * Python pipeline input data structure
 */
export interface PythonPipelineInput {
  repository_info: RepositoryInfo;
  pattern_matches: PatternMatch[];
  scan_config: ScanConfig;
}

/**
 * Code block from Python pipeline
 */
export interface CodeBlock {
  block_id: string;
  file_path: string;
  line_start: number;
  line_end: number;
  source_code: string;
  language: string;
  semantic_category?: string;
  tags: string[];
  complexity_metrics?: {
    cyclomatic: number;
    cognitive: number;
    halstead: Record<string, number>;
  };
}

/**
 * Duplicate group from Python pipeline
 */
export interface DuplicateGroup {
  group_id: string;
  block_ids: string[];
  similarity_score: number;
  group_type: 'exact' | 'structural' | 'semantic';
  total_lines: number;
  potential_reduction: number;
  impact_score?: number;
}

/**
 * Consolidation suggestion from Python pipeline
 */
export interface ConsolidationSuggestion {
  suggestion_id: string;
  group_id: string;
  suggestion_type: string;
  priority: 'critical' | 'high' | 'medium' | 'low';
  estimated_effort: 'minimal' | 'low' | 'medium' | 'high';
  potential_reduction: number;
  implementation_notes?: string;
  migration_steps?: Array<{
    order: number;
    description: string;
    code_snippet?: string;
  }>;
}

/**
 * Scan metrics from Python pipeline
 */
export interface ScanMetrics {
  total_code_blocks: number;
  code_blocks_by_category?: Record<string, number>;
  code_blocks_by_language?: Record<string, number>;
  total_duplicate_groups: number;
  exact_duplicates: number;
  structural_duplicates: number;
  semantic_duplicates: number;
  total_duplicated_lines: number;
  potential_loc_reduction: number;
  duplication_percentage: number;
  total_suggestions: number;
  quick_wins?: number;
  high_priority_suggestions?: number;
}

/**
 * Python pipeline output structure
 */
export interface PythonPipelineOutput {
  code_blocks: CodeBlock[];
  duplicate_groups: DuplicateGroup[];
  suggestions: ConsolidationSuggestion[];
  metrics: ScanMetrics;
  repository_info: RepositoryInfo;
  scan_type?: 'single-project' | 'inter-project';
  error?: string;
  warnings?: string[];
}

/**
 * Scan result with metadata
 */
export interface ScanResult extends PythonPipelineOutput {
  scan_metadata: {
    duration_seconds: number;
    scanned_at: string;
    repository_path: string;
  };
  report_paths?: ReportPaths;
}

/**
 * Report generation paths
 */
export interface ReportPaths {
  html?: string;
  markdown?: string;
  summary?: string;
  json?: string;
}

/**
 * Report generation options
 */
export interface ReportOptions {
  outputDir?: string;
  baseName?: string;
  title?: string;
  html?: boolean;
  markdown?: boolean;
  summary?: boolean;
  json?: boolean;
  includeDetails?: boolean;
  maxDuplicates?: number;
  maxSuggestions?: number;
}

/**
 * Scan orchestrator constructor options
 */
export interface ScanOrchestratorOptions {
  scanner?: Record<string, any>;
  detector?: Record<string, any>;
  pythonPath?: string;
  extractorScript?: string;
  reports?: ReportOptions;
  outputDir?: string;
  autoGenerateReports?: boolean;
  config?: Record<string, any>;
}

/**
 * Cross-repository duplicate group
 */
export interface CrossRepositoryDuplicate {
  group_id: string;
  pattern_id: string;
  content_hash: string;
  member_blocks: CodeBlock[];
  occurrence_count: number;
  repository_count: number;
  affected_repositories: string[];
  affected_files: string[];
  category: string;
  language: string;
  total_lines: number;
  similarity_score: number;
  similarity_method: string;
  impact_score: number;
}

/**
 * Cross-repository consolidation suggestion
 */
export interface CrossRepositorySuggestion {
  suggestion_id: string;
  group_id: string;
  suggestion_type: string;
  priority: 'critical' | 'high' | 'medium' | 'low';
  estimated_effort: 'minimal' | 'low' | 'medium' | 'high';
  potential_reduction: number;
  affected_repositories: string[];
  implementation_notes?: string;
  migration_steps?: Array<{
    order: number;
    description: string;
    code_snippet?: string;
  }>;
}

/**
 * Multi-repository scan result
 */
export interface MultiRepositoryScanResult {
  repositories: Array<ScanResult | { error: string; repository_path: string }>;
  total_scanned: number;
  successful: number;
  failed: number;
  cross_repository_duplicates?: CrossRepositoryDuplicate[];
  cross_repository_suggestions?: CrossRepositorySuggestion[];
  scan_type?: 'single-project' | 'inter-project';
  metrics?: ScanMetrics & {
    total_cross_repository_groups?: number;
    cross_repository_occurrences?: number;
    cross_repository_duplicated_lines?: number;
  };
}

/**
 * Repository scan output from RepositoryScanner
 */
interface RepositoryScanOutput {
  repository_info: RepositoryInfo;
  metadata?: {
    totalFiles: number;
    totalLines: number;
    languages: string[];
  };
  repomix_output?: any;
}

/**
 * Pattern detection output from AstGrepPatternDetector
 */
interface PatternDetectionOutput {
  matches: PatternMatch[];
  statistics: {
    total_matches: number;
    rules_applied: number;
    files_scanned: number;
    scan_duration_ms: number;
  };
}

// ============================================================================
// Main ScanOrchestrator Class
// ============================================================================

/**
 * Scan Orchestrator - Coordinates the duplicate detection pipeline
 */
export class ScanOrchestrator {
  private readonly repositoryScanner: RepositoryScanner;
  private readonly patternDetector: AstGrepPatternDetector;
  private readonly pythonPath: string;
  private readonly extractorScript: string;
  private readonly reportConfig: ReportOptions;
  private readonly outputDir: string;
  private readonly autoGenerateReports: boolean;
  private readonly config: Record<string, any>;

  constructor(options: ScanOrchestratorOptions = {}) {
    // JavaScript components
    this.repositoryScanner = new RepositoryScanner(options.scanner || {});
    this.patternDetector = new AstGrepPatternDetector(options.detector || {});

    // Python components (called via subprocess)
    // Use venv python by default if it exists, otherwise fall back to system python3
    const venvPython = path.join(process.cwd(), 'venv/bin/python3');
    this.pythonPath = options.pythonPath || venvPython;
    this.extractorScript = options.extractorScript || path.join(process.cwd(), 'sidequest/pipeline-core/extractors/extract_blocks.py');

    // Report generation configuration
    this.reportConfig = options.reports || {};
    this.outputDir = options.outputDir || path.join(process.cwd(), 'output', 'reports');
    this.autoGenerateReports = options.autoGenerateReports !== false; // Default: true

    // Configuration
    this.config = options.config || {};
  }

  /**
   * Scan a single repository for duplicates
   */
  async scanRepository(repoPath: string, scanConfig: ScanConfig = {}): Promise<ScanResult> {
    const startTime = Date.now();

    // Validate repoPath
    if (!repoPath || typeof repoPath !== 'string') {
      const error = new ScanError(
        `Invalid repository path: ${repoPath === undefined ? 'undefined' : repoPath === null ? 'null' : typeof repoPath}. ` +
        `Expected a valid file system path string.`
      );
      logger.error({ repoPath, type: typeof repoPath }, 'Invalid repository path provided');
      Sentry.captureException(error, {
        tags: {
          error_type: 'validation_error',
          component: 'ScanOrchestrator'
        },
        extra: {
          repoPath,
          repoPathType: typeof repoPath,
          scanConfig
        }
      });
      throw error;
    }

    logger.info({ repoPath }, 'Starting repository duplicate scan');

    try {
      // Stage 1: Repository scanning
      logger.info('Stage 1/7: Scanning repository with repomix');
      const repoScan: RepositoryScanOutput = await this.repositoryScanner.scanRepository(
        repoPath,
        scanConfig.scan_config || {}
      );

      // Stage 2: Pattern detection
      logger.info('Stage 2/7: Detecting patterns with ast-grep');
      const patterns: PatternDetectionOutput = await this.patternDetector.detectPatterns(
        repoPath,
        scanConfig.pattern_config || {}
      );

      // Stage 3-7: Python pipeline
      logger.info('Stage 3-7: Running Python extraction and analysis pipeline');
      const pythonResult = await this.runPythonPipeline({
        repository_info: repoScan.repository_info,
        pattern_matches: patterns.matches,
        scan_config: scanConfig
      });

      const duration = (Date.now() - startTime) / 1000;

      logger.info({
        repoPath,
        duration,
        blocks: pythonResult.metrics?.total_code_blocks || 0,
        groups: pythonResult.metrics?.total_duplicate_groups || 0,
        suggestions: pythonResult.metrics?.total_suggestions || 0
      }, 'Repository scan completed successfully');

      const scanResult: ScanResult = {
        ...pythonResult,
        scan_metadata: {
          duration_seconds: duration,
          scanned_at: new Date().toISOString(),
          repository_path: repoPath
        }
      };

      // Auto-generate reports if enabled
      if (this.autoGenerateReports && scanConfig.generateReports !== false) {
        logger.info('Auto-generating reports');
        try {
          const reportPaths = await this.generateReports(scanResult, this.reportConfig);
          scanResult.report_paths = reportPaths;
          logger.info({ reportPaths }, 'Reports auto-generated successfully');
        } catch (error) {
          logger.warn({ error }, 'Report auto-generation failed, continuing');
        }
      }

      return scanResult;

    } catch (error) {
      logger.error({ repoPath, error }, 'Repository scan failed');
      throw new ScanError(`Scan failed for ${repoPath}: ${(error as Error).message}`, {
        cause: error
      });
    }
  }

  /**
   * Run Python pipeline for extraction, grouping, and reporting
   */
  private async runPythonPipeline(data: PythonPipelineInput): Promise<PythonPipelineOutput> {
    return new Promise<PythonPipelineOutput>((resolve, reject) => {
      logger.debug('Launching Python extraction pipeline');

      const proc: ChildProcess = spawn(this.pythonPath, [this.extractorScript], {
        timeout: 600000, // 10 minute timeout
      });

      let stdout = '';
      let stderr = '';

      // Send input data via stdin
      const jsonData = JSON.stringify(data);
      logger.debug({
        patternMatchCount: data.pattern_matches?.length,
        repoPath: data.repository_info?.path
      }, 'Sending data to Python pipeline');

      proc.stdin?.write(jsonData);
      proc.stdin?.end();

      proc.stdout?.on('data', (data: Buffer) => {
        stdout += data.toString();
      });

      proc.stderr?.on('data', (data: Buffer) => {
        const stderrText = data.toString();
        stderr += stderrText;
        // Log warnings at warn level so they're visible
        if (stderrText.includes('Warning:')) {
          logger.warn({ stderr: stderrText }, 'Python pipeline warning');
        } else {
          logger.debug({ stderr: stderrText }, 'Python pipeline stderr');
        }
      });

      proc.on('close', (code: number | null) => {
        if (code === 0) {
          try {
            const result: PythonPipelineOutput = JSON.parse(stdout);
            resolve(result);
          } catch (error) {
            logger.error({ stdout, stderr }, 'Failed to parse Python pipeline output');
            reject(new Error(`Failed to parse Python output: ${(error as Error).message}`));
          }
        } else {
          logger.error({ code, stderr }, 'Python pipeline failed');
          reject(new Error(`Python pipeline exited with code ${code}: ${stderr}`));
        }
      });

      proc.on('error', (error: NodeJS.ErrnoException) => {
        if (error.code === 'ENOENT') {
          reject(new Error(`Python not found at: ${this.pythonPath}`));
        } else {
          reject(error);
        }
      });
    });
  }

  /**
   * Generate reports from scan results
   */
  async generateReports(scanResult: ScanResult, options: ReportOptions = {}): Promise<ReportPaths> {
    const repoInfo = scanResult.repository_info || {} as RepositoryInfo;
    const repoName = repoInfo.name || 'scan';
    const isInterProject = scanResult.scan_type === 'inter-project';

    const timestamp = new Date().toISOString().replace(/[:.]/g, '-').slice(0, -5);
    const baseName = options.baseName || `${repoName}-${timestamp}`;

    const outputDir = options.outputDir || this.outputDir;
    await fs.mkdir(outputDir, { recursive: true });

    const reportPaths: ReportPaths = {};

    try {
      // Generate HTML report
      if (options.html !== false) {
        logger.info({ outputDir }, 'Generating HTML report');
        const htmlPath = path.join(outputDir, `${baseName}.html`);

        await HTMLReportGenerator.saveReport(scanResult, htmlPath, {
          title: options.title || (isInterProject
            ? 'Inter-Project Duplicate Detection Report'
            : `Duplicate Detection Report - ${repoName}`)
        });

        reportPaths.html = htmlPath;
        logger.info({ path: htmlPath }, 'HTML report generated');
      }

      // Generate Markdown report
      if (options.markdown !== false) {
        logger.info({ outputDir }, 'Generating Markdown report');
        const markdownPath = path.join(outputDir, `${baseName}.md`);

        await MarkdownReportGenerator.saveReport(scanResult, markdownPath, {
          includeDetails: options.includeDetails !== false,
          maxDuplicates: options.maxDuplicates || 10,
          maxSuggestions: options.maxSuggestions || 10
        });

        reportPaths.markdown = markdownPath;
        logger.info({ path: markdownPath }, 'Markdown report generated');
      }

      // Generate concise summary
      if (options.summary !== false) {
        logger.info({ outputDir }, 'Generating summary');
        const summaryPath = path.join(outputDir, `${baseName}-summary.md`);

        await MarkdownReportGenerator.saveSummary(scanResult, summaryPath);

        reportPaths.summary = summaryPath;
        logger.info({ path: summaryPath }, 'Summary generated');
      }

      return reportPaths;

    } catch (error) {
      logger.error({ error }, 'Report generation failed');
      throw new ScanError(`Report generation failed: ${(error as Error).message}`, {
        cause: error
      });
    }
  }

  /**
   * Scan multiple repositories (inter-project analysis)
   */
  async scanMultipleRepositories(
    repoPaths: string[],
    scanConfig: ScanConfig = {}
  ): Promise<MultiRepositoryScanResult> {
    logger.info({ count: repoPaths.length }, 'Starting multi-repository scan');

    // Use InterProjectScanner for cross-repository analysis
    const interProjectScanner = new InterProjectScanner({
      orchestrator: {
        scanner: {},
        detector: {},
        pythonPath: this.pythonPath,
        extractorScript: this.extractorScript,
        outputDir: this.outputDir,
        autoGenerateReports: this.autoGenerateReports,
        reports: this.reportConfig
      },
      outputDir: this.outputDir
    });

    try {
      // Delegate to InterProjectScanner for full cross-repository analysis
      const interProjectResult = await interProjectScanner.scanRepositories(repoPaths, scanConfig);

      // Transform InterProjectScanner result to MultiRepositoryScanResult
      const results: Array<ScanResult | { error: string; repository_path: string }> = [];

      for (const repoScan of interProjectResult.repository_scans) {
        if (repoScan.error) {
          results.push({
            error: repoScan.error,
            repository_path: repoScan.repository_path
          });
        } else if (repoScan.scan_result) {
          results.push(repoScan.scan_result);
        }
      }

      return {
        repositories: results,
        total_scanned: repoPaths.length,
        successful: results.filter(r => !('error' in r)).length,
        failed: results.filter(r => 'error' in r).length,
        cross_repository_duplicates: interProjectResult.cross_repository_duplicates,
        cross_repository_suggestions: interProjectResult.cross_repository_suggestions,
        scan_type: 'inter-project',
        metrics: interProjectResult.metrics
      };
    } catch (error) {
      logger.error({ error }, 'Multi-repository scan failed');

      // Fallback: scan each repository individually without cross-repo analysis
      logger.warn('Falling back to individual repository scans without cross-repository analysis');

      const results: Array<ScanResult | { error: string; repository_path: string }> = [];

      for (const repoPath of repoPaths) {
        try {
          const result = await this.scanRepository(repoPath, scanConfig);
          results.push(result);
        } catch (scanError) {
          logger.warn({ repoPath, error: scanError }, 'Repository scan failed, continuing');
          results.push({
            error: (scanError as Error).message,
            repository_path: repoPath
          });
        }
      }

      return {
        repositories: results,
        total_scanned: repoPaths.length,
        successful: results.filter(r => !('error' in r)).length,
        failed: results.filter(r => 'error' in r).length,
        scan_type: 'single-project'
      };
    }
  }
}

// ============================================================================
// Error Class
// ============================================================================

/**
 * Custom error class for scan orchestration errors
 */
export class ScanError extends Error {
  constructor(message: string, options?: { cause?: unknown }) {
    super(message);
    if (options?.cause) {
      (this as any).cause = options.cause;
    }
    this.name = 'ScanError';
  }
}
</file>

<file path="pipeline-runners/claude-health-pipeline.js">
#!/usr/bin/env node
// @ts-nocheck

/**
 * Claude Health Check Pipeline - AlephAuto Integration
 *
 * Automated monitoring and health checking for Claude Code environment.
 * Runs comprehensive checks including:
 * - Environment setup and direnv configuration
 * - Directory structure verification
 * - Configuration validation
 * - Hook permissions and registration
 * - Plugin analysis and duplicate detection
 * - Performance monitoring
 *
 * Usage:
 *   npm run claude:health           # Run immediate health check
 *   npm run claude:health:detailed  # Run detailed health check
 *   npm run claude:health:schedule  # Start cron scheduler (daily 8 AM)
 *   RUN_ON_STARTUP=true npm run claude:health  # Run on startup
 *
 * Environment Variables:
 *   CLAUDE_HEALTH_CRON_SCHEDULE - Cron schedule (default: "0 8 * * *" - daily 8 AM)
 *   RUN_ON_STARTUP - Run immediately on startup (default: false)
 *   DETAILED - Include detailed component listing (default: false)
 *   SKIP_VALIDATION - Skip configuration validation (default: false)
 *   SKIP_PERFORMANCE - Skip performance log analysis (default: false)
 *   SKIP_PLUGINS - Skip plugin analysis (default: false)
 */

import { ClaudeHealthWorker } from '../workers/claude-health-worker.js';
import { createComponentLogger } from '../utils/logger.js';
import { config } from '../core/config.js';
import cron from 'node-cron';

const logger = createComponentLogger('ClaudeHealthPipeline');

/**
 * Claude Health Check Pipeline
 */
class ClaudeHealthPipeline {
  constructor(options = {}) {
    this.worker = new ClaudeHealthWorker({
      maxConcurrent: 1,
      logDir: config.logDir,
      sentryDsn: config.sentryDsn,
      ...options
    });

    this.options = {
      detailed: process.env.DETAILED === 'true',
      skipValidation: process.env.SKIP_VALIDATION === 'true',
      skipPerformance: process.env.SKIP_PERFORMANCE === 'true',
      skipPlugins: process.env.SKIP_PLUGINS === 'true'
    };

    this.setupEventListeners();
  }

  /**
   * Setup event listeners for job events
   */
  setupEventListeners() {
    this.worker.on('job:created', (job) => {
      logger.info({ jobId: job.id }, 'Health check job created');
    });

    this.worker.on('job:started', (job) => {
      logger.info({ jobId: job.id }, 'Health check job started');
    });

    this.worker.on('job:completed', (job) => {
      const result = job.result;

      logger.info({
        jobId: job.id,
        healthScore: result.summary.healthScore,
        status: result.summary.status,
        criticalIssues: result.summary.criticalIssues,
        warnings: result.summary.warnings,
        duration: result.duration
      }, 'Health check job completed');

      // Display results
      this.displayResults(result);
    });

    this.worker.on('job:failed', (job) => {
      logger.error({
        jobId: job.id,
        error: job.error
      }, 'Health check job failed');
    });
  }

  /**
   * Display health check results
   * @param {Object} result - Health check results
   */
  displayResults(result) {
    printSummary(result);

    if (result.recommendations.length > 0) {
      printRecommendations(result.recommendations);
    }

    if (this.options.detailed) {
      printDetailedChecks(result.checks);
    }
  }

  /**
   * Run a single health check
   * @param {Object} options - Check options
   */
  async runHealthCheck(options = {}) {
    logger.info({ options }, 'Starting health check');

    const startTime = Date.now();

    try {
      // Create health check job
      const job = this.worker.addJob({
        detailed: this.options.detailed,
        validateConfig: !this.options.skipValidation,
        checkPerformance: !this.options.skipPerformance,
        analyzePlugins: !this.options.skipPlugins,
        ...options
      });

      logger.info({ jobId: job.id }, 'Health check job created');

      // Wait for completion
      await this.waitForCompletion();

      const duration = Date.now() - startTime;
      const stats = this.worker.getStats();

      logger.info({
        duration,
        stats
      }, 'Health check pipeline completed');

      return stats;
    } catch (error) {
      logger.error({ error }, 'Health check pipeline failed');
      throw error;
    }
  }

  /**
   * Wait for all jobs to complete
   */
  async waitForCompletion() {
    return new Promise((resolve) => {
      const checkInterval = setInterval(() => {
        const stats = this.worker.getStats();
        if (stats.active === 0 && stats.queued === 0) {
          clearInterval(checkInterval);
          resolve();
        }
      }, 100);
    });
  }

  /**
   * Schedule automatic health checks
   * @param {string} cronSchedule - Cron schedule string
   * @returns {Object} Cron task
   */
  scheduleHealthChecks(cronSchedule) {
    logger.info({ cronSchedule }, 'Scheduling health checks');

    const task = cron.schedule(cronSchedule, () => {
      logger.info('Cron triggered health check');
      this.runHealthCheck().catch(error => {
        logger.error({ error }, 'Scheduled health check failed');
      });
    });

    return task;
  }

  /**
   * Get worker statistics
   */
  getStats() {
    return this.worker.getStats();
  }
}

// Print functions
function printSummary(result) {
  console.log('\n‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó');
  console.log('‚ïë          Claude Code Health Check Summary                     ‚ïë');
  console.log('‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n');

  console.log(`Health Score: ${getScoreColor(result.summary.healthScore)}${result.summary.healthScore}/100\x1b[0m`);
  console.log(`Status:       ${result.summary.message}\n`);

  // Component inventory
  if (result.checks.components) {
    console.log('Component Inventory:');
    console.log(`  Skills:        ${result.checks.components.skills}`);
    console.log(`  Agents:        ${result.checks.components.agents}`);
    console.log(`  Commands:      ${result.checks.components.commands}`);
    console.log(`  Hooks:         ${result.checks.hooks?.executableHooks || 0}/${result.checks.hooks?.totalHooks || 0} executable`);
    console.log(`  Registered:    ${result.checks.hooks?.registeredHooks || 0} hook types`);
    if (result.checks.plugins) {
      console.log(`  Plugins:       ${result.checks.plugins.totalPlugins}`);
    }
    console.log(`  Active Tasks:  ${result.checks.components.activeTasks}`);
    console.log(`  Archived:      ${result.checks.components.archivedTasks}`);
    console.log('');
  }

  // Environment
  if (result.checks.environment) {
    const env = result.checks.environment;
    console.log('Environment:');
    console.log(`  Node.js:       ${env.nodeVersion || 'not found'}`);
    console.log(`  npm:           ${env.npmVersion || 'not found'}`);
    console.log(`  direnv:        ${env.direnv ? '‚úì installed' : '‚úó not installed'}`);
    if (env.direnv && !env.direnvAllowed) {
      console.log(`  \x1b[33m‚ö†  Environment variables not loaded\x1b[0m`);
    }
    console.log('');
  }

  // Summary statistics
  console.log('Summary:');
  console.log(`  Critical Issues: ${result.summary.criticalIssues}`);
  console.log(`  Warnings:        ${result.summary.warnings}`);
  console.log(`  Duration:        ${result.duration}ms\n`);
}

function printRecommendations(recommendations) {
  console.log('‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó');
  console.log('‚ïë          Recommendations                                       ‚ïë');
  console.log('‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n');

  for (const rec of recommendations) {
    const icon = rec.priority === 'high' ? 'üî¥' : rec.priority === 'medium' ? 'üü°' : '‚úÖ';
    const priority = rec.priority.toUpperCase();

    console.log(`${icon} [${priority}] ${rec.type}`);
    console.log(`   ${rec.message}`);
    console.log(`   Action: ${rec.action}`);

    if (rec.details) {
      console.log('   Details:');
      for (const detail of rec.details) {
        if (detail.category && detail.plugins) {
          console.log(`     ‚Ä¢ ${detail.category}: ${detail.plugins.join(', ')}`);
          if (detail.suggestion) {
            console.log(`       ‚Üí ${detail.suggestion}`);
          }
        }
      }
    }
    console.log('');
  }
}

function printDetailedChecks(checks) {
  console.log('‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó');
  console.log('‚ïë          Detailed Check Results                                ‚ïë');
  console.log('‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n');

  // Configuration
  if (checks.configuration) {
    console.log('Configuration Files:');
    console.log(`  settings.json:     ${checks.configuration.settingsJson.valid ? '‚úì' : '‚úó'} valid`);
    console.log(`  skill-rules.json:  ${checks.configuration.skillRulesJson.valid ? '‚úì' : '‚úó'} valid`);
    console.log(`  package.json:      ${checks.configuration.packageJson.valid ? '‚úì' : '‚úó'} valid`);
    console.log(`  .envrc:            ${checks.configuration.envrc.exists ? '‚úì' : '‚úó'} exists`);
    console.log('');
  }

  // Hooks
  if (checks.hooks && checks.hooks.hooks) {
    console.log('Hook Details:');
    for (const hook of checks.hooks.hooks) {
      const status = hook.executable ? '‚úì' : '‚úó';
      console.log(`  ${status} ${hook.name}`);
    }
    console.log('');
  }

  // Plugins
  if (checks.plugins && checks.plugins.duplicateCategories && checks.plugins.duplicateCategories.length > 0) {
    console.log('Duplicate Plugin Categories:');
    for (const cat of checks.plugins.duplicateCategories) {
      console.log(`  ${cat.category} (${cat.count} plugins):`);
      for (const plugin of cat.plugins) {
        console.log(`    ‚Ä¢ ${plugin}`);
      }
    }
    console.log('');
  }

  // Performance
  if (checks.performance && checks.performance.logExists) {
    console.log('Performance:');
    console.log(`  Log size:       ${formatBytes(checks.performance.logSize)}`);
    console.log(`  Total entries:  ${checks.performance.totalEntries}`);
    console.log(`  Slow hooks:     ${checks.performance.slowHooks}`);
    console.log(`  Failures:       ${checks.performance.failures}`);

    if (checks.performance.slowHookDetails && checks.performance.slowHookDetails.length > 0) {
      console.log('\n  Slowest hooks:');
      for (const hook of checks.performance.slowHookDetails) {
        console.log(`    ‚Ä¢ ${hook.hook}: ${hook.duration}ms`);
      }
    }
    console.log('');
  }
}

function getScoreColor(score) {
  if (score >= 90) return '\x1b[32m'; // Green
  if (score >= 70) return '\x1b[33m'; // Yellow
  return '\x1b[31m'; // Red
}

function formatBytes(bytes) {
  if (bytes === 0) return '0 Bytes';
  const k = 1024;
  const sizes = ['Bytes', 'KB', 'MB', 'GB'];
  const i = Math.floor(Math.log(bytes) / Math.log(k));
  return Math.round(bytes / Math.pow(k, i) * 100) / 100 + ' ' + sizes[i];
}

// Initialize pipeline
const pipeline = new ClaudeHealthPipeline();

// Configuration
const CRON_SCHEDULE = process.env.CLAUDE_HEALTH_CRON_SCHEDULE || '0 8 * * *';
const RUN_ON_STARTUP = process.env.RUN_ON_STARTUP === 'true';
const CRON_ENABLED = process.argv.includes('--cron');

logger.info('Claude Health Pipeline initialized', {
  cronEnabled: CRON_ENABLED,
  cronSchedule: CRON_SCHEDULE,
  runOnStartup: RUN_ON_STARTUP
});

// Main execution
(async () => {
  try {
    // Run immediately if requested
    if (RUN_ON_STARTUP) {
      logger.info('Running health check on startup');
      await pipeline.runHealthCheck();

      if (!CRON_ENABLED) {
        const stats = pipeline.getStats();
        const hasFailures = stats.failed > 0;
        process.exit(hasFailures ? 1 : 0);
      }
    }

    // Setup cron if enabled
    if (CRON_ENABLED) {
      logger.info('Setting up cron schedule', { schedule: CRON_SCHEDULE });
      pipeline.scheduleHealthChecks(CRON_SCHEDULE);

      console.log(`\n‚úì Claude Health Check scheduled: ${CRON_SCHEDULE}`);
      console.log('  Press Ctrl+C to stop\n');

      // Keep process alive
      process.stdin.resume();
    } else if (!RUN_ON_STARTUP) {
      // Run once immediately if not cron mode and not startup
      await pipeline.runHealthCheck();
      const stats = pipeline.getStats();
      const hasFailures = stats.failed > 0;
      process.exit(hasFailures ? 1 : 0);
    }
  } catch (error) {
    logger.error({ error }, 'Pipeline execution failed');
    process.exit(1);
  }
})();

// Handle graceful shutdown
process.on('SIGINT', () => {
  logger.info('Received SIGINT, shutting down gracefully');
  process.exit(0);
});

process.on('SIGTERM', () => {
  logger.info('Received SIGTERM, shutting down gracefully');
  process.exit(0);
});
</file>

<file path="pipeline-runners/collect_git_activity.py">
#!/usr/bin/env python3
"""
Comprehensive Git Activity Report Generator

Scans multiple repositories, analyzes commits, generates visualizations,
and creates a formatted markdown report for Jekyll sites.

Usage:
    python3 collect_git_activity.py --start-date 2025-07-07 --end-date 2025-11-16
    python3 collect_git_activity.py --days 7  # Last 7 days
    python3 collect_git_activity.py --weekly  # Last week
"""

import argparse
import json
import math
import os
import subprocess
from collections import defaultdict
from datetime import datetime, timedelta
from pathlib import Path


# Configuration
CODE_DIR = Path.home() / 'code'
EXCLUDE_PATTERNS = ['vim/bundle', 'node_modules', '.git', 'venv', '.venv']
DEFAULT_MAX_DEPTH = 2

# Language/File Type Mapping
LANGUAGE_EXTENSIONS = {
    'Python': ['.py', '.pyw'],
    'JavaScript': ['.js', '.mjs', '.cjs'],
    'TypeScript': ['.ts', '.tsx'],
    'Ruby': ['.rb', '.rake', '.gemspec'],
    'HTML': ['.html', '.htm'],
    'CSS/SCSS': ['.css', '.scss', '.sass', '.less'],
    'Markdown': ['.md', '.markdown'],
    'JSON': ['.json'],
    'YAML': ['.yml', '.yaml'],
    'Shell': ['.sh', '.bash', '.zsh'],
    'SQL': ['.sql'],
    'Go': ['.go'],
    'Rust': ['.rs'],
    'C/C++': ['.c', '.cpp', '.cc', '.h', '.hpp'],
    'Java': ['.java'],
    'PHP': ['.php'],
    'Lock Files': ['.lock', 'package-lock.json', 'Gemfile.lock', 'yarn.lock', 'pnpm-lock.yaml'],
    'SVG': ['.svg'],
    'Images': ['.png', '.jpg', '.jpeg', '.gif', '.webp', '.ico', '.bmp'],
    'Data Files': ['.csv', '.xml', '.tsv', '.parquet'],
    'Text Files': ['.txt', '.log', '.ini', '.conf']
}


def find_git_repos(max_depth=DEFAULT_MAX_DEPTH):
    """Find all git repositories, excluding specified patterns"""
    print(f"Scanning for repositories in {CODE_DIR} (depth: {max_depth})...")
    cmd = f"find {CODE_DIR} -maxdepth {max_depth} -name .git -type d"
    result = subprocess.run(cmd.split(), capture_output=True, text=True)

    repos = []
    for line in result.stdout.strip().split('\n'):
        if line:
            repo_path = Path(line).parent
            # Exclude patterns
            if not any(pattern in str(repo_path) for pattern in EXCLUDE_PATTERNS):
                repos.append(repo_path)

    print(f"Found {len(repos)} repositories")
    return sorted(repos)


def get_repo_stats(repo_path, since_date, until_date=None):
    """Get commit statistics for a repository"""
    try:
        os.chdir(repo_path)

        # Build git log command
        git_cmd = f"git log --since={since_date} --all --oneline"
        if until_date:
            git_cmd += f" --until={until_date}"

        result = subprocess.run(git_cmd.split(), capture_output=True, text=True)
        commits = len(result.stdout.strip().split('\n')) if result.stdout.strip() else 0

        # Get file changes for language analysis
        git_cmd = f"git log --since={since_date} --all --name-only --pretty=format:"
        if until_date:
            git_cmd += f" --until={until_date}"

        result = subprocess.run(git_cmd.split(), capture_output=True, text=True)
        files = [f for f in result.stdout.strip().split('\n') if f]

        # Get parent directory (for organization/grouping)
        parent = repo_path.parent.name if repo_path.parent != CODE_DIR else None

        return {
            'path': str(repo_path),
            'name': repo_path.name,
            'parent': parent,
            'commits': commits,
            'files': files
        }
    except Exception as e:
        print(f"Error processing {repo_path}: {e}")
        return None


def analyze_languages(all_files):
    """Analyze file changes by programming language"""
    language_stats = defaultdict(int)

    for file_path in all_files:
        file_ext = Path(file_path).suffix.lower()
        file_name = Path(file_path).name

        # Map to language
        found = False
        for language, extensions in LANGUAGE_EXTENSIONS.items():
            if file_ext in extensions or file_name in extensions:
                language_stats[language] += 1
                found = True
                break

        if not found and file_ext:
            language_stats['Other'] += 1

    return dict(language_stats)


def find_project_websites(repositories):
    """Scan for CNAME files to discover GitHub Pages websites"""
    websites = {}
    for repo in repositories:
        repo_path = Path(repo['path'])
        cname_file = repo_path / 'CNAME'

        if cname_file.exists():
            try:
                website = cname_file.read_text().strip()
                if website and '.' in website:  # Basic validation
                    websites[repo['name']] = f"https://{website}"
            except Exception:
                pass

    return websites


def categorize_repositories(repositories):
    """Categorize repositories by project type"""
    categories = {
        'Data & Analytics': [],
        'Personal Sites': [],
        'Infrastructure': [],
        'MCP Servers': [],
        'Client Work': [],
        'Business Apps': [],
        'Legacy': []
    }

    for repo in repositories:
        name = repo['name'].lower()
        commits = repo['commits']

        # Categorization logic (customize based on your projects)
        if 'scraper' in name or 'analytics' in name or 'bot' in name:
            categories['Data & Analytics'].append(repo)
        elif 'personalsite' in name or 'github.io' in name:
            categories['Personal Sites'].append(repo)
        elif 'mcp' in name or 'server' in name:
            categories['MCP Servers'].append(repo)
        elif commits < 5:
            categories['Legacy'].append(repo)
        elif 'integrity' in name or 'studio' in name or 'visualizer' in name:
            categories['Infrastructure'].append(repo)
        elif 'inventory' in name or 'financial' in name:
            categories['Business Apps'].append(repo)
        else:
            categories['Client Work'].append(repo)

    return categories


def create_pie_chart_svg(data, title, output_file, width=800, height=600):
    """Create SVG pie chart without matplotlib dependency"""
    cx, cy = width / 2, height / 2
    radius = min(width, height) / 3

    colors = [
        '#0066cc', '#4da6ff', '#99ccff', '#00994d', '#ffcc00',
        '#ff6600', '#cc0000', '#9966cc', '#66cc99', '#ff6699'
    ]

    total = sum(data.values())
    if total == 0:
        return

    svg_parts = [
        f'<svg width="{width}" height="{height}" xmlns="http://www.w3.org/2000/svg">',
        f'<text x="{cx}" y="30" text-anchor="middle" font-size="20" font-weight="bold">{title}</text>'
    ]

    start_angle = 0
    legend_y = 50

    for i, (label, value) in enumerate(data.items()):
        if value == 0:
            continue

        percent = (value / total) * 100
        angle = (value / total) * 360
        end_angle = start_angle + angle

        # Convert to radians
        start_rad = math.radians(start_angle - 90)
        end_rad = math.radians(end_angle - 90)

        # Calculate arc path
        x1 = cx + radius * math.cos(start_rad)
        y1 = cy + radius * math.sin(start_rad)
        x2 = cx + radius * math.cos(end_rad)
        y2 = cy + radius * math.sin(end_rad)

        large_arc = 1 if angle > 180 else 0

        # Create pie slice
        path = f'M {cx},{cy} L {x1},{y1} A {radius},{radius} 0 {large_arc},1 {x2},{y2} Z'
        color = colors[i % len(colors)]
        svg_parts.append(f'<path d="{path}" fill="{color}" stroke="white" stroke-width="2"/>')

        # Add legend
        legend_x = width - 200
        svg_parts.append(f'<rect x="{legend_x}" y="{legend_y}" width="15" height="15" fill="{color}"/>')
        svg_parts.append(f'<text x="{legend_x + 20}" y="{legend_y + 12}" font-size="12">{label}: {value} ({percent:.1f}%)</text>')
        legend_y += 25

        start_angle = end_angle

    svg_parts.append('</svg>')

    # Write to file
    output_file.parent.mkdir(parents=True, exist_ok=True)
    output_file.write_text('\n'.join(svg_parts))
    print(f"Created: {output_file.name}")


def create_bar_chart_svg(data, title, output_file, width=800, height=600):
    """Create SVG horizontal bar chart"""
    max_value = max(data.values()) if data else 1
    bar_height = 30
    spacing = 10
    chart_height = len(data) * (bar_height + spacing)
    margin_left = 250
    margin_top = 50

    actual_height = chart_height + margin_top + 50

    svg_parts = [
        f'<svg width="{width}" height="{actual_height}" xmlns="http://www.w3.org/2000/svg">',
        f'<text x="{width/2}" y="30" text-anchor="middle" font-size="20" font-weight="bold">{title}</text>'
    ]

    for i, (label, value) in enumerate(data.items()):
        y = margin_top + i * (bar_height + spacing)
        bar_width = ((width - margin_left - 100) * value / max_value)

        # Bar
        svg_parts.append(f'<rect x="{margin_left}" y="{y}" width="{bar_width}" height="{bar_height}" fill="#0066cc" stroke="#333" stroke-width="1"/>')

        # Label
        svg_parts.append(f'<text x="{margin_left - 10}" y="{y + bar_height/2 + 5}" text-anchor="end" font-size="14">{label}</text>')

        # Value
        svg_parts.append(f'<text x="{margin_left + bar_width + 5}" y="{y + bar_height/2 + 5}" font-size="14" font-weight="bold">{value}</text>')

    svg_parts.append('</svg>')

    # Write to file
    output_file.parent.mkdir(parents=True, exist_ok=True)
    output_file.write_text('\n'.join(svg_parts))
    print(f"Created: {output_file.name}")


def generate_visualizations(data, output_dir):
    """Generate all SVG visualizations"""
    print("\nGenerating SVG visualizations...")

    # Monthly commits (if available in data)
    if 'monthly' in data:
        monthly_data = {month: count for month, count in data['monthly'].items()}
        create_pie_chart_svg(
            monthly_data,
            f"Commits by Month ({data['total_commits']} total)",
            output_dir / 'monthly-commits.svg'
        )

    # Top 10 repositories
    top_10 = {}
    for repo in data['repositories'][:10]:
        name = repo['name']
        if repo['parent']:
            name = f"{repo['parent']}/{name}"
        top_10[name] = repo['commits']

    create_bar_chart_svg(
        top_10,
        'Top 10 Repositories by Commits',
        output_dir / 'top-10-repos.svg'
    )

    # Project categories
    if 'categories' in data:
        category_data = {cat: len(repos) for cat, repos in data['categories'].items() if repos}
        create_pie_chart_svg(
            category_data,
            f"Project Categories ({len(data['repositories'])} repos)",
            output_dir / 'project-categories.svg'
        )

    # Language distribution
    if 'languages' in data:
        language_data = data['languages']
        create_pie_chart_svg(
            language_data,
            f"File Changes by Language ({sum(language_data.values())} total)",
            output_dir / 'language-distribution.svg',
            width=900
        )


def main():
    parser = argparse.ArgumentParser(description='Generate comprehensive git activity report')
    parser.add_argument('--start-date', help='Start date (YYYY-MM-DD)')
    parser.add_argument('--end-date', help='End date (YYYY-MM-DD)', default=None)
    parser.add_argument('--days', type=int, help='Number of days back from today')
    parser.add_argument('--weekly', action='store_true', help='Last 7 days')
    parser.add_argument('--monthly', action='store_true', help='Last 30 days')
    parser.add_argument('--max-depth', type=int, default=DEFAULT_MAX_DEPTH, help='Max directory depth')
    parser.add_argument('--output-dir', help='Output directory for visualizations')
    parser.add_argument('--json-output', help='Output JSON data file')

    args = parser.parse_args()

    # Calculate date range
    if args.weekly:
        args.days = 7
    elif args.monthly:
        args.days = 30

    if args.days:
        end_date = datetime.now()
        start_date = end_date - timedelta(days=args.days)
        since_date = start_date.strftime('%Y-%m-%d')
        until_date = end_date.strftime('%Y-%m-%d')
    elif args.start_date:
        since_date = args.start_date
        until_date = args.end_date
    else:
        print("Error: Must specify --start-date, --days, --weekly, or --monthly")
        return 1

    print(f"\n{'='*60}")
    print(f"Git Activity Report Generator")
    print(f"{'='*60}")
    print(f"Date range: {since_date} to {until_date or 'now'}")
    print(f"Scan depth: {args.max_depth} directories")
    print(f"{'='*60}\n")

    # Find repositories
    repos = find_git_repos(args.max_depth)

    # Collect statistics
    print("\nCollecting commit statistics...")
    repositories = []
    all_files = []

    for repo in repos:
        stats = get_repo_stats(repo, since_date, until_date)
        if stats and stats['commits'] > 0:
            repositories.append(stats)
            all_files.extend(stats['files'])

    repositories.sort(key=lambda x: x['commits'], reverse=True)

    # Analyze languages
    print("\nAnalyzing programming languages...")
    language_stats = analyze_languages(all_files)

    # Find websites
    print("\nDiscovering project websites...")
    websites = find_project_websites(repositories)

    # Categorize projects
    print("\nCategorizing projects...")
    categories = categorize_repositories(repositories)

    # Compile data
    data = {
        'date_range': {
            'start': since_date,
            'end': until_date or datetime.now().strftime('%Y-%m-%d')
        },
        'total_commits': sum(r['commits'] for r in repositories),
        'total_repositories': len(repositories),
        'total_files': len(all_files),
        'repositories': repositories,
        'languages': language_stats,
        'websites': websites,
        'categories': {cat: [{'name': r['name'], 'commits': r['commits']}
                             for r in repos]
                       for cat, repos in categories.items()}
    }

    # Save JSON
    json_file = args.json_output or '/tmp/git_activity_comprehensive.json'
    with open(json_file, 'w') as f:
        json.dump(data, f, indent=2)
    print(f"\n‚úÖ Saved data to: {json_file}")

    # Generate visualizations
    if args.output_dir:
        output_dir = Path(args.output_dir)
    else:
        year = datetime.now().year
        output_dir = Path.home() / 'code' / 'PersonalSite' / 'assets' / 'images' / f'git-activity-{year}'

    generate_visualizations(data, output_dir)

    # Print summary
    print(f"\n{'='*60}")
    print(f"Summary")
    print(f"{'='*60}")
    print(f"Total commits: {data['total_commits']}")
    print(f"Active repositories: {len(repositories)}")
    print(f"File changes: {len(all_files)}")
    print(f"Languages detected: {len(language_stats)}")
    print(f"Websites found: {len(websites)}")
    print(f"\nTop 5 repositories:")
    for i, repo in enumerate(repositories[:5], 1):
        print(f"  {i}. {repo['name']}: {repo['commits']} commits")

    print(f"\nTop 5 languages:")
    sorted_langs = sorted(language_stats.items(), key=lambda x: x[1], reverse=True)
    for i, (lang, count) in enumerate(sorted_langs[:5], 1):
        print(f"  {i}. {lang}: {count} files")

    print(f"\n‚úÖ Complete! Visualizations saved to: {output_dir}")
    print(f"{'='*60}\n")

    return 0


if __name__ == '__main__':
    exit(main())
</file>

<file path="pipeline-runners/duplicate-detection-pipeline.js">
#!/usr/bin/env node

/**
 * Duplicate Detection Pipeline
 *
 * Automated duplicate detection scanning system with cron scheduling.
 * Scans repositories on a configured schedule, detects duplicates, generates reports.
 *
 * Features:
 * - Cron-based scheduling
 * - Repository prioritization and frequency management
 * - Inter-project and intra-project scanning
 * - Redis-based job queue (optional)
 * - Retry logic with exponential backoff
 * - Sentry error tracking
 * - Progress tracking and metrics
 *
 * Usage:
 *   node duplicate-detection-pipeline.js                    # Start cron server
 *   RUN_ON_STARTUP=true node duplicate-detection-pipeline.js # Run immediately
 */

import { DuplicateDetectionWorker } from '../workers/duplicate-detection-worker.js';
import { createComponentLogger } from '../utils/logger.js';
import { config } from '../core/config.js';
import * as Sentry from '@sentry/node';
import cron from 'node-cron';

const logger = createComponentLogger('DuplicateDetectionPipeline');

/**
 * Main execution
 */
async function main() {
  const cronSchedule = config.duplicateScanCronSchedule || process.env.DUPLICATE_SCAN_CRON_SCHEDULE || '0 2 * * *';
  const runOnStartup = process.env.RUN_ON_STARTUP === 'true';

  console.log('‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó');
  console.log('‚ïë     DUPLICATE DETECTION AUTOMATED PIPELINE              ‚ïë');
  console.log('‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n');

  try {
    // Initialize worker
    const worker = new DuplicateDetectionWorker({
      maxConcurrentScans: config.maxConcurrentDuplicateScans || 3,
      enablePRCreation: process.env.ENABLE_PR_CREATION === 'true',
      baseBranch: process.env.PR_BASE_BRANCH || 'main',
      dryRun: process.env.PR_DRY_RUN === 'true'
    });

    // Event listeners for job lifecycle
    worker.on('job:created', (job) => {
      logger.info({
        jobId: job.id,
        type: job.data.type,
        scanType: job.data.scanType
      }, 'Job created');
    });

    worker.on('job:started', (job) => {
      logger.info({
        jobId: job.id,
        scanType: job.data.scanType,
        repositories: job.data.repositories?.length || 0
      }, 'Job started');
    });

    worker.on('job:completed', (job) => {
      const result = job.result || {};
      logger.info({
        jobId: job.id,
        scanType: result.scanType,
        duplicates: result.duplicates || result.crossRepoDuplicates || 0,
        suggestions: result.suggestions || 0,
        duration: result.duration,
        prResults: result.prResults
      }, 'Job completed');

      worker.scanMetrics.successfulScans++;
    });

    worker.on('job:failed', (job, error) => {
      logger.error({
        jobId: job.id,
        error: job.error || error?.message,
        scanType: job.data.scanType
      }, 'Job failed');

      worker.scanMetrics.failedScans++;

      Sentry.captureException(error || new Error(job.error), {
        tags: {
          component: 'duplicate-detection-pipeline',
          job_id: job.id,
          scan_type: job.data.scanType
        }
      });
    });

    // Event listeners for pipeline-specific events
    worker.on('initialized', (stats) => {
      logger.info({
        totalRepositories: stats.totalRepositories,
        enabledRepositories: stats.enabledRepositories,
        groups: stats.groups
      }, 'Worker initialized with configuration');
    });

    worker.on('pipeline:status', (status) => {
      logger.info({ pipelineStatus: status }, 'Pipeline status update');
    });

    worker.on('scan:completed', (scanInfo) => {
      logger.info({
        jobId: scanInfo.jobId,
        scanType: scanInfo.scanType,
        metrics: scanInfo.metrics
      }, 'Scan completed');
    });

    worker.on('pr:created', (prInfo) => {
      logger.info({
        repository: prInfo.repository,
        prsCreated: prInfo.prsCreated,
        prUrls: prInfo.prUrls
      }, 'Pull requests created');
    });

    worker.on('pr:failed', (prInfo) => {
      logger.error({
        repository: prInfo.repository,
        error: prInfo.error
      }, 'Failed to create pull requests');
    });

    worker.on('high-impact:detected', (info) => {
      logger.warn({
        count: info.count,
        threshold: info.threshold,
        topScore: info.topImpactScore
      }, 'High-impact duplicates detected');
    });

    worker.on('retry:scheduled', (retryInfo) => {
      logger.info({
        jobId: retryInfo.jobId,
        attempt: retryInfo.attempt,
        delay: retryInfo.delay
      }, 'Retry scheduled');
    });

    worker.on('retry:warning', (retryInfo) => {
      logger.warn({
        jobId: retryInfo.jobId,
        attempts: retryInfo.attempts,
        maxAttempts: retryInfo.maxAttempts
      }, 'Approaching retry limit');
    });

    worker.on('retry:circuit-breaker', (retryInfo) => {
      logger.error({
        jobId: retryInfo.jobId,
        attempts: retryInfo.attempts
      }, 'Circuit breaker triggered');
    });

    worker.on('metrics:updated', (metrics) => {
      logger.debug({ metrics }, 'Metrics updated');
    });

    // Initialize the worker
    await worker.initialize();

    console.log('‚úÖ Duplicate detection pipeline initialized\n');

    const stats = worker.configLoader.getStats();
    console.log('üìä Configuration:');
    console.log(`   Total repositories: ${stats.totalRepositories}`);
    console.log(`   Enabled repositories: ${stats.enabledRepositories}`);
    console.log(`   Repository groups: ${stats.groups}\n`);

    // Schedule cron job or run immediately
    if (!runOnStartup) {
      console.log(`‚è∞ Scheduling nightly scans: ${cronSchedule}\n`);

      cron.schedule(cronSchedule, async () => {
        logger.info('Cron job triggered');
        try {
          await worker.runNightlyScan();
        } catch (error) {
          logger.error({ error }, 'Nightly scan failed');
          Sentry.captureException(error);
        }
      });

      console.log('üöÄ Pipeline is running. Press Ctrl+C to stop.\n');

      // Notify PM2 that process is ready (fork mode)
      if (process.send) {
        process.send('ready');
        logger.info('Sent ready signal to PM2');
      }

      // Keep-alive: prevent process from exiting
      // The cron scheduler keeps the event loop active, but we add this as a safeguard
      setInterval(() => {
        logger.debug('Worker keep-alive heartbeat');
      }, 300000); // 5 minutes

      // Graceful shutdown handlers
      process.on('SIGTERM', () => {
        logger.info('Received SIGTERM, shutting down gracefully');
        process.exit(0);
      });

      process.on('SIGINT', () => {
        logger.info('Received SIGINT, shutting down gracefully');
        process.exit(0);
      });

    } else {
      console.log('‚ñ∂Ô∏è  Running scan immediately (RUN_ON_STARTUP=true)\n');
      await worker.runNightlyScan();

      // Wait for all jobs to complete
      const waitForCompletion = () => {
        return new Promise((resolve) => {
          const checkInterval = setInterval(() => {
            const stats = worker.getStats();
            if (stats.active === 0 && stats.queued === 0) {
              clearInterval(checkInterval);
              resolve();
            }
          }, 1000);
        });
      };

      await waitForCompletion();

      console.log('\n‚úÖ Startup scan completed');
      const metrics = worker.getScanMetrics();
      console.log('\nüìä Scan Metrics:');
      console.log(`   Total scans: ${metrics.totalScans}`);
      console.log(`   Successful: ${metrics.successfulScans}`);
      console.log(`   Failed: ${metrics.failedScans}`);
      console.log(`   Duplicates found: ${metrics.totalDuplicatesFound}`);
      console.log(`   Suggestions generated: ${metrics.totalSuggestionsGenerated}`);
      console.log(`   High-impact duplicates: ${metrics.highImpactDuplicates}`);

      if (worker.enablePRCreation) {
        console.log('\nüîÄ PR Creation:');
        console.log(`   PRs created: ${metrics.prsCreated}`);
        console.log(`   PR creation errors: ${metrics.prCreationErrors}`);
      }

      console.log('\nüìà Retry Metrics:');
      const retryMetrics = metrics.retryMetrics;
      console.log(`   Active retries: ${retryMetrics.activeRetries}`);
      console.log(`   Total retry attempts: ${retryMetrics.totalRetryAttempts}`);
      if (retryMetrics.retryDistribution.nearingLimit > 0) {
        console.log(`   ‚ö†Ô∏è  Jobs nearing retry limit: ${retryMetrics.retryDistribution.nearingLimit}`);
      }

      console.log('');
      process.exit(0);
    }

  } catch (error) {
    console.error('\n‚ùå Error:', error.message);
    logger.error({ error }, 'Pipeline initialization failed');
    Sentry.captureException(error);
    process.exit(1);
  }
}

// Export worker class for testing and external use
export { DuplicateDetectionWorker };

// Run the pipeline
// Check if running directly (not imported as module)
// Also check for PM2 execution (pm_id is set by PM2)
const isDirectExecution = import.meta.url === `file://${process.argv[1]}` || process.env.pm_id !== undefined;

if (isDirectExecution) {
  await main();
}
</file>

<file path="pipeline-runners/duplicate-detection-pipeline.ts">
#!/usr/bin/env -S npx tsx

/**
 * Duplicate Detection Pipeline - TypeScript Version
 *
 * Automated duplicate detection scanning system with cron scheduling.
 * Scans repositories on a configured schedule, detects duplicates, generates reports.
 *
 * Features:
 * - Cron-based scheduling
 * - Repository prioritization and frequency management
 * - Inter-project and intra-project scanning
 * - Redis-based job queue (optional)
 * - Retry logic with exponential backoff
 * - Sentry error tracking
 * - Progress tracking and metrics
 *
 * Usage:
 *   tsx duplicate-detection-pipeline.ts                    # Start cron server
 *   RUN_ON_STARTUP=true tsx duplicate-detection-pipeline.ts # Run immediately
 */

import { SidequestServer } from '../core/server.js';
import { RepositoryConfigLoader } from '../pipeline-core/config/repository-config-loader.js';
import { InterProjectScanner } from '../pipeline-core/inter-project-scanner.js';
import { ScanOrchestrator } from '../pipeline-core/scan-orchestrator.js';
import { ReportCoordinator } from '../pipeline-core/reports/report-coordinator.js';
import { PRCreator } from '../pipeline-core/git/pr-creator.js';
import { createComponentLogger } from '../utils/logger.js';
import { config } from '../core/config.js';
import { isRetryable, getErrorInfo } from '../pipeline-core/errors/error-classifier.js';
import * as cron from 'node-cron';
import * as path from 'path';
import * as Sentry from '@sentry/node';

// Type imports
import type { Logger } from 'pino';

/**
 * Job status enum
 */
export enum JobStatus {
  QUEUED = 'queued',
  RUNNING = 'running',
  COMPLETED = 'completed',
  FAILED = 'failed'
}

/**
 * Scan type enum
 */
export enum ScanType {
  INTER_PROJECT = 'inter-project',
  INTRA_PROJECT = 'intra-project'
}

/**
 * Interface for job data
 */
export interface JobData {
  scanType: ScanType | string;
  repositories?: RepositoryConfig[];
  groupName?: string | null;
  type?: string;
}

/**
 * Interface for a job
 */
export interface Job {
  id: string;
  status: JobStatus;
  data: JobData;
  createdAt: Date;
  startedAt: Date | null;
  completedAt: Date | null;
  error: Error | null;
  result: any;
}

/**
 * Interface for repository configuration
 */
export interface RepositoryConfig {
  name: string;
  path: string;
  enabled?: boolean;
  frequency?: string;
  lastScanned?: string | null;
  priority?: number;
  groups?: string[];
  scanHistory?: ScanHistoryEntry[];
}

/**
 * Interface for scan history entry
 */
export interface ScanHistoryEntry {
  date: string;
  status: 'success' | 'failure';
  duration: number;
  duplicatesFound: number;
}

/**
 * Interface for retry information
 */
export interface RetryInfo {
  attempts: number;
  lastAttempt: number;
  maxAttempts: number;
  delay: number;
}

/**
 * Interface for scan metrics
 */
export interface ScanMetrics {
  totalScans: number;
  successfulScans: number;
  failedScans: number;
  totalDuplicatesFound: number;
  totalSuggestionsGenerated: number;
  highImpactDuplicates: number;
  prsCreated: number;
  prCreationErrors: number;
}

/**
 * Interface for retry metrics
 */
export interface RetryMetrics {
  activeRetries: number;
  totalRetryAttempts: number;
  jobsBeingRetried: Array<{
    jobId: string;
    attempts: number;
    maxAttempts: number;
    lastAttempt: string;
  }>;
  retryDistribution: {
    attempt1: number;
    attempt2: number;
    attempt3Plus: number;
    nearingLimit: number;
  };
}

/**
 * Interface for scan result
 */
export interface ScanResult {
  scan_type: 'single-project' | 'inter-project' | 'intra-project';
  scan_metadata?: {
    duration_seconds: number;
    [key: string]: any;
  };
  metrics: {
    total_duplicate_groups?: number;
    total_cross_repository_groups?: number;
    total_suggestions?: number;
    [key: string]: any;
  };
  duplicate_groups?: DuplicateGroup[];
  cross_repository_duplicates?: DuplicateGroup[];
  suggestions?: Suggestion[];
  [key: string]: any;
}

/**
 * Interface for duplicate group
 */
export interface DuplicateGroup {
  id: string;
  impact_score: number;
  files: Array<{
    path: string;
    repository?: string;
  }>;
  [key: string]: any;
}

/**
 * Interface for suggestion
 */
export interface Suggestion {
  id: string;
  type: string;
  impact: number;
  files: string[];
  [key: string]: any;
}

/**
 * Interface for PR creation result
 */
export interface PRCreationResult {
  prsCreated: number;
  prUrls: string[];
  errors: Array<{
    message: string;
    [key: string]: any;
  }>;
}

/**
 * Interface for worker options
 */
export interface DuplicateDetectionWorkerOptions {
  maxConcurrentScans?: number;
  logDir?: string;
  sentryDsn?: string;
  configPath?: string;
  baseBranch?: string;
  branchPrefix?: string;
  dryRun?: boolean;
  maxSuggestionsPerPR?: number;
  enablePRCreation?: boolean;
}

/**
 * Interface for inter-project scan result
 */
export interface InterProjectScanJobResult {
  scanType: 'inter-project';
  repositories: number;
  crossRepoDuplicates: number;
  suggestions: number;
  duration: number;
}

/**
 * Interface for intra-project scan result
 */
export interface IntraProjectScanJobResult {
  scanType: 'intra-project';
  repository: string;
  duplicates: number;
  suggestions: number;
  duration: number;
  prResults: {
    prsCreated: number;
    prUrls: string[];
    errors: number;
  } | null;
}

/**
 * Type for job result
 */
export type JobResult = InterProjectScanJobResult | IntraProjectScanJobResult;

const logger: Logger = createComponentLogger('DuplicateDetectionPipeline');

// Circuit breaker: Absolute maximum retry attempts to prevent infinite loops
const MAX_ABSOLUTE_RETRIES: number = 5;

/**
 * Duplicate Detection Worker
 *
 * Extends SidequestServer to handle duplicate detection scanning jobs
 */
class DuplicateDetectionWorker extends SidequestServer {
  private configLoader: RepositoryConfigLoader;
  private interProjectScanner: InterProjectScanner;
  private orchestrator: ScanOrchestrator;
  private reportCoordinator: ReportCoordinator;
  private prCreator: PRCreator;
  private scanMetrics: ScanMetrics;
  private enablePRCreation: boolean;
  private retryQueue: Map<string, RetryInfo>;

  constructor(options: DuplicateDetectionWorkerOptions = {}) {
    super({
      maxConcurrent: options.maxConcurrentScans || 3,
      logDir: options.logDir || path.join(process.cwd(), 'logs', 'duplicate-detection'),
      sentryDsn: options.sentryDsn
    });

    this.configLoader = new RepositoryConfigLoader(options.configPath);
    this.interProjectScanner = new InterProjectScanner({
      outputDir: path.join(process.cwd(), 'output', 'automated-scans')
    });
    this.orchestrator = new ScanOrchestrator({
      pythonPath: path.join(process.cwd(), 'venv', 'bin', 'python3')
    });
    this.reportCoordinator = new ReportCoordinator(
      path.join(process.cwd(), 'output', 'reports')
    );
    this.prCreator = new PRCreator({
      baseBranch: options.baseBranch || 'main',
      branchPrefix: options.branchPrefix || 'consolidate',
      dryRun: options.dryRun ?? (process.env.PR_DRY_RUN === 'true'),
      maxSuggestionsPerPR: options.maxSuggestionsPerPR || 5
    });

    this.scanMetrics = {
      totalScans: 0,
      successfulScans: 0,
      failedScans: 0,
      totalDuplicatesFound: 0,
      totalSuggestionsGenerated: 0,
      highImpactDuplicates: 0,
      prsCreated: 0,
      prCreationErrors: 0
    };

    this.enablePRCreation = options.enablePRCreation ?? (process.env.ENABLE_PR_CREATION === 'true');

    this.retryQueue = new Map<string, RetryInfo>();
  }

  /**
   * Initialize the worker
   */
  async initialize(): Promise<void> {
    try {
      // Load configuration
      await this.configLoader.load();

      // Validate configuration
      this.configLoader.validate();

      const stats = this.configLoader.getStats();
      logger.info({
        ...stats
      }, 'Duplicate detection pipeline initialized');

      // @ts-ignore - emit is inherited from EventEmitter through SidequestServer
      this.emit('initialized', stats);
    } catch (error) {
      logger.error({ error }, 'Failed to initialize duplicate detection pipeline');
      Sentry.captureException(error);
      throw error;
    }
  }

  /**
   * Run job handler (required by SidequestServer)
   */
  async runJobHandler(job: Job): Promise<JobResult> {
    const { scanType, repositories, groupName } = job.data;

    logger.info({
      jobId: job.id,
      scanType,
      repositories: repositories?.length || 0,
      groupName
    }, 'Starting duplicate detection scan job');

    try {
      if (scanType === ScanType.INTER_PROJECT || scanType === 'inter-project') {
        return await this._runInterProjectScan(job, repositories!);
      } else if (scanType === ScanType.INTRA_PROJECT || scanType === 'intra-project') {
        return await this._runIntraProjectScan(job, repositories![0]);
      } else {
        throw new Error(`Unknown scan type: ${scanType}`);
      }
    } catch (error) {
      // Handle retry logic
      const shouldRetry = await this._handleRetry(job, error as Error);

      if (shouldRetry) {
        logger.info({ jobId: job.id }, 'Job will be retried');
        throw error; // Re-throw to mark job as failed, will be retried by retry handler
      } else {
        logger.error({ jobId: job.id, error }, 'Job failed after all retry attempts');
        throw error;
      }
    }
  }

  /**
   * Extract original job ID by stripping all retry suffixes
   * @param jobId - Job ID (may contain retry suffixes)
   * @returns Original job ID without retry suffixes
   * @private
   */
  private _getOriginalJobId(jobId: string): string {
    // Strip all -retryN suffixes to get the original job ID
    // Example: "scan-intra-project-123-retry1-retry1-retry1" -> "scan-intra-project-123"
    return jobId.replace(/-retry\d+/g, '');
  }

  /**
   * Handle retry logic with exponential backoff
   */
  private async _handleRetry(job: Job, error: Error): Promise<boolean> {
    const scanConfig = this.configLoader.getScanConfig();
    const maxRetries = scanConfig.retryAttempts || 0;
    const baseDelay = scanConfig.retryDelay || 60000;

    // Get original job ID to track retries correctly
    const originalJobId = this._getOriginalJobId(job.id);

    // Classify error to determine if retry is appropriate
    const errorInfo = getErrorInfo(error);

    if (!errorInfo.retryable) {
      logger.warn({
        jobId: job.id,
        originalJobId,
        errorCode: errorInfo.code,
        errorMessage: errorInfo.message,
        classification: errorInfo.category,
        reason: errorInfo.reason
      }, 'Error is non-retryable - skipping retry');
      this.retryQueue.delete(originalJobId);
      return false;
    }

    if (!this.retryQueue.has(originalJobId)) {
      // First failure - initialize retry tracking
      this.retryQueue.set(originalJobId, {
        attempts: 0,
        lastAttempt: Date.now(),
        maxAttempts: maxRetries,
        delay: baseDelay
      });
    }

    const retryInfo = this.retryQueue.get(originalJobId)!;
    retryInfo.attempts++;

    // Circuit breaker: Check against absolute maximum
    if (retryInfo.attempts >= MAX_ABSOLUTE_RETRIES) {
      logger.error({
        jobId: job.id,
        originalJobId,
        attempts: retryInfo.attempts,
        maxAbsolute: MAX_ABSOLUTE_RETRIES
      }, 'Circuit breaker triggered: Maximum absolute retry attempts reached');

      // Send Sentry alert for circuit breaker
      Sentry.captureMessage('Circuit breaker triggered: Excessive retry attempts', {
        level: 'error',
        tags: {
          component: 'retry-logic',
          jobId: originalJobId,
          errorType: (error as any).code || error.name
        },
        extra: {
          jobId: job.id,
          originalJobId,
          attempts: retryInfo.attempts,
          maxAbsolute: MAX_ABSOLUTE_RETRIES,
          errorMessage: error.message,
          errorCode: errorInfo.code,
          errorClassification: errorInfo.category
        }
      });

      this.retryQueue.delete(originalJobId);
      return false;
    }

    // Check against configured maximum
    if (retryInfo.attempts >= retryInfo.maxAttempts) {
      logger.warn({
        jobId: job.id,
        originalJobId,
        attempts: retryInfo.attempts,
        maxConfigured: retryInfo.maxAttempts
      }, 'Maximum configured retry attempts reached');

      // Send Sentry alert for max retries reached
      Sentry.captureMessage('Maximum configured retry attempts reached', {
        level: 'warning',
        tags: {
          component: 'retry-logic',
          jobId: originalJobId,
          errorType: (error as any).code || error.name
        },
        extra: {
          jobId: job.id,
          originalJobId,
          attempts: retryInfo.attempts,
          maxConfigured: retryInfo.maxAttempts,
          errorMessage: error.message,
          errorCode: errorInfo.code,
          errorClassification: errorInfo.category
        }
      });

      this.retryQueue.delete(originalJobId);
      return false;
    }

    // Alert when approaching circuit breaker (3+ attempts)
    if (retryInfo.attempts >= 3) {
      Sentry.captureMessage('Warning: Approaching retry limit', {
        level: 'warning',
        tags: {
          component: 'retry-logic',
          jobId: originalJobId,
          errorType: (error as any).code || error.name
        },
        extra: {
          jobId: job.id,
          originalJobId,
          attempts: retryInfo.attempts,
          maxAttempts: retryInfo.maxAttempts,
          maxAbsolute: MAX_ABSOLUTE_RETRIES,
          errorMessage: error.message,
          errorCode: errorInfo.code,
          errorClassification: errorInfo.category
        }
      });
    }

    // Calculate exponential backoff delay
    // Use suggested delay from error classifier as base
    const baseRetryDelay = errorInfo.suggestedDelay || retryInfo.delay;
    const delay = baseRetryDelay * Math.pow(2, retryInfo.attempts - 1);

    logger.info({
      jobId: job.id,
      originalJobId,
      attempt: retryInfo.attempts,
      maxAttempts: retryInfo.maxAttempts,
      maxAbsolute: MAX_ABSOLUTE_RETRIES,
      delayMs: delay,
      error: error.message,
      errorClassification: errorInfo.category,
      errorReason: errorInfo.reason,
      suggestedDelay: errorInfo.suggestedDelay
    }, 'Scheduling retry with exponential backoff');

    // Schedule retry
    setTimeout(() => {
      logger.info({ jobId: job.id, originalJobId, attempt: retryInfo.attempts }, 'Retrying failed job');
      // Use original job ID + retry count for new job ID
      // @ts-ignore - createJob is inherited from SidequestServer
      this.createJob(`${originalJobId}-retry${retryInfo.attempts}`, job.data);
    }, delay);

    return true;
  }

  /**
   * Run inter-project scan
   */
  private async _runInterProjectScan(job: Job, repositoryConfigs: RepositoryConfig[]): Promise<InterProjectScanJobResult> {
    const repoPaths = repositoryConfigs.map(r => r.path);

    logger.info({
      jobId: job.id,
      repositories: repoPaths.length
    }, 'Running inter-project scan');

    const result = await this.interProjectScanner.scanRepositories(repoPaths) as unknown as ScanResult;

    // Generate reports
    await this.reportCoordinator.generateAllReports(result, {
      title: `Automated Inter-Project Scan: ${repoPaths.length} Repositories`,
      includeDetails: true,
      includeSourceCode: true,
      includeCodeBlocks: true
    });

    // Update scan metrics
    this._updateMetrics(result);

    // Update repository configurations
    await this._updateRepositoryConfigs(repositoryConfigs, result);

    // Check for high-impact duplicates
    await this._checkForHighImpactDuplicates(result);

    return {
      scanType: 'inter-project',
      repositories: repoPaths.length,
      crossRepoDuplicates: result.metrics.total_cross_repository_groups || 0,
      suggestions: result.metrics.total_suggestions || 0,
      duration: result.scan_metadata?.duration_seconds || 0
    };
  }

  /**
   * Run intra-project scan
   */
  private async _runIntraProjectScan(job: Job, repositoryConfig: RepositoryConfig): Promise<IntraProjectScanJobResult> {
    // Validate repository config
    if (!repositoryConfig) {
      const error = new Error('Repository configuration is undefined');
      logger.error({ jobId: job.id }, 'No repository configuration provided for intra-project scan');
      Sentry.captureException(error, {
        tags: {
          error_type: 'validation_error',
          component: 'DuplicateDetectionPipeline',
          scan_type: 'intra-project'
        },
        extra: { jobId: job.id }
      });
      throw error;
    }

    if (!repositoryConfig.path) {
      const error = new Error(`Repository configuration missing 'path' property. Config: ${JSON.stringify(repositoryConfig)}`);
      logger.error({
        jobId: job.id,
        repositoryConfig
      }, 'Repository configuration missing path property');
      Sentry.captureException(error, {
        tags: {
          error_type: 'validation_error',
          component: 'DuplicateDetectionPipeline',
          scan_type: 'intra-project'
        },
        extra: {
          jobId: job.id,
          repositoryConfig
        }
      });
      throw error;
    }

    const repoPath = repositoryConfig.path;

    logger.info({
      jobId: job.id,
      repository: repoPath
    }, 'Running intra-project scan');

    const result = await this.orchestrator.scanRepository(repoPath) as unknown as ScanResult;

    // Generate reports
    await this.reportCoordinator.generateAllReports(result, {
      title: `Automated Scan: ${repositoryConfig.name}`,
      includeDetails: true,
      includeSourceCode: true,
      includeCodeBlocks: true
    });

    // Update scan metrics
    this._updateMetrics(result);

    // Update repository configuration
    await this._updateRepositoryConfigs([repositoryConfig], result);

    // Check for high-impact duplicates
    await this._checkForHighImpactDuplicates(result);

    // Create PRs if enabled
    let prResults: PRCreationResult | null = null;
    if (this.enablePRCreation && result.suggestions && result.suggestions.length > 0) {
      try {
        logger.info({
          jobId: job.id,
          repository: repositoryConfig.name,
          suggestions: result.suggestions.length
        }, 'Creating PRs for consolidation suggestions');

        prResults = await this.prCreator.createPRsForSuggestions(result, repoPath);

        this.scanMetrics.prsCreated += prResults.prsCreated;

        if (prResults.errors.length > 0) {
          this.scanMetrics.prCreationErrors += prResults.errors.length;
          logger.warn({
            errors: prResults.errors
          }, 'Some PRs failed to create');
        }

        logger.info({
          prsCreated: prResults.prsCreated,
          prUrls: prResults.prUrls,
          errors: prResults.errors.length
        }, 'PR creation completed');

      } catch (error) {
        logger.error({ error }, 'Failed to create PRs for suggestions');
        this.scanMetrics.prCreationErrors++;
        Sentry.captureException(error, {
          tags: {
            component: 'pr-creation',
            repository: repositoryConfig.name
          }
        });
      }
    }

    return {
      scanType: 'intra-project',
      repository: repositoryConfig.name,
      duplicates: result.metrics.total_duplicate_groups || 0,
      suggestions: result.metrics.total_suggestions || 0,
      duration: result.scan_metadata?.duration_seconds || 0,
      prResults: prResults ? {
        prsCreated: prResults.prsCreated,
        prUrls: prResults.prUrls,
        errors: prResults.errors.length
      } : null
    };
  }

  /**
   * Update scan metrics
   */
  private _updateMetrics(scanResult: ScanResult): void {
    this.scanMetrics.totalScans++;

    if (scanResult.scan_type === 'inter-project') {
      this.scanMetrics.totalDuplicatesFound += scanResult.metrics.total_cross_repository_groups || 0;
      this.scanMetrics.totalSuggestionsGenerated += scanResult.metrics.total_suggestions || 0;

      // Count high-impact duplicates
      const highImpactDuplicates = (scanResult.cross_repository_duplicates || [])
        .filter(dup => dup.impact_score >= 75);
      this.scanMetrics.highImpactDuplicates += highImpactDuplicates.length;
    } else {
      this.scanMetrics.totalDuplicatesFound += scanResult.metrics.total_duplicate_groups || 0;
      this.scanMetrics.totalSuggestionsGenerated += scanResult.metrics.total_suggestions || 0;

      // Count high-impact duplicates
      const highImpactDuplicates = (scanResult.duplicate_groups || [])
        .filter(dup => dup.impact_score >= 75);
      this.scanMetrics.highImpactDuplicates += highImpactDuplicates.length;
    }
  }

  /**
   * Update repository configurations with scan results
   */
  private async _updateRepositoryConfigs(repositoryConfigs: RepositoryConfig[], scanResult: ScanResult): Promise<void> {
    const status = scanResult.scan_metadata ? 'success' : 'failure';
    const duration = scanResult.scan_metadata?.duration_seconds || 0;
    const duplicatesFound = scanResult.scan_type === 'inter-project'
      ? scanResult.metrics.total_cross_repository_groups || 0
      : scanResult.metrics.total_duplicate_groups || 0;

    for (const repoConfig of repositoryConfigs) {
      try {
        // Update last scanned timestamp
        await this.configLoader.updateLastScanned(repoConfig.name);

        // Add scan history entry
        await this.configLoader.addScanHistory(repoConfig.name, {
          status,
          duration,
          duplicatesFound
        });
      } catch (error) {
        logger.warn({
          error,
          repository: repoConfig.name
        }, 'Failed to update repository config');
      }
    }
  }

  /**
   * Check for high-impact duplicates and send notifications
   */
  private async _checkForHighImpactDuplicates(scanResult: ScanResult): Promise<void> {
    const notificationSettings = this.configLoader.getNotificationSettings();

    if (!notificationSettings.enabled || !notificationSettings.onHighImpactDuplicates) {
      return;
    }

    const threshold = notificationSettings.highImpactThreshold || 75;
    const duplicates = scanResult.scan_type === 'inter-project'
      ? scanResult.cross_repository_duplicates || []
      : scanResult.duplicate_groups || [];

    const highImpactDuplicates = duplicates.filter(dup => dup.impact_score >= threshold);

    if (highImpactDuplicates.length > 0) {
      logger.warn({
        count: highImpactDuplicates.length,
        threshold,
        topImpactScore: Math.max(...highImpactDuplicates.map(d => d.impact_score))
      }, 'High-impact duplicates detected');

      // Send Sentry notification
      Sentry.captureMessage(`High-impact duplicates detected: ${highImpactDuplicates.length} duplicates with impact score >= ${threshold}`, {
        level: 'warning',
        tags: {
          component: 'duplicate-detection',
          scanType: scanResult.scan_type
        },
        contexts: {
          duplicates: {
            count: highImpactDuplicates.length,
            threshold,
            topImpactScore: Math.max(...highImpactDuplicates.map(d => d.impact_score))
          }
        }
      });
    }
  }

  /**
   * Schedule a scan job
   */
  public scheduleScan(scanType: ScanType | string, repositories: RepositoryConfig[], groupName: string | null = null): Job {
    const jobId = `scan-${scanType}-${Date.now()}`;
    const jobData: JobData = {
      scanType,
      repositories,
      groupName,
      type: 'duplicate-detection'
    };

    // @ts-ignore - createJob is inherited from SidequestServer
    return this.createJob(jobId, jobData);
  }

  /**
   * Run nightly scan (called by cron)
   */
  public async runNightlyScan(): Promise<void> {
    logger.info('Starting nightly duplicate detection scan');

    const scanConfig = this.configLoader.getScanConfig();

    if (!scanConfig.enabled) {
      logger.info('Automated scanning is disabled');
      return;
    }

    // Get repositories to scan tonight
    const repositoriesToScan: RepositoryConfig[] = this.configLoader.getRepositoriesToScanTonight();

    logger.info({
      repositoryCount: repositoriesToScan.length
    }, 'Repositories selected for scanning');

    if (repositoriesToScan.length === 0) {
      logger.info('No repositories to scan tonight');
      return;
    }

    // Scan individual repositories (intra-project)
    for (const repo of repositoriesToScan) {
      this.scheduleScan(ScanType.INTRA_PROJECT, [repo]);
    }

    // Scan repository groups (inter-project)
    const groups = this.configLoader.getEnabledGroups();
    for (const group of groups) {
      const groupRepos: RepositoryConfig[] = this.configLoader.getGroupRepositories(group.name);
      if (groupRepos.length >= 2) {
        this.scheduleScan(ScanType.INTER_PROJECT, groupRepos, group.name);
      }
    }

    logger.info({
      individualScans: repositoriesToScan.length,
      groupScans: groups.length
    }, 'Nightly scan scheduled');
  }

  /**
   * Get retry metrics
   */
  public getRetryMetrics(): RetryMetrics {
    const retryStats: RetryMetrics = {
      activeRetries: this.retryQueue.size,
      totalRetryAttempts: 0,
      jobsBeingRetried: [],
      retryDistribution: {
        attempt1: 0,
        attempt2: 0,
        attempt3Plus: 0,
        nearingLimit: 0  // 3+ attempts
      }
    };

    Array.from(this.retryQueue.entries()).forEach(([jobId, retryInfo]) => {
      retryStats.totalRetryAttempts += retryInfo.attempts;
      retryStats.jobsBeingRetried.push({
        jobId,
        attempts: retryInfo.attempts,
        maxAttempts: retryInfo.maxAttempts,
        lastAttempt: new Date(retryInfo.lastAttempt).toISOString()
      });

      // Distribution
      if (retryInfo.attempts === 1) {
        retryStats.retryDistribution.attempt1++;
      } else if (retryInfo.attempts === 2) {
        retryStats.retryDistribution.attempt2++;
      } else {
        retryStats.retryDistribution.attempt3Plus++;
      }

      if (retryInfo.attempts >= 3) {
        retryStats.retryDistribution.nearingLimit++;
      }
    });

    return retryStats;
  }

  /**
   * Get scan metrics
   */
  public getScanMetrics(): ScanMetrics & { queueStats: any; retryMetrics: RetryMetrics } {
    return {
      ...this.scanMetrics,
      // @ts-ignore - getStats is inherited from SidequestServer
      queueStats: this.getStats(),
      retryMetrics: this.getRetryMetrics()
    };
  }
}

/**
 * Main execution
 */
async function main(): Promise<void> {
  const cronSchedule = config.duplicateScanCronSchedule || process.env.DUPLICATE_SCAN_CRON_SCHEDULE || '0 2 * * *';
  const runOnStartup = process.env.RUN_ON_STARTUP === 'true';

  console.log('‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó');
  console.log('‚ïë     DUPLICATE DETECTION AUTOMATED PIPELINE              ‚ïë');
  console.log('‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n');

  try {
    // Initialize worker
    const worker = new DuplicateDetectionWorker({
      maxConcurrentScans: config.maxConcurrentDuplicateScans || 3
    });

    await worker.initialize();

    console.log('‚úÖ Duplicate detection pipeline initialized\n');

    // @ts-ignore - configLoader is private but needed for initialization log
    const stats = worker.configLoader.getStats();
    console.log('üìä Configuration:');
    console.log(`   Total repositories: ${stats.totalRepositories}`);
    console.log(`   Enabled repositories: ${stats.enabledRepositories}`);
    console.log(`   Repository groups: ${stats.groups}\n`);

    // Schedule cron job
    if (!runOnStartup) {
      console.log(`‚è∞ Scheduling nightly scans: ${cronSchedule}\n`);

      cron.schedule(cronSchedule, async () => {
        logger.info('Cron job triggered');
        try {
          await worker.runNightlyScan();
        } catch (error) {
          logger.error({ error }, 'Nightly scan failed');
          Sentry.captureException(error);
        }
      });

      console.log('üöÄ Pipeline is running. Press Ctrl+C to stop.\n');

      // Notify PM2 that process is ready (fork mode)
      if (process.send) {
        process.send('ready');
        logger.info('Sent ready signal to PM2');
      }

      // Keep-alive: prevent process from exiting
      // The cron scheduler keeps the event loop active, but we add this as a safeguard
      setInterval(() => {
        logger.debug('Worker keep-alive heartbeat');
      }, 300000); // 5 minutes
    } else {
      console.log('‚ñ∂Ô∏è  Running scan immediately (RUN_ON_STARTUP=true)\n');
      await worker.runNightlyScan();

      console.log('\n‚úÖ Startup scan completed');
      const metrics = worker.getScanMetrics();
      console.log('\nüìä Scan Metrics:');
      console.log(`   Total scans: ${metrics.totalScans}`);
      console.log(`   Duplicates found: ${metrics.totalDuplicatesFound}`);
      console.log(`   Suggestions generated: ${metrics.totalSuggestionsGenerated}`);
      console.log(`   High-impact duplicates: ${metrics.highImpactDuplicates}`);

      // @ts-ignore - enablePRCreation is private but needed for metrics display
      if (worker.enablePRCreation) {
        console.log('\nüîÄ PR Creation:');
        console.log(`   PRs created: ${metrics.prsCreated}`);
        console.log(`   PR creation errors: ${metrics.prCreationErrors}`);
      }

      console.log('');
      process.exit(0);
    }

  } catch (error) {
    console.error('\n‚ùå Error:', (error as Error).message);
    logger.error({ error }, 'Pipeline initialization failed');
    Sentry.captureException(error);
    process.exit(1);
  }
}

// Run the pipeline
// Check if running directly (not imported as module)
// Also check for PM2 execution (pm_id is set by PM2)
// @ts-ignore - import.meta not available in ES2022 target
const isDirectExecution = typeof import.meta !== 'undefined' && import.meta.url === `file://${process.argv[1]}` || process.env.pm_id !== undefined;

if (isDirectExecution) {
  // @ts-ignore - top-level await needs ES2022 module
  main().catch((error) => {
    console.error('Fatal error:', error);
    process.exit(1);
  });
}

// Re-export the main class for external usage
export { DuplicateDetectionWorker };
</file>

<file path="pipeline-runners/git-activity-pipeline.js">
#!/usr/bin/env node
import cron from 'node-cron';
import { GitActivityWorker } from '../workers/git-activity-worker.js';
import { config } from '../core/config.js';
import { createComponentLogger } from '../utils/logger.js';

const logger = createComponentLogger('GitActivityPipeline');

/**
 * Git Activity Report Pipeline
 * Automatically generates git activity reports on a schedule
 */
class GitActivityPipeline {
  constructor(options = {}) {
    this.worker = new GitActivityWorker({
      maxConcurrent: config.maxConcurrent || 2,
      logDir: config.logDir,
      sentryDsn: config.sentryDsn,
      codeBaseDir: config.codeBaseDir,
      ...options
    });

    this.reportType = options.reportType || 'weekly';
    this.setupEventListeners();
  }

  /**
   * Setup event listeners for job events
   */
  setupEventListeners() {
    this.worker.on('job:created', (job) => {
      logger.info({ jobId: job.id, reportType: job.data.reportType }, 'Job created');
    });

    this.worker.on('job:started', (job) => {
      logger.info({
        jobId: job.id,
        reportType: job.data.reportType,
        days: job.data.days
      }, 'Job started');
    });

    this.worker.on('job:completed', (job) => {
      const duration = job.completedAt - job.startedAt;
      logger.info({
        jobId: job.id,
        duration,
        reportType: job.result.reportType,
        totalCommits: job.result.stats.totalCommits,
        totalRepositories: job.result.stats.totalRepositories,
        filesGenerated: job.result.outputFiles.length
      }, 'Job completed');

      // Log output file locations
      job.result.outputFiles.forEach(file => {
        if (file.exists) {
          logger.info({
            path: file.path,
            size: file.size
          }, 'Output file generated');
        }
      });
    });

    this.worker.on('job:failed', (job) => {
      logger.error({
        jobId: job.id,
        error: job.error
      }, 'Job failed');
    });
  }

  /**
   * Run a single report
   */
  async runReport(options = {}) {
    logger.info({ options }, 'Starting git activity report');

    const startTime = Date.now();

    try {
      // Create job based on options
      let job;

      if (options.sinceDate && options.untilDate) {
        job = this.worker.createCustomReportJob(options.sinceDate, options.untilDate);
      } else if (options.reportType === 'monthly' || options.days === 30) {
        job = this.worker.createMonthlyReportJob();
      } else if (options.reportType === 'weekly' || options.days === 7) {
        job = this.worker.createWeeklyReportJob();
      } else {
        job = this.worker.createReportJob(options);
      }

      logger.info({ jobId: job.id }, 'Report job created');

      // Wait for completion
      await this.waitForCompletion();

      const duration = Date.now() - startTime;
      const stats = this.worker.getStats();

      logger.info({
        duration,
        stats
      }, 'Git activity report pipeline completed');

      return stats;
    } catch (error) {
      logger.error({ error }, 'Git activity report pipeline failed');
      throw error;
    }
  }

  /**
   * Wait for all jobs to complete
   */
  async waitForCompletion() {
    return new Promise((resolve) => {
      const checkInterval = setInterval(() => {
        const stats = this.worker.getStats();
        if (stats.active === 0 && stats.queued === 0) {
          clearInterval(checkInterval);
          resolve();
        }
      }, 1000);
    });
  }

  /**
   * Schedule weekly reports
   */
  scheduleWeeklyReports(cronSchedule = '0 20 * * 0') {
    logger.info({ cronSchedule }, 'Scheduling weekly git activity reports');

    if (!cron.validate(cronSchedule)) {
      throw new Error(`Invalid cron schedule: ${cronSchedule}`);
    }

    const task = cron.schedule(cronSchedule, async () => {
      logger.info('Cron triggered - starting weekly report');
      try {
        await this.runReport({ reportType: 'weekly' });
      } catch (error) {
        logger.error({ error }, 'Scheduled weekly report failed');
      }
    });

    logger.info('Weekly git activity reports scheduled');
    return task;
  }

  /**
   * Schedule monthly reports
   */
  scheduleMonthlyReports(cronSchedule = '0 0 1 * *') {
    logger.info({ cronSchedule }, 'Scheduling monthly git activity reports');

    if (!cron.validate(cronSchedule)) {
      throw new Error(`Invalid cron schedule: ${cronSchedule}`);
    }

    const task = cron.schedule(cronSchedule, async () => {
      logger.info('Cron triggered - starting monthly report');
      try {
        await this.runReport({ reportType: 'monthly' });
      } catch (error) {
        logger.error({ error }, 'Scheduled monthly report failed');
      }
    });

    logger.info('Monthly git activity reports scheduled');
    return task;
  }
}

// Run if executed directly
if (import.meta.url === `file://${process.argv[1]}`) {
  const pipeline = new GitActivityPipeline();

  const runOnStartup = process.env.RUN_ON_STARTUP === 'true';
  const gitCronSchedule = process.env.GIT_CRON_SCHEDULE || '0 20 * * 0'; // Sunday 8 PM

  if (runOnStartup) {
    logger.info('Running git activity report immediately');

    // Parse command line arguments
    const args = process.argv.slice(2);
    const options = {};

    for (let i = 0; i < args.length; i++) {
      if (args[i] === '--weekly') {
        options.reportType = 'weekly';
      } else if (args[i] === '--monthly') {
        options.reportType = 'monthly';
      } else if (args[i] === '--since' && args[i + 1]) {
        options.sinceDate = args[i + 1];
        i++;
      } else if (args[i] === '--until' && args[i + 1]) {
        options.untilDate = args[i + 1];
        i++;
      } else if (args[i] === '--days' && args[i + 1]) {
        options.days = parseInt(args[i + 1], 10);
        i++;
      }
    }

    // Default to weekly if no options specified
    if (!options.reportType && !options.sinceDate && !options.days) {
      options.reportType = 'weekly';
    }

    pipeline.runReport(options)
      .then(() => {
        logger.info('Report completed successfully');
        process.exit(0);
      })
      .catch((error) => {
        logger.error({ error }, 'Report failed');
        process.exit(1);
      });
  } else {
    logger.info({ cronSchedule: gitCronSchedule }, 'Starting git activity pipeline in scheduled mode');
    pipeline.scheduleWeeklyReports(gitCronSchedule);
    logger.info('Git activity pipeline running. Press Ctrl+C to stop.');
  }
}

export { GitActivityPipeline };
</file>

<file path="pipeline-runners/gitignore-pipeline.js">
#!/usr/bin/env node
// @ts-nocheck
import { GitignoreWorker } from '../workers/gitignore-worker.js';
import { createComponentLogger } from '../utils/logger.js';
import * as Sentry from '@sentry/node';
import cron from 'node-cron';
import path from 'path';
import os from 'os';

const logger = createComponentLogger('GitignorePipeline');

/**
 * Gitignore Update Pipeline
 *
 * Automated .gitignore file management across all git repositories.
 * Adds repomix-output.xml to .gitignore files to prevent tracking of generated files.
 *
 * Features:
 * - Scheduled updates via cron
 * - Batch processing with job queue
 * - Dry-run mode for testing
 * - Sentry error tracking
 * - Event-driven architecture
 *
 * Environment Variables:
 * - GITIGNORE_CRON_SCHEDULE: Cron schedule (default: "0 4 * * *" - Daily at 4 AM)
 * - GITIGNORE_BASE_DIR: Base directory to scan (default: ~/code)
 * - GITIGNORE_MAX_DEPTH: Maximum scan depth (default: 10)
 * - GITIGNORE_DRY_RUN: Dry run mode (default: false)
 * - RUN_ON_STARTUP: Run immediately on startup (default: false)
 */

const CRON_SCHEDULE = process.env.GITIGNORE_CRON_SCHEDULE || '0 4 * * *'; // Daily at 4 AM
const BASE_DIR = process.env.GITIGNORE_BASE_DIR || path.join(os.homedir(), 'code');
const MAX_DEPTH = parseInt(process.env.GITIGNORE_MAX_DEPTH || '10', 10);
const DRY_RUN = process.env.GITIGNORE_DRY_RUN === 'true';
const RUN_ON_STARTUP = process.env.RUN_ON_STARTUP === 'true';

async function main() {
  logger.info({
    cronSchedule: CRON_SCHEDULE,
    baseDir: BASE_DIR,
    maxDepth: MAX_DEPTH,
    dryRun: DRY_RUN,
    runOnStartup: RUN_ON_STARTUP
  }, 'Starting Gitignore Update Pipeline');

  // Create worker instance
  const worker = new GitignoreWorker({
    baseDir: BASE_DIR,
    maxDepth: MAX_DEPTH,
    maxConcurrent: 1, // Process one batch at a time
  });

  // Event listeners
  worker.on('job:created', (job) => {
    logger.info({
      jobId: job.id,
      type: job.data.type,
      dryRun: job.data.dryRun
    }, 'Job created');
  });

  worker.on('job:started', (job) => {
    logger.info({
      jobId: job.id,
      baseDir: job.data.baseDir,
      dryRun: job.data.dryRun
    }, 'Job started');
  });

  worker.on('job:completed', (job) => {
    const { totalRepositories, summary } = job.result || {};

    logger.info({
      jobId: job.id,
      totalRepositories,
      added: summary?.added || 0,
      skipped: summary?.skipped || 0,
      wouldAdd: summary?.would_add || 0,
      errors: summary?.error || 0
    }, 'Job completed');

    // Log summary for monitoring
    if (summary) {
      logger.info({
        summary: {
          total: totalRepositories,
          added: summary.added,
          skipped: summary.skipped,
          wouldAdd: summary.would_add,
          errors: summary.error,
        }
      }, 'Gitignore update summary');
    }
  });

  worker.on('job:failed', (job) => {
    logger.error({
      jobId: job.id,
      error: job.error,
      retries: job.retries
    }, 'Job failed');

    Sentry.captureException(new Error(job.error), {
      tags: {
        component: 'gitignore-pipeline',
        job_id: job.id,
      },
      extra: {
        baseDir: job.data.baseDir,
        dryRun: job.data.dryRun,
        retries: job.retries,
      },
    });
  });

  // Run immediately if requested
  if (RUN_ON_STARTUP) {
    logger.info('Running gitignore update immediately (RUN_ON_STARTUP=true)');
    try {
      const job = worker.createUpdateAllJob({
        baseDir: BASE_DIR,
        dryRun: DRY_RUN,
        maxDepth: MAX_DEPTH,
      });

      logger.info({
        jobId: job.id,
        dryRun: DRY_RUN
      }, 'Startup job created');
    } catch (error) {
      logger.error({ error }, 'Failed to create startup job');
      Sentry.captureException(error, {
        tags: { component: 'gitignore-pipeline', phase: 'startup' },
      });
    }
  }

  // Schedule cron job
  logger.info({ schedule: CRON_SCHEDULE }, 'Scheduling gitignore updates');

  cron.schedule(CRON_SCHEDULE, () => {
    logger.info('Cron triggered gitignore update');
    try {
      const job = worker.createUpdateAllJob({
        baseDir: BASE_DIR,
        dryRun: DRY_RUN,
        maxDepth: MAX_DEPTH,
      });

      logger.info({
        jobId: job.id,
        dryRun: DRY_RUN
      }, 'Scheduled job created');
    } catch (error) {
      logger.error({ error }, 'Failed to create scheduled job');
      Sentry.captureException(error, {
        tags: { component: 'gitignore-pipeline', phase: 'cron' },
      });
    }
  });

  logger.info('Gitignore Update Pipeline is running');

  // Keep process alive
  process.on('SIGTERM', () => {
    logger.info('Received SIGTERM, shutting down gracefully');
    process.exit(0);
  });

  process.on('SIGINT', () => {
    logger.info('Received SIGINT, shutting down gracefully');
    process.exit(0);
  });
}

// Run the pipeline
main().catch((error) => {
  logger.error({ error }, 'Fatal error in gitignore pipeline');
  Sentry.captureException(error, {
    tags: { component: 'gitignore-pipeline', phase: 'startup' },
  });
  process.exit(1);
});
</file>

<file path="pipeline-runners/plugin-management-pipeline.js">
#!/usr/bin/env node
// @ts-nocheck
import cron from 'node-cron';
import { PluginManagerWorker } from '../utils/plugin-manager.js';
import { config } from '../core/config.js';
import { createComponentLogger } from '../utils/logger.js';

const logger = createComponentLogger('PluginPipeline');

/**
 * Plugin Management Pipeline
 * Automatically audits Claude Code plugins on a schedule
 */
class PluginManagementPipeline {
  constructor(options = {}) {
    this.worker = new PluginManagerWorker({
      maxConcurrent: 1,
      logDir: config.logDir,
      sentryDsn: config.sentryDsn,
      ...options
    });

    this.setupEventListeners();
  }

  /**
   * Setup event listeners for job events
   */
  setupEventListeners() {
    this.worker.on('job:created', (job) => {
      logger.info({ jobId: job.id }, 'Plugin audit job created');
    });

    this.worker.on('job:started', (job) => {
      logger.info({
        jobId: job.id,
        detailed: job.data.detailed
      }, 'Plugin audit started');
    });

    this.worker.on('job:completed', (job) => {
      const duration = job.completedAt - job.startedAt;
      logger.info({
        jobId: job.id,
        duration,
        totalPlugins: job.result.totalPlugins,
        duplicateCategories: job.result.duplicateCategories?.length || 0,
        recommendations: job.result.recommendations?.length || 0
      }, 'Plugin audit completed');

      // Display recommendations
      this.displayRecommendations(job.result);
    });

    this.worker.on('job:failed', (job) => {
      logger.error({
        jobId: job.id,
        error: job.error
      }, 'Plugin audit failed');
    });
  }

  /**
   * Display audit recommendations
   * @param {Object} result - Audit results
   */
  displayRecommendations(result) {
    if (!result.recommendations || result.recommendations.length === 0) {
      return;
    }

    console.log('\n‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó');
    console.log('‚ïë          Plugin Audit Recommendations                          ‚ïë');
    console.log('‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n');

    result.recommendations.forEach((rec) => {
      const priorityIcon = {
        high: 'üî¥',
        medium: 'üü°',
        info: '‚úÖ'
      }[rec.priority] || 'üìå';

      console.log(`${priorityIcon} [${rec.priority.toUpperCase()}] ${rec.type}`);
      console.log(`   ${rec.message}`);
      console.log(`   Action: ${rec.action}`);

      if (rec.details) {
        console.log('   Details:');
        rec.details.forEach(detail => {
          console.log(`     ‚Ä¢ ${detail.category}: ${detail.plugins.join(', ')}`);
          console.log(`       ‚Üí ${detail.suggestion}`);
        });
      }
      console.log('');
    });
  }

  /**
   * Run a single audit
   * @param {Object} options - Audit options
   */
  async runAudit(options = {}) {
    logger.info({ options }, 'Starting plugin audit');

    const startTime = Date.now();

    try {
      // Create audit job
      const job = this.worker.addJob({
        detailed: options.detailed || false
      });

      logger.info({ jobId: job.id }, 'Audit job created');

      // Wait for completion
      await this.waitForCompletion();

      const duration = Date.now() - startTime;
      const stats = this.worker.getStats();

      logger.info({
        duration,
        stats
      }, 'Plugin audit pipeline completed');

      return stats;
    } catch (error) {
      logger.error({ error }, 'Plugin audit pipeline failed');
      throw error;
    }
  }

  /**
   * Wait for all jobs to complete
   */
  async waitForCompletion() {
    return new Promise((resolve) => {
      const checkInterval = setInterval(() => {
        const stats = this.worker.getStats();
        if (stats.active === 0 && stats.queued === 0) {
          clearInterval(checkInterval);
          resolve();
        }
      }, 1000);
    });
  }

  /**
   * Schedule automatic plugin audits
   * @param {string} cronSchedule - Cron schedule string
   * @returns {Object} Cron task
   */
  scheduleAudits(cronSchedule = '0 9 * * 1') {
    logger.info({ cronSchedule }, 'Scheduling plugin audits');

    if (!cron.validate(cronSchedule)) {
      throw new Error(`Invalid cron schedule: ${cronSchedule}`);
    }

    const task = cron.schedule(cronSchedule, async () => {
      logger.info('Cron triggered - starting plugin audit');
      try {
        await this.runAudit({ detailed: false });
      } catch (error) {
        logger.error({ error }, 'Scheduled plugin audit failed');
      }
    });

    logger.info('Plugin audits scheduled');
    return task;
  }
}

// Run if executed directly
if (import.meta.url === `file://${process.argv[1]}`) {
  const pipeline = new PluginManagementPipeline();

  const runOnStartup = process.env.RUN_ON_STARTUP === 'true';
  const detailed = process.env.DETAILED === 'true';
  const pluginCronSchedule = process.env.PLUGIN_CRON_SCHEDULE || '0 9 * * 1'; // Monday 9 AM

  if (runOnStartup) {
    logger.info('Running plugin audit immediately', { detailed });

    pipeline.runAudit({ detailed })
      .then(() => {
        logger.info('Plugin audit completed successfully');
        process.exit(0);
      })
      .catch((error) => {
        logger.error({ error }, 'Plugin audit failed');
        process.exit(1);
      });
  } else {
    logger.info('Starting plugin audit scheduler', { cronSchedule: pluginCronSchedule });
    pipeline.scheduleAudits(pluginCronSchedule);

    // Keep process alive
    logger.info('Plugin audit scheduler running. Press Ctrl+C to exit.');
  }

  // Graceful shutdown
  process.on('SIGINT', () => {
    logger.info('Received SIGINT, shutting down...');
    process.exit(0);
  });

  process.on('SIGTERM', () => {
    logger.info('Received SIGTERM, shutting down...');
    process.exit(0);
  });
}

export { PluginManagementPipeline };
</file>

<file path="pipeline-runners/repo-cleanup-pipeline.js">
#!/usr/bin/env node
// @ts-nocheck
import { RepoCleanupWorker } from '../workers/repo-cleanup-worker.js';
import { createComponentLogger } from '../utils/logger.js';
import * as Sentry from '@sentry/node';
import cron from 'node-cron';
import path from 'path';
import os from 'os';

const logger = createComponentLogger('RepoCleanupPipeline');

/**
 * Repository Cleanup Pipeline
 *
 * Automated repository cleanup across directories.
 * Removes common bloat files: Python venvs, .DS_Store, build artifacts, temp files.
 *
 * Features:
 * - Scheduled cleanup via cron
 * - Dry-run mode for testing
 * - Job queue integration
 * - Sentry error tracking
 * - Event-driven architecture
 *
 * Environment Variables:
 * - CLEANUP_CRON_SCHEDULE: Cron schedule (default: "0 3 * * 0" - Weekly Sunday 3 AM)
 * - CLEANUP_TARGET_DIR: Directory to clean (default: ~/code)
 * - CLEANUP_DRY_RUN: Dry run mode (default: false)
 * - RUN_ON_STARTUP: Run immediately on startup (default: false)
 *
 * Usage:
 *   npm run cleanup           # Start cron server
 *   npm run cleanup:once      # Run once and exit
 *   npm run cleanup:dryrun    # Dry run preview
 */

const CRON_SCHEDULE = process.env.CLEANUP_CRON_SCHEDULE || '0 3 * * 0'; // Weekly Sunday 3 AM
const TARGET_DIR = process.env.CLEANUP_TARGET_DIR || path.join(os.homedir(), 'code');
const DRY_RUN = process.env.CLEANUP_DRY_RUN === 'true';
const RUN_ON_STARTUP = process.env.RUN_ON_STARTUP === 'true';

async function main() {
  logger.info({
    cronSchedule: CRON_SCHEDULE,
    targetDir: TARGET_DIR,
    dryRun: DRY_RUN,
    runOnStartup: RUN_ON_STARTUP,
  }, 'Starting Repository Cleanup Pipeline');

  // Create worker instance
  const worker = new RepoCleanupWorker({
    baseDir: TARGET_DIR,
    maxConcurrent: 1, // Process one cleanup at a time
  });

  // Event listeners
  worker.on('job:created', (job) => {
    logger.info({
      jobId: job.id,
      type: job.data.type,
      targetDir: job.data.targetDir,
      dryRun: job.data.dryRun,
    }, 'Job created');
  });

  worker.on('job:started', (job) => {
    logger.info({
      jobId: job.id,
      targetDir: job.data.targetDir,
      dryRun: job.data.dryRun,
    }, 'Job started');
  });

  worker.on('job:completed', (job) => {
    const { initialSize, finalSize, totalItems, summary } = job.result || {};

    logger.info({
      jobId: job.id,
      targetDir: job.data.targetDir,
      initialSize,
      finalSize,
      totalItems,
      venvs: summary?.venvs || 0,
      tempFiles: summary?.tempFiles || 0,
      buildArtifacts: summary?.buildArtifacts || 0,
      redundantDirs: summary?.redundantDirs || 0,
    }, 'Job completed');

    // Log detailed summary
    if (summary) {
      logger.info({
        summary: {
          totalItems,
          categories: {
            venvs: summary.venvs,
            tempFiles: summary.tempFiles,
            outputFiles: summary.outputFiles,
            buildArtifacts: summary.buildArtifacts,
            redundantDirs: summary.redundantDirs,
          },
        },
      }, 'Cleanup summary');
    }
  });

  worker.on('job:failed', (job) => {
    logger.error({
      jobId: job.id,
      error: job.error,
      retries: job.retries,
    }, 'Job failed');

    Sentry.captureException(new Error(job.error), {
      tags: {
        component: 'repo-cleanup-pipeline',
        job_id: job.id,
      },
      extra: {
        targetDir: job.data.targetDir,
        dryRun: job.data.dryRun,
        retries: job.retries,
      },
    });
  });

  // Run immediately if requested
  if (RUN_ON_STARTUP) {
    logger.info('Running cleanup immediately (RUN_ON_STARTUP=true)');
    try {
      const job = worker.createCleanupJob(TARGET_DIR, {
        dryRun: DRY_RUN,
      });

      logger.info({
        jobId: job.id,
        dryRun: DRY_RUN,
      }, 'Startup job created');
    } catch (error) {
      logger.error({ error }, 'Failed to create startup job');
      Sentry.captureException(error, {
        tags: { component: 'repo-cleanup-pipeline', phase: 'startup' },
      });
    }
  }

  // Schedule cron job
  logger.info({ schedule: CRON_SCHEDULE }, 'Scheduling repository cleanup');

  cron.schedule(CRON_SCHEDULE, () => {
    logger.info('Cron triggered cleanup');
    try {
      const job = worker.createCleanupJob(TARGET_DIR, {
        dryRun: DRY_RUN,
      });

      logger.info({
        jobId: job.id,
        dryRun: DRY_RUN,
      }, 'Scheduled job created');
    } catch (error) {
      logger.error({ error }, 'Failed to create scheduled job');
      Sentry.captureException(error, {
        tags: { component: 'repo-cleanup-pipeline', phase: 'cron' },
      });
    }
  });

  logger.info('Repository Cleanup Pipeline is running');

  // Keep process alive
  process.on('SIGTERM', () => {
    logger.info('Received SIGTERM, shutting down gracefully');
    process.exit(0);
  });

  process.on('SIGINT', () => {
    logger.info('Received SIGINT, shutting down gracefully');
    process.exit(0);
  });
}

// Run the pipeline
main().catch((error) => {
  logger.error({ error }, 'Fatal error in cleanup pipeline');
  Sentry.captureException(error, {
    tags: { component: 'repo-cleanup-pipeline', phase: 'startup' },
  });
  process.exit(1);
});
</file>

<file path="pipeline-runners/schema-enhancement-pipeline.js">
#!/usr/bin/env node
import { SchemaEnhancementWorker } from '../workers/schema-enhancement-worker.js';
import { config } from '../core/config.js';
import { createComponentLogger } from '../utils/logger.js';
import cron from 'node-cron';
import fs from 'fs/promises';
import path from 'path';

const logger = createComponentLogger('SchemaEnhancementPipeline');

/**
 * Schema Enhancement Pipeline
 * Automatically enhances README files with Schema.org structured data
 */
class SchemaEnhancementPipeline {
  constructor(options = {}) {
    this.worker = new SchemaEnhancementWorker({
      maxConcurrent: config.maxConcurrent || 2,
      logDir: config.logDir,
      sentryDsn: config.sentryDsn,
      gitWorkflowEnabled: options.gitWorkflowEnabled ?? config.enableGitWorkflow,
      gitBranchPrefix: options.gitBranchPrefix || 'docs',
      gitBaseBranch: options.gitBaseBranch || config.gitBaseBranch,
      gitDryRun: options.gitDryRun ?? config.gitDryRun,
      outputBaseDir: options.outputBaseDir || './document-enhancement-impact-measurement',
      dryRun: options.dryRun || false,
      ...options
    });

    this.excludeDirs = new Set([
      'node_modules',
      '.git',
      'dist',
      'build',
      'coverage',
      'venv',
      '__pycache__'
    ]);

    this.baseDir = options.baseDir || config.codeBaseDir || process.env.HOME;
    this.setupEventListeners();
  }

  /**
   * Setup event listeners for job events
   */
  setupEventListeners() {
    this.worker.on('job:created', (job) => {
      logger.info({
        jobId: job.id,
        readmePath: job.data.relativePath
      }, 'Schema enhancement job created');
    });

    this.worker.on('job:started', (job) => {
      logger.info({
        jobId: job.id,
        readmePath: job.data.relativePath
      }, 'Schema enhancement job started');
    });

    this.worker.on('job:completed', (job) => {
      const duration = job.completedAt - job.startedAt;
      logger.info({
        jobId: job.id,
        duration,
        status: job.result.status,
        schemaType: job.result.schemaType,
        impactScore: job.result.impact?.impactScore
      }, 'Schema enhancement job completed');
    });

    this.worker.on('job:failed', (job) => {
      logger.error({
        jobId: job.id,
        readmePath: job.data.relativePath,
        error: job.error
      }, 'Schema enhancement job failed');
    });
  }

  /**
   * Recursively scan directory for README files
   */
  async scanForReadmes(dir, baseDir = dir, results = []) {
    try {
      const entries = await fs.readdir(dir, { withFileTypes: true });

      for (const entry of entries) {
        const fullPath = path.join(dir, entry.name);

        if (entry.isDirectory()) {
          // Skip excluded directories
          if (this.excludeDirs.has(entry.name)) {
            continue;
          }

          // Recursively scan subdirectories
          await this.scanForReadmes(fullPath, baseDir, results);
        } else if (entry.name === 'README.md') {
          const relativePath = path.relative(baseDir, fullPath);
          results.push({
            fullPath,
            relativePath,
            name: entry.name,
            dirPath: path.dirname(fullPath)
          });
        }
      }
    } catch (error) {
      logger.error({ dir, error: error.message }, 'Error scanning directory');
    }

    return results;
  }

  /**
   * Scan directory for README files
   */
  async scanDirectory(directory) {
    logger.info({ directory }, 'Scanning for README files');

    const readmeFiles = await this.scanForReadmes(directory);

    logger.info({
      readmeFiles: readmeFiles.length
    }, 'Directory scan complete');

    return readmeFiles;
  }

  /**
   * Create enhancement jobs for README files
   */
  async createEnhancementJobs(readmeFiles) {
    const context = {
      totalReadmes: readmeFiles.length,
      baseDir: this.baseDir
    };

    const jobs = [];

    for (const readme of readmeFiles) {
      // Add context about the repository
      const repoContext = {
        ...context,
        hasPackageJson: false, // Would need to check
        hasPyproject: false,   // Would need to check
        gitRemote: null        // Would need to extract
      };

      const job = await this.worker.createEnhancementJob(readme, repoContext);
      jobs.push(job);
    }

    logger.info({
      jobsCreated: jobs.length
    }, 'Enhancement jobs created');

    return jobs;
  }

  /**
   * Run enhancement on directory
   */
  async runEnhancement(directory = this.baseDir) {
    logger.info({ directory }, 'Starting schema enhancement pipeline');

    const startTime = Date.now();

    try {
      // Scan for README files
      const readmeFiles = await this.scanDirectory(directory);

      if (readmeFiles.length === 0) {
        logger.warn({ directory }, 'No README files found');
        return {
          status: 'completed',
          readmesFound: 0,
          enhanced: 0
        };
      }

      // Create enhancement jobs
      await this.createEnhancementJobs(readmeFiles);

      // Wait for completion
      await this.waitForCompletion();

      const duration = Date.now() - startTime;
      const stats = this.worker.getEnhancementStats();
      const jobStats = this.worker.getStats();

      // Generate summary report
      await this.worker.generateSummaryReport();

      logger.info({
        duration,
        stats,
        jobStats
      }, 'Schema enhancement pipeline completed');

      return {
        status: 'completed',
        duration,
        ...stats,
        jobs: jobStats
      };
    } catch (error) {
      logger.error({ error }, 'Schema enhancement pipeline failed');
      throw error;
    }
  }

  /**
   * Wait for all jobs to complete
   */
  async waitForCompletion() {
    return new Promise((resolve) => {
      const checkInterval = setInterval(() => {
        const stats = this.worker.getStats();
        if (stats.active === 0 && stats.queued === 0) {
          clearInterval(checkInterval);
          resolve();
        }
      }, 1000);
    });
  }

  /**
   * Schedule enhancement runs
   */
  scheduleEnhancements(cronSchedule = '0 3 * * 0') {
    logger.info({ cronSchedule }, 'Scheduling schema enhancement runs');

    if (!cron.validate(cronSchedule)) {
      throw new Error(`Invalid cron schedule: ${cronSchedule}`);
    }

    const task = cron.schedule(cronSchedule, async () => {
      logger.info('Cron triggered - starting schema enhancement');
      try {
        await this.runEnhancement();
      } catch (error) {
        logger.error({ error }, 'Scheduled enhancement failed');
      }
    });

    logger.info('Schema enhancement scheduled');
    return task;
  }
}

// Run if executed directly
if (import.meta.url === `file://${process.argv[1]}`) {
  const runOnStartup = process.env.RUN_ON_STARTUP === 'true';
  const cronSchedule = process.env.SCHEMA_ENHANCEMENT_CRON_SCHEDULE || '0 3 * * 0'; // Sunday 3 AM

  // Parse command line arguments
  const args = process.argv.slice(2);
  const options = {
    dryRun: false,
    gitWorkflowEnabled: config.enableGitWorkflow
  };

  let directory = config.codeBaseDir || process.env.HOME;

  for (let i = 0; i < args.length; i++) {
    if (args[i] === '--dry-run') {
      options.dryRun = true;
    } else if (args[i] === '--dir' && args[i + 1]) {
      directory = args[i + 1];
      i++;
    } else if (args[i] === '--git-workflow') {
      options.gitWorkflowEnabled = true;
    } else if (args[i] === '--no-git-workflow') {
      options.gitWorkflowEnabled = false;
    }
  }

  const pipeline = new SchemaEnhancementPipeline(options);

  if (runOnStartup) {
    logger.info({ directory, options }, 'Running schema enhancement immediately');

    pipeline.runEnhancement(directory)
      .then((result) => {
        logger.info({ result }, 'Enhancement completed successfully');
        process.exit(0);
      })
      .catch((error) => {
        logger.error({ error }, 'Enhancement failed');
        process.exit(1);
      });
  } else {
    logger.info({ cronSchedule }, 'Starting schema enhancement pipeline in scheduled mode');
    pipeline.scheduleEnhancements(cronSchedule);
    logger.info('Schema enhancement pipeline running. Press Ctrl+C to stop.');
  }
}

export { SchemaEnhancementPipeline };
</file>

<file path="pipeline-runners/test-refactor-pipeline.ts">
#!/usr/bin/env node
/**
 * Test Refactor Pipeline
 *
 * Scans repositories for test suites and generates modular utility files.
 * Part of the AlephAuto framework.
 *
 * Usage:
 *   node pipelines/test-refactor-pipeline.js                    # Scan all repos
 *   node pipelines/test-refactor-pipeline.js /path/to/project   # Single project
 *   DRY_RUN=true node pipelines/test-refactor-pipeline.js       # Analysis only
 */

import { TestRefactorWorker } from '../workers/test-refactor-worker.js';
import { DirectoryScanner } from '../utils/directory-scanner.js';
import { createComponentLogger } from '../utils/logger.js';
import { config } from '../core/config.js';
import cron from 'node-cron';
import path from 'path';

const logger = createComponentLogger('TestRefactorPipeline');

// Configuration
const CODE_BASE_DIR = process.env.CODE_BASE_DIR || config.codeBaseDir || process.env.HOME + '/code';
const CRON_SCHEDULE = process.env.TEST_REFACTOR_CRON || '0 4 * * 0'; // Sunday 4 AM
const RUN_ON_STARTUP = process.env.RUN_ON_STARTUP !== 'false';
const DRY_RUN = process.env.DRY_RUN === 'true';
const ENABLE_GIT_WORKFLOW = process.env.ENABLE_GIT_WORKFLOW === 'true';

interface DirectoryInfo {
  path: string;
  name: string;
}

interface PipelineMetrics {
  totalProjects: number;
  successfulRefactors: number;
  failedRefactors: number;
  filesGenerated: number;
  patternsDetected: number;
  stringsExtracted: number;
  recommendationsGenerated: number;
}

interface PipelineStats {
  total: number;
  queued: number;
  active: number;
  completed: number;
  failed: number;
}

interface PipelineResult {
  metrics: PipelineMetrics;
  stats: PipelineStats;
}

/**
 * Run the test refactoring pipeline
 */
async function runPipeline(targetPath: string | null = null): Promise<PipelineResult> {
  logger.info({
    codeBaseDir: CODE_BASE_DIR,
    targetPath,
    dryRun: DRY_RUN
  }, 'Starting test refactor pipeline');

  const worker = new TestRefactorWorker({
    dryRun: DRY_RUN,
    gitWorkflowEnabled: ENABLE_GIT_WORKFLOW,
    maxConcurrent: parseInt(process.env.MAX_CONCURRENT || '3', 10)
  });

  // Set up event handlers
  worker.on('job:created', (job: { id: string; data: { repository: string } }) => {
    logger.debug({ jobId: job.id, project: job.data.repository }, 'Job created');
  });

  worker.on('job:started', (job: { id: string; data: { repository: string } }) => {
    logger.info({ jobId: job.id, project: job.data.repository }, 'Job started');
  });

  worker.on('job:completed', (job: { id: string; data: { repository: string }; result: { generatedFiles?: string[]; recommendations?: string[] } }) => {
    const result = job.result;
    logger.info({
      jobId: job.id,
      project: job.data.repository,
      filesGenerated: result.generatedFiles?.length || 0,
      recommendations: result.recommendations?.length || 0
    }, 'Job completed');
  });

  worker.on('job:failed', (job: { id: string; data: { repository: string } }, error: Error) => {
    logger.error({
      jobId: job.id,
      project: job.data.repository,
      error: error.message
    }, 'Job failed');
  });

  try {
    if (targetPath) {
      // Single project mode
      const resolvedPath = path.resolve(targetPath);
      worker.queueProject(resolvedPath);
    } else {
      // Scan all repositories
      const scanner = new DirectoryScanner({
        baseDir: CODE_BASE_DIR,
        maxDepth: 2,
        excludePatterns: [
          'node_modules',
          '.git',
          'dist',
          'build',
          'coverage',
          '__pycache__',
          '.venv',
          'venv'
        ]
      });

      const directories: DirectoryInfo[] = await scanner.scanDirectories();

      // Filter to directories that have test files
      for (const dir of directories) {
        const hasTests = await hasTestDirectory(dir.path);
        if (hasTests) {
          worker.queueProject(dir.path);
        }
      }

      logger.info({
        totalDirectories: directories.length,
        queuedJobs: worker.queue.length
      }, 'Scan complete, jobs queued');
    }

    // Wait for all jobs to complete
    await waitForCompletion(worker);

    const metrics = worker.getMetrics();
    const stats = worker.getStats();

    logger.info({
      ...metrics,
      ...stats
    }, 'Pipeline completed');

    return { metrics, stats };

  } catch (error) {
    logger.error({ err: error }, 'Pipeline failed');
    throw error;
  }
}

/**
 * Check if directory has test files
 */
async function hasTestDirectory(dirPath: string): Promise<boolean> {
  const { glob } = await import('glob');

  const testFiles = await glob('**/*.{test,spec}.{ts,tsx,js,jsx}', {
    cwd: dirPath,
    ignore: ['**/node_modules/**'],
    nodir: true
  });

  return testFiles.length > 0;
}

/**
 * Wait for all jobs to complete
 */
function waitForCompletion(worker: TestRefactorWorker): Promise<void> {
  return new Promise((resolve) => {
    const checkCompletion = () => {
      const stats = worker.getStats();
      if (stats.queued === 0 && stats.active === 0) {
        resolve();
      } else {
        setTimeout(checkCompletion, 1000);
      }
    };
    checkCompletion();
  });
}

// Main execution
async function main(): Promise<void> {
  const targetPath = process.argv[2];

  if (targetPath) {
    // Single project mode
    await runPipeline(targetPath);
  } else if (RUN_ON_STARTUP) {
    // Run immediately
    await runPipeline();
  }

  // Schedule cron job
  if (!targetPath) {
    logger.info({ schedule: CRON_SCHEDULE }, 'Scheduling cron job');

    cron.schedule(CRON_SCHEDULE, async () => {
      logger.info('Running scheduled test refactor pipeline');
      await runPipeline();
    });

    // Keep process alive
    process.on('SIGINT', () => {
      logger.info('Shutting down');
      process.exit(0);
    });
  }
}

main().catch(error => {
  logger.error({ err: error }, 'Fatal error');
  process.exit(1);
});

export { runPipeline };
</file>

<file path="pipeline-runners/universal-repo-cleanup.sh">
#!/bin/bash

################################################################################
# Universal Repository Cleanup Script
#
# Purpose: Remove common bloat files and directories from any repository
# Created: 2025-11-17
#
# This script performs cleanup tasks common to most repositories:
# 1. Remove Python virtual environments
# 2. Remove .DS_Store files (macOS system files)
# 3. Remove build artifacts and temporary files
# 4. Remove common duplicate/redundant directories
#
# Usage:
#   ./universal-repo-cleanup.sh [directory]
#
#   If no directory is provided, uses current working directory
#
# Examples:
#   ./universal-repo-cleanup.sh                    # Clean current directory
#   ./universal-repo-cleanup.sh /path/to/repo      # Clean specific directory
#   ./universal-repo-cleanup.sh ~/projects/myapp   # Clean using home path
################################################################################

set -e  # Exit on error
set -u  # Exit on undefined variable

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Target directory (first argument or current directory)
TARGET_DIR="${1:-$(pwd)}"

# Resolve to absolute path
TARGET_DIR="$(cd "$TARGET_DIR" 2>/dev/null && pwd)" || {
    echo -e "${RED}‚úó Error: Directory '$1' does not exist${NC}"
    exit 1
}

################################################################################
# Configuration - Customize what gets cleaned
################################################################################

# Python virtual environment directory names (common patterns)
VENV_PATTERNS=(
    "venv"
    ".venv"
    "env"
    ".env"
    "virtualenv"
    "*.venv"
    "personal_site"  # Example from original repo
)

# Build artifact patterns
BUILD_ARTIFACTS=(
    ".jekyll-cache"
    ".sass-cache"
    ".bundle"
    "node_modules/.cache"
    "dist"
    "build"
    ".next"
    ".nuxt"
    "out"
    ".output"
    "target"
    ".gradle"
)

# Temporary/cache file patterns
TEMP_FILE_PATTERNS=(
    ".DS_Store"
    "*.pyc"
    "*.pyo"
    "__pycache__"
    "*.swp"
    "*.swo"
    "*~"
    ".*.swp"
    "Thumbs.db"
    "desktop.ini"
)

# Output file patterns (files that are generated)
OUTPUT_FILE_PATTERNS=(
    "repomix-output.xml"
    "*.log"
    "npm-debug.log*"
    "yarn-debug.log*"
    "yarn-error.log*"
)

# Common redundant directory names
REDUNDANT_DIRS=(
    "drafts"
    "temp"
    "tmp"
    "backup"
    "backups"
    "old"
    "archive"
    "deprecated"
)

################################################################################
# Helper Functions
################################################################################

print_header() {
    echo -e "\n${BLUE}========================================${NC}"
    echo -e "${BLUE}$1${NC}"
    echo -e "${BLUE}========================================${NC}\n"
}

print_success() {
    echo -e "${GREEN}‚úì $1${NC}"
}

print_warning() {
    echo -e "${YELLOW}‚ö† $1${NC}"
}

print_error() {
    echo -e "${RED}‚úó $1${NC}"
}

print_info() {
    echo -e "${BLUE}‚Üí $1${NC}"
}

# Get directory/file size in human readable format
get_size() {
    local path="$1"
    if [ -e "$path" ]; then
        du -sh "$path" 2>/dev/null | cut -f1
    else
        echo "N/A"
    fi
}

# Count files in directory or matching pattern
count_files() {
    local path="$1"
    if [ -d "$path" ]; then
        find "$path" -type f 2>/dev/null | wc -l | tr -d ' '
    else
        echo "0"
    fi
}

################################################################################
# Scanning Functions
################################################################################

scan_venvs() {
    print_info "Scanning for Python virtual environments..."
    local found=()

    for pattern in "${VENV_PATTERNS[@]}"; do
        while IFS= read -r -d '' dir; do
            # Skip if inside node_modules
            if [[ ! "$dir" =~ node_modules ]]; then
                found+=("$dir")
            fi
        done < <(find "$TARGET_DIR" -maxdepth 3 -type d -name "$pattern" -print0 2>/dev/null)
    done

    # Remove duplicates
    printf '%s\n' "${found[@]}" | sort -u
}

scan_temp_files() {
    print_info "Scanning for temporary/cache files..."
    local found=()

    for pattern in "${TEMP_FILE_PATTERNS[@]}"; do
        while IFS= read -r -d '' file; do
            found+=("$file")
        done < <(find "$TARGET_DIR" -name "$pattern" -print0 2>/dev/null)
    done

    printf '%s\n' "${found[@]}" | sort -u
}

scan_output_files() {
    print_info "Scanning for output/generated files..."
    local found=()

    for pattern in "${OUTPUT_FILE_PATTERNS[@]}"; do
        while IFS= read -r -d '' file; do
            # Keep root-level repomix-output.xml
            if [[ "$file" != "$TARGET_DIR/repomix-output.xml" ]]; then
                found+=("$file")
            fi
        done < <(find "$TARGET_DIR" -name "$pattern" -print0 2>/dev/null)
    done

    printf '%s\n' "${found[@]}" | sort -u
}

scan_build_artifacts() {
    print_info "Scanning for build artifacts..."
    local found=()

    for artifact in "${BUILD_ARTIFACTS[@]}"; do
        local path="$TARGET_DIR/$artifact"
        if [ -d "$path" ]; then
            found+=("$path")
        fi
    done

    printf '%s\n' "${found[@]}" | sort -u
}

scan_redundant_dirs() {
    print_info "Scanning for redundant directories..."
    local found=()

    for dir_name in "${REDUNDANT_DIRS[@]}"; do
        local path="$TARGET_DIR/$dir_name"
        if [ -d "$path" ]; then
            # Check if it's in .gitignore
            if [ -f "$TARGET_DIR/.gitignore" ] && grep -q "^${dir_name}/\?$" "$TARGET_DIR/.gitignore" 2>/dev/null; then
                found+=("$path (in .gitignore)")
            else
                found+=("$path")
            fi
        fi
    done

    printf '%s\n' "${found[@]}" | sort -u
}

################################################################################
# Confirmation and Preview
################################################################################

show_preview() {
    print_header "Repository Cleanup Preview"

    echo "Target Directory: $TARGET_DIR"
    echo "Current Size: $(get_size "$TARGET_DIR")"
    echo ""

    # Scan all categories
    local venvs=($(scan_venvs))
    local temp_files=($(scan_temp_files))
    local output_files=($(scan_output_files))
    local build_artifacts=($(scan_build_artifacts))
    local redundant_dirs=($(scan_redundant_dirs))

    local total_items=0

    # Python venvs
    if [ ${#venvs[@]} -gt 0 ]; then
        echo -e "${YELLOW}Python Virtual Environments (${#venvs[@]} found):${NC}"
        for venv in "${venvs[@]}"; do
            [ -n "$venv" ] && echo "  - $venv ($(get_size "$venv"))" && ((total_items++))
        done
        echo ""
    fi

    # Temporary files
    if [ ${#temp_files[@]} -gt 0 ]; then
        echo -e "${YELLOW}Temporary/Cache Files (${#temp_files[@]} found):${NC}"
        local count=0
        for file in "${temp_files[@]}"; do
            [ -n "$file" ] && ((count++)) && ((total_items++))
        done
        echo "  - $count files (.DS_Store, __pycache__, .swp, etc.)"
        echo ""
    fi

    # Output files
    if [ ${#output_files[@]} -gt 0 ]; then
        echo -e "${YELLOW}Output/Generated Files (${#output_files[@]} found):${NC}"
        for file in "${output_files[@]}"; do
            [ -n "$file" ] && echo "  - $file" && ((total_items++))
        done
        echo ""
    fi

    # Build artifacts
    if [ ${#build_artifacts[@]} -gt 0 ]; then
        echo -e "${YELLOW}Build Artifacts (${#build_artifacts[@]} found):${NC}"
        for artifact in "${build_artifacts[@]}"; do
            [ -n "$artifact" ] && echo "  - $artifact ($(get_size "$artifact"))" && ((total_items++))
        done
        echo ""
    fi

    # Redundant directories
    if [ ${#redundant_dirs[@]} -gt 0 ]; then
        echo -e "${YELLOW}Redundant Directories (${#redundant_dirs[@]} found):${NC}"
        for dir in "${redundant_dirs[@]}"; do
            [ -n "$dir" ] && echo "  - $dir" && ((total_items++))
        done
        echo ""
    fi

    if [ $total_items -eq 0 ]; then
        print_success "No items found to clean - repository is already clean!"
        exit 0
    fi

    echo -e "${YELLOW}Total items to remove: $total_items${NC}"
    echo ""

    # Store for cleanup functions
    export FOUND_VENVS="${venvs[*]}"
    export FOUND_TEMP_FILES="${temp_files[*]}"
    export FOUND_OUTPUT_FILES="${output_files[*]}"
    export FOUND_BUILD_ARTIFACTS="${build_artifacts[*]}"
    export FOUND_REDUNDANT_DIRS="${redundant_dirs[*]}"
}

confirm_cleanup() {
    read -p "Do you want to proceed with cleanup? (yes/no): " response

    if [[ ! "$response" =~ ^[Yy][Ee][Ss]$ ]]; then
        print_warning "Cleanup cancelled by user"
        exit 0
    fi

    echo ""
}

################################################################################
# Cleanup Functions
################################################################################

cleanup_venvs() {
    print_header "Step 1: Removing Python Virtual Environments"

    local venvs=($FOUND_VENVS)
    local removed=0

    if [ ${#venvs[@]} -eq 0 ] || [ -z "${venvs[0]}" ]; then
        print_info "No virtual environments to remove"
        return
    fi

    for venv in "${venvs[@]}"; do
        if [ -n "$venv" ] && [ -d "$venv" ]; then
            local size=$(get_size "$venv")
            print_info "Removing $(basename "$venv") ($size)..."
            rm -rf "$venv"
            print_success "Removed $venv"
            ((removed++))
        fi
    done

    print_success "Removed $removed virtual environment(s)"
}

cleanup_temp_files() {
    print_header "Step 2: Removing Temporary/Cache Files"

    local temp_files=($FOUND_TEMP_FILES)
    local removed=0

    if [ ${#temp_files[@]} -eq 0 ] || [ -z "${temp_files[0]}" ]; then
        print_info "No temporary files to remove"
        return
    fi

    for file in "${temp_files[@]}"; do
        if [ -n "$file" ] && [ -e "$file" ]; then
            rm -rf "$file" 2>/dev/null && ((removed++))
        fi
    done

    print_success "Removed $removed temporary file(s)"
}

cleanup_output_files() {
    print_header "Step 3: Removing Output/Generated Files"

    local output_files=($FOUND_OUTPUT_FILES)
    local removed=0

    if [ ${#output_files[@]} -eq 0 ] || [ -z "${output_files[0]}" ]; then
        print_info "No output files to remove"
        return
    fi

    for file in "${output_files[@]}"; do
        if [ -n "$file" ] && [ -f "$file" ]; then
            print_info "Removing $(basename "$file")..."
            rm -f "$file"
            print_success "Removed $file"
            ((removed++))
        fi
    done

    print_success "Removed $removed output file(s)"
}

cleanup_build_artifacts() {
    print_header "Step 4: Removing Build Artifacts"

    local build_artifacts=($FOUND_BUILD_ARTIFACTS)
    local removed=0

    if [ ${#build_artifacts[@]} -eq 0 ] || [ -z "${build_artifacts[0]}" ]; then
        print_info "No build artifacts to remove"
        return
    fi

    for artifact in "${build_artifacts[@]}"; do
        if [ -n "$artifact" ] && [ -d "$artifact" ]; then
            local size=$(get_size "$artifact")
            print_info "Removing $(basename "$artifact") ($size)..."
            rm -rf "$artifact"
            print_success "Removed $artifact"
            ((removed++))
        fi
    done

    print_success "Removed $removed build artifact(s)"
}

cleanup_redundant_dirs() {
    print_header "Step 5: Removing Redundant Directories"

    local redundant_dirs=($FOUND_REDUNDANT_DIRS)
    local removed=0

    if [ ${#redundant_dirs[@]} -eq 0 ] || [ -z "${redundant_dirs[0]}" ]; then
        print_info "No redundant directories to remove"
        return
    fi

    for dir in "${redundant_dirs[@]}"; do
        # Remove " (in .gitignore)" suffix if present
        dir="${dir% (in .gitignore)}"

        if [ -n "$dir" ] && [ -d "$dir" ]; then
            local size=$(get_size "$dir")
            print_info "Removing $(basename "$dir") ($size)..."
            rm -rf "$dir"
            print_success "Removed $dir"
            ((removed++))
        fi
    done

    print_success "Removed $removed redundant director(ies)"
}

################################################################################
# Summary
################################################################################

print_summary() {
    print_header "Cleanup Summary"

    local final_size=$(get_size "$TARGET_DIR")

    echo "Cleanup completed successfully!"
    echo ""
    echo "Target Directory: $TARGET_DIR"
    echo "Final Size: $final_size"
    echo ""
    echo "Cleaned up:"
    echo "  ‚úì Python virtual environments"
    echo "  ‚úì Temporary/cache files (.DS_Store, __pycache__, etc.)"
    echo "  ‚úì Output/generated files (logs, repomix files, etc.)"
    echo "  ‚úì Build artifacts (.jekyll-cache, dist/, etc.)"
    echo "  ‚úì Redundant directories (drafts/, temp/, backup/, etc.)"
    echo ""

    print_success "Repository cleanup completed!"
}

recommend_gitignore() {
    print_header "Recommendations"

    if [ ! -f "$TARGET_DIR/.gitignore" ]; then
        print_warning "No .gitignore found - consider creating one"
        return
    fi

    echo "Consider adding these patterns to .gitignore if not already present:"
    echo ""
    echo "  # Python"
    echo "  venv/"
    echo "  .venv/"
    echo "  *.pyc"
    echo "  __pycache__/"
    echo ""
    echo "  # System files"
    echo "  .DS_Store"
    echo "  Thumbs.db"
    echo ""
    echo "  # Build artifacts"
    echo "  dist/"
    echo "  build/"
    echo "  *.log"
    echo ""
    echo "  # Temporary"
    echo "  temp/"
    echo "  tmp/"
    echo "  *.swp"
    echo ""
}

################################################################################
# Main Execution
################################################################################

main() {
    print_header "Universal Repository Cleanup Script"

    echo "Target: $TARGET_DIR"
    echo ""

    # Show preview and get confirmation
    show_preview
    confirm_cleanup

    # Execute cleanup tasks
    cleanup_venvs
    cleanup_temp_files
    cleanup_output_files
    cleanup_build_artifacts
    cleanup_redundant_dirs

    # Print summary
    print_summary

    # Print recommendations
    recommend_gitignore

    print_success "All cleanup tasks completed successfully!"
}

# Run main function
main "$@"
</file>

<file path="types/duplicate-detection-types.js">
/**
 * Type Definitions for Duplicate Detection Worker
 *
 * Comprehensive TypeScript types for the duplicate detection pipeline,
 * including job data, scan results, metrics, and configuration.
 *
 * @module sidequest/types/duplicate-detection-types
 */
import { z } from 'zod';
// ============================================================================
// Scan Types & Enums
// ============================================================================
/**
 * Scan Type Enum
 */
export const ScanTypeSchema = z.enum(['inter-project', 'intra-project']);
/**
 * Scan Frequency Enum
 */
export const ScanFrequencySchema = z.enum(['daily', 'weekly', 'monthly', 'on-demand']);
/**
 * Priority Level Enum
 */
export const PriorityLevelSchema = z.enum(['critical', 'high', 'medium', 'low']);
// ============================================================================
// Job Data Types
// ============================================================================
/**
 * Duplicate Detection Job Data Schema
 */
export const DuplicateDetectionJobDataSchema = z.object({
    type: z.literal('duplicate-detection'),
    scanType: ScanTypeSchema,
    repositories: z.array(z.object({
        name: z.string(),
        path: z.string(),
        enabled: z.boolean(),
        priority: PriorityLevelSchema,
        scanFrequency: ScanFrequencySchema
    })),
    groupName: z.string().optional()
}).strict();
// ============================================================================
// Type Guards
// ============================================================================
/**
 * Type guard for IntraProjectScanResult
 */
export function isIntraProjectScanResult(result) {
    return result.scan_type === 'intra-project';
}
/**
 * Type guard for InterProjectScanResult
 */
export function isInterProjectScanResult(result) {
    return result.scan_type === 'inter-project';
}
/**
 * Type guard for IntraProjectJobResult
 */
export function isIntraProjectJobResult(result) {
    return result.scanType === 'intra-project';
}
/**
 * Type guard for InterProjectJobResult
 */
export function isInterProjectJobResult(result) {
    return result.scanType === 'inter-project';
}
</file>

<file path="types/duplicate-detection-types.ts">
/**
 * Type Definitions for Duplicate Detection Worker
 *
 * Comprehensive TypeScript types for the duplicate detection pipeline,
 * including job data, scan results, metrics, and configuration.
 *
 * @module sidequest/types/duplicate-detection-types
 */

import { z } from 'zod';

// ============================================================================
// Scan Types & Enums
// ============================================================================

/**
 * Scan Type Enum
 */
export const ScanTypeSchema = z.enum(['inter-project', 'intra-project']);
export type ScanType = z.infer<typeof ScanTypeSchema>;

/**
 * Scan Frequency Enum
 */
export const ScanFrequencySchema = z.enum(['daily', 'weekly', 'monthly', 'on-demand']);
export type ScanFrequency = z.infer<typeof ScanFrequencySchema>;

/**
 * Priority Level Enum
 */
export const PriorityLevelSchema = z.enum(['critical', 'high', 'medium', 'low']);
export type PriorityLevel = z.infer<typeof PriorityLevelSchema>;

// ============================================================================
// Repository Configuration Types
// ============================================================================

/**
 * Repository Configuration
 */
export interface RepositoryConfig {
  name: string;
  path: string;
  enabled: boolean;
  priority: PriorityLevel;
  scanFrequency: ScanFrequency;
  lastScanned?: string; // ISO timestamp
  scanHistory?: ScanHistoryEntry[];
}

/**
 * Scan History Entry
 */
export interface ScanHistoryEntry {
  timestamp: string; // ISO timestamp
  status: 'success' | 'failure';
  duration: number; // seconds
  duplicatesFound: number;
}

/**
 * Repository Group Configuration
 */
export interface RepositoryGroupConfig {
  name: string;
  description?: string;
  enabled: boolean;
  scanType: 'inter-project';
  repositories: string[]; // Repository names
}

// ============================================================================
// Job Data Types
// ============================================================================

/**
 * Duplicate Detection Job Data Schema
 */
export const DuplicateDetectionJobDataSchema = z.object({
  type: z.literal('duplicate-detection'),
  scanType: ScanTypeSchema,
  repositories: z.array(z.object({
    name: z.string(),
    path: z.string(),
    enabled: z.boolean(),
    priority: PriorityLevelSchema,
    scanFrequency: ScanFrequencySchema
  })),
  groupName: z.string().optional()
}).strict();

export type DuplicateDetectionJobData = z.infer<typeof DuplicateDetectionJobDataSchema>;

// ============================================================================
// Scan Result Types
// ============================================================================

/**
 * Scan Metadata
 */
export interface ScanMetadata {
  scan_started: string; // ISO timestamp
  scan_completed: string; // ISO timestamp
  duration_seconds: number;
  total_files_scanned: number;
  scan_type: ScanType;
}

/**
 * Scan Metrics
 */
export interface ScanMetrics {
  total_duplicate_groups?: number;
  total_cross_repository_groups?: number;
  total_suggestions: number;
  high_impact_duplicates: number;
}

/**
 * Duplicate Group
 */
export interface DuplicateGroup {
  group_id: string;
  instances: Array<{
    file_path: string;
    start_line: number;
    end_line: number;
    repository?: string;
  }>;
  impact_score: number;
  similarity_score: number;
  suggestion?: string;
}

/**
 * Scan Result (Intra-Project)
 */
export interface IntraProjectScanResult {
  scan_type: 'intra-project';
  scan_metadata: ScanMetadata;
  metrics: ScanMetrics;
  duplicate_groups: DuplicateGroup[];
  suggestions?: string[];
}

/**
 * Scan Result (Inter-Project)
 */
export interface InterProjectScanResult {
  scan_type: 'inter-project';
  scan_metadata: ScanMetadata;
  metrics: ScanMetrics;
  cross_repository_duplicates: DuplicateGroup[];
  suggestions?: string[];
}

/**
 * Union type for all scan results
 */
export type ScanResult = IntraProjectScanResult | InterProjectScanResult;

// ============================================================================
// PR Creation Types
// ============================================================================

/**
 * PR Creation Result
 */
export interface PRCreationResult {
  prsCreated: number;
  prUrls: string[];
  errors: Array<{
    repository?: string;
    error: string;
  }>;
}

// ============================================================================
// Job Result Types
// ============================================================================

/**
 * Intra-Project Job Result
 */
export interface IntraProjectJobResult {
  scanType: 'intra-project';
  repository: string;
  duplicates: number;
  suggestions: number;
  duration: number; // seconds
  prResults?: {
    prsCreated: number;
    prUrls: string[];
    errors: number;
  } | null;
}

/**
 * Inter-Project Job Result
 */
export interface InterProjectJobResult {
  scanType: 'inter-project';
  repositories: number;
  crossRepoDuplicates: number;
  suggestions: number;
  duration: number; // seconds
}

/**
 * Union type for all job results
 */
export type JobResult = IntraProjectJobResult | InterProjectJobResult;

// ============================================================================
// Metrics Types
// ============================================================================

/**
 * Worker Scan Metrics
 */
export interface WorkerScanMetrics {
  totalScans: number;
  successfulScans: number;
  failedScans: number;
  totalDuplicatesFound: number;
  totalSuggestionsGenerated: number;
  highImpactDuplicates: number;
  prsCreated: number;
  prCreationErrors: number;
}

/**
 * Retry Information
 */
export interface RetryInfo {
  attempts: number;
  lastAttempt: number; // timestamp
  maxAttempts: number;
  delay: number; // milliseconds
}

/**
 * Retry Metrics
 */
export interface RetryMetrics {
  activeRetries: number;
  totalRetryAttempts: number;
  jobsBeingRetried: Array<{
    jobId: string;
    attempts: number;
    maxAttempts: number;
    lastAttempt: string; // ISO timestamp
  }>;
  retryDistribution: {
    attempt1: number;
    attempt2: number;
    attempt3Plus: number;
    nearingLimit: number; // 3+ attempts
  };
}

/**
 * Complete Scan Metrics (includes queue and retry metrics)
 */
export interface CompleteScanMetrics extends WorkerScanMetrics {
  queueStats: {
    queued: number;
    active: number;
    completed: number;
    failed: number;
  };
  retryMetrics: RetryMetrics;
}

// ============================================================================
// Worker Options Types
// ============================================================================

/**
 * Duplicate Detection Worker Options
 */
export interface DuplicateDetectionWorkerOptions {
  maxConcurrentScans?: number;
  maxConcurrent?: number;
  logDir?: string;
  configPath?: string;
  enablePRCreation?: boolean;
  baseBranch?: string;
  branchPrefix?: string;
  dryRun?: boolean;
  maxSuggestionsPerPR?: number;
  sentryDsn?: string;
}

// ============================================================================
// Event Payload Types
// ============================================================================

/**
 * Initialized Event Payload
 */
export interface InitializedEventPayload {
  totalRepositories: number;
  enabledRepositories: number;
  groups: number;
  byPriority: Record<PriorityLevel, number>;
  byFrequency: Record<ScanFrequency, number>;
}

/**
 * Pipeline Status Event Payload
 */
export type PipelineStatusEventPayload =
  | { status: 'initialized'; stats: InitializedEventPayload }
  | { status: 'scanning'; jobId: string; scanType: ScanType; repositories: number }
  | { status: 'failed'; jobId: string; error: string }
  | { status: 'completed'; jobId: string; result: JobResult };

/**
 * Scan Completed Event Payload
 */
export interface ScanCompletedEventPayload {
  jobId: string;
  scanType: ScanType;
  metrics: {
    duplicates: number;
    suggestions: number;
    duration: number;
  };
}

/**
 * PR Created Event Payload
 */
export interface PRCreatedEventPayload {
  repository: string;
  prsCreated: number;
  prUrls: string[];
}

/**
 * PR Failed Event Payload
 */
export interface PRFailedEventPayload {
  repository: string;
  error: string;
}

/**
 * High Impact Detected Event Payload
 */
export interface HighImpactDetectedEventPayload {
  count: number;
  threshold: number;
  topImpactScore: number;
}

/**
 * Retry Event Payloads
 */
export interface RetryScheduledEventPayload {
  jobId: string;
  attempt: number;
  delay: number;
}

export interface RetryWarningEventPayload {
  jobId: string;
  attempts: number;
  maxAttempts: number;
}

export interface RetryCircuitBreakerEventPayload {
  jobId: string;
  attempts: number;
  maxAbsolute: number;
}

export interface MetricsUpdatedEventPayload {
  metrics: CompleteScanMetrics;
}

// ============================================================================
// Configuration Types
// ============================================================================

/**
 * Scan Configuration
 */
export interface ScanConfig {
  enabled: boolean;
  retryAttempts: number;
  retryDelay: number; // milliseconds
  maxConcurrentScans: number;
}

/**
 * Notification Settings
 */
export interface NotificationSettings {
  enabled: boolean;
  onHighImpactDuplicates: boolean;
  highImpactThreshold: number;
}

// ============================================================================
// Type Guards
// ============================================================================

/**
 * Type guard for IntraProjectScanResult
 */
export function isIntraProjectScanResult(result: ScanResult): result is IntraProjectScanResult {
  return result.scan_type === 'intra-project';
}

/**
 * Type guard for InterProjectScanResult
 */
export function isInterProjectScanResult(result: ScanResult): result is InterProjectScanResult {
  return result.scan_type === 'inter-project';
}

/**
 * Type guard for IntraProjectJobResult
 */
export function isIntraProjectJobResult(result: JobResult): result is IntraProjectJobResult {
  return result.scanType === 'intra-project';
}

/**
 * Type guard for InterProjectJobResult
 */
export function isInterProjectJobResult(result: JobResult): result is InterProjectJobResult {
  return result.scanType === 'inter-project';
}
</file>

<file path="utils/directory-scanner.js">
import fs from 'fs/promises';
import path from 'path';
import os from 'os';
import { createComponentLogger } from './logger.js';

const logger = createComponentLogger('DirectoryScanner');

/**
 * DirectoryScanner - Recursively scans directories
 */
export class DirectoryScanner {
  constructor(options = {}) {
    this.baseDir = options.baseDir || path.join(os.homedir(), 'code');
    this.outputDir = options.outputDir || './directory-scan-reports';
    this.excludeDirs = new Set(options.excludeDirs || [
      'node_modules',
      '.git',
      'dist',
      'build',
      'coverage',
      '.next',
      '.nuxt',
      'vendor',
      '__pycache__',
      '.venv',
      'venv',
      'jobs',
      'logs',
      '.claude',
      'python',
      'node',
      'go',
      'php',
      'rust',
      'recovery',
    ]);
    this.maxDepth = options.maxDepth || 10;
  }

  /**
   * Scan for git repository root directories only
   */
  async scanDirectories() {
    const directories = [];
    await this.scanRecursive(this.baseDir, '', 0, directories);
    return directories;
  }

  /**
   * Check if a directory is a git repository root
   */
  async isGitRepository(dirPath) {
    try {
      const gitPath = path.join(dirPath, '.git');
      const stat = await fs.stat(gitPath);
      return stat.isDirectory();
    } catch (error) {
      return false;
    }
  }

  /**
   * Recursively scan for git repository root directories
   */
  async scanRecursive(currentPath, relativePath, depth, results) {
    // Check depth limit
    if (depth > this.maxDepth) {
      return;
    }

    try {
      // Check if current directory is a git repository
      const isGitRepo = await this.isGitRepository(currentPath);

      if (isGitRepo) {
        // This is a git repository root - add it and stop recursing
        results.push({
          fullPath: currentPath,
          relativePath: relativePath || path.basename(currentPath),
          name: path.basename(currentPath),
          depth,
          isGitRepo: true,
        });
        logger.info({ path: currentPath, relativePath }, 'Found git repository');
        return; // Don't scan subdirectories of git repos
      }

      const entries = await fs.readdir(currentPath, { withFileTypes: true });

      for (const entry of entries) {
        if (!entry.isDirectory()) continue;

        // Skip excluded directories
        if (this.excludeDirs.has(entry.name)) {
          continue;
        }

        // Skip hidden directories (except .git is checked separately)
        if (entry.name.startsWith('.')) {
          continue;
        }

        const fullPath = path.join(currentPath, entry.name);
        const newRelativePath = relativePath
          ? path.join(relativePath, entry.name)
          : entry.name;

        // Recurse into subdirectories
        await this.scanRecursive(fullPath, newRelativePath, depth + 1, results);
      }
    } catch (error) {
      // Log but don't fail on permission errors
      logger.warn({ path: currentPath, error: error.message }, 'Cannot access directory');
    }
  }

  /**
   * Check if a directory should be processed
   */
  async shouldProcess(dirPath) {
    try {
      const stat = await fs.stat(dirPath);
      if (!stat.isDirectory()) return false;

      // Check if directory has any files (not just subdirectories)
      const entries = await fs.readdir(dirPath);
      return entries.length > 0;
    } catch (error) {
      return false;
    }
  }

  /**
   * Get directory info
   */
  async getDirectoryInfo(dirPath) {
    const stat = await fs.stat(dirPath);
    const entries = await fs.readdir(dirPath);

    return {
      path: dirPath,
      size: stat.size,
      fileCount: entries.length,
      modifiedAt: stat.mtime,
    };
  }

  /**
   * Generate scan statistics
   */
  generateScanStats(directories) {
    const stats = {
      total: directories.length,
      byDepth: {},
      totalSize: 0,
      byName: {},
    };

    for (const dir of directories) {
      // Count by depth
      stats.byDepth[dir.depth] = (stats.byDepth[dir.depth] || 0) + 1;

      // Count by name (for detecting common project types)
      stats.byName[dir.name] = (stats.byName[dir.name] || 0) + 1;
    }

    // Get top directory names
    const sortedNames = Object.entries(stats.byName)
      .sort((a, b) => b[1] - a[1])
      .slice(0, 10);

    stats.topDirectoryNames = sortedNames.map(([name, count]) => ({ name, count }));

    return stats;
  }

  /**
   * Save scan report to output directory
   */
  async saveScanReport(directories, stats) {
    await fs.mkdir(this.outputDir, { recursive: true });

    const timestamp = Date.now();
    const report = {
      timestamp: new Date().toISOString(),
      baseDir: this.baseDir,
      scanStats: stats,
      directories: directories.map(d => ({
        relativePath: d.relativePath,
        name: d.name,
        depth: d.depth,
      })),
    };

    const reportPath = path.join(this.outputDir, `scan-report-${timestamp}.json`);
    await fs.writeFile(reportPath, JSON.stringify(report, null, 2));

    return reportPath;
  }

  /**
   * Generate directory tree visualization
   */
  generateDirectoryTree(directories) {
    const tree = [];

    // Group by depth for easier visualization
    const byDepth = {};
    for (const dir of directories) {
      if (!byDepth[dir.depth]) byDepth[dir.depth] = [];
      byDepth[dir.depth].push(dir);
    }

    // Generate tree structure
    tree.push('Directory Tree:');
    tree.push('==============');
    tree.push('');
    tree.push(this.baseDir);

    for (const dir of directories) {
      const indent = '  '.repeat(dir.depth + 1);
      const prefix = dir.depth === 0 ? '‚îú‚îÄ‚îÄ ' : '‚îî‚îÄ‚îÄ ';
      tree.push(`${indent}${prefix}${dir.name}/`);
    }

    return tree.join('\n');
  }

  /**
   * Save directory tree to file
   */
  async saveDirectoryTree(directories) {
    await fs.mkdir(this.outputDir, { recursive: true });

    const tree = this.generateDirectoryTree(directories);
    const timestamp = Date.now();
    const treePath = path.join(this.outputDir, `directory-tree-${timestamp}.txt`);

    await fs.writeFile(treePath, tree);

    return treePath;
  }

  /**
   * Generate and save complete scan results
   */
  async generateAndSaveScanResults(directories) {
    const stats = this.generateScanStats(directories);

    // Save JSON report
    const reportPath = await this.saveScanReport(directories, stats);

    // Save tree visualization
    const treePath = await this.saveDirectoryTree(directories);

    // Create summary
    const summary = {
      timestamp: new Date().toISOString(),
      baseDir: this.baseDir,
      totalDirectories: directories.length,
      maxDepth: Math.max(...directories.map(d => d.depth)),
      reportPath,
      treePath,
      stats,
    };

    const summaryPath = path.join(this.outputDir, `scan-summary-${Date.now()}.json`);
    await fs.writeFile(summaryPath, JSON.stringify(summary, null, 2));

    return {
      summary,
      reportPath,
      treePath,
      summaryPath,
    };
  }
}
</file>

<file path="utils/doppler-resilience.example.js">
/**
 * Doppler Resilience Usage Example
 *
 * This example demonstrates how to integrate the DopplerResilience circuit breaker
 * into your application to handle Doppler API failures gracefully.
 */

import { DopplerResilience } from './doppler-resilience.js';
import { config } from '../core/config.js';
import { createComponentLogger } from './logger.js';

const logger = createComponentLogger('DopplerExample');

/**
 * Example 1: Basic Usage - Wrapping process.env access
 */
class DopplerConfigManager extends DopplerResilience {
  constructor(options = {}) {
    super({
      ...config.doppler,
      ...options
    });
  }

  /**
   * Override fetchFromDoppler to use process.env
   * (which is populated by `doppler run` command)
   */
  async fetchFromDoppler() {
    // In reality, when running with `doppler run`, process.env is already populated
    // This would just return the current environment
    // But we simulate checking if Doppler API is accessible by checking for sentinel value

    if (!process.env.NODE_ENV) {
      throw new Error('Doppler secrets not available - NODE_ENV missing');
    }

    // Return all environment variables
    return process.env;
  }

  /**
   * Get a specific secret with circuit breaker protection
   */
  async getSecret(key, defaultValue = null) {
    try {
      const secrets = await this.getSecrets();
      return secrets[key] ?? defaultValue;
    } catch (error) {
      logger.error({ key, error }, 'Failed to get secret');
      return defaultValue;
    }
  }
}

/**
 * Example 2: Using the circuit breaker in your application
 */
async function exampleUsage() {
  const dopplerConfig = new DopplerConfigManager();

  // Get secrets with circuit breaker protection
  try {
    const apiKey = await dopplerConfig.getSecret('API_KEY', 'fallback-key');
    const dbPassword = await dopplerConfig.getSecret('DB_PASSWORD');

    logger.info({ apiKey: '***', dbPassword: '***' }, 'Secrets loaded');
  } catch (error) {
    logger.error({ error }, 'Failed to load secrets');
  }

  // Check health status
  const health = dopplerConfig.getHealth();
  logger.info({ health }, 'Circuit breaker health');

  if (health.circuitState === 'OPEN') {
    logger.warn({
      waitTimeMs: health.waitTimeMs,
      failureCount: health.failureCount
    }, 'Circuit breaker is OPEN - using cached secrets');
  }
}

/**
 * Example 3: Express middleware for Doppler health monitoring
 */
export function createDopplerHealthMiddleware(dopplerConfig) {
  return async (req, res) => {
    const health = dopplerConfig.getHealth();

    const statusCode = health.healthy ? 200 : 503;
    const status = health.circuitState === 'CLOSED' ? 'healthy' : 'degraded';

    res.status(statusCode).json({
      status,
      circuitState: health.circuitState,
      healthy: health.healthy,
      usingFallback: health.usingFallback,
      metrics: {
        successRate: health.metrics.successRate,
        totalRequests: health.metrics.totalRequests,
        totalFailures: health.metrics.totalFailures
      },
      recovery: health.circuitState === 'OPEN' ? {
        waitTimeMs: health.waitTimeMs,
        nextAttemptTime: health.nextAttemptTime
      } : null,
      lastError: health.metrics.lastError
    });
  };
}

/**
 * Example 4: Integration with existing DopplerHealthMonitor
 */
export class IntegratedDopplerMonitor {
  constructor(options = {}) {
    this.resilience = new DopplerConfigManager(options);
  }

  /**
   * Get comprehensive health status
   */
  async getComprehensiveHealth() {
    const circuitHealth = this.resilience.getHealth();

    // Also check cache file age
    let cacheAge = null;
    if (circuitHealth.cacheLoadedAt) {
      cacheAge = Date.now() - new Date(circuitHealth.cacheLoadedAt).getTime();
    }

    return {
      // Circuit breaker status
      circuit: {
        state: circuitHealth.circuitState,
        healthy: circuitHealth.healthy,
        failureCount: circuitHealth.failureCount,
        successRate: circuitHealth.metrics.successRate
      },

      // Cache status
      cache: {
        usingFallback: circuitHealth.usingFallback,
        loadedAt: circuitHealth.cacheLoadedAt,
        ageMs: cacheAge,
        ageHours: cacheAge ? Math.floor(cacheAge / (60 * 60 * 1000)) : null
      },

      // Recovery information
      recovery: circuitHealth.circuitState === 'OPEN' ? {
        waitTimeMs: circuitHealth.waitTimeMs,
        nextAttemptTime: circuitHealth.nextAttemptTime,
        currentBackoffMs: circuitHealth.currentBackoffMs
      } : null,

      // Recommendations
      recommendations: this.generateRecommendations(circuitHealth, cacheAge)
    };
  }

  /**
   * Generate actionable recommendations based on health status
   */
  generateRecommendations(health, cacheAge) {
    const recommendations = [];

    if (health.circuitState === 'OPEN') {
      recommendations.push({
        severity: 'critical',
        message: `Circuit breaker is OPEN due to ${health.failureCount} consecutive failures`,
        action: 'Check Doppler API status at https://status.doppler.com'
      });

      recommendations.push({
        severity: 'info',
        message: `Circuit will attempt recovery in ${health.waitTimeMs}ms`,
        action: 'Wait for automatic recovery or manually reset if API is restored'
      });
    }

    if (health.usingFallback) {
      recommendations.push({
        severity: 'warning',
        message: 'Using cached secrets - secrets may be stale',
        action: 'Verify no critical secrets have been rotated'
      });
    }

    if (cacheAge && cacheAge > 24 * 60 * 60 * 1000) { // > 24 hours
      recommendations.push({
        severity: 'critical',
        message: `Cache is ${Math.floor(cacheAge / (60 * 60 * 1000))} hours old`,
        action: 'Restart application with `doppler run` to refresh secrets'
      });
    }

    if (health.metrics.successRate && parseFloat(health.metrics.successRate) < 50) {
      recommendations.push({
        severity: 'warning',
        message: `Low success rate: ${health.metrics.successRate}`,
        action: 'Investigate Doppler connectivity issues or increase failure threshold'
      });
    }

    return recommendations;
  }

  /**
   * Manual recovery trigger (for operational use)
   */
  async triggerRecovery() {
    logger.info('Manually triggering circuit breaker recovery');
    this.resilience.reset();

    // Attempt to fetch fresh secrets
    try {
      await this.resilience.getSecrets();
      logger.info('Circuit breaker recovered successfully');
      return { success: true, message: 'Circuit breaker reset and secrets refreshed' };
    } catch (error) {
      logger.error({ error }, 'Failed to recover circuit breaker');
      return { success: false, message: error.message };
    }
  }
}

/**
 * Example 5: Periodic health monitoring
 */
export class DopplerHealthService {
  constructor(options = {}) {
    this.monitor = new IntegratedDopplerMonitor(options);
    this.checkIntervalMs = options.checkIntervalMs || 60000; // 1 minute
    this.intervalId = null;
  }

  /**
   * Start periodic health checks
   */
  start() {
    if (this.intervalId) {
      logger.warn('Health monitoring already started');
      return;
    }

    logger.info({ checkIntervalMs: this.checkIntervalMs }, 'Starting Doppler health monitoring');

    this.intervalId = setInterval(async () => {
      const health = await this.monitor.getComprehensiveHealth();

      if (!health.circuit.healthy) {
        logger.warn({ health }, 'Doppler health check - circuit degraded');
      } else {
        logger.debug({ health }, 'Doppler health check - healthy');
      }

      // Alert on critical recommendations
      const critical = health.recommendations.filter(r => r.severity === 'critical');
      if (critical.length > 0) {
        logger.error({ recommendations: critical }, 'Critical Doppler issues detected');
      }
    }, this.checkIntervalMs);
  }

  /**
   * Stop periodic health checks
   */
  stop() {
    if (this.intervalId) {
      clearInterval(this.intervalId);
      this.intervalId = null;
      logger.info('Stopped Doppler health monitoring');
    }
  }

  /**
   * Get current health status
   */
  async getHealth() {
    return await this.monitor.getComprehensiveHealth();
  }
}

// Export example instances for testing
export {
  DopplerConfigManager,
  exampleUsage
};
</file>

<file path="utils/doppler-resilience.js">
/**
 * Doppler Resilience Module
 *
 * Implements circuit breaker pattern with exponential backoff to handle
 * Doppler API HTTP 500 errors gracefully.
 *
 * Features:
 * - Circuit breaker with 3 states: CLOSED, OPEN, HALF_OPEN
 * - Exponential backoff (1s, 2s, 4s, 8s, max 10s)
 * - Graceful fallback to cached secrets
 * - Health check endpoint integration
 * - Sentry error tracking
 *
 * Usage:
 *   import { DopplerResilience } from './sidequest/utils/doppler-resilience.js';
 *
 *   const doppler = new DopplerResilience();
 *   const secrets = await doppler.getSecrets();
 *
 *   // Get health status
 *   const health = doppler.getHealth();
 */

import { createComponentLogger } from './logger.js';
import Sentry from '@sentry/node';
import fs from 'fs/promises';
import path from 'path';
import os from 'os';

const logger = createComponentLogger('DopplerResilience');

// Circuit breaker states
const CircuitState = {
  CLOSED: 'CLOSED',     // Normal operation
  OPEN: 'OPEN',         // Failures detected, using fallback
  HALF_OPEN: 'HALF_OPEN' // Testing if service recovered
};

export class DopplerResilience {
  constructor(options = {}) {
    // Circuit breaker configuration
    this.failureThreshold = options.failureThreshold || 3; // Open circuit after N failures
    this.successThreshold = options.successThreshold || 2; // Close circuit after N successes in HALF_OPEN
    this.timeout = options.timeout || 5000; // 5s timeout before attempting HALF_OPEN
    this.maxBackoffMs = options.maxBackoffMs || 10000; // 10s max backoff

    // Exponential backoff configuration
    this.baseDelayMs = options.baseDelayMs || 1000; // Start with 1s
    this.backoffMultiplier = options.backoffMultiplier || 2; // Double each retry

    // State tracking
    this.state = CircuitState.CLOSED;
    this.failureCount = 0;
    this.successCount = 0;
    this.lastFailureTime = null;
    this.lastSuccessTime = null;
    this.consecutiveFailures = 0;
    this.nextAttemptTime = null;

    // Fallback cache configuration
    this.cacheFile = options.cacheFile || path.join(os.homedir(), '.doppler', '.fallback.json');
    this.cachedSecrets = null;
    this.cacheLoadedAt = null;

    // Metrics for health monitoring
    this.metrics = {
      totalRequests: 0,
      totalFailures: 0,
      totalSuccesses: 0,
      currentBackoffMs: 0,
      lastError: null
    };
  }

  /**
   * Get secrets with circuit breaker protection
   *
   * @returns {Promise<Object>} Secrets object
   * @throws {Error} If circuit is open and no fallback available
   */
  async getSecrets() {
    this.metrics.totalRequests++;

    // Check circuit state
    if (this.state === CircuitState.OPEN) {
      // Check if timeout has elapsed to attempt recovery
      if (Date.now() >= this.nextAttemptTime) {
        logger.info('Circuit breaker timeout elapsed, attempting HALF_OPEN state');
        this.state = CircuitState.HALF_OPEN;
        this.successCount = 0;
      } else {
        // Circuit still open, use fallback
        const waitMs = this.nextAttemptTime - Date.now();
        logger.warn({
          state: this.state,
          waitMs,
          failureCount: this.failureCount
        }, 'Circuit breaker OPEN, using cached secrets');

        return await this.getFallbackSecrets();
      }
    }

    try {
      // Attempt to fetch from Doppler API
      const secrets = await this.fetchFromDoppler();

      // Success handling
      this.handleSuccess();

      return secrets;
    } catch (error) {
      // Failure handling
      this.handleFailure(error);

      // Fallback to cached secrets
      return await this.getFallbackSecrets();
    }
  }

  /**
   * Fetch secrets from Doppler API (to be implemented by consumer)
   * This is a placeholder that should be overridden
   *
   * @returns {Promise<Object>} Secrets from Doppler
   * @throws {Error} If Doppler API fails
   */
  async fetchFromDoppler() {
    // This is a placeholder - in actual usage, this would call Doppler CLI
    // or use process.env which is populated by `doppler run`
    throw new Error('fetchFromDoppler must be implemented by consumer');
  }

  /**
   * Get fallback secrets from cache file
   *
   * @returns {Promise<Object>} Cached secrets
   * @throws {Error} If cache file doesn't exist
   */
  async getFallbackSecrets() {
    try {
      // Load from cache if not already loaded or cache is stale
      if (!this.cachedSecrets || this.isCacheStale()) {
        logger.info({ cacheFile: this.cacheFile }, 'Loading fallback secrets from cache');

        const cacheContent = await fs.readFile(this.cacheFile, 'utf-8');
        this.cachedSecrets = JSON.parse(cacheContent);
        this.cacheLoadedAt = Date.now();

        logger.info('Fallback secrets loaded successfully');
      }

      return this.cachedSecrets;
    } catch (error) {
      logger.error({ error, cacheFile: this.cacheFile }, 'Failed to load fallback secrets');

      Sentry.captureException(error, {
        tags: {
          component: 'doppler-resilience',
          operation: 'get-fallback-secrets'
        },
        extra: {
          cacheFile: this.cacheFile,
          circuitState: this.state
        }
      });

      throw new Error(`Doppler API unavailable and no fallback cache: ${error.message}`);
    }
  }

  /**
   * Check if cache is stale (older than 5 minutes)
   *
   * @returns {boolean} True if cache should be reloaded
   */
  isCacheStale() {
    if (!this.cacheLoadedAt) return true;

    const cacheAgeMs = Date.now() - this.cacheLoadedAt;
    const staleThresholdMs = 5 * 60 * 1000; // 5 minutes

    return cacheAgeMs > staleThresholdMs;
  }

  /**
   * Handle successful Doppler API call
   */
  handleSuccess() {
    this.lastSuccessTime = Date.now();
    this.consecutiveFailures = 0;
    this.metrics.totalSuccesses++;
    this.metrics.currentBackoffMs = 0;

    if (this.state === CircuitState.HALF_OPEN) {
      this.successCount++;

      logger.info({
        successCount: this.successCount,
        threshold: this.successThreshold
      }, 'Success in HALF_OPEN state');

      if (this.successCount >= this.successThreshold) {
        // Enough successes, close circuit
        this.state = CircuitState.CLOSED;
        this.failureCount = 0;
        this.successCount = 0;

        logger.info('Circuit breaker CLOSED - service recovered');

        Sentry.captureMessage('Doppler circuit breaker recovered', {
          level: 'info',
          tags: {
            component: 'doppler-resilience',
            circuitState: 'CLOSED'
          }
        });
      }
    } else if (this.state === CircuitState.CLOSED) {
      // Reset failure count on success in CLOSED state
      this.failureCount = 0;
    }
  }

  /**
   * Handle failed Doppler API call
   *
   * @param {Error} error - The error that occurred
   */
  handleFailure(error) {
    this.lastFailureTime = Date.now();
    this.failureCount++;
    this.consecutiveFailures++;
    this.metrics.totalFailures++;
    this.metrics.lastError = {
      message: error.message,
      timestamp: new Date().toISOString()
    };

    // Calculate exponential backoff
    const backoffMs = Math.min(
      this.baseDelayMs * Math.pow(this.backoffMultiplier, this.consecutiveFailures - 1),
      this.maxBackoffMs
    );
    this.metrics.currentBackoffMs = backoffMs;

    logger.warn({
      error: error.message,
      failureCount: this.failureCount,
      consecutiveFailures: this.consecutiveFailures,
      backoffMs,
      state: this.state
    }, 'Doppler API call failed');

    if (this.state === CircuitState.HALF_OPEN) {
      // Failure in HALF_OPEN, reopen circuit
      this.state = CircuitState.OPEN;
      this.nextAttemptTime = Date.now() + this.timeout;

      logger.warn({
        nextAttemptTime: new Date(this.nextAttemptTime).toISOString()
      }, 'Circuit breaker reopened due to failure in HALF_OPEN state');

      Sentry.captureMessage('Doppler circuit breaker reopened', {
        level: 'warning',
        tags: {
          component: 'doppler-resilience',
          circuitState: 'OPEN'
        },
        extra: {
          failureCount: this.failureCount,
          nextAttemptTime: new Date(this.nextAttemptTime).toISOString()
        }
      });
    } else if (this.state === CircuitState.CLOSED && this.failureCount >= this.failureThreshold) {
      // Too many failures, open circuit
      this.state = CircuitState.OPEN;
      this.nextAttemptTime = Date.now() + this.timeout;

      logger.error({
        failureCount: this.failureCount,
        threshold: this.failureThreshold,
        nextAttemptTime: new Date(this.nextAttemptTime).toISOString()
      }, 'Circuit breaker OPEN - failure threshold exceeded');

      Sentry.captureException(error, {
        level: 'error',
        tags: {
          component: 'doppler-resilience',
          circuitState: 'OPEN'
        },
        extra: {
          failureCount: this.failureCount,
          failureThreshold: this.failureThreshold,
          consecutiveFailures: this.consecutiveFailures,
          backoffMs
        }
      });
    }
  }

  /**
   * Get current health status for monitoring
   *
   * @returns {Object} Health status object
   */
  getHealth() {
    const now = Date.now();

    return {
      healthy: this.state === CircuitState.CLOSED,
      circuitState: this.state,
      failureCount: this.failureCount,
      consecutiveFailures: this.consecutiveFailures,
      successCount: this.successCount,
      lastFailureTime: this.lastFailureTime
        ? new Date(this.lastFailureTime).toISOString()
        : null,
      lastSuccessTime: this.lastSuccessTime
        ? new Date(this.lastSuccessTime).toISOString()
        : null,
      nextAttemptTime: this.nextAttemptTime
        ? new Date(this.nextAttemptTime).toISOString()
        : null,
      waitTimeMs: this.nextAttemptTime
        ? Math.max(0, this.nextAttemptTime - now)
        : 0,
      currentBackoffMs: this.metrics.currentBackoffMs,
      metrics: {
        totalRequests: this.metrics.totalRequests,
        totalFailures: this.metrics.totalFailures,
        totalSuccesses: this.metrics.totalSuccesses,
        successRate: this.metrics.totalRequests > 0
          ? (this.metrics.totalSuccesses / this.metrics.totalRequests * 100).toFixed(2) + '%'
          : 'N/A',
        lastError: this.metrics.lastError
      },
      usingFallback: this.state !== CircuitState.CLOSED,
      cacheLoadedAt: this.cacheLoadedAt
        ? new Date(this.cacheLoadedAt).toISOString()
        : null
    };
  }

  /**
   * Manually reset circuit breaker (for testing or manual intervention)
   */
  reset() {
    logger.info('Manually resetting circuit breaker');

    this.state = CircuitState.CLOSED;
    this.failureCount = 0;
    this.successCount = 0;
    this.consecutiveFailures = 0;
    this.lastFailureTime = null;
    this.lastSuccessTime = null;
    this.nextAttemptTime = null;
    this.metrics.currentBackoffMs = 0;
  }

  /**
   * Get circuit state (for testing)
   *
   * @returns {string} Current circuit state
   */
  getState() {
    return this.state;
  }
}

export default DopplerResilience;
</file>

<file path="utils/gitignore-repomix-updater.js">
import fs from 'fs/promises';
import path from 'path';
import os from 'os';
import { createComponentLogger } from './logger.js';

const logger = createComponentLogger('GitignoreRepomixUpdater');

/**
 * GitignoreRepomixUpdater - Adds repomix-output.xml to .gitignore in all git repositories
 */
export class GitignoreRepomixUpdater {
  constructor(options = {}) {
    this.baseDir = options.baseDir || path.join(os.homedir(), 'code');
    this.excludeDirs = new Set(options.excludeDirs || [
      'node_modules',
      '.git',
      'dist',
      'build',
      'coverage',
      '.next',
      '.nuxt',
      'vendor',
      '__pycache__',
      '.venv',
      'venv',
    ]);
    this.maxDepth = options.maxDepth || 10;
    this.dryRun = options.dryRun || false;
    this.gitignoreEntry = 'repomix-output.xml';
  }

  /**
   * Find all git repositories recursively
   */
  async findGitRepositories() {
    const repositories = [];
    await this.scanForGitRepos(this.baseDir, 0, repositories);
    return repositories;
  }

  /**
   * Recursively scan for git repositories
   */
  async scanForGitRepos(currentPath, depth, results) {
    // Check depth limit
    if (depth > this.maxDepth) {
      return;
    }

    try {
      const entries = await fs.readdir(currentPath, { withFileTypes: true });

      // Check if current directory is a git repository
      const hasGit = entries.some(entry => entry.name === '.git' && entry.isDirectory());

      if (hasGit) {
        results.push({
          fullPath: currentPath,
          depth,
        });
        // Don't scan subdirectories of a git repo for nested repos
        // (remove this return if you want to find nested repos)
        return;
      }

      // Scan subdirectories
      for (const entry of entries) {
        if (!entry.isDirectory()) continue;

        // Skip excluded directories
        if (this.excludeDirs.has(entry.name)) {
          continue;
        }

        // Skip hidden directories except .git
        if (entry.name.startsWith('.') && entry.name !== '.git') {
          continue;
        }

        const fullPath = path.join(currentPath, entry.name);
        await this.scanForGitRepos(fullPath, depth + 1, results);
      }
    } catch (error) {
      // Log but don't fail on permission errors
      logger.warn({ path: currentPath, error: error.message }, 'Cannot access directory');
    }
  }

  /**
   * Check if .gitignore already contains the entry
   */
  async gitignoreContainsEntry(gitignorePath) {
    try {
      const content = await fs.readFile(gitignorePath, 'utf8');
      const lines = content.split('\n').map(line => line.trim());

      // Check for exact match or pattern that would match
      return lines.some(line =>
        line === this.gitignoreEntry ||
        line === `/${this.gitignoreEntry}` ||
        line === `**/${this.gitignoreEntry}` ||
        line === `**/repomix-output.xml`
      );
    } catch (error) {
      if (error.code === 'ENOENT') {
        return false; // File doesn't exist
      }
      throw error;
    }
  }

  /**
   * Add entry to .gitignore file
   */
  async addToGitignore(repoPath) {
    const gitignorePath = path.join(repoPath, '.gitignore');

    try {
      // Check if entry already exists
      const alreadyExists = await this.gitignoreContainsEntry(gitignorePath);

      if (alreadyExists) {
        return {
          path: gitignorePath,
          action: 'skipped',
          reason: 'Entry already exists',
        };
      }

      if (this.dryRun) {
        return {
          path: gitignorePath,
          action: 'would_add',
          reason: 'Dry run mode',
        };
      }

      // Read existing content or start with empty string
      let content = '';
      try {
        content = await fs.readFile(gitignorePath, 'utf8');
      } catch (error) {
        if (error.code !== 'ENOENT') {
          throw error;
        }
      }

      // Ensure content ends with newline before adding new entry
      if (content.length > 0 && !content.endsWith('\n')) {
        content += '\n';
      }

      // Add comment and entry
      if (content.length > 0) {
        content += '\n';
      }
      content += '# Repomix output files\n';
      content += `${this.gitignoreEntry}\n`;

      // Write back to file
      await fs.writeFile(gitignorePath, content, 'utf8');

      return {
        path: gitignorePath,
        action: 'added',
        reason: 'Entry added successfully',
      };
    } catch (error) {
      return {
        path: gitignorePath,
        action: 'error',
        reason: error.message,
      };
    }
  }

  /**
   * Process all repositories
   */
  async processRepositories() {
    logger.info({
      baseDir: this.baseDir,
      dryRun: this.dryRun
    }, 'Scanning for git repositories');

    const repositories = await this.findGitRepositories();
    logger.info({ count: repositories.length }, 'Git repositories found');

    const results = [];

    for (const repo of repositories) {
      logger.info({ repository: repo.fullPath }, 'Processing repository');
      const result = await this.addToGitignore(repo.fullPath);
      results.push({
        repository: repo.fullPath,
        ...result,
      });
      logger.info({
        repository: repo.fullPath,
        action: result.action,
        reason: result.reason
      }, 'Repository processed');
    }

    return {
      totalRepositories: repositories.length,
      results,
      summary: this.generateSummary(results),
    };
  }

  /**
   * Generate summary statistics
   */
  generateSummary(results) {
    const summary = {
      added: 0,
      skipped: 0,
      would_add: 0,
      error: 0,
    };

    for (const result of results) {
      if (summary.hasOwnProperty(result.action)) {
        summary[result.action]++;
      }
    }

    return summary;
  }

  /**
   * Save results to JSON file
   */
  async saveResults(results, outputPath) {
    const report = {
      timestamp: new Date().toISOString(),
      baseDir: this.baseDir,
      dryRun: this.dryRun,
      gitignoreEntry: this.gitignoreEntry,
      ...results,
    };

    await fs.writeFile(outputPath, JSON.stringify(report, null, 2));
    logger.info({ outputPath }, 'Results saved');

    return report;
  }
}

/**
 * Main execution function
 */
export async function main() {
  const args = process.argv.slice(2);
  const dryRun = args.includes('--dry-run') || args.includes('-d');
  const baseDir = args.find(arg => !arg.startsWith('-')) || path.join(os.homedir(), 'code');

  const updater = new GitignoreRepomixUpdater({
    baseDir,
    dryRun,
  });

  try {
    const results = await updater.processRepositories();

    // Print summary
    logger.info({
      totalRepositories: results.totalRepositories,
      added: results.summary.added,
      skipped: results.summary.skipped,
      wouldAdd: results.summary.would_add,
      errors: results.summary.error
    }, 'Summary');

    // Save results
    const timestamp = Date.now();
    const outputPath = path.join(
      process.cwd(),
      `gitignore-update-report-${timestamp}.json`
    );
    await updater.saveResults(results, outputPath);

  } catch (error) {
    logger.error({ err: error }, 'Fatal error');
    process.exit(1);
  }
}

// Run if called directly
if (import.meta.url === `file://${process.argv[1]}`) {
  main();
}
</file>

<file path="utils/logger.d.ts">
/**
 * Type definitions for sidequest/logger.js
 */

import type { Logger } from 'pino';

/**
 * Main logger instance
 */
export const logger: Logger;

/**
 * Create a child logger with additional context
 */
export function createChildLogger(bindings: Record<string, any>): Logger;

/**
 * Create a logger for a specific component
 */
export function createComponentLogger(component: string): Logger;

export default logger;
</file>

<file path="utils/logger.js">
import pino from 'pino';
import { config } from '../core/config.js';

/**
 * Create a structured logger instance using Pino
 *
 * Logs are JSON-formatted for easy parsing and analysis.
 * Use pino-pretty in development for human-readable output.
 *
 * Usage:
 *   logger.info('Simple message');
 *   logger.info({ jobId: 'job-123', path: '/foo' }, 'Job started');
 *   logger.error({ err }, 'Operation failed');
 */
export const logger = pino({
  level: config.logLevel,

  // Base fields included in every log
  base: {
    pid: process.pid,
    hostname: undefined, // Exclude hostname for cleaner logs
  },

  // Timestamp format
  timestamp: pino.stdTimeFunctions.isoTime,

  // Error serialization
  serializers: {
    err: pino.stdSerializers.err,
    error: pino.stdSerializers.err,
  },

  // Pretty printing for development (disabled in production for performance)
  transport: config.nodeEnv !== 'production' ? {
    target: 'pino-pretty',
    options: {
      colorize: true,
      translateTime: 'SYS:standard',
      ignore: 'pid,hostname',
      singleLine: false,
      messageFormat: '{levelLabel} - {msg}',
    }
  } : undefined,
});

/**
 * Create a child logger with additional context
 *
 * @param {Object} bindings - Context to add to all logs from this child
 * @returns {pino.Logger} Child logger instance
 *
 * @example
 * const jobLogger = createChildLogger({ jobId: 'job-123' });
 * jobLogger.info('Job started'); // Automatically includes jobId in log
 */
export function createChildLogger(bindings) {
  return logger.child(bindings);
}

/**
 * Create a logger for a specific component
 *
 * @param {string} component - Component name (e.g., 'RepomixWorker', 'DirectoryScanner')
 * @returns {pino.Logger} Component logger instance
 *
 * @example
 * const workerLogger = createComponentLogger('RepomixWorker');
 * workerLogger.info('Worker initialized');
 */
export function createComponentLogger(component) {
  return logger.child({ component });
}

export default logger;
</file>

<file path="utils/pipeline-names.js">
/**
 * Pipeline ID to Display Name Mapping
 *
 * Centralized mapping for all pipeline identifiers to human-readable names.
 * Used by API endpoints and dashboard UI for consistent naming.
 *
 * @module sidequest/utils/pipeline-names
 */

/**
 * Pipeline display name mappings
 * @type {Object.<string, string>}
 */
export const PIPELINE_NAMES = {
  'duplicate-detection': 'Duplicate Detection',
  'repomix': 'Repomix Automation',
  'git-activity': 'Git Activity Reporter',
  'claude-health': 'Claude Health Monitor',
  'gitignore-manager': 'Gitignore Manager',
  'plugin-manager': 'Plugin Manager',
  'doc-enhancement': 'Doc Enhancement',
  'test-refactor': 'Test Refactor',
  'schema-enhancement': 'Schema Enhancement',
  'bugfix-audit': 'Bugfix Audit',
  'unknown': 'Unknown Pipeline'
};

/**
 * Get human-readable display name for a pipeline ID
 *
 * @param {string} id - Pipeline identifier (e.g., 'duplicate-detection')
 * @returns {string} Display name (e.g., 'Duplicate Detection')
 *
 * @example
 * getPipelineName('duplicate-detection')
 * // Returns: 'Duplicate Detection'
 *
 * getPipelineName('custom-pipeline')
 * // Returns: 'custom-pipeline' (fallback to ID)
 */
export function getPipelineName(id) {
  return PIPELINE_NAMES[id] || id;
}

/**
 * Get all known pipeline IDs
 *
 * @returns {string[]} Array of pipeline identifiers
 */
export function getAllKnownPipelineIds() {
  return Object.keys(PIPELINE_NAMES);
}

/**
 * Check if a pipeline ID is known
 *
 * @param {string} id - Pipeline identifier
 * @returns {boolean} True if pipeline is in the known list
 */
export function isKnownPipeline(id) {
  return id in PIPELINE_NAMES;
}
</file>

<file path="utils/plugin-manager.js">
// @ts-nocheck
/**
 * Plugin Management Worker - AlephAuto Integration
 *
 * Monitors and audits Claude Code plugin configurations.
 * Identifies duplicate plugins, unused plugins, and provides cleanup recommendations.
 *
 * @extends SidequestServer
 */

import { SidequestServer } from './server.js';
import { config } from '../core/config.js';
import { createComponentLogger } from './logger.js';
import { exec } from 'child_process';
import { promisify } from 'util';
import fs from 'fs/promises';
import path from 'path';

const execAsync = promisify(exec);
const logger = createComponentLogger('PluginManager');

class PluginManagerWorker extends SidequestServer {
  constructor(options = {}) {
    super({
      maxConcurrent: options.maxConcurrent ?? 1, // Single concurrent audit
      ...options
    });

    this.auditScriptPath = path.join(
      process.env.HOME,
      'code/jobs/sidequest/plugin-management-audit.sh'
    );
    this.configPath = path.join(process.env.HOME, '.claude/settings.json');
    this.thresholds = {
      maxPlugins: 30,
      warnPlugins: 20
    };

    logger.info('Plugin Manager Worker initialized', {
      auditScriptPath: this.auditScriptPath
    });
  }

  /**
   * Run plugin audit job
   * @param {Object} job - Job configuration
   * @param {boolean} job.detailed - Include detailed plugin listing
   * @param {boolean} job.autoCleanup - Attempt automatic cleanup (future)
   * @returns {Promise<Object>} Audit results
   */
  async runJobHandler(job) {
    const startTime = Date.now();
    logger.info('Starting plugin audit', { jobId: job.id, detailed: job.data?.detailed });

    try {
      // Run audit script
      const auditResults = await this.runAuditScript(job.data?.detailed || false);

      // Parse results
      const analysis = await this.analyzeResults(auditResults);

      // Generate recommendations
      const recommendations = this.generateRecommendations(analysis);

      const result = {
        success: true,
        timestamp: new Date().toISOString(),
        duration: Date.now() - startTime,
        ...analysis,
        recommendations
      };

      logger.info('Plugin audit completed', {
        jobId: job.id,
        totalPlugins: analysis.totalPlugins,
        duplicateCount: analysis.duplicateCategories?.length || 0,
        duration: result.duration
      });

      return result;
    } catch (error) {
      logger.error({ err: error, jobId: job.id }, 'Plugin audit failed');
      throw error;
    }
  }

  /**
   * Run the audit shell script
   * @param {boolean} detailed - Include detailed listing
   * @returns {Promise<Object>} Parsed JSON results
   */
  async runAuditScript(detailed = false) {
    try {
      const args = ['--json'];
      if (detailed) args.push('--detailed');

      // Use Homebrew bash if available
      const bashPath = '/opt/homebrew/bin/bash';
      const cmd = `${bashPath} ${this.auditScriptPath} ${args.join(' ')}`;

      const { stdout, stderr } = await execAsync(cmd);

      if (stderr) {
        logger.warn('Audit script warnings', { stderr });
      }

      return JSON.parse(stdout);
    } catch (error) {
      // Script exits with 1 if issues found, but still provides JSON
      if (error.stdout) {
        try {
          return JSON.parse(error.stdout);
        } catch (parseError) {
          logger.error({ err: parseError }, 'Failed to parse audit output');
          throw parseError;
        }
      }
      throw error;
    }
  }

  /**
   * Analyze audit results
   * @param {Object} auditResults - Raw audit results
   * @returns {Promise<Object>} Analysis
   */
  async analyzeResults(auditResults) {
    const { total_enabled, enabled_plugins, potential_duplicates } = auditResults;

    // Load plugin metadata if available
    const pluginMetadata = await this.loadPluginMetadata();

    // Identify duplicate categories
    const duplicateCategories = Object.entries(potential_duplicates || {}).map(
      ([category, plugins]) => ({
        category,
        plugins,
        count: plugins.length
      })
    );

    // Check thresholds
    const exceededThresholds = {
      maxPlugins: total_enabled > this.thresholds.maxPlugins,
      warnPlugins: total_enabled > this.thresholds.warnPlugins
    };

    return {
      totalPlugins: total_enabled,
      enabledPlugins: enabled_plugins,
      duplicateCategories,
      exceededThresholds,
      pluginMetadata
    };
  }

  /**
   * Load plugin metadata from Claude config
   * @returns {Promise<Object>} Plugin metadata
   */
  async loadPluginMetadata() {
    try {
      const configData = await fs.readFile(this.configPath, 'utf-8');
      const config = JSON.parse(configData);

      // Extract useful metadata if available
      return {
        enabledPlugins: config.enabledPlugins || {},
        pluginSettings: config.pluginSettings || {},
        lastModified: (await fs.stat(this.configPath)).mtime
      };
    } catch (error) {
      logger.warn({ err: error }, 'Failed to load plugin metadata');
      return {};
    }
  }

  /**
   * Generate cleanup recommendations
   * @param {Object} analysis - Analysis results
   * @returns {Array<Object>} Recommendations
   */
  generateRecommendations(analysis) {
    const recommendations = [];

    // High plugin count warning
    if (analysis.exceededThresholds.maxPlugins) {
      recommendations.push({
        priority: 'high',
        type: 'plugin_count',
        message: `You have ${analysis.totalPlugins} enabled plugins (threshold: ${this.thresholds.maxPlugins})`,
        action: 'Review and disable unused plugins to reduce overhead'
      });
    } else if (analysis.exceededThresholds.warnPlugins) {
      recommendations.push({
        priority: 'medium',
        type: 'plugin_count',
        message: `You have ${analysis.totalPlugins} enabled plugins (warning: ${this.thresholds.warnPlugins})`,
        action: 'Consider reviewing plugin usage'
      });
    }

    // Duplicate category recommendations
    if (analysis.duplicateCategories.length > 0) {
      recommendations.push({
        priority: 'medium',
        type: 'duplicate_categories',
        message: `Found ${analysis.duplicateCategories.length} categories with multiple plugins`,
        action: 'Review duplicate categories and consolidate',
        details: analysis.duplicateCategories.map(cat => ({
          category: cat.category,
          plugins: cat.plugins,
          suggestion: `Keep only the plugin you actively use in ${cat.category}`
        }))
      });
    }

    // Success message if no issues
    if (recommendations.length === 0) {
      recommendations.push({
        priority: 'info',
        type: 'healthy',
        message: `Plugin configuration looks healthy (${analysis.totalPlugins} plugins)`,
        action: 'No action needed'
      });
    }

    return recommendations;
  }

  /**
   * Create a plugin audit job
   * @param {Object} options - Job options
   * @returns {Object} Created job
   */
  addJob(options = {}) {
    const jobId = `plugin-audit-${Date.now()}`;
    return this.createJob(jobId, {
      detailed: options.detailed || false
    });
  }
}

// Export worker class
export { PluginManagerWorker };
</file>

<file path="utils/refactor-test-suite.ts">
#!/usr/bin/env npx ts-node
/**
 * Test Suite Refactoring Script
 *
 * Analyzes a test suite and generates modular utility files to reduce duplication.
 * Run with: npx ts-node refactor-test-suite.ts <project-path>
 *
 * This script:
 * 1. Scans test files for common patterns
 * 2. Generates utility modules for assertions, validators, constants, etc.
 * 3. Provides refactoring recommendations
 */

import * as fs from 'fs';
import * as path from 'path';
import { glob } from 'glob';

interface RefactorConfig {
  projectPath: string;
  testsDir: string;
  utilsDir: string;
  e2eDir: string;
  framework: 'vitest' | 'jest' | 'playwright';
}

interface AnalysisResult {
  testFiles: string[];
  patterns: {
    renderWaitFor: number;
    linkValidation: number;
    semanticChecks: number;
    formInteractions: number;
    hardcodedStrings: string[];
    duplicateAssertions: string[];
  };
  recommendations: string[];
}

// Default configuration
const defaultConfig: Partial<RefactorConfig> = {
  testsDir: 'tests',
  utilsDir: 'tests/utils',
  e2eDir: 'tests/e2e',
  framework: 'vitest'
};

/**
 * Detect test framework from package.json
 */
function detectFramework(projectPath: string): 'vitest' | 'jest' | 'playwright' {
  const packageJsonPath = path.join(projectPath, 'package.json');

  if (!fs.existsSync(packageJsonPath)) {
    return 'vitest';
  }

  const packageJson = JSON.parse(fs.readFileSync(packageJsonPath, 'utf-8'));
  const deps = { ...packageJson.dependencies, ...packageJson.devDependencies };

  if (deps['vitest']) return 'vitest';
  if (deps['@playwright/test']) return 'playwright';
  return 'jest';
}

/**
 * Find all test files in the project
 */
async function findTestFiles(config: RefactorConfig): Promise<string[]> {
  const patterns = [
    `${config.testsDir}/**/*.test.{ts,tsx,js,jsx}`,
    `${config.testsDir}/**/*.spec.{ts,tsx,js,jsx}`,
    `src/**/*.test.{ts,tsx,js,jsx}`,
    `src/**/*.spec.{ts,tsx,js,jsx}`
  ];

  const files: string[] = [];

  for (const pattern of patterns) {
    const matches = await glob(pattern, {
      cwd: config.projectPath,
      ignore: ['**/node_modules/**']
    });
    files.push(...matches);
  }

  return [...new Set(files)];
}

/**
 * Analyze test files for refactoring opportunities
 */
async function analyzeTestFiles(config: RefactorConfig, testFiles: string[]): Promise<AnalysisResult> {
  const result: AnalysisResult = {
    testFiles,
    patterns: {
      renderWaitFor: 0,
      linkValidation: 0,
      semanticChecks: 0,
      formInteractions: 0,
      hardcodedStrings: [],
      duplicateAssertions: []
    },
    recommendations: []
  };

  const stringCounts = new Map<string, number>();
  const assertionCounts = new Map<string, number>();

  for (const file of testFiles) {
    const filePath = path.join(config.projectPath, file);
    const content = fs.readFileSync(filePath, 'utf-8');

    // Count render + waitFor patterns
    const renderWaitMatches = content.match(/render\s*\([^)]+\);\s*await\s+waitFor/g);
    result.patterns.renderWaitFor += renderWaitMatches?.length || 0;

    // Count link validation patterns
    const linkMatches = content.match(/toHaveAttribute\s*\(\s*['"]href['"]/g);
    result.patterns.linkValidation += linkMatches?.length || 0;

    // Count semantic checks
    const semanticMatches = content.match(/getByRole\s*\(\s*['"]heading['"]|querySelector\s*\(\s*['"]section#/g);
    result.patterns.semanticChecks += semanticMatches?.length || 0;

    // Count form interactions
    const formMatches = content.match(/userEvent\.type|fireEvent\.click|getByRole\s*\(\s*['"]form['"]/g);
    result.patterns.formInteractions += formMatches?.length || 0;

    // Extract hardcoded strings (quoted strings in expect statements)
    const stringMatches = content.matchAll(/expect\s*\([^)]+\)\.toBeInTheDocument\(\)|getByText\s*\(\s*['"]([^'"]+)['"]\)/g);
    for (const match of stringMatches) {
      if (match[1]) {
        const count = stringCounts.get(match[1]) || 0;
        stringCounts.set(match[1], count + 1);
      }
    }

    // Extract assertion patterns
    const assertionMatches = content.matchAll(/(expect\([^)]+\)\.[^;]+);/g);
    for (const match of assertionMatches) {
      const assertion = match[1].replace(/['"][^'"]+['"]/g, '""');
      const count = assertionCounts.get(assertion) || 0;
      assertionCounts.set(assertion, count + 1);
    }
  }

  // Find duplicated strings (appearing 3+ times)
  for (const [str, count] of stringCounts) {
    if (count >= 3 && str.length > 5) {
      result.patterns.hardcodedStrings.push(str);
    }
  }

  // Find duplicated assertions (appearing 3+ times)
  for (const [assertion, count] of assertionCounts) {
    if (count >= 3) {
      result.patterns.duplicateAssertions.push(assertion);
    }
  }

  // Generate recommendations
  if (result.patterns.renderWaitFor > 5) {
    result.recommendations.push('Create renderAndWait helper to reduce render + waitFor boilerplate');
  }
  if (result.patterns.linkValidation > 5) {
    result.recommendations.push('Create link assertion helpers (expectExternalLink, expectInternalLink, etc.)');
  }
  if (result.patterns.semanticChecks > 5) {
    result.recommendations.push('Create semantic validators (expectSectionWithId, expectHeadingLevel, etc.)');
  }
  if (result.patterns.formInteractions > 5) {
    result.recommendations.push('Create form helpers (fillContactForm, expectFormAccessibility, etc.)');
  }
  if (result.patterns.hardcodedStrings.length > 10) {
    result.recommendations.push('Extract hardcoded strings to test-constants.ts');
  }
  if (result.patterns.duplicateAssertions.length > 5) {
    result.recommendations.push('Extract duplicate assertions to custom assertion helpers');
  }

  return result;
}

/**
 * Generate the assertions utility file
 */
function generateAssertionsFile(framework: string): string {
  const importStatement = framework === 'vitest'
    ? "import { expect } from 'vitest';"
    : "import { expect } from '@jest/globals';";

  return `/**
 * Test Assertions Utilities
 *
 * Common assertion helpers for link validation and DOM element checks.
 * Reduces duplication across component tests.
 */

${importStatement}

/**
 * Asserts that an element is an external link with proper security attributes
 */
export function expectExternalLink(element: HTMLElement, href: string) {
  const link = element.closest('a');
  expect(link).toHaveAttribute('href', href);
  expect(link).toHaveAttribute('target', '_blank');
  expect(link).toHaveAttribute('rel', 'noopener noreferrer');
}

/**
 * Asserts that an element is an internal link with correct href
 */
export function expectInternalLink(element: HTMLElement, href: string) {
  const link = element.closest('a');
  expect(link).toHaveAttribute('href', href);
}

/**
 * Asserts that an element is a mailto link with correct email
 */
export function expectMailtoLink(element: HTMLElement, email: string) {
  const link = element.closest('a');
  expect(link).toHaveAttribute('href', \`mailto:\${email}\`);
}

/**
 * Asserts that an element is a tel link with correct phone number
 */
export function expectTelLink(element: HTMLElement, phone: string) {
  const link = element.closest('a');
  expect(link).toHaveAttribute('href', \`tel:\${phone}\`);
}

/**
 * Asserts that all external links in a container have proper security attributes
 */
export function expectAllExternalLinksSecure(container: HTMLElement) {
  const links = container.querySelectorAll('a[target="_blank"]');
  links.forEach(link => {
    expect(link).toHaveAttribute('rel', 'noopener noreferrer');
    expect(link).toHaveAttribute('href');
  });
  return links.length;
}

/**
 * Asserts that a link points to a specific section anchor
 */
export function expectSectionLink(element: HTMLElement, sectionId: string) {
  expectInternalLink(element, \`#\${sectionId}\`);
}

/**
 * Asserts that an element has specific aria-label attribute
 */
export function expectAriaLabel(element: HTMLElement, label: string) {
  expect(element).toHaveAttribute('aria-label', label);
}

/**
 * Asserts that an element has aria-describedby attribute
 */
export function expectAriaDescribedBy(element: HTMLElement) {
  expect(element).toHaveAttribute('aria-describedby');
}

/**
 * Asserts that a button has correct type attribute
 */
export function expectButtonType(element: HTMLElement, type: 'submit' | 'button' | 'reset') {
  expect(element).toHaveAttribute('type', type);
}

/**
 * Asserts that an element is not disabled
 */
export function expectEnabled(element: HTMLElement) {
  expect(element).not.toBeDisabled();
}

/**
 * Asserts that an element is disabled
 */
export function expectDisabled(element: HTMLElement) {
  expect(element).toBeDisabled();
}
`;
}

/**
 * Generate the semantic validators utility file
 */
function generateSemanticValidatorsFile(framework: string): string {
  const importStatement = framework === 'vitest'
    ? "import { expect } from 'vitest';"
    : "import { expect } from '@jest/globals';";

  return `/**
 * Semantic Validation Utilities
 *
 * Helpers for validating semantic HTML structure in component tests.
 * Ensures accessibility and proper document outline.
 */

import { screen } from '@testing-library/react';
${importStatement}

/**
 * Validates that a section exists with the specified ID
 */
export function expectSectionWithId(id: string) {
  const section = document.querySelector(\`section#\${id}\`);
  expect(section).toBeInTheDocument();
  return section;
}

/**
 * Validates that a heading exists at the specified level with given name
 */
export function expectHeadingLevel(level: 1 | 2 | 3 | 4 | 5 | 6, name: string) {
  const heading = screen.getByRole('heading', { level, name });
  expect(heading).toBeInTheDocument();
  return heading;
}

/**
 * Validates the count of articles in the document
 */
export function expectArticleCount(count: number) {
  const articles = screen.getAllByRole('article');
  expect(articles).toHaveLength(count);
  return articles;
}

/**
 * Validates the count of list items (at minimum)
 */
export function expectListItemCount(minCount: number) {
  const listItems = screen.getAllByRole('listitem');
  expect(listItems.length).toBeGreaterThanOrEqual(minCount);
  return listItems;
}

/**
 * Validates that a navigation landmark exists
 */
export function expectNavigation(name?: string) {
  const nav = name
    ? screen.getByRole('navigation', { name })
    : screen.getByRole('navigation');
  expect(nav).toBeInTheDocument();
  return nav;
}

/**
 * Validates that a contentinfo (footer) landmark exists
 */
export function expectContentInfo() {
  const footer = screen.getByRole('contentinfo');
  expect(footer).toBeInTheDocument();
  return footer;
}

/**
 * Validates that a form exists with proper accessibility
 */
export function expectAccessibleForm() {
  const form = screen.getByRole('form');
  expect(form).toBeInTheDocument();
  expect(form).toHaveAttribute('noValidate');
  expect(form).toHaveAttribute('aria-describedby');
  return form;
}

/**
 * Validates that a specific number of headings exist at a level
 */
export function expectHeadingCount(level: 1 | 2 | 3 | 4 | 5 | 6, count: number) {
  const headings = screen.getAllByRole('heading', { level });
  expect(headings).toHaveLength(count);
  return headings;
}

/**
 * Validates that lists exist in the document (at minimum)
 */
export function expectListCount(minCount: number) {
  const lists = screen.getAllByRole('list');
  expect(lists.length).toBeGreaterThanOrEqual(minCount);
  return lists;
}

/**
 * Validates that an image has proper alt text
 */
export function expectImageWithAlt(altText: string) {
  const img = screen.getByAltText(altText);
  expect(img).toBeInTheDocument();
  return img;
}

/**
 * Validates that a button exists with specific name
 */
export function expectButton(name: string | RegExp) {
  const button = screen.getByRole('button', { name });
  expect(button).toBeInTheDocument();
  return button;
}

/**
 * Validates that a link exists with specific name
 */
export function expectLink(name: string | RegExp) {
  const link = screen.getByRole('link', { name });
  expect(link).toBeInTheDocument();
  return link;
}
`;
}

/**
 * Generate the form helpers utility file
 */
function generateFormHelpersFile(): string {
  return `/**
 * Form Testing Utilities
 *
 * Helpers for testing form interactions, validation, and submission.
 * Reduces duplication in form-heavy component tests.
 */

import { screen, within, waitFor } from '@testing-library/react';
import { expect } from 'vitest';
import type { UserEvent } from '@testing-library/user-event';

/**
 * Gets the form element from the document
 */
export function getForm() {
  return screen.getByRole('form');
}

/**
 * Gets all text inputs within a form
 */
export function getFormInputs(form: HTMLElement) {
  return within(form).getAllByRole('textbox');
}

/**
 * Gets the submit button within a form
 */
export function getSubmitButton(form: HTMLElement) {
  return within(form).getByRole('button', { name: /send|submit/i });
}

/**
 * Fills a contact form with provided data
 */
export async function fillContactForm(
  user: UserEvent,
  data: {
    name: string;
    email: string;
    organization?: string;
    message?: string;
  }
) {
  const form = getForm();
  const inputs = getFormInputs(form);

  await user.type(inputs[0], data.name);
  await user.type(inputs[1], data.email);

  if (data.organization && inputs[2]) {
    await user.type(inputs[2], data.organization);
  }

  if (data.message) {
    const messageField = inputs.find(input =>
      input.tagName === 'TEXTAREA' || input.getAttribute('rows')
    );
    if (messageField) {
      await user.type(messageField, data.message);
    }
  }

  return { form, inputs };
}

/**
 * Validates that form has proper accessibility attributes
 */
export function expectFormAccessibility(form: HTMLElement) {
  expect(form).toHaveAttribute('noValidate');
  expect(form).toHaveAttribute('aria-describedby');
}

/**
 * Validates that email input has correct attributes
 */
export function expectEmailInput(inputs: HTMLElement[]) {
  const emailInput = inputs.find(input =>
    input.getAttribute('type') === 'email'
  );
  expect(emailInput).toBeInTheDocument();

  if (emailInput) {
    expect(emailInput).toHaveAttribute('type', 'email');
    expect(emailInput).toHaveAttribute('autocomplete', 'email');
  }

  return emailInput;
}

/**
 * Validates that inputs have aria-describedby attributes
 */
export function expectInputsHaveDescriptions(inputs: HTMLElement[]) {
  const inputsWithDescriptions = inputs.filter(input =>
    input.hasAttribute('aria-describedby')
  );
  expect(inputsWithDescriptions.length).toBeGreaterThan(0);
  return inputsWithDescriptions;
}

/**
 * Validates form field values match expected data
 */
export function expectFormValues(
  inputs: HTMLElement[],
  expected: { name?: string; email?: string; organization?: string }
) {
  if (expected.name) {
    expect(inputs[0]).toHaveValue(expected.name);
  }
  if (expected.email) {
    expect(inputs[1]).toHaveValue(expected.email);
  }
  if (expected.organization && inputs[2]) {
    expect(inputs[2]).toHaveValue(expected.organization);
  }
}

/**
 * Helper to wait for form to be ready
 */
export async function waitForForm() {
  await waitFor(() => {
    expect(screen.getByRole('form')).toBeInTheDocument();
  });
  return getForm();
}

/**
 * Gets the minimum required number of text inputs for a form
 */
export function expectMinimumInputs(form: HTMLElement, minCount: number) {
  const inputs = getFormInputs(form);
  expect(inputs.length).toBeGreaterThanOrEqual(minCount);
  return inputs;
}
`;
}

/**
 * Generate the test constants file template
 */
function generateConstantsTemplate(hardcodedStrings: string[]): string {
  const uniqueStrings = [...new Set(hardcodedStrings)].slice(0, 20);

  return `/**
 * Test Constants
 *
 * Centralized content strings and expected data for tests.
 * Makes tests more maintainable when content changes.
 */

// TODO: Organize these strings into meaningful groups
export const EXTRACTED_STRINGS = [
${uniqueStrings.map(s => `  '${s.replace(/'/g, "\\'")}',`).join('\n')}
] as const;

// Example constant groups - customize for your project:

// Navigation links
export const NAV_LINKS = [
  { label: 'Home', href: '/' },
  { label: 'About', href: '#about' },
  // Add more...
] as const;

// Company/App info
export const APP_INFO = {
  name: 'Your App Name',
  email: 'contact@example.com',
  // Add more...
} as const;

// Form field names
export const FORM_FIELDS = {
  name: 'name',
  email: 'email',
  message: 'message',
} as const;

// Accessibility labels
export const A11Y_LABELS = {
  openMenu: 'Open menu',
  closeMenu: 'Close menu',
  // Add more...
} as const;
`;
}

/**
 * Generate the render helpers to add to test-utils
 */
function generateRenderHelpers(): string {
  return `
// ============================================================================
// Render Helpers - Add these to your existing test-utils file
// ============================================================================

/**
 * Renders a component and waits for assertions to pass
 * Reduces boilerplate in tests that follow render + waitFor pattern
 */
export async function renderAndWait(
  ui: React.ReactElement,
  assertions: () => void
): Promise<void> {
  const { waitFor } = await import('@testing-library/react');
  render(ui);
  await waitFor(assertions);
}

/**
 * Renders a component and waits for it to be visible
 * Useful for components that have async loading states
 */
export async function renderAndWaitForElement(
  ui: React.ReactElement,
  elementQuery: () => HTMLElement
): Promise<HTMLElement> {
  const { waitFor } = await import('@testing-library/react');
  render(ui);
  let element: HTMLElement;
  await waitFor(() => {
    element = elementQuery();
    expect(element).toBeInTheDocument();
  });
  return element!;
}
`;
}

/**
 * Generate E2E navigation fixtures for Playwright
 */
function generateE2EFixtures(): string {
  return `/**
 * E2E Navigation Fixtures
 *
 * Playwright helpers for navigation, viewport management, and common page interactions.
 * Reduces duplication across E2E test files.
 */

import { Page, expect } from '@playwright/test';

// ============================================================================
// Navigation Helpers
// ============================================================================

/**
 * Navigates to a section by clicking a navigation link
 */
export async function navigateToSection(page: Page, section: string) {
  await page.click(\`text=\${section}\`);
  await expect(page.url()).toContain(\`#\${section.toLowerCase()}\`);
}

/**
 * Navigates to homepage
 */
export async function goToHomepage(page: Page) {
  await page.goto('/');
}

/**
 * Waits for page to fully load
 */
export async function waitForPageLoad(page: Page) {
  await page.waitForLoadState('networkidle');
}

// ============================================================================
// Viewport Helpers
// ============================================================================

/**
 * Sets viewport to mobile dimensions
 */
export async function setMobileViewport(page: Page) {
  await page.setViewportSize({ width: 375, height: 667 });
}

/**
 * Sets viewport to tablet dimensions
 */
export async function setTabletViewport(page: Page) {
  await page.setViewportSize({ width: 768, height: 1024 });
}

/**
 * Sets viewport to desktop dimensions
 */
export async function setDesktopViewport(page: Page) {
  await page.setViewportSize({ width: 1280, height: 720 });
}

// ============================================================================
// Mobile Menu Helpers
// ============================================================================

/**
 * Opens the mobile navigation menu
 */
export async function openMobileMenu(page: Page) {
  await page.click('[aria-label="Open menu"]');
  await expect(page.locator('[aria-label="Close menu"]')).toBeVisible();
}

/**
 * Closes the mobile navigation menu
 */
export async function closeMobileMenu(page: Page) {
  await page.click('[aria-label="Close menu"]');
  await expect(page.locator('[aria-label="Open menu"]')).toBeVisible();
}

// ============================================================================
// Element Visibility Helpers
// ============================================================================

/**
 * Expects text to be visible on the page
 */
export async function expectTextVisible(page: Page, text: string) {
  await expect(page.locator(\`text=\${text}\`)).toBeVisible();
}

/**
 * Expects heading with specific text to be visible
 */
export async function expectHeadingVisible(page: Page, name: string) {
  await expect(page.getByRole('heading', { name })).toBeVisible();
}

/**
 * Expects multiple text elements to be visible
 */
export async function expectAllTextVisible(page: Page, texts: string[]) {
  for (const text of texts) {
    await expect(page.locator(\`text=\${text}\`)).toBeVisible();
  }
}

// ============================================================================
// External Link Helpers
// ============================================================================

/**
 * Clicks an external link and returns the popup page
 */
export async function clickExternalLink(page: Page, linkText: string) {
  const [popup] = await Promise.all([
    page.waitForEvent('popup'),
    page.click(\`text=\${linkText}\`)
  ]);
  return popup;
}
`;
}

/**
 * Generate index file for clean imports
 */
function generateIndexFile(): string {
  return `/**
 * Test Utilities Index
 *
 * Central export for all test utilities.
 * Import from this file for cleaner imports.
 */

// Core test utilities and render functions
export * from './test-utils';

// Assertion helpers
export * from './assertions';

// Semantic validation helpers
export * from './semantic-validators';

// Form testing helpers
export * from './form-helpers';

// Test constants and expected data
export * from './test-constants';
`;
}

/**
 * Main execution
 */
async function main() {
  const projectPath = process.argv[2] || process.cwd();

  console.log('üîç Test Suite Refactoring Script');
  console.log('================================\n');
  console.log(`Project: ${projectPath}\n`);

  // Build configuration
  const config: RefactorConfig = {
    ...defaultConfig as RefactorConfig,
    projectPath,
    framework: detectFramework(projectPath)
  };

  console.log(`Detected framework: ${config.framework}`);

  // Find test files
  const testFiles = await findTestFiles(config);
  console.log(`Found ${testFiles.length} test files\n`);

  if (testFiles.length === 0) {
    console.log('No test files found. Please check your project structure.');
    process.exit(1);
  }

  // Analyze test files
  console.log('Analyzing test files for patterns...\n');
  const analysis = await analyzeTestFiles(config, testFiles);

  // Print analysis results
  console.log('Analysis Results:');
  console.log('‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ');
  console.log(`  render + waitFor patterns: ${analysis.patterns.renderWaitFor}`);
  console.log(`  Link validation patterns: ${analysis.patterns.linkValidation}`);
  console.log(`  Semantic checks: ${analysis.patterns.semanticChecks}`);
  console.log(`  Form interactions: ${analysis.patterns.formInteractions}`);
  console.log(`  Hardcoded strings (3+ occurrences): ${analysis.patterns.hardcodedStrings.length}`);
  console.log(`  Duplicate assertions: ${analysis.patterns.duplicateAssertions.length}`);
  console.log();

  // Print recommendations
  if (analysis.recommendations.length > 0) {
    console.log('Recommendations:');
    console.log('‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ');
    analysis.recommendations.forEach((rec, i) => {
      console.log(`  ${i + 1}. ${rec}`);
    });
    console.log();
  }

  // Create utils directory
  const utilsPath = path.join(projectPath, config.utilsDir);
  if (!fs.existsSync(utilsPath)) {
    fs.mkdirSync(utilsPath, { recursive: true });
    console.log(`Created directory: ${config.utilsDir}`);
  }

  // Generate utility files
  const filesToGenerate = [
    { name: 'assertions.ts', content: generateAssertionsFile(config.framework) },
    { name: 'semantic-validators.ts', content: generateSemanticValidatorsFile(config.framework) },
    { name: 'form-helpers.ts', content: generateFormHelpersFile() },
    { name: 'test-constants.ts', content: generateConstantsTemplate(analysis.patterns.hardcodedStrings) },
    { name: 'index.ts', content: generateIndexFile() },
  ];

  console.log('\nGenerating utility files:');
  console.log('‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ');

  for (const file of filesToGenerate) {
    const filePath = path.join(utilsPath, file.name);
    if (!fs.existsSync(filePath)) {
      fs.writeFileSync(filePath, file.content);
      console.log(`  ‚úì Created ${config.utilsDir}/${file.name}`);
    } else {
      console.log(`  ‚äò Skipped ${config.utilsDir}/${file.name} (already exists)`);
    }
  }

  // Generate render helpers (to be added to existing test-utils)
  const renderHelpersPath = path.join(utilsPath, 'render-helpers.ts');
  fs.writeFileSync(renderHelpersPath, generateRenderHelpers());
  console.log(`  ‚úì Created ${config.utilsDir}/render-helpers.ts (add to test-utils.tsx)`);

  // Generate E2E fixtures if e2e directory exists
  const e2ePath = path.join(projectPath, config.e2eDir);
  if (fs.existsSync(e2ePath)) {
    const fixturesPath = path.join(e2ePath, 'fixtures');
    if (!fs.existsSync(fixturesPath)) {
      fs.mkdirSync(fixturesPath, { recursive: true });
    }
    const navFixturesPath = path.join(fixturesPath, 'navigation.ts');
    if (!fs.existsSync(navFixturesPath)) {
      fs.writeFileSync(navFixturesPath, generateE2EFixtures());
      console.log(`  ‚úì Created ${config.e2eDir}/fixtures/navigation.ts`);
    }
  }

  console.log('\n‚úÖ Refactoring complete!');
  console.log('\nNext steps:');
  console.log('  1. Review generated files and customize for your project');
  console.log('  2. Add render helpers to your existing test-utils.tsx');
  console.log('  3. Update your test files to import from the new utilities');
  console.log('  4. Run your test suite to verify everything works');
}

main().catch(console.error);
</file>

<file path="utils/report-generator.js">
/**
 * Generic Report Generator
 *
 * Generates HTML and JSON reports for all pipeline types.
 * Provides a consistent reporting interface across all AlephAuto workers.
 *
 * Usage:
 *   const { generateReport } = await import('./report-generator.js');
 *   const reportPaths = await generateReport({
 *     jobId: 'job-123',
 *     jobType: 'claude-health',
 *     status: 'completed',
 *     result: { ... },
 *     startTime: Date.now(),
 *     endTime: Date.now(),
 *     parameters: { ... }
 *   });
 */

import fs from 'fs/promises';
import path from 'path';
import { createComponentLogger } from './logger.js';

const logger = createComponentLogger('ReportGenerator');

/**
 * Default output directory for reports
 */
const DEFAULT_OUTPUT_DIR = path.join(process.cwd(), 'output/reports');

/**
 * Generate reports for a completed job
 *
 * @param {Object} options - Report generation options
 * @param {string} options.jobId - Job identifier
 * @param {string} options.jobType - Type of job (e.g., 'claude-health', 'git-activity')
 * @param {string} options.status - Job status ('completed', 'failed', etc.)
 * @param {Object} options.result - Job result data
 * @param {number} options.startTime - Job start timestamp
 * @param {number} options.endTime - Job end timestamp
 * @param {Object} [options.parameters] - Job parameters/configuration
 * @param {Object} [options.metadata] - Additional metadata
 * @param {string} [options.outputDir] - Custom output directory
 * @returns {Promise<Object>} - Report paths { html, json, timestamp }
 */
export async function generateReport(options) {
  const {
    jobId,
    jobType,
    status,
    result,
    startTime,
    endTime,
    parameters = {},
    metadata = {},
    outputDir = DEFAULT_OUTPUT_DIR
  } = options;

  // Validate required fields
  if (!jobId || !jobType || !status) {
    throw new Error('Missing required fields: jobId, jobType, status');
  }

  const timestamp = new Date().toISOString().replace(/[:.]/g, '-').slice(0, -5);
  const baseName = `${jobType}-${timestamp}`;

  // Ensure output directory exists
  await fs.mkdir(outputDir, { recursive: true });

  const reportPaths = {
    timestamp: new Date().toISOString()
  };

  try {
    // Generate HTML report
    const htmlPath = path.join(outputDir, `${baseName}.html`);
    const htmlContent = generateHTMLReport({
      jobId,
      jobType,
      status,
      result,
      startTime,
      endTime,
      parameters,
      metadata,
      timestamp
    });
    await fs.writeFile(htmlPath, htmlContent);
    reportPaths.html = htmlPath;
    logger.info({ path: htmlPath }, 'HTML report generated');

    // Generate JSON report
    const jsonPath = path.join(outputDir, `${baseName}.json`);
    const jsonContent = generateJSONReport({
      jobId,
      jobType,
      status,
      result,
      startTime,
      endTime,
      parameters,
      metadata,
      timestamp
    });
    await fs.writeFile(jsonPath, JSON.stringify(jsonContent, null, 2));
    reportPaths.json = jsonPath;
    logger.info({ path: jsonPath }, 'JSON report generated');

    return reportPaths;

  } catch (error) {
    logger.error({ error }, 'Report generation failed');
    throw new Error(`Report generation failed: ${error.message}`);
  }
}

/**
 * Generate HTML report content
 *
 * @private
 */
function generateHTMLReport(data) {
  const {
    jobId,
    jobType,
    status,
    result,
    startTime,
    endTime,
    parameters,
    metadata,
    timestamp
  } = data;

  const duration = endTime && startTime ? ((endTime - startTime) / 1000).toFixed(2) : 'N/A';
  const title = getJobTypeTitle(jobType);
  const statusClass = status === 'completed' ? 'success' : status === 'failed' ? 'error' : 'warning';

  return `<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>${escapeHtml(title)} - ${escapeHtml(jobId)}</title>
    <style>
        ${getHTMLStyles()}
    </style>
</head>
<body>
    <div class="container">
        ${generateHTMLHeader(title, jobId, status, statusClass, timestamp, duration)}
        ${generateHTMLParameters(parameters)}
        ${generateHTMLMetadata(metadata)}
        ${generateHTMLResults(result, jobType)}
        ${generateHTMLFooter(timestamp)}
    </div>
</body>
</html>`;
}

/**
 * Generate JSON report content
 *
 * @private
 */
function generateJSONReport(data) {
  const {
    jobId,
    jobType,
    status,
    result,
    startTime,
    endTime,
    parameters,
    metadata,
    timestamp
  } = data;

  return {
    report_version: '1.0.0',
    generated_at: timestamp,
    job: {
      id: jobId,
      type: jobType,
      status,
      duration_seconds: endTime && startTime ? (endTime - startTime) / 1000 : null,
      started_at: startTime ? new Date(startTime).toISOString() : null,
      completed_at: endTime ? new Date(endTime).toISOString() : null
    },
    parameters,
    metadata,
    result
  };
}

/**
 * Generate HTML header section
 *
 * @private
 */
function generateHTMLHeader(title, jobId, status, statusClass, timestamp, duration) {
  return `
    <header>
        <h1>üìã ${escapeHtml(title)}</h1>
        <div class="header-meta">
            <span class="meta-item">
                <strong>Job ID:</strong> ${escapeHtml(jobId)}
            </span>
            <span class="meta-item status-${statusClass}">
                <strong>Status:</strong> ${escapeHtml(status)}
            </span>
            <span class="meta-item">
                <strong>Generated:</strong> ${new Date(timestamp).toLocaleString()}
            </span>
            <span class="meta-item">
                <strong>Duration:</strong> ${duration}s
            </span>
        </div>
    </header>`;
}

/**
 * Generate HTML parameters section
 *
 * @private
 */
function generateHTMLParameters(parameters) {
  if (!parameters || Object.keys(parameters).length === 0) {
    return '';
  }

  const paramItems = Object.entries(parameters)
    .map(([key, value]) => `
      <div class="param-item">
        <span class="param-key">${escapeHtml(key)}:</span>
        <span class="param-value">${escapeHtml(formatValue(value))}</span>
      </div>
    `)
    .join('');

  return `
    <section class="parameters">
        <h2>‚öôÔ∏è Parameters</h2>
        <div class="params-grid">
            ${paramItems}
        </div>
    </section>`;
}

/**
 * Generate HTML metadata section
 *
 * @private
 */
function generateHTMLMetadata(metadata) {
  if (!metadata || Object.keys(metadata).length === 0) {
    return '';
  }

  const metaItems = Object.entries(metadata)
    .map(([key, value]) => `
      <div class="meta-item-detail">
        <span class="meta-key">${escapeHtml(key)}:</span>
        <span class="meta-value">${escapeHtml(formatValue(value))}</span>
      </div>
    `)
    .join('');

  return `
    <section class="metadata">
        <h2>‚ÑπÔ∏è Metadata</h2>
        <div class="meta-grid">
            ${metaItems}
        </div>
    </section>`;
}

/**
 * Generate HTML results section
 *
 * @private
 */
function generateHTMLResults(result, jobType) {
  if (!result) {
    return `
    <section class="results">
        <h2>üìä Results</h2>
        <p class="empty-state">No results available</p>
    </section>`;
  }

  // Try to detect metrics/stats in result
  const metrics = extractMetrics(result);
  const details = extractDetails(result);

  return `
    <section class="results">
        <h2>üìä Results</h2>
        ${metrics ? generateMetricsSection(metrics) : ''}
        ${details ? generateDetailsSection(details, jobType) : ''}
    </section>`;
}

/**
 * Generate metrics cards
 *
 * @private
 */
function generateMetricsSection(metrics) {
  const metricCards = Object.entries(metrics)
    .map(([key, value]) => {
      const label = key.replace(/_/g, ' ').replace(/\b\w/g, l => l.toUpperCase());
      return `
        <div class="metric-card">
            <div class="metric-value">${escapeHtml(String(value))}</div>
            <div class="metric-label">${escapeHtml(label)}</div>
        </div>
      `;
    })
    .join('');

  return `
    <div class="metrics-section">
        <h3>Key Metrics</h3>
        <div class="metrics-grid">
            ${metricCards}
        </div>
    </div>`;
}

/**
 * Generate details section
 *
 * @private
 */
function generateDetailsSection(details, jobType) {
  return `
    <div class="details-section">
        <h3>Details</h3>
        <pre class="details-content">${escapeHtml(JSON.stringify(details, null, 2))}</pre>
    </div>`;
}

/**
 * Generate HTML footer
 *
 * @private
 */
function generateHTMLFooter(timestamp) {
  return `
    <footer>
        <p>Generated by AlephAuto Pipeline Framework | ${new Date(timestamp).toLocaleString()}</p>
    </footer>`;
}

/**
 * Extract metrics from result object
 *
 * @private
 */
function extractMetrics(result) {
  // Look for common metric field names
  const metricKeys = [
    'metrics', 'stats', 'statistics', 'summary',
    'totalFiles', 'totalItems', 'totalRepositories',
    'enhanced', 'skipped', 'failed', 'success',
    'totalCommits', 'linesAdded', 'linesDeleted',
    'healthScore', 'issueCount', 'warningCount'
  ];

  const metrics = {};

  for (const key of metricKeys) {
    if (result[key] !== undefined) {
      if (typeof result[key] === 'object' && !Array.isArray(result[key])) {
        // If it's an object, merge its properties
        Object.assign(metrics, result[key]);
      } else {
        metrics[key] = result[key];
      }
    }
  }

  return Object.keys(metrics).length > 0 ? metrics : null;
}

/**
 * Extract details from result object (non-metrics)
 *
 * @private
 */
function extractDetails(result) {
  const metricKeys = [
    'metrics', 'stats', 'statistics', 'summary',
    'totalFiles', 'totalItems', 'totalRepositories',
    'enhanced', 'skipped', 'failed', 'success',
    'totalCommits', 'linesAdded', 'linesDeleted',
    'healthScore', 'issueCount', 'warningCount'
  ];

  const details = {};

  for (const [key, value] of Object.entries(result)) {
    if (!metricKeys.includes(key)) {
      details[key] = value;
    }
  }

  return Object.keys(details).length > 0 ? details : null;
}

/**
 * Get human-readable title for job type
 *
 * @private
 */
function getJobTypeTitle(jobType) {
  const titles = {
    'claude-health': 'Claude Health Check Report',
    'git-activity': 'Git Activity Report',
    'gitignore-update': 'Gitignore Update Report',
    'repo-cleanup': 'Repository Cleanup Report',
    'repomix': 'Repomix Report',
    'schema-enhancement': 'Schema Enhancement Report',
    'duplicate-detection': 'Duplicate Detection Report',
    'test-refactor': 'Test Refactor Report'
  };

  return titles[jobType] || `${jobType} Report`;
}

/**
 * Format value for display
 *
 * @private
 */
function formatValue(value) {
  if (value === null || value === undefined) {
    return 'N/A';
  }
  if (typeof value === 'boolean') {
    return value ? 'Yes' : 'No';
  }
  if (typeof value === 'object') {
    if (Array.isArray(value)) {
      return value.length > 0 ? value.join(', ') : 'None';
    }
    return JSON.stringify(value);
  }
  return String(value);
}

/**
 * Escape HTML special characters
 *
 * @private
 */
function escapeHtml(text) {
  if (!text) return '';
  return String(text)
    .replace(/&/g, '&amp;')
    .replace(/</g, '&lt;')
    .replace(/>/g, '&gt;')
    .replace(/"/g, '&quot;')
    .replace(/'/g, '&#039;');
}

/**
 * Get HTML styles
 *
 * @private
 */
function getHTMLStyles() {
  return `
    * { margin: 0; padding: 0; box-sizing: border-box; }
    body {
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
        line-height: 1.6;
        color: #333;
        background: #f5f7fa;
    }
    .container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 20px;
    }
    header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 30px;
        border-radius: 10px;
        margin-bottom: 30px;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }
    header h1 { margin-bottom: 15px; font-size: 2em; }
    .header-meta {
        display: flex;
        gap: 20px;
        flex-wrap: wrap;
    }
    .meta-item {
        background: rgba(255,255,255,0.2);
        padding: 8px 15px;
        border-radius: 5px;
    }
    .status-success { background: rgba(72, 187, 120, 0.3) !important; }
    .status-error { background: rgba(245, 101, 101, 0.3) !important; }
    .status-warning { background: rgba(237, 137, 54, 0.3) !important; }
    section {
        background: white;
        padding: 25px;
        margin-bottom: 20px;
        border-radius: 10px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
    }
    section h2 {
        margin-bottom: 20px;
        color: #2d3748;
        border-bottom: 2px solid #e2e8f0;
        padding-bottom: 10px;
    }
    section h3 {
        margin: 20px 0 15px 0;
        color: #4a5568;
        font-size: 1.2em;
    }
    .params-grid, .meta-grid {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
        gap: 15px;
    }
    .param-item, .meta-item-detail {
        padding: 12px;
        background: #f7fafc;
        border-radius: 6px;
        border: 1px solid #e2e8f0;
    }
    .param-key, .meta-key {
        font-weight: 600;
        color: #4a5568;
        margin-right: 8px;
    }
    .param-value, .meta-value {
        color: #2d3748;
    }
    .metrics-grid {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
        gap: 15px;
        margin-top: 15px;
    }
    .metric-card {
        padding: 20px;
        text-align: center;
        border-radius: 8px;
        background: #f7fafc;
        border: 2px solid #e2e8f0;
    }
    .metric-value {
        font-size: 2.5em;
        font-weight: bold;
        color: #2d3748;
    }
    .metric-label {
        color: #718096;
        margin-top: 5px;
        font-size: 0.9em;
    }
    .details-content {
        background: #2d3748;
        color: #e2e8f0;
        padding: 20px;
        border-radius: 6px;
        overflow-x: auto;
        font-family: 'Courier New', monospace;
        font-size: 0.9em;
        line-height: 1.5;
    }
    .empty-state {
        text-align: center;
        color: #a0aec0;
        padding: 40px;
        font-style: italic;
    }
    footer {
        text-align: center;
        color: #a0aec0;
        padding: 30px;
        font-size: 0.9em;
    }
  `;
}
</file>

<file path="utils/schema-mcp-tools.js">
import { config } from '../core/config.js';

/**
 * Schema.org MCP Tools Integration
 * Provides wrapper methods for Schema.org MCP server tools
 */

export class SchemaMCPTools {
  constructor(options = {}) {
    this.mcpServerUrl = options.mcpServerUrl || config.schemaMcpUrl;
    this.useRealMCP = options.useRealMCP || false;
  }

  /**
   * Get appropriate schema type for content
   * Maps to MCP tool: get_schema_type
   */
  async getSchemaType(readmePath, content, context) {
    // In real implementation, this would call the MCP server
    // For now, we'll use heuristics like the Python version

    const pathLower = readmePath.toLowerCase();
    const contentLower = content.toLowerCase();

    // Test documentation
    if (pathLower.includes('test') || contentLower.includes('testing guide')) {
      return 'HowTo';
    }

    // API documentation
    if (pathLower.includes('api') ||
        contentLower.includes('api reference') ||
        contentLower.includes('endpoints')) {
      return 'APIReference';
    }

    // Software application
    if (context.hasPackageJson || context.hasPyproject) {
      return 'SoftwareApplication';
    }

    // Tutorial/Guide
    if (contentLower.includes('tutorial') ||
        contentLower.includes('getting started') ||
        contentLower.includes('guide')) {
      return 'HowTo';
    }

    // Code repository/technical documentation
    if (context.gitRemote) {
      return 'SoftwareSourceCode';
    }

    // Default to TechArticle
    return 'TechArticle';
  }

  /**
   * Generate JSON-LD schema markup
   * Maps to MCP tool: generate_example
   */
  async generateSchema(readmePath, content, context, schemaType) {
    const schema = {
      '@context': 'https://schema.org',
      '@type': schemaType,
    };

    // Extract title from first heading
    const titleMatch = content.match(/^#\s+(.+)$/m);
    if (titleMatch) {
      schema.name = titleMatch[1].trim();
    } else {
      // Fallback to directory name
      const dirName = readmePath.split('/').slice(-2, -1)[0];
      schema.name = dirName || 'Documentation';
    }

    // Extract description from content
    const description = this.extractDescription(content);
    if (description) {
      schema.description = description;
    }

    // Add common properties based on schema type
    if (schemaType === 'SoftwareApplication' || schemaType === 'SoftwareSourceCode') {
      if (context.gitRemote) {
        schema.codeRepository = context.gitRemote;
      }

      if (context.languages && context.languages.length > 0) {
        schema.programmingLanguage = context.languages.map(lang => ({
          '@type': 'ComputerLanguage',
          name: lang,
        }));
      }

      if (schemaType === 'SoftwareApplication') {
        schema.applicationCategory = 'DeveloperApplication';
        schema.operatingSystem = 'Cross-platform';
      }
    }

    if (schemaType === 'TechArticle' || schemaType === 'HowTo') {
      schema.dateModified = new Date().toISOString();
      schema.inLanguage = 'en-US';
    }

    if (schemaType === 'APIReference') {
      schema.additionalType = 'https://schema.org/TechArticle';
      if (context.gitRemote) {
        schema.url = context.gitRemote;
      }
    }

    return schema;
  }

  /**
   * Extract description from README content
   */
  extractDescription(content) {
    // Try to get the first paragraph after the title
    const lines = content.split('\n');
    let foundTitle = false;
    let description = '';

    for (const line of lines) {
      const trimmed = line.trim();

      // Skip title
      if (trimmed.startsWith('#')) {
        foundTitle = true;
        continue;
      }

      // Skip empty lines and code blocks
      if (!trimmed || trimmed.startsWith('```') || trimmed.startsWith('<')) {
        if (description) break; // Stop at first empty line after description
        continue;
      }

      // Found description
      if (foundTitle && trimmed.length > 10) {
        description = trimmed;
        break;
      }
    }

    // Limit description length
    if (description.length > 200) {
      description = description.substring(0, 197) + '...';
    }

    return description || 'Technical documentation and guides';
  }

  /**
   * Validate schema markup
   * Maps to Schema.org validation tools
   */
  async validateSchema(schema) {
    // Basic validation
    const errors = [];
    const warnings = [];

    // Check required fields
    if (!schema['@context']) {
      errors.push('Missing @context');
    }
    if (!schema['@type']) {
      errors.push('Missing @type');
    }
    if (!schema.name) {
      warnings.push('Missing name property');
    }
    if (!schema.description) {
      warnings.push('Missing description property');
    }

    // Validate JSON-LD format
    try {
      JSON.stringify(schema);
    } catch (e) {
      errors.push(`Invalid JSON: ${e.message}`);
    }

    return {
      valid: errors.length === 0,
      errors,
      warnings,
    };
  }

  /**
   * Analyze schema impact on SEO/performance
   * Maps to MCP tool: analyze_schema_impact
   */
  async analyzeSchemaImpact(originalContent, enhancedContent, schema) {
    const impact = {
      timestamp: new Date().toISOString(),
      schemaType: schema['@type'],
      metrics: {
        contentSize: {
          original: originalContent.length,
          enhanced: enhancedContent.length,
          increase: enhancedContent.length - originalContent.length,
        },
        schemaProperties: Object.keys(schema).length,
        structuredDataAdded: true,
      },
      seoImprovements: [],
      richResultsEligibility: [],
    };

    // Analyze SEO improvements
    if (schema.name) {
      impact.seoImprovements.push('Added structured name/title');
    }
    if (schema.description) {
      impact.seoImprovements.push('Added structured description');
    }
    if (schema.codeRepository) {
      impact.seoImprovements.push('Linked to code repository');
    }
    if (schema.programmingLanguage) {
      impact.seoImprovements.push('Specified programming languages');
    }

    // Check Rich Results eligibility
    const schemaType = schema['@type'];
    if (schemaType === 'HowTo') {
      impact.richResultsEligibility.push('How-to rich results');
    }
    if (schemaType === 'SoftwareApplication') {
      impact.richResultsEligibility.push('Software app rich results');
    }
    if (schemaType === 'TechArticle') {
      impact.richResultsEligibility.push('Article rich results');
    }

    // Calculate impact score (0-100)
    let score = 0;
    score += impact.seoImprovements.length * 15;
    score += impact.richResultsEligibility.length * 20;
    score += schema.description ? 20 : 0;
    score += schema.codeRepository ? 15 : 0;

    impact.impactScore = Math.min(100, score);
    impact.rating = this.getRating(impact.impactScore);

    return impact;
  }

  /**
   * Get rating based on impact score
   */
  getRating(score) {
    if (score >= 80) return 'Excellent';
    if (score >= 60) return 'Good';
    if (score >= 40) return 'Fair';
    return 'Needs Improvement';
  }

  /**
   * Create JSON-LD script tag
   */
  createJSONLDScript(schema) {
    const jsonStr = JSON.stringify(schema, null, 2);
    return `<script type="application/ld+json">\n${jsonStr}\n</script>`;
  }

  /**
   * Inject schema into README content
   */
  injectSchema(content, schema) {
    const jsonldScript = this.createJSONLDScript(schema);
    const lines = content.split('\n');

    // Find first heading
    let insertIndex = 0;
    for (let i = 0; i < lines.length; i++) {
      if (lines[i].trim().startsWith('#')) {
        insertIndex = i + 1;
        break;
      }
    }

    // Insert schema after first heading with blank lines
    lines.splice(insertIndex, 0, '', jsonldScript, '');

    return lines.join('\n');
  }
}
</file>

<file path="workers/claude-health-worker.js">
// @ts-nocheck
/**
 * Claude Health Check Worker - AlephAuto Integration
 *
 * Comprehensive health monitoring for Claude Code environment including:
 * - Skills, hooks, agents, commands inventory
 * - Configuration validation (settings.json, skill-rules.json)
 * - Hook permissions and registration
 * - Plugin analysis and duplicate detection
 * - Directory structure verification
 * - Environment variable status
 * - Performance log analysis
 *
 * @extends SidequestServer
 */

import { SidequestServer } from '../core/server.js';
import { config } from '../core/config.js';
import { createComponentLogger } from '../utils/logger.js';
import { generateReport } from '../utils/report-generator.js';
import { exec } from 'child_process';
import { promisify } from 'util';
import fs from 'fs/promises';
import path from 'path';

const execAsync = promisify(exec);
const logger = createComponentLogger('ClaudeHealth');

class ClaudeHealthWorker extends SidequestServer {
  constructor(options = {}) {
    super({
      maxConcurrent: options.maxConcurrent ?? 1, // Single concurrent health check
      ...options
    });

    this.claudeDir = path.join(process.env.HOME, '.claude');
    this.devDir = path.join(process.env.HOME, 'dev');
    this.healthScriptPath = path.join(
      process.env.HOME,
      'code/jobs/sidequest/claude-health-check.sh'
    );

    this.thresholds = {
      maxPlugins: 30,
      warnPlugins: 20,
      maxHookExecutionTime: 1000, // ms
      minDiskSpace: 1024 * 1024 * 100, // 100MB
      maxLogSize: 1024 * 1024 * 10 // 10MB
    };

    logger.info('Claude Health Worker initialized', {
      claudeDir: this.claudeDir,
      healthScriptPath: this.healthScriptPath
    });
  }

  /**
   * Run comprehensive health check job
   * @param {Object} job - Job configuration
   * @param {boolean} job.detailed - Include detailed component listing
   * @param {boolean} job.validateConfig - Run configuration validation
   * @param {boolean} job.checkPerformance - Analyze hook performance logs
   * @param {boolean} job.analyzePlugins - Run plugin duplicate detection
   * @returns {Promise<Object>} Health check results
   */
  async runJobHandler(job) {
    const startTime = Date.now();
    logger.info('Starting Claude health check', { jobId: job.id });

    try {
      const checks = await this.runAllChecks(job.data || {});
      const analysis = this.analyzeResults(checks);
      const recommendations = this.generateRecommendations(analysis);

      const result = {
        success: true,
        timestamp: new Date().toISOString(),
        duration: Date.now() - startTime,
        checks,
        analysis,
        recommendations,
        summary: this.generateSummary(analysis, recommendations)
      };

      logger.info('Claude health check completed', {
        jobId: job.id,
        issueCount: recommendations.filter(r => r.priority === 'high').length,
        duration: result.duration
      });

      // Generate HTML/JSON reports
      const endTime = Date.now();
      const reportPaths = await generateReport({
        jobId: job.id,
        jobType: 'claude-health',
        status: 'completed',
        result,
        startTime,
        endTime,
        parameters: job.data || {},
        metadata: {
          healthScore: analysis.healthScore,
          criticalIssues: recommendations.filter(r => r.priority === 'high').length,
          warnings: recommendations.filter(r => r.priority === 'medium').length
        }
      });

      result.reportPaths = reportPaths;
      logger.info({ reportPaths }, 'Health check reports generated');

      return result;
    } catch (error) {
      logger.error({ err: error, jobId: job.id }, 'Claude health check failed');
      throw error;
    }
  }

  /**
   * Run all health checks
   * @param {Object} options - Check options
   * @returns {Promise<Object>} Check results
   */
  async runAllChecks(options = {}) {
    const checks = {};

    // Run checks in parallel where possible
    const [
      environment,
      directories,
      configuration,
      hooks,
      components,
      plugins,
      performance
    ] = await Promise.all([
      this.checkEnvironment(),
      this.checkDirectories(),
      options.validateConfig !== false ? this.checkConfiguration() : null,
      this.checkHooks(),
      this.checkComponents(options.detailed),
      options.analyzePlugins !== false ? this.checkPlugins() : null,
      options.checkPerformance !== false ? this.checkPerformance() : null
    ]);

    return {
      environment,
      directories,
      configuration,
      hooks,
      components,
      plugins,
      performance
    };
  }

  /**
   * Check environment variables and dependencies
   * @returns {Promise<Object>}
   */
  async checkEnvironment() {
    const checks = {
      direnv: false,
      direnvConfigured: false,
      direnvAllowed: false,
      nodeVersion: null,
      npmVersion: null,
      envVars: {}
    };

    try {
      // Check direnv
      try {
        await execAsync('which direnv');
        checks.direnv = true;
      } catch (e) {
        checks.direnv = false;
      }

      // Check if direnv hook is in shell config
      try {
        const { stdout } = await execAsync('grep -q "direnv hook" ~/.zshrc && echo "configured" || echo "not configured"');
        checks.direnvConfigured = stdout.trim() === 'configured';
      } catch (e) {
        checks.direnvConfigured = false;
      }

      // Check Node.js version
      const { stdout: nodeVersion } = await execAsync('node --version');
      checks.nodeVersion = nodeVersion.trim();

      // Check npm version
      const { stdout: npmVersion } = await execAsync('npm --version');
      checks.npmVersion = npmVersion.trim();

      // Check critical environment variables
      const varNames = [
        'CLAUDE_CONFIG_DIR',
        'CLAUDE_PROJECT_DIR',
        'CLAUDE_ACTIVE_DOCS',
        'CLAUDE_HOOKS_DIR'
      ];

      for (const varName of varNames) {
        checks.envVars[varName] = process.env[varName] || null;
      }

      checks.direnvAllowed = Object.values(checks.envVars).every(v => v !== null);

    } catch (error) {
      logger.warn({ err: error }, 'Error checking environment');
    }

    return checks;
  }

  /**
   * Check directory structure
   * @returns {Promise<Object>}
   */
  async checkDirectories() {
    const requiredDirs = {
      '.claude': this.claudeDir,
      'skills': path.join(this.claudeDir, 'skills'),
      'hooks': path.join(this.claudeDir, 'hooks'),
      'agents': path.join(this.claudeDir, 'agents'),
      'commands': path.join(this.claudeDir, 'commands'),
      'scripts': path.join(this.claudeDir, 'scripts'),
      'logs': path.join(this.claudeDir, 'logs'),
      'dev': this.devDir,
      'dev/active': path.join(this.devDir, 'active'),
      'dev/archive': path.join(this.devDir, 'archive'),
      'dev/templates': path.join(this.devDir, 'templates')
    };

    const checks = {};

    for (const [name, dirPath] of Object.entries(requiredDirs)) {
      try {
        const stats = await fs.stat(dirPath);
        checks[name] = {
          exists: stats.isDirectory(),
          path: dirPath,
          size: null
        };
      } catch (error) {
        checks[name] = {
          exists: false,
          path: dirPath,
          error: error.code
        };
      }
    }

    return checks;
  }

  /**
   * Check configuration files
   * @returns {Promise<Object>}
   */
  async checkConfiguration() {
    const checks = {
      settingsJson: {},
      skillRulesJson: {},
      packageJson: {},
      envrc: {}
    };

    // Check settings.json
    const settingsPath = path.join(this.claudeDir, 'settings.json');
    try {
      const data = await fs.readFile(settingsPath, 'utf-8');
      const settings = JSON.parse(data);
      checks.settingsJson = {
        exists: true,
        valid: true,
        hooks: settings.hooks ? Object.keys(settings.hooks).length : 0,
        enabledPlugins: settings.enabledPlugins ? Object.keys(settings.enabledPlugins).length : 0
      };
    } catch (error) {
      checks.settingsJson = {
        exists: error.code !== 'ENOENT',
        valid: false,
        error: error.message
      };
    }

    // Check skill-rules.json
    const skillRulesPath = path.join(this.claudeDir, 'skills', 'skill-rules.json');
    try {
      const data = await fs.readFile(skillRulesPath, 'utf-8');
      const skillRules = JSON.parse(data);
      checks.skillRulesJson = {
        exists: true,
        valid: true,
        skillCount: skillRules.skills ? Object.keys(skillRules.skills).length : 0
      };
    } catch (error) {
      checks.skillRulesJson = {
        exists: error.code !== 'ENOENT',
        valid: false,
        error: error.message
      };
    }

    // Check package.json
    const packagePath = path.join(this.claudeDir, 'package.json');
    try {
      const data = await fs.readFile(packagePath, 'utf-8');
      const pkg = JSON.parse(data);
      checks.packageJson = {
        exists: true,
        valid: true,
        scripts: pkg.scripts ? Object.keys(pkg.scripts).length : 0
      };
    } catch (error) {
      checks.packageJson = {
        exists: error.code !== 'ENOENT',
        valid: false,
        error: error.message
      };
    }

    // Check .envrc
    const envrcPath = path.join(this.claudeDir, '.envrc');
    try {
      await fs.access(envrcPath);
      checks.envrc = { exists: true };
    } catch (error) {
      checks.envrc = { exists: false };
    }

    return checks;
  }

  /**
   * Check hooks
   * @returns {Promise<Object>}
   */
  async checkHooks() {
    const hooksDir = path.join(this.claudeDir, 'hooks');
    const checks = {
      totalHooks: 0,
      executableHooks: 0,
      registeredHooks: 0,
      hooks: []
    };

    try {
      const files = await fs.readdir(hooksDir);
      const shellHooks = files.filter(f => f.endsWith('.sh'));

      checks.totalHooks = shellHooks.length;

      for (const hook of shellHooks) {
        const hookPath = path.join(hooksDir, hook);
        const stats = await fs.stat(hookPath);
        const isExecutable = (stats.mode & 0o111) !== 0;

        if (isExecutable) checks.executableHooks++;

        checks.hooks.push({
          name: hook,
          executable: isExecutable,
          size: stats.size
        });
      }

      // Check registered hooks
      const settingsPath = path.join(this.claudeDir, 'settings.json');
      try {
        const data = await fs.readFile(settingsPath, 'utf-8');
        const settings = JSON.parse(data);
        checks.registeredHooks = settings.hooks ? Object.keys(settings.hooks).length : 0;
      } catch (e) {
        logger.warn({ err: e }, 'Could not check registered hooks');
      }

    } catch (error) {
      logger.warn({ err: error }, 'Error checking hooks');
    }

    return checks;
  }

  /**
   * Check component counts
   * @param {boolean} detailed - Include detailed listing
   * @returns {Promise<Object>}
   */
  async checkComponents(detailed = false) {
    const counts = {
      skills: 0,
      agents: 0,
      commands: 0,
      activeTasks: 0,
      archivedTasks: 0,
      templates: 0
    };

    try {
      // Count skills
      const skillsDir = path.join(this.claudeDir, 'skills');
      const skillFiles = await fs.readdir(skillsDir);
      counts.skills = skillFiles.filter(f => f.endsWith('.md') || fs.stat(path.join(skillsDir, f)).then(s => s.isDirectory())).length;

      // Count agents
      const agentsDir = path.join(this.claudeDir, 'agents');
      const agentFiles = await fs.readdir(agentsDir);
      counts.agents = agentFiles.filter(f => f.endsWith('.md')).length;

      // Count commands
      const commandsDir = path.join(this.claudeDir, 'commands');
      const commandFiles = await fs.readdir(commandsDir);
      counts.commands = commandFiles.filter(f => f.endsWith('.md')).length;

      // Count active tasks
      const activeDir = path.join(this.devDir, 'active');
      try {
        const activeTasks = await fs.readdir(activeDir);
        counts.activeTasks = activeTasks.filter(f => !f.startsWith('.')).length;
      } catch (e) {
        counts.activeTasks = 0;
      }

      // Count archived tasks
      const archiveDir = path.join(this.devDir, 'archive');
      try {
        const archivedTasks = await fs.readdir(archiveDir);
        counts.archivedTasks = archivedTasks.filter(f => !f.startsWith('.')).length;
      } catch (e) {
        counts.archivedTasks = 0;
      }

      // Count templates
      const templatesDir = path.join(this.devDir, 'templates');
      try {
        const templates = await fs.readdir(templatesDir);
        counts.templates = templates.filter(f => f.endsWith('.md')).length;
      } catch (e) {
        counts.templates = 0;
      }

    } catch (error) {
      logger.warn({ err: error }, 'Error checking components');
    }

    return counts;
  }

  /**
   * Check plugins (integrate with plugin manager)
   * @returns {Promise<Object>}
   */
  async checkPlugins() {
    try {
      const settingsPath = path.join(this.claudeDir, 'settings.json');
      const data = await fs.readFile(settingsPath, 'utf-8');
      const settings = JSON.parse(data);

      const enabledPlugins = settings.enabledPlugins || {};
      const totalPlugins = Object.keys(enabledPlugins).length;

      // Basic duplicate detection (category-based)
      const categories = {};
      for (const plugin of Object.keys(enabledPlugins)) {
        const category = plugin.split(/[@-]/)[0];
        if (!categories[category]) categories[category] = [];
        categories[category].push(plugin);
      }

      const duplicateCategories = Object.entries(categories)
        .filter(([_, plugins]) => plugins.length > 1)
        .map(([category, plugins]) => ({ category, plugins, count: plugins.length }));

      return {
        totalPlugins,
        enabledPlugins: Object.keys(enabledPlugins),
        duplicateCategories,
        exceededThresholds: {
          maxPlugins: totalPlugins > this.thresholds.maxPlugins,
          warnPlugins: totalPlugins > this.thresholds.warnPlugins
        }
      };
    } catch (error) {
      logger.warn({ err: error }, 'Error checking plugins');
      return null;
    }
  }

  /**
   * Check performance logs
   * @returns {Promise<Object>}
   */
  async checkPerformance() {
    const perfLogPath = path.join(this.claudeDir, 'logs', 'hook-performance.log');

    try {
      const stats = await fs.stat(perfLogPath);
      const logSize = stats.size;

      // Read recent entries
      const data = await fs.readFile(perfLogPath, 'utf-8');
      const lines = data.split('\n').filter(l => l.trim());
      const recentEntries = lines.slice(-100); // Last 100 entries

      // Parse slow hooks
      const slowHooks = recentEntries
        .filter(line => line.includes('SLOW'))
        .map(line => {
          const match = line.match(/\[([^\]]+)\].*?(\d+)ms/);
          return match ? { hook: match[1], duration: parseInt(match[2]) } : null;
        })
        .filter(Boolean);

      // Parse failures
      const failures = recentEntries.filter(line => line.includes('failed')).length;

      return {
        logExists: true,
        logSize,
        totalEntries: lines.length,
        recentEntries: recentEntries.length,
        slowHooks: slowHooks.length,
        failures,
        slowHookDetails: slowHooks.slice(0, 5) // Top 5 slowest
      };
    } catch (error) {
      if (error.code === 'ENOENT') {
        return {
          logExists: false,
          message: 'Performance log not created yet'
        };
      }
      logger.warn({ err: error }, 'Error checking performance');
      return null;
    }
  }

  /**
   * Analyze health check results
   * @param {Object} checks - All check results
   * @returns {Object} Analysis
   */
  analyzeResults(checks) {
    const issues = [];
    const warnings = [];
    const successes = [];

    // Environment analysis
    if (!checks.environment?.direnv) {
      warnings.push({ type: 'environment', message: 'direnv not installed' });
    } else if (!checks.environment?.direnvConfigured) {
      warnings.push({ type: 'environment', message: 'direnv hook not configured in shell' });
    } else if (!checks.environment?.direnvAllowed) {
      warnings.push({ type: 'environment', message: 'Environment variables not loaded (run: direnv allow)' });
    } else {
      successes.push({ type: 'environment', message: 'Environment properly configured' });
    }

    // Directory analysis
    const missingDirs = Object.entries(checks.directories || {})
      .filter(([_, info]) => !info.exists)
      .map(([name, _]) => name);

    if (missingDirs.length > 0) {
      issues.push({ type: 'directories', message: `Missing directories: ${missingDirs.join(', ')}` });
    } else {
      successes.push({ type: 'directories', message: 'All required directories exist' });
    }

    // Configuration analysis
    if (!checks.configuration?.settingsJson?.valid) {
      issues.push({ type: 'configuration', message: 'settings.json is invalid or missing' });
    }
    if (!checks.configuration?.skillRulesJson?.valid) {
      issues.push({ type: 'configuration', message: 'skill-rules.json is invalid or missing' });
    }
    if (checks.configuration?.settingsJson?.valid && checks.configuration?.skillRulesJson?.valid) {
      successes.push({ type: 'configuration', message: 'Configuration files valid' });
    }

    // Hooks analysis
    const nonExecutableHooks = (checks.hooks?.totalHooks || 0) - (checks.hooks?.executableHooks || 0);
    if (nonExecutableHooks > 0) {
      issues.push({
        type: 'hooks',
        message: `${nonExecutableHooks} hook(s) not executable`,
        action: 'Run: chmod +x ~/.claude/hooks/*.sh'
      });
    }

    if ((checks.hooks?.registeredHooks || 0) === 0) {
      warnings.push({ type: 'hooks', message: 'No hooks registered in settings.json' });
    }

    // Plugin analysis
    if (checks.plugins?.exceededThresholds?.maxPlugins) {
      issues.push({
        type: 'plugins',
        message: `Too many plugins enabled (${checks.plugins.totalPlugins} > ${this.thresholds.maxPlugins})`,
        action: 'Review and disable unused plugins'
      });
    } else if (checks.plugins?.exceededThresholds?.warnPlugins) {
      warnings.push({
        type: 'plugins',
        message: `High plugin count (${checks.plugins.totalPlugins})`,
        action: 'Consider reviewing plugin usage'
      });
    }

    if (checks.plugins?.duplicateCategories?.length > 0) {
      warnings.push({
        type: 'plugins',
        message: `Found ${checks.plugins.duplicateCategories.length} categories with duplicate plugins`,
        action: 'Review and consolidate duplicate plugins'
      });
    }

    // Performance analysis
    if (checks.performance?.logExists) {
      if (checks.performance.slowHooks > 0) {
        warnings.push({
          type: 'performance',
          message: `${checks.performance.slowHooks} slow hook executions detected`,
          action: 'Review hook performance'
        });
      }
      if (checks.performance.failures > 0) {
        issues.push({
          type: 'performance',
          message: `${checks.performance.failures} hook failures detected`,
          action: 'Check hook logs for errors'
        });
      }
    }

    return {
      issues,
      warnings,
      successes,
      healthScore: this.calculateHealthScore(issues, warnings, successes)
    };
  }

  /**
   * Calculate overall health score (0-100)
   * @param {Array} issues - Critical issues
   * @param {Array} warnings - Warnings
   * @param {Array} successes - Successful checks
   * @returns {number} Health score
   */
  calculateHealthScore(issues, warnings, successes) {
    const totalChecks = issues.length + warnings.length + successes.length;
    if (totalChecks === 0) return 100;

    const issueWeight = 20;
    const warningWeight = 5;
    const successWeight = 10;

    const deductions = (issues.length * issueWeight) + (warnings.length * warningWeight);
    const additions = successes.length * successWeight;

    return Math.max(0, Math.min(100, 100 - deductions + (additions / totalChecks)));
  }

  /**
   * Generate recommendations
   * @param {Object} analysis - Analysis results
   * @returns {Array<Object>} Recommendations
   */
  generateRecommendations(analysis) {
    const recommendations = [];

    // Critical issues first
    for (const issue of analysis.issues) {
      recommendations.push({
        priority: 'high',
        type: issue.type,
        message: issue.message,
        action: issue.action || 'Review and fix'
      });
    }

    // Warnings
    for (const warning of analysis.warnings) {
      recommendations.push({
        priority: 'medium',
        type: warning.type,
        message: warning.message,
        action: warning.action || 'Review recommended'
      });
    }

    // Success message if health score is high
    if (analysis.healthScore >= 90 && recommendations.length === 0) {
      recommendations.push({
        priority: 'info',
        type: 'healthy',
        message: `Claude environment is healthy (Score: ${analysis.healthScore}/100)`,
        action: 'No action needed'
      });
    }

    return recommendations;
  }

  /**
   * Generate summary
   * @param {Object} analysis - Analysis results
   * @param {Array} recommendations - Recommendations
   * @returns {Object} Summary
   */
  generateSummary(analysis, recommendations) {
    const critical = recommendations.filter(r => r.priority === 'high').length;
    const warnings = recommendations.filter(r => r.priority === 'medium').length;

    return {
      healthScore: analysis.healthScore,
      status: analysis.healthScore >= 90 ? 'healthy' : analysis.healthScore >= 70 ? 'warning' : 'critical',
      criticalIssues: critical,
      warnings,
      message: this.getStatusMessage(analysis.healthScore, critical, warnings)
    };
  }

  /**
   * Get status message based on health score
   * @param {number} healthScore - Health score
   * @param {number} critical - Critical issue count
   * @param {number} warnings - Warning count
   * @returns {string} Status message
   */
  getStatusMessage(healthScore, critical, warnings) {
    if (healthScore >= 90 && critical === 0 && warnings === 0) {
      return '‚úÖ Claude environment is healthy';
    } else if (healthScore >= 70) {
      return `‚ö†Ô∏è  Claude environment has ${warnings} warning(s)`;
    } else {
      return `üî¥ Claude environment has ${critical} critical issue(s)`;
    }
  }

  /**
   * Create a health check job
   * @param {Object} options - Job options
   * @returns {Object} Created job
   */
  addJob(options = {}) {
    const jobId = `claude-health-${Date.now()}`;
    return this.createJob(jobId, {
      detailed: options.detailed || false,
      validateConfig: options.validateConfig !== false,
      checkPerformance: options.checkPerformance !== false,
      analyzePlugins: options.analyzePlugins !== false
    });
  }
}

// Export worker class
export { ClaudeHealthWorker };
</file>

<file path="workers/duplicate-detection-worker.js">
/**
 * Duplicate Detection Worker
 *
 * Extends SidequestServer to handle duplicate detection scanning jobs
 * within the AlephAuto framework.
 *
 * Features:
 * - Inter-project and intra-project scanning
 * - Intelligent retry logic with circuit breaker
 * - Auto-PR creation for consolidation suggestions
 * - Repository configuration management
 * - High-impact duplicate notifications
 * - Comprehensive metrics tracking
 */

import { SidequestServer } from '../core/server.js';
import { RepositoryConfigLoader } from '../pipeline-core/config/repository-config-loader.js';
import { InterProjectScanner } from '../pipeline-core/inter-project-scanner.js';
import { ScanOrchestrator } from '../pipeline-core/scan-orchestrator.js';
import { ReportCoordinator } from '../pipeline-core/reports/report-coordinator.js';
import { PRCreator } from '../pipeline-core/git/pr-creator.js';
import { createComponentLogger } from '../utils/logger.js';
import { isRetryable, getErrorInfo } from '../pipeline-core/errors/error-classifier.js';
import path from 'path';
import * as Sentry from '@sentry/node';

const logger = createComponentLogger('DuplicateDetectionWorker');

// Circuit breaker: Absolute maximum retry attempts to prevent infinite loops
const MAX_ABSOLUTE_RETRIES = 5;

/**
 * DuplicateDetectionWorker
 *
 * Handles duplicate detection scanning jobs with retry logic,
 * PR creation, and comprehensive reporting.
 */
export class DuplicateDetectionWorker extends SidequestServer {
  constructor(options = {}) {
    super({
      maxConcurrent: options.maxConcurrentScans || 3,
      logDir: path.join(process.cwd(), 'logs', 'duplicate-detection'),
      ...options
    });

    this.configLoader = new RepositoryConfigLoader(options.configPath);
    this.interProjectScanner = new InterProjectScanner({
      outputDir: path.join(process.cwd(), 'output', 'automated-scans')
    });
    this.orchestrator = new ScanOrchestrator({
      pythonPath: path.join(process.cwd(), 'venv', 'bin', 'python3')
    });
    this.reportCoordinator = new ReportCoordinator(
      path.join(process.cwd(), 'output', 'reports')
    );
    this.prCreator = new PRCreator({
      baseBranch: options.baseBranch || 'main',
      branchPrefix: options.branchPrefix || 'consolidate',
      dryRun: options.dryRun ?? (process.env.PR_DRY_RUN === 'true'),
      maxSuggestionsPerPR: options.maxSuggestionsPerPR || 5
    });

    this.scanMetrics = {
      totalScans: 0,
      successfulScans: 0,
      failedScans: 0,
      totalDuplicatesFound: 0,
      totalSuggestionsGenerated: 0,
      highImpactDuplicates: 0,
      prsCreated: 0,
      prCreationErrors: 0
    };

    this.enablePRCreation = options.enablePRCreation ?? (process.env.ENABLE_PR_CREATION === 'true');

    this.retryQueue = new Map(); // jobId -> { attempts, lastAttempt, maxAttempts, delay }
  }

  /**
   * Initialize the worker
   */
  async initialize() {
    try {
      // Load configuration
      await this.configLoader.load();

      // Validate configuration
      this.configLoader.validate();

      const stats = this.configLoader.getStats();
      logger.info({
        ...stats
      }, 'Duplicate detection worker initialized');

      this.emit('initialized', stats);
      this.emit('pipeline:status', {
        status: 'initialized',
        stats
      });
    } catch (error) {
      logger.error({ error }, 'Failed to initialize duplicate detection worker');
      Sentry.captureException(error);
      throw error;
    }
  }

  /**
   * Run job handler (required by SidequestServer)
   */
  async runJobHandler(job) {
    const { scanType, repositories, groupName } = job.data;

    logger.info({
      jobId: job.id,
      scanType,
      repositories: repositories?.length || 0,
      groupName
    }, 'Starting duplicate detection scan job');

    this.emit('pipeline:status', {
      status: 'scanning',
      jobId: job.id,
      scanType,
      repositories: repositories?.length || 0
    });

    try {
      if (scanType === 'inter-project') {
        return await this._runInterProjectScan(job, repositories);
      } else if (scanType === 'intra-project') {
        return await this._runIntraProjectScan(job, repositories[0]);
      } else {
        throw new Error(`Unknown scan type: ${scanType}`);
      }
    } catch (error) {
      // Handle retry logic
      const shouldRetry = await this._handleRetry(job, error);

      if (shouldRetry) {
        logger.info({ jobId: job.id }, 'Job will be retried');
        throw error; // Re-throw to mark job as failed, will be retried by retry handler
      } else {
        logger.error({ jobId: job.id, error }, 'Job failed after all retry attempts');
        this.emit('pipeline:status', {
          status: 'failed',
          jobId: job.id,
          error: error.message
        });
        throw error;
      }
    }
  }

  /**
   * Extract original job ID by stripping all retry suffixes
   * @param {string} jobId - Job ID (may contain retry suffixes)
   * @returns {string} Original job ID without retry suffixes
   * @private
   */
  _getOriginalJobId(jobId) {
    // Strip all -retryN suffixes to get the original job ID
    // Example: "scan-intra-project-123-retry1-retry1-retry1" -> "scan-intra-project-123"
    return jobId.replace(/-retry\d+/g, '');
  }

  /**
   * Handle retry logic with exponential backoff
   */
  async _handleRetry(job, error) {
    const scanConfig = this.configLoader.getScanConfig();
    const maxRetries = scanConfig.retryAttempts || 0;
    const baseDelay = scanConfig.retryDelay || 60000;

    // Get original job ID to track retries correctly
    const originalJobId = this._getOriginalJobId(job.id);

    // Classify error to determine if retry is appropriate
    const errorInfo = getErrorInfo(error);

    if (!errorInfo.retryable) {
      logger.warn({
        jobId: job.id,
        originalJobId,
        errorCode: errorInfo.code,
        errorMessage: errorInfo.message,
        classification: errorInfo.category,
        reason: errorInfo.reason
      }, 'Error is non-retryable - skipping retry');
      this.retryQueue.delete(originalJobId);
      return false;
    }

    if (!this.retryQueue.has(originalJobId)) {
      // First failure - initialize retry tracking
      this.retryQueue.set(originalJobId, {
        attempts: 0,
        lastAttempt: Date.now(),
        maxAttempts: maxRetries,
        delay: baseDelay
      });
    }

    const retryInfo = this.retryQueue.get(originalJobId);
    retryInfo.attempts++;

    // Circuit breaker: Check against absolute maximum
    if (retryInfo.attempts >= MAX_ABSOLUTE_RETRIES) {
      logger.error({
        jobId: job.id,
        originalJobId,
        attempts: retryInfo.attempts,
        maxAbsolute: MAX_ABSOLUTE_RETRIES
      }, 'Circuit breaker triggered: Maximum absolute retry attempts reached');

      // Send Sentry alert for circuit breaker
      Sentry.captureMessage('Circuit breaker triggered: Excessive retry attempts', {
        level: 'error',
        tags: {
          component: 'retry-logic',
          jobId: originalJobId,
          errorType: error.code || error.name
        },
        extra: {
          jobId: job.id,
          originalJobId,
          attempts: retryInfo.attempts,
          maxAbsolute: MAX_ABSOLUTE_RETRIES,
          errorMessage: error.message,
          errorCode: errorInfo.code,
          errorClassification: errorInfo.category
        }
      });

      this.retryQueue.delete(originalJobId);
      this.emit('retry:circuit-breaker', {
        jobId: originalJobId,
        attempts: retryInfo.attempts
      });
      return false;
    }

    // Check against configured maximum
    if (retryInfo.attempts >= retryInfo.maxAttempts) {
      logger.warn({
        jobId: job.id,
        originalJobId,
        attempts: retryInfo.attempts,
        maxConfigured: retryInfo.maxAttempts
      }, 'Maximum configured retry attempts reached');

      // Send Sentry alert for max retries reached
      Sentry.captureMessage('Maximum configured retry attempts reached', {
        level: 'warning',
        tags: {
          component: 'retry-logic',
          jobId: originalJobId,
          errorType: error.code || error.name
        },
        extra: {
          jobId: job.id,
          originalJobId,
          attempts: retryInfo.attempts,
          maxConfigured: retryInfo.maxAttempts,
          errorMessage: error.message,
          errorCode: errorInfo.code,
          errorClassification: errorInfo.category
        }
      });

      this.retryQueue.delete(originalJobId);
      this.emit('retry:max-attempts', {
        jobId: originalJobId,
        attempts: retryInfo.attempts
      });
      return false;
    }

    // Alert when approaching circuit breaker (3+ attempts)
    if (retryInfo.attempts >= 3) {
      Sentry.captureMessage('Warning: Approaching retry limit', {
        level: 'warning',
        tags: {
          component: 'retry-logic',
          jobId: originalJobId,
          errorType: error.code || error.name
        },
        extra: {
          jobId: job.id,
          originalJobId,
          attempts: retryInfo.attempts,
          maxAttempts: retryInfo.maxAttempts,
          maxAbsolute: MAX_ABSOLUTE_RETRIES,
          errorMessage: error.message,
          errorCode: errorInfo.code,
          errorClassification: errorInfo.category
        }
      });

      this.emit('retry:warning', {
        jobId: originalJobId,
        attempts: retryInfo.attempts,
        maxAttempts: retryInfo.maxAttempts
      });
    }

    // Calculate exponential backoff delay
    // Use suggested delay from error classifier as base
    const baseRetryDelay = errorInfo.suggestedDelay || retryInfo.delay;
    const delay = baseRetryDelay * Math.pow(2, retryInfo.attempts - 1);

    logger.info({
      jobId: job.id,
      originalJobId,
      attempt: retryInfo.attempts,
      maxAttempts: retryInfo.maxAttempts,
      maxAbsolute: MAX_ABSOLUTE_RETRIES,
      delayMs: delay,
      error: error.message,
      errorClassification: errorInfo.category,
      errorReason: errorInfo.reason,
      suggestedDelay: errorInfo.suggestedDelay
    }, 'Scheduling retry with exponential backoff');

    this.emit('retry:scheduled', {
      jobId: originalJobId,
      attempt: retryInfo.attempts,
      delay
    });

    // Schedule retry
    setTimeout(() => {
      logger.info({ jobId: job.id, originalJobId, attempt: retryInfo.attempts }, 'Retrying failed job');
      // Use original job ID + retry count for new job ID
      this.createJob(`${originalJobId}-retry${retryInfo.attempts}`, job.data);
    }, delay);

    return true;
  }

  /**
   * Run inter-project scan
   */
  async _runInterProjectScan(job, repositoryConfigs) {
    const repoPaths = repositoryConfigs.map(r => r.path);

    logger.info({
      jobId: job.id,
      repositories: repoPaths.length
    }, 'Running inter-project scan');

    const result = await this.interProjectScanner.scanRepositories(repoPaths);

    // Generate reports
    await this.reportCoordinator.generateAllReports(result, {
      title: `Automated Inter-Project Scan: ${repoPaths.length} Repositories`,
      includeDetails: true,
      includeSourceCode: true,
      includeCodeBlocks: true
    });

    // Update scan metrics
    this._updateMetrics(result);

    // Update repository configurations
    await this._updateRepositoryConfigs(repositoryConfigs, result);

    // Check for high-impact duplicates
    await this._checkForHighImpactDuplicates(result);

    this.emit('scan:completed', {
      jobId: job.id,
      scanType: 'inter-project',
      metrics: result.metrics
    });

    return {
      scanType: 'inter-project',
      repositories: repoPaths.length,
      crossRepoDuplicates: result.metrics.total_cross_repository_groups || 0,
      suggestions: result.metrics.total_suggestions || 0,
      duration: result.scan_metadata?.duration_seconds || 0
    };
  }

  /**
   * Run intra-project scan
   */
  async _runIntraProjectScan(job, repositoryConfig) {
    // Validate repository config
    if (!repositoryConfig) {
      const error = new Error('Repository configuration is undefined');
      logger.error({ jobId: job.id }, 'No repository configuration provided for intra-project scan');
      Sentry.captureException(error, {
        tags: {
          error_type: 'validation_error',
          component: 'DuplicateDetectionWorker',
          scan_type: 'intra-project'
        },
        extra: { jobId: job.id }
      });
      throw error;
    }

    if (!repositoryConfig.path) {
      const error = new Error(`Repository configuration missing 'path' property. Config: ${JSON.stringify(repositoryConfig)}`);
      logger.error({
        jobId: job.id,
        repositoryConfig
      }, 'Repository configuration missing path property');
      Sentry.captureException(error, {
        tags: {
          error_type: 'validation_error',
          component: 'DuplicateDetectionWorker',
          scan_type: 'intra-project'
        },
        extra: {
          jobId: job.id,
          repositoryConfig
        }
      });
      throw error;
    }

    const repoPath = repositoryConfig.path;

    logger.info({
      jobId: job.id,
      repository: repoPath
    }, 'Running intra-project scan');

    const result = await this.orchestrator.scanRepository(repoPath);

    // Generate reports
    await this.reportCoordinator.generateAllReports(result, {
      title: `Automated Scan: ${repositoryConfig.name}`,
      includeDetails: true,
      includeSourceCode: true,
      includeCodeBlocks: true
    });

    // Update scan metrics
    this._updateMetrics(result);

    // Update repository configuration
    await this._updateRepositoryConfigs([repositoryConfig], result);

    // Check for high-impact duplicates
    await this._checkForHighImpactDuplicates(result);

    // Create PRs if enabled
    let prResults = null;
    if (this.enablePRCreation && result.suggestions && result.suggestions.length > 0) {
      try {
        logger.info({
          jobId: job.id,
          repository: repositoryConfig.name,
          suggestions: result.suggestions.length
        }, 'Creating PRs for consolidation suggestions');

        prResults = await this.prCreator.createPRsForSuggestions(result, repoPath);

        this.scanMetrics.prsCreated += prResults.prsCreated;

        if (prResults.errors.length > 0) {
          this.scanMetrics.prCreationErrors += prResults.errors.length;
          logger.warn({
            errors: prResults.errors
          }, 'Some PRs failed to create');
        }

        logger.info({
          prsCreated: prResults.prsCreated,
          prUrls: prResults.prUrls,
          errors: prResults.errors.length
        }, 'PR creation completed');

        this.emit('pr:created', {
          jobId: job.id,
          repository: repositoryConfig.name,
          prsCreated: prResults.prsCreated,
          prUrls: prResults.prUrls
        });

      } catch (error) {
        logger.error({ error }, 'Failed to create PRs for suggestions');
        this.scanMetrics.prCreationErrors++;
        Sentry.captureException(error, {
          tags: {
            component: 'pr-creation',
            repository: repositoryConfig.name
          }
        });

        this.emit('pr:failed', {
          jobId: job.id,
          repository: repositoryConfig.name,
          error: error.message
        });
      }
    }

    this.emit('scan:completed', {
      jobId: job.id,
      scanType: 'intra-project',
      repository: repositoryConfig.name,
      metrics: result.metrics,
      prResults
    });

    return {
      scanType: 'intra-project',
      repository: repositoryConfig.name,
      duplicates: result.metrics.total_duplicate_groups || 0,
      suggestions: result.metrics.total_suggestions || 0,
      duration: result.scan_metadata?.duration_seconds || 0,
      prResults: prResults ? {
        prsCreated: prResults.prsCreated,
        prUrls: prResults.prUrls,
        errors: prResults.errors.length
      } : null
    };
  }

  /**
   * Update scan metrics
   */
  _updateMetrics(scanResult) {
    this.scanMetrics.totalScans++;

    if (scanResult.scan_type === 'inter-project') {
      this.scanMetrics.totalDuplicatesFound += scanResult.metrics.total_cross_repository_groups || 0;
      this.scanMetrics.totalSuggestionsGenerated += scanResult.metrics.total_suggestions || 0;

      // Count high-impact duplicates
      const highImpactDuplicates = (scanResult.cross_repository_duplicates || [])
        .filter(dup => dup.impact_score >= 75);
      this.scanMetrics.highImpactDuplicates += highImpactDuplicates.length;
    } else {
      this.scanMetrics.totalDuplicatesFound += scanResult.metrics.total_duplicate_groups || 0;
      this.scanMetrics.totalSuggestionsGenerated += scanResult.metrics.total_suggestions || 0;

      // Count high-impact duplicates
      const highImpactDuplicates = (scanResult.duplicate_groups || [])
        .filter(dup => dup.impact_score >= 75);
      this.scanMetrics.highImpactDuplicates += highImpactDuplicates.length;
    }

    this.emit('metrics:updated', this.scanMetrics);
  }

  /**
   * Update repository configurations with scan results
   */
  async _updateRepositoryConfigs(repositoryConfigs, scanResult) {
    const status = scanResult.scan_metadata ? 'success' : 'failure';
    const duration = scanResult.scan_metadata?.duration_seconds || 0;
    const duplicatesFound = scanResult.scan_type === 'inter-project'
      ? scanResult.metrics.total_cross_repository_groups || 0
      : scanResult.metrics.total_duplicate_groups || 0;

    for (const repoConfig of repositoryConfigs) {
      try {
        // Update last scanned timestamp
        await this.configLoader.updateLastScanned(repoConfig.name);

        // Add scan history entry
        await this.configLoader.addScanHistory(repoConfig.name, {
          status,
          duration,
          duplicatesFound
        });
      } catch (error) {
        logger.warn({
          error,
          repository: repoConfig.name
        }, 'Failed to update repository config');
      }
    }
  }

  /**
   * Check for high-impact duplicates and send notifications
   */
  async _checkForHighImpactDuplicates(scanResult) {
    const notificationSettings = this.configLoader.getNotificationSettings();

    if (!notificationSettings.enabled || !notificationSettings.onHighImpactDuplicates) {
      return;
    }

    const threshold = notificationSettings.highImpactThreshold || 75;
    const duplicates = scanResult.scan_type === 'inter-project'
      ? scanResult.cross_repository_duplicates || []
      : scanResult.duplicate_groups || [];

    const highImpactDuplicates = duplicates.filter(dup => dup.impact_score >= threshold);

    if (highImpactDuplicates.length > 0) {
      logger.warn({
        count: highImpactDuplicates.length,
        threshold,
        topImpactScore: Math.max(...highImpactDuplicates.map(d => d.impact_score))
      }, 'High-impact duplicates detected');

      // Send Sentry notification
      Sentry.captureMessage(`High-impact duplicates detected: ${highImpactDuplicates.length} duplicates with impact score >= ${threshold}`, {
        level: 'warning',
        tags: {
          component: 'duplicate-detection',
          scanType: scanResult.scan_type
        },
        contexts: {
          duplicates: {
            count: highImpactDuplicates.length,
            threshold,
            topImpactScore: Math.max(...highImpactDuplicates.map(d => d.impact_score))
          }
        }
      });

      this.emit('high-impact:detected', {
        count: highImpactDuplicates.length,
        threshold,
        topImpactScore: Math.max(...highImpactDuplicates.map(d => d.impact_score))
      });
    }
  }

  /**
   * Schedule a scan job
   */
  scheduleScan(scanType, repositories, groupName = null) {
    const jobId = `scan-${scanType}-${Date.now()}`;
    const jobData = {
      scanType,
      repositories,
      groupName,
      type: 'duplicate-detection'
    };

    return this.createJob(jobId, jobData);
  }

  /**
   * Run nightly scan (called by cron or startup)
   */
  async runNightlyScan() {
    logger.info('Starting nightly duplicate detection scan');

    const scanConfig = this.configLoader.getScanConfig();

    if (!scanConfig.enabled) {
      logger.info('Automated scanning is disabled');
      this.emit('pipeline:status', {
        status: 'disabled'
      });
      return;
    }

    // Get repositories to scan tonight
    const repositoriesToScan = this.configLoader.getRepositoriesToScanTonight();

    logger.info({
      repositoryCount: repositoriesToScan.length
    }, 'Repositories selected for scanning');

    if (repositoriesToScan.length === 0) {
      logger.info('No repositories to scan tonight');
      this.emit('pipeline:status', {
        status: 'idle',
        message: 'No repositories scheduled for scanning'
      });
      return;
    }

    this.emit('pipeline:status', {
      status: 'scheduling',
      individualScans: repositoriesToScan.length
    });

    // Scan individual repositories (intra-project)
    for (const repo of repositoriesToScan) {
      this.scheduleScan('intra-project', [repo]);
    }

    // Scan repository groups (inter-project)
    const groups = this.configLoader.getEnabledGroups();
    for (const group of groups) {
      const groupRepos = this.configLoader.getGroupRepositories(group.name);
      if (groupRepos.length >= 2) {
        this.scheduleScan('inter-project', groupRepos, group.name);
      }
    }

    logger.info({
      individualScans: repositoriesToScan.length,
      groupScans: groups.length
    }, 'Nightly scan scheduled');

    this.emit('pipeline:status', {
      status: 'scheduled',
      individualScans: repositoriesToScan.length,
      groupScans: groups.length
    });
  }

  /**
   * Get retry metrics
   */
  getRetryMetrics() {
    const retryStats = {
      activeRetries: this.retryQueue.size,
      totalRetryAttempts: 0,
      jobsBeingRetried: [],
      retryDistribution: {
        attempt1: 0,
        attempt2: 0,
        attempt3Plus: 0,
        nearingLimit: 0  // 3+ attempts
      }
    };

    for (const [jobId, retryInfo] of this.retryQueue.entries()) {
      retryStats.totalRetryAttempts += retryInfo.attempts;
      retryStats.jobsBeingRetried.push({
        jobId,
        attempts: retryInfo.attempts,
        maxAttempts: retryInfo.maxAttempts,
        lastAttempt: new Date(retryInfo.lastAttempt).toISOString()
      });

      // Distribution
      if (retryInfo.attempts === 1) {
        retryStats.retryDistribution.attempt1++;
      } else if (retryInfo.attempts === 2) {
        retryStats.retryDistribution.attempt2++;
      } else {
        retryStats.retryDistribution.attempt3Plus++;
      }

      if (retryInfo.attempts >= 3) {
        retryStats.retryDistribution.nearingLimit++;
      }
    }

    return retryStats;
  }

  /**
   * Get scan metrics
   */
  getScanMetrics() {
    return {
      ...this.scanMetrics,
      queueStats: this.getStats(),
      retryMetrics: this.getRetryMetrics()
    };
  }
}
</file>

<file path="workers/git-activity-worker.js">
// @ts-nocheck
import { SidequestServer } from '../core/server.js';
import { generateReport } from '../utils/report-generator.js';
import { spawn } from 'child_process';
import fs from 'fs/promises';
import path from 'path';
import os from 'os';
import { createComponentLogger } from '../utils/logger.js';

const logger = createComponentLogger('GitActivityWorker');

/**
 * GitActivityWorker - Executes git activity report jobs
 *
 * Integrates the Python git activity collector into the AlephAuto framework,
 * providing job queue management, event tracking, and Sentry error monitoring.
 */
export class GitActivityWorker extends SidequestServer {
  constructor(options = {}) {
    super(options);
    this.codeBaseDir = options.codeBaseDir || path.join(os.homedir(), 'code');
    this.pythonScript = options.pythonScript || path.join(
      path.dirname(new URL(import.meta.url).pathname),
      '..',
      'pipeline-runners',
      'collect_git_activity.py'
    );
    this.personalSiteDir = options.personalSiteDir || path.join(
      os.homedir(),
      'code',
      'PersonalSite'
    );
    this.outputDir = options.outputDir || '/tmp';
    this.logDir = options.logDir || path.join(
      path.dirname(new URL(import.meta.url).pathname),
      'logs'
    );
  }

  /**
   * Run git activity report job
   */
  async runJobHandler(job) {
    const startTime = Date.now();
    const {
      reportType,
      days,
      sinceDate,
      untilDate,
      outputFormat = 'json',
      generateVisualizations = true
    } = job.data;

    logger.info({
      jobId: job.id,
      reportType,
      days,
      sinceDate,
      untilDate
    }, 'Running git activity report');

    try {
      // Build command arguments
      const args = this.#buildPythonArgs({
        reportType,
        days,
        sinceDate,
        untilDate,
        outputFormat,
        generateVisualizations
      });

      // Execute Python script
      const { stdout, stderr, outputFiles } = await this.#runPythonScript(args);

      if (stderr) {
        logger.warn({ jobId: job.id, stderr }, 'Git activity warnings');
      }

      // Parse JSON output to get statistics
      const stats = this.#parseStats(stdout);

      // Verify output files exist
      const verifiedFiles = await this.#verifyOutputFiles(outputFiles);

      logger.info({
        jobId: job.id,
        stats,
        filesGenerated: verifiedFiles.length
      }, 'Git activity report completed');

      const result = {
        reportType,
        days: days || this.#calculateDays(sinceDate, untilDate),
        sinceDate,
        untilDate,
        stats,
        outputFiles: verifiedFiles,
        timestamp: new Date().toISOString(),
      };

      // Generate HTML/JSON reports
      const endTime = Date.now();
      const reportPaths = await generateReport({
        jobId: job.id,
        jobType: 'git-activity',
        status: 'completed',
        result,
        startTime,
        endTime,
        parameters: job.data,
        metadata: {
          reportType,
          filesGenerated: verifiedFiles.length
        }
      });

      result.reportPaths = reportPaths;
      logger.info({ reportPaths }, 'Git activity reports generated');

      return result;
    } catch (error) {
      logger.error({
        jobId: job.id,
        error: error.message,
        stack: error.stack
      }, 'Git activity report failed');
      throw error;
    }
  }

  /**
   * Build Python script arguments
   * @private
   */
  #buildPythonArgs({ reportType, days, sinceDate, untilDate, outputFormat, generateVisualizations }) {
    const args = [];

    // Add date range arguments
    if (sinceDate && untilDate) {
      args.push('--since', sinceDate);
      args.push('--until', untilDate);
    } else if (reportType === 'weekly' || days === 7) {
      args.push('--weekly');
    } else if (reportType === 'monthly' || days === 30) {
      args.push('--monthly');
    } else if (days) {
      args.push('--days', String(days));
    } else {
      // Default to weekly
      args.push('--weekly');
    }

    // Add output format
    if (outputFormat !== 'json') {
      args.push('--output-format', outputFormat);
    }

    // Add visualization flag
    if (!generateVisualizations) {
      args.push('--no-visualizations');
    }

    return args;
  }

  /**
   * Execute Python script
   * @private
   */
  #runPythonScript(args) {
    return new Promise((resolve, reject) => {
      logger.debug({ args }, 'Executing Python script');

      const proc = spawn('python3', [this.pythonScript, ...args], {
        cwd: path.dirname(this.pythonScript),
        timeout: 300000, // 5 minute timeout
        maxBuffer: 10 * 1024 * 1024, // 10MB buffer
      });

      let stdout = '';
      let stderr = '';

      proc.stdout.on('data', (data) => {
        stdout += data.toString();
      });

      proc.stderr.on('data', (data) => {
        stderr += data.toString();
      });

      proc.on('close', (code) => {
        if (code === 0) {
          // Extract output files from stdout if present
          const outputFiles = this.#extractOutputFiles(stdout);
          resolve({ stdout, stderr, outputFiles });
        } else {
          const error = new Error(`Python script exited with code ${code}`);
          error.code = code;
          error.stdout = stdout;
          error.stderr = stderr;
          reject(error);
        }
      });

      proc.on('error', (error) => {
        error.stdout = stdout;
        error.stderr = stderr;
        reject(error);
      });
    });
  }

  /**
   * Extract output file paths from script output
   * @private
   */
  #extractOutputFiles(stdout) {
    const files = [];

    // Look for JSON output file
    const jsonMatch = stdout.match(/(?:Saving|Saved) (?:to|data to):\s*(.+\.json)/i);
    if (jsonMatch) {
      files.push(jsonMatch[1].trim());
    }

    // Look for visualization files
    const svgMatches = stdout.matchAll(/(?:Generating|Generated|Saving).*?:\s*(.+\.svg)/gi);
    for (const match of svgMatches) {
      files.push(match[1].trim());
    }

    return files;
  }

  /**
   * Parse statistics from script output
   * @private
   */
  #parseStats(stdout) {
    const stats = {
      totalCommits: 0,
      totalRepositories: 0,
      linesAdded: 0,
      linesDeleted: 0,
    };

    // Try to parse from JSON output in stdout
    try {
      const jsonMatch = stdout.match(/\{[\s\S]*"total_commits"[\s\S]*\}/);
      if (jsonMatch) {
        const data = JSON.parse(jsonMatch[0]);
        stats.totalCommits = data.total_commits || 0;
        stats.totalRepositories = data.repositories?.length || 0;
        stats.linesAdded = data.total_additions || 0;
        stats.linesDeleted = data.total_deletions || 0;
      }
    } catch (error) {
      logger.warn({ error: error.message }, 'Could not parse stats from output');
    }

    return stats;
  }

  /**
   * Calculate days between two dates
   * @private
   */
  #calculateDays(sinceDate, untilDate) {
    if (!sinceDate || !untilDate) return null;

    const since = new Date(sinceDate);
    const until = new Date(untilDate);
    const diffTime = Math.abs(until - since);
    const diffDays = Math.ceil(diffTime / (1000 * 60 * 60 * 24));

    return diffDays;
  }

  /**
   * Verify output files exist
   * @private
   */
  async #verifyOutputFiles(files) {
    const verified = [];

    for (const file of files) {
      try {
        await fs.access(file);
        const stats = await fs.stat(file);
        verified.push({
          path: file,
          size: stats.size,
          exists: true,
        });
      } catch (error) {
        logger.warn({ file, error: error.message }, 'Output file not found');
        verified.push({
          path: file,
          exists: false,
        });
      }
    }

    return verified;
  }

  /**
   * Create a weekly report job
   */
  createWeeklyReportJob() {
    const jobId = `git-activity-weekly-${Date.now()}`;

    return this.createJob(jobId, {
      reportType: 'weekly',
      days: 7,
      type: 'git-activity-report',
    });
  }

  /**
   * Create a monthly report job
   */
  createMonthlyReportJob() {
    const jobId = `git-activity-monthly-${Date.now()}`;

    return this.createJob(jobId, {
      reportType: 'monthly',
      days: 30,
      type: 'git-activity-report',
    });
  }

  /**
   * Create a custom date range report job
   */
  createCustomReportJob(sinceDate, untilDate) {
    const jobId = `git-activity-custom-${Date.now()}`;

    return this.createJob(jobId, {
      reportType: 'custom',
      sinceDate,
      untilDate,
      type: 'git-activity-report',
    });
  }

  /**
   * Create a report job with custom parameters
   */
  createReportJob(options = {}) {
    const jobId = options.jobId || `git-activity-${Date.now()}`;

    return this.createJob(jobId, {
      reportType: options.reportType || 'weekly',
      days: options.days,
      sinceDate: options.sinceDate,
      untilDate: options.untilDate,
      outputFormat: options.outputFormat || 'json',
      generateVisualizations: options.generateVisualizations !== false,
      type: 'git-activity-report',
    });
  }
}
</file>

<file path="workers/gitignore-worker.js">
// @ts-nocheck
import { SidequestServer } from '../core/server.js';
import { GitignoreRepomixUpdater } from './gitignore-repomix-updater.js';
import { generateReport } from '../utils/report-generator.js';
import { createComponentLogger } from '../utils/logger.js';
import * as Sentry from '@sentry/node';
import path from 'path';
import os from 'os';

const logger = createComponentLogger('GitignoreWorker');

/**
 * GitignoreWorker - Executes gitignore update jobs
 *
 * Integrates GitignoreRepomixUpdater into the AlephAuto framework,
 * providing job queue management, event tracking, and Sentry error monitoring.
 */
export class GitignoreWorker extends SidequestServer {
  constructor(options = {}) {
    super(options);
    this.baseDir = options.baseDir || path.join(os.homedir(), 'code');
    this.excludeDirs = options.excludeDirs || [
      'node_modules',
      '.git',
      'dist',
      'build',
      'coverage',
      '.next',
      '.nuxt',
      'vendor',
      '__pycache__',
      '.venv',
      'venv',
    ];
    this.maxDepth = options.maxDepth || 10;
    this.gitignoreEntry = options.gitignoreEntry || 'repomix-output.xml';
  }

  /**
   * Run gitignore update job
   */
  async runJobHandler(job) {
    const startTime = Date.now();
    const {
      baseDir = this.baseDir,
      excludeDirs = this.excludeDirs,
      maxDepth = this.maxDepth,
      gitignoreEntry = this.gitignoreEntry,
      dryRun = false,
      repositories = null, // Optional: specific repositories to process
    } = job.data;

    logger.info({
      jobId: job.id,
      baseDir,
      dryRun,
      specificRepos: repositories ? repositories.length : 'all'
    }, 'Running gitignore update job');

    try {
      // Create updater instance
      const updater = new GitignoreRepomixUpdater({
        baseDir,
        excludeDirs,
        maxDepth,
        dryRun,
      });

      // Override gitignore entry if specified
      if (gitignoreEntry) {
        updater.gitignoreEntry = gitignoreEntry;
      }

      let results;
      if (repositories && repositories.length > 0) {
        // Process specific repositories
        results = await this.#processSpecificRepositories(updater, repositories);
      } else {
        // Process all repositories
        results = await updater.processRepositories();
      }

      logger.info({
        jobId: job.id,
        totalRepositories: results.totalRepositories,
        added: results.summary.added,
        skipped: results.summary.skipped,
        wouldAdd: results.summary.would_add,
        errors: results.summary.error
      }, 'Gitignore update job completed');

      const result = {
        ...results,
        timestamp: new Date().toISOString(),
        dryRun,
        gitignoreEntry,
      };

      // Generate HTML/JSON reports
      const endTime = Date.now();
      const reportPaths = await generateReport({
        jobId: job.id,
        jobType: 'gitignore-update',
        status: 'completed',
        result,
        startTime,
        endTime,
        parameters: job.data,
        metadata: {
          totalRepositories: results.totalRepositories,
          dryRun
        }
      });

      result.reportPaths = reportPaths;
      logger.info({ reportPaths }, 'Gitignore update reports generated');

      return result;
    } catch (error) {
      logger.error({
        jobId: job.id,
        error: error.message,
        stack: error.stack
      }, 'Gitignore update job failed');

      Sentry.captureException(error, {
        tags: {
          component: 'gitignore-worker',
          job_id: job.id,
        },
        extra: {
          baseDir,
          dryRun,
          gitignoreEntry,
        },
      });

      throw error;
    }
  }

  /**
   * Process specific repositories
   * @private
   */
  async #processSpecificRepositories(updater, repositories) {
    const results = [];

    logger.info({
      count: repositories.length
    }, 'Processing specific repositories');

    for (const repoPath of repositories) {
      logger.info({ repository: repoPath }, 'Processing repository');
      const result = await updater.addToGitignore(repoPath);
      results.push({
        repository: repoPath,
        ...result,
      });
      logger.info({
        repository: repoPath,
        action: result.action,
        reason: result.reason
      }, 'Repository processed');
    }

    return {
      totalRepositories: repositories.length,
      results,
      summary: updater.generateSummary(results),
    };
  }

  /**
   * Create a job to update all repositories
   */
  createUpdateAllJob(options = {}) {
    const jobId = `gitignore-update-all-${Date.now()}`;

    return this.createJob(jobId, {
      type: 'gitignore-update',
      baseDir: options.baseDir || this.baseDir,
      dryRun: options.dryRun ?? false,
      gitignoreEntry: options.gitignoreEntry || this.gitignoreEntry,
      excludeDirs: options.excludeDirs || this.excludeDirs,
      maxDepth: options.maxDepth || this.maxDepth,
    });
  }

  /**
   * Create a job to update specific repositories
   */
  createUpdateRepositoriesJob(repositories, options = {}) {
    const jobId = `gitignore-update-repos-${Date.now()}`;

    return this.createJob(jobId, {
      type: 'gitignore-update',
      repositories,
      dryRun: options.dryRun ?? false,
      gitignoreEntry: options.gitignoreEntry || this.gitignoreEntry,
    });
  }

  /**
   * Create a dry-run job to preview changes
   */
  createDryRunJob(options = {}) {
    const jobId = `gitignore-dryrun-${Date.now()}`;

    return this.createJob(jobId, {
      type: 'gitignore-update',
      baseDir: options.baseDir || this.baseDir,
      dryRun: true,
      gitignoreEntry: options.gitignoreEntry || this.gitignoreEntry,
      excludeDirs: options.excludeDirs || this.excludeDirs,
      maxDepth: options.maxDepth || this.maxDepth,
    });
  }
}
</file>

<file path="workers/repo-cleanup-worker.js">
// @ts-nocheck
import { SidequestServer } from '../core/server.js';
import { generateReport } from '../utils/report-generator.js';
import { createComponentLogger } from '../utils/logger.js';
import * as Sentry from '@sentry/node';
import { execFile } from 'child_process';
import { promisify } from 'util';
import path from 'path';
import os from 'os';
import fs from 'fs/promises';
import { fileURLToPath } from 'url';

const execFileAsync = promisify(execFile);
const logger = createComponentLogger('RepoCleanupWorker');

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

/**
 * RepoCleanupWorker - Executes repository cleanup jobs
 *
 * Integrates universal-repo-cleanup.sh into the AlephAuto framework,
 * providing job queue management, event tracking, and Sentry error monitoring.
 *
 * Cleanup targets:
 * - Python virtual environments (venv, .venv, etc.)
 * - Temporary/cache files (.DS_Store, __pycache__, .swp)
 * - Build artifacts (.jekyll-cache, dist, build, node_modules/.cache)
 * - Output files (repomix-output.xml, *.log)
 * - Redundant directories (drafts, temp, tmp, backup)
 */
export class RepoCleanupWorker extends SidequestServer {
  constructor(options = {}) {
    super({
      ...options,
      jobType: 'repo-cleanup',
    });
    this.baseDir = options.baseDir || path.join(os.homedir(), 'code');
    this.scriptPath = path.join(__dirname, '../pipeline-runners/universal-repo-cleanup.sh');
  }

  /**
   * Run repository cleanup job
   */
  async runJobHandler(job) {
    const startTime = Date.now();
    const {
      targetDir = this.baseDir,
      dryRun = false,
    } = job.data;

    logger.info({
      jobId: job.id,
      targetDir,
      dryRun,
    }, 'Running repository cleanup job');

    try {
      // Check script exists
      await fs.access(this.scriptPath);

      // Get initial size
      const initialSize = await this.#getDirectorySize(targetDir);

      let result;
      if (dryRun) {
        // Scan only, don't clean
        result = await this.#scanRepository(targetDir);
      } else {
        // Run actual cleanup
        result = await this.#runCleanup(targetDir);
      }

      // Get final size
      const finalSize = await this.#getDirectorySize(targetDir);

      const output = {
        targetDir,
        dryRun,
        initialSize,
        finalSize,
        savedSpace: dryRun ? 'N/A (dry run)' : this.#formatSizeDiff(initialSize, finalSize),
        ...result,
        timestamp: new Date().toISOString(),
      };

      logger.info({
        jobId: job.id,
        targetDir,
        initialSize,
        finalSize,
        itemsFound: result.totalItems,
      }, 'Repository cleanup job completed');

      // Generate HTML/JSON reports
      const endTime = Date.now();
      const reportPaths = await generateReport({
        jobId: job.id,
        jobType: 'repo-cleanup',
        status: 'completed',
        result: output,
        startTime,
        endTime,
        parameters: job.data,
        metadata: {
          targetDir,
          dryRun,
          totalItems: result.totalItems
        }
      });

      output.reportPaths = reportPaths;
      logger.info({ reportPaths }, 'Repository cleanup reports generated');

      return output;
    } catch (error) {
      logger.error({
        jobId: job.id,
        error: error.message,
        stack: error.stack,
      }, 'Repository cleanup job failed');

      Sentry.captureException(error, {
        tags: {
          component: 'repo-cleanup-worker',
          job_id: job.id,
        },
        extra: {
          targetDir,
          dryRun,
        },
      });

      throw error;
    }
  }

  /**
   * Scan repository without cleaning
   * @private
   */
  async #scanRepository(targetDir) {
    const categories = {
      venvs: [],
      tempFiles: [],
      outputFiles: [],
      buildArtifacts: [],
      redundantDirs: [],
    };

    // Python venvs
    const venvPatterns = ['venv', '.venv', 'env', 'virtualenv'];
    for (const pattern of venvPatterns) {
      try {
        const { stdout } = await execFileAsync('find', [
          targetDir, '-maxdepth', '3', '-type', 'd', '-name', pattern
        ], { maxBuffer: 10 * 1024 * 1024 });
        const dirs = stdout.trim().split('\n').filter(Boolean);
        categories.venvs.push(...dirs);
      } catch {
        // Pattern not found
      }
    }

    // Temp files
    const tempPatterns = ['.DS_Store', '__pycache__', '*.pyc', '*.swp'];
    for (const pattern of tempPatterns) {
      try {
        const { stdout } = await execFileAsync('find', [
          targetDir, '-name', pattern
        ], { maxBuffer: 10 * 1024 * 1024 });
        const files = stdout.trim().split('\n').filter(Boolean);
        categories.tempFiles.push(...files);
      } catch {
        // Pattern not found
      }
    }

    // Build artifacts
    const buildArtifacts = ['.jekyll-cache', 'dist', 'build', '.next', 'node_modules/.cache'];
    for (const artifact of buildArtifacts) {
      const artifactPath = path.join(targetDir, artifact);
      try {
        await fs.access(artifactPath);
        categories.buildArtifacts.push(artifactPath);
      } catch {
        // Not found
      }
    }

    // Redundant dirs
    const redundantDirs = ['drafts', 'temp', 'tmp', 'backup', 'backups'];
    for (const dir of redundantDirs) {
      const dirPath = path.join(targetDir, dir);
      try {
        await fs.access(dirPath);
        categories.redundantDirs.push(dirPath);
      } catch {
        // Not found
      }
    }

    const totalItems = Object.values(categories).reduce((sum, arr) => sum + arr.length, 0);

    return {
      categories,
      totalItems,
      summary: {
        venvs: categories.venvs.length,
        tempFiles: categories.tempFiles.length,
        outputFiles: categories.outputFiles.length,
        buildArtifacts: categories.buildArtifacts.length,
        redundantDirs: categories.redundantDirs.length,
      },
    };
  }

  /**
   * Run actual cleanup using bash script
   * @private
   */
  async #runCleanup(targetDir) {
    // Run cleanup non-interactively by piping "yes"
    const { stdout, stderr } = await execFileAsync('bash', [
      '-c',
      `echo "yes" | ${this.scriptPath} "${targetDir}"`
    ], { maxBuffer: 10 * 1024 * 1024 });

    // Parse output to extract results
    const result = this.#parseCleanupOutput(stdout);

    if (stderr) {
      logger.warn({ stderr }, 'Cleanup script stderr');
    }

    return result;
  }

  /**
   * Parse cleanup script output
   * @private
   */
  #parseCleanupOutput(output) {
    const summary = {
      venvs: 0,
      tempFiles: 0,
      outputFiles: 0,
      buildArtifacts: 0,
      redundantDirs: 0,
    };

    // Extract counts from output
    const venvMatch = output.match(/Removed (\d+) virtual environment/);
    if (venvMatch) summary.venvs = parseInt(venvMatch[1], 10);

    const tempMatch = output.match(/Removed (\d+) temporary file/);
    if (tempMatch) summary.tempFiles = parseInt(tempMatch[1], 10);

    const outputMatch = output.match(/Removed (\d+) output file/);
    if (outputMatch) summary.outputFiles = parseInt(outputMatch[1], 10);

    const buildMatch = output.match(/Removed (\d+) build artifact/);
    if (buildMatch) summary.buildArtifacts = parseInt(buildMatch[1], 10);

    const dirMatch = output.match(/Removed (\d+) redundant director/);
    if (dirMatch) summary.redundantDirs = parseInt(dirMatch[1], 10);

    const totalItems = Object.values(summary).reduce((sum, val) => sum + val, 0);

    return {
      totalItems,
      summary,
      rawOutput: output,
    };
  }

  /**
   * Get directory size
   * @private
   */
  async #getDirectorySize(dirPath) {
    try {
      const { stdout } = await execFileAsync('du', ['-sh', dirPath]);
      return stdout.trim().split('\t')[0];
    } catch {
      return 'N/A';
    }
  }

  /**
   * Format size difference
   * @private
   */
  #formatSizeDiff(initial, final) {
    // Simple string comparison for now
    return `${initial} -> ${final}`;
  }

  /**
   * Create a job to clean a specific repository
   */
  createCleanupJob(targetDir, options = {}) {
    const jobId = `repo-cleanup-${Date.now()}`;

    return this.createJob(jobId, {
      type: 'repo-cleanup',
      targetDir: targetDir || this.baseDir,
      dryRun: options.dryRun ?? false,
    });
  }

  /**
   * Create a dry-run job to preview cleanup
   */
  createDryRunJob(targetDir, options = {}) {
    const jobId = `repo-cleanup-dryrun-${Date.now()}`;

    return this.createJob(jobId, {
      type: 'repo-cleanup',
      targetDir: targetDir || this.baseDir,
      dryRun: true,
    });
  }
}
</file>

<file path="workers/repomix-worker.js">
// @ts-nocheck
import { SidequestServer } from '../core/server.js';
import { generateReport } from '../utils/report-generator.js';
import { spawn } from 'child_process';
import fs from 'fs/promises';
import path from 'path';
import os from 'os';
import { createComponentLogger } from '../utils/logger.js';
import { execSync } from 'child_process';
import { config } from '../core/config.js';

const logger = createComponentLogger('RepomixWorker');

/**
 * RepomixWorker - Executes repomix jobs
 *
 * Respects .gitignore files by default - any directories or files listed
 * in .gitignore will be automatically excluded from processing.
 *
 * Options:
 * - respectGitignore: Respect .gitignore files (default: true)
 * - additionalIgnorePatterns: Array of additional patterns to ignore (default: [])
 * - outputBaseDir: Base directory for output files
 * - codeBaseDir: Base directory for source code
 */
export class RepomixWorker extends SidequestServer {
  constructor(options = {}) {
    super(options);
    this.outputBaseDir = options.outputBaseDir || './condense';
    this.codeBaseDir = options.codeBaseDir || path.join(os.homedir(), 'code');

    // Gitignore handling (respects .gitignore by default)
    this.respectGitignore = options.respectGitignore !== false; // Default: true

    // Default ignore patterns from config (includes README.md and markdown files)
    // Can be overridden via options or environment variable
    this.additionalIgnorePatterns = options.additionalIgnorePatterns || config.repomixIgnorePatterns || [];

    if (this.additionalIgnorePatterns.length > 0) {
      logger.info(
        { patterns: this.additionalIgnorePatterns },
        'RepomixWorker configured with ignore patterns (README files will be skipped)'
      );
    }

    // Pre-flight check: Verify repomix is available
    this.#verifyRepomixAvailable();
  }

  /**
   * Verify repomix is available via npx
   * Throws if repomix cannot be found
   * @private
   */
  #verifyRepomixAvailable() {
    try {
      execSync('npx repomix --version', {
        stdio: 'ignore',
        timeout: 30000, // 30 seconds - allow time during heavy system load
        env: process.env, // Inherit full PATH from parent
      });
      logger.info('Pre-flight check: repomix is available');
    } catch (error) {
      // ETIMEDOUT can happen under heavy load - treat as available but warn
      if (error.code === 'ETIMEDOUT') {
        logger.warn('Pre-flight check timed out after 30s - assuming repomix is available');
        return;
      }
      const errorMessage =
        'repomix is not available. Please install it:\n' +
        '  npm install\n' +
        'Or verify package.json includes "repomix" dependency.\n' +
        `PATH: ${process.env.PATH}`;
      logger.error({ error }, errorMessage);
      throw new Error(errorMessage);
    }
  }

  /**
   * Run repomix for a specific directory
   */
  async runJobHandler(job) {
    const startTime = Date.now();
    const { sourceDir, relativePath } = job.data;

    // Create output directory matching the source structure
    const outputDir = path.join(this.outputBaseDir, relativePath);
    await fs.mkdir(outputDir, { recursive: true });

    const outputFile = path.join(outputDir, 'repomix-output.txt');

    logger.info({ jobId: job.id, sourceDir, outputFile }, 'Running repomix');

    try {
      const { stdout, stderr } = await this.#runRepomixCommand(sourceDir);

      // Save the output to the appropriate location
      await fs.writeFile(outputFile, stdout);

      if (stderr) {
        logger.warn({ jobId: job.id, stderr }, 'Repomix warnings');
      }

      const result = {
        sourceDir,
        outputFile,
        relativePath,
        size: (await fs.stat(outputFile)).size,
        timestamp: new Date().toISOString(),
      };

      // Generate HTML/JSON reports
      const endTime = Date.now();
      const repoName = path.basename(sourceDir);
      const reportPaths = await generateReport({
        jobId: job.id,
        jobType: 'repomix',
        status: 'completed',
        result,
        startTime,
        endTime,
        parameters: job.data,
        metadata: {
          repoName,
          outputSize: result.size
        }
      });

      result.reportPaths = reportPaths;
      logger.info({ reportPaths }, 'Repomix reports generated');

      return result;
    } catch (error) {
      // Even if command fails, try to save any output
      if (error.stdout) {
        await fs.writeFile(outputFile, error.stdout);
      }
      throw error;
    }
  }

  /**
   * Securely run repomix command using npx (prevents command injection)
   * Uses npx to ensure local repomix from node_modules is used
   *
   * By default, repomix respects .gitignore files and excludes:
   * - Files and directories listed in .gitignore
   * - Common patterns (node_modules, .git, etc.)
   *
   * @param {string} cwd - Current working directory to run repomix in
   * @private
   */
  #runRepomixCommand(cwd) {
    return new Promise((resolve, reject) => {
      // Build repomix arguments
      const args = ['repomix'];

      // Disable gitignore if requested (NOT recommended)
      if (!this.respectGitignore) {
        args.push('--no-gitignore');
        logger.warn({ cwd }, 'Running repomix with --no-gitignore (not recommended)');
      }

      // Add additional ignore patterns if specified
      if (this.additionalIgnorePatterns.length > 0) {
        args.push('--ignore', this.additionalIgnorePatterns.join(','));
        logger.info(
          { cwd, patterns: this.additionalIgnorePatterns },
          'Adding additional ignore patterns'
        );
      }

      logger.debug({ cwd, args }, 'Spawning repomix with arguments');

      const proc = spawn('npx', args, {
        cwd,
        timeout: 600000, // 10 minute timeout
        maxBuffer: 50 * 1024 * 1024, // 50MB buffer
        env: process.env, // Inherit full PATH from parent to locate npx
      });

      let stdout = '';
      let stderr = '';

      proc.stdout.on('data', (data) => {
        stdout += data.toString();
      });

      proc.stderr.on('data', (data) => {
        stderr += data.toString();
      });

      proc.on('close', (code) => {
        if (code === 0) {
          resolve({ stdout, stderr });
        } else {
          const error = new Error(`repomix exited with code ${code}`);
          error.code = code;
          error.stdout = stdout;
          error.stderr = stderr;
          reject(error);
        }
      });

      proc.on('error', (error) => {
        error.stdout = stdout;
        error.stderr = stderr;
        reject(error);
      });
    });
  }

  /**
   * Create a repomix job for a directory
   */
  createRepomixJob(sourceDir, relativePath) {
    const jobId = `repomix-${relativePath.replace(/\//g, '-')}-${Date.now()}`;

    return this.createJob(jobId, {
      sourceDir,
      relativePath,
      type: 'repomix',
    });
  }
}
</file>

<file path="workers/schema-enhancement-worker.js">
import { SidequestServer } from '../core/server.js';
import { SchemaMCPTools } from '../utils/schema-mcp-tools.js';
import { generateReport } from '../utils/report-generator.js';
import { createComponentLogger } from '../utils/logger.js';
import { config } from '../core/config.js';
import fs from 'fs/promises';
import path from 'path';

const logger = createComponentLogger('SchemaEnhancementWorker');

/**
 * SchemaEnhancementWorker - Enhances README files with Schema.org markup
 */
export class SchemaEnhancementWorker extends SidequestServer {
  constructor(options = {}) {
    // Enable git workflow with schema-specific settings
    super({
      ...options,
      jobType: 'schema-enhancement',
      gitWorkflowEnabled: options.gitWorkflowEnabled ?? config.enableGitWorkflow,
      gitBranchPrefix: options.gitBranchPrefix || config.gitBranchPrefix || 'docs',
      gitBaseBranch: options.gitBaseBranch || config.gitBaseBranch,
      gitDryRun: options.gitDryRun ?? config.gitDryRun
    });

    this.outputBaseDir = options.outputBaseDir || './document-enhancement-impact-measurement';
    this.mcpTools = new SchemaMCPTools(options);
    this.dryRun = options.dryRun || false;
    this.stats = {
      enhanced: 0,
      skipped: 0,
      failed: 0,
    };
  }

  /**
   * Run enhancement for a specific README file
   */
  async runJobHandler(job) {
    const startTime = Date.now();
    const { readmePath, relativePath, context } = job.data;

    logger.info({ jobId: job.id, readmePath }, 'Enhancing README');

    try {
      // Read README content
      const originalContent = await fs.readFile(readmePath, 'utf-8');

      // Check if already has schema
      if (originalContent.includes('<script type="application/ld+json">')) {
        logger.info({ jobId: job.id }, 'Skipped - already has schema markup');
        this.stats.skipped++;
        return {
          status: 'skipped',
          reason: 'Already has schema markup',
          readmePath,
          relativePath,
        };
      }

      // Get appropriate schema type
      const schemaType = await this.mcpTools.getSchemaType(
        readmePath,
        originalContent,
        context
      );

      logger.info({ jobId: job.id, schemaType }, 'Schema type detected');

      // Generate schema markup
      const schema = await this.mcpTools.generateSchema(
        readmePath,
        originalContent,
        context,
        schemaType
      );

      // Validate schema
      const validation = await this.mcpTools.validateSchema(schema);
      if (!validation.valid) {
        throw new Error(`Schema validation failed: ${validation.errors.join(', ')}`);
      }

      if (validation.warnings.length > 0) {
        logger.warn({
          jobId: job.id,
          warnings: validation.warnings
        }, 'Schema validation warnings');
      }

      // Inject schema into content
      const enhancedContent = this.mcpTools.injectSchema(originalContent, schema);

      // Analyze impact
      const impact = await this.mcpTools.analyzeSchemaImpact(
        originalContent,
        enhancedContent,
        schema
      );

      logger.info({
        jobId: job.id,
        impactScore: impact.impactScore,
        rating: impact.rating
      }, 'Impact analysis complete');

      // Save enhanced README
      if (!this.dryRun) {
        await fs.writeFile(readmePath, enhancedContent, 'utf-8');
        logger.info({ jobId: job.id }, 'Enhanced README saved');
      } else {
        logger.info({ jobId: job.id }, 'Dry run - no changes made');
      }

      // Save impact report
      await this.saveImpactReport(relativePath, schema, impact);

      // Save enhanced copy to output directory
      await this.saveEnhancedCopy(relativePath, enhancedContent);

      this.stats.enhanced++;

      const result = {
        status: 'enhanced',
        readmePath,
        relativePath,
        schemaType,
        schema,
        impact,
        validation,
        timestamp: new Date().toISOString(),
      };

      // Generate HTML/JSON reports
      const endTime = Date.now();
      const reportPaths = await generateReport({
        jobId: job.id,
        jobType: 'schema-enhancement',
        status: 'completed',
        result,
        startTime,
        endTime,
        parameters: job.data,
        metadata: {
          schemaType,
          impactScore: impact.impactScore,
          rating: impact.rating
        }
      });

      result.reportPaths = reportPaths;
      logger.info({ reportPaths }, 'Schema enhancement reports generated');

      return result;

    } catch (error) {
      this.stats.failed++;
      throw error;
    }
  }

  /**
   * Save impact report to output directory
   */
  async saveImpactReport(relativePath, schema, impact) {
    const reportDir = path.join(
      this.outputBaseDir,
      'impact-reports',
      path.dirname(relativePath)
    );

    await fs.mkdir(reportDir, { recursive: true });

    const reportPath = path.join(
      reportDir,
      `${path.basename(relativePath, '.md')}-impact.json`
    );

    const report = {
      relativePath,
      schema,
      impact,
      timestamp: new Date().toISOString(),
    };

    await fs.writeFile(reportPath, JSON.stringify(report, null, 2));
  }

  /**
   * Save enhanced copy to output directory
   */
  async saveEnhancedCopy(relativePath, enhancedContent) {
    const outputDir = path.join(
      this.outputBaseDir,
      'enhanced-readmes',
      path.dirname(relativePath)
    );

    await fs.mkdir(outputDir, { recursive: true });

    const outputPath = path.join(outputDir, path.basename(relativePath));

    await fs.writeFile(outputPath, enhancedContent, 'utf-8');
  }

  /**
   * Find git repository root from a directory path
   * Walks up the directory tree until .git is found
   * @private
   */
  async findGitRoot(startPath) {
    let currentPath = startPath;
    const root = path.parse(currentPath).root;

    while (currentPath !== root) {
      try {
        const gitPath = path.join(currentPath, '.git');
        const stats = await fs.stat(gitPath);

        if (stats.isDirectory()) {
          return currentPath;
        }
      } catch (error) {
        // .git not found, continue up
      }

      currentPath = path.dirname(currentPath);
    }

    // No git repository found
    return null;
  }

  /**
   * Create an enhancement job for a README
   */
  async createEnhancementJob(readme, context) {
    const jobId = `schema-${readme.relativePath.replace(/\//g, '-')}-${Date.now()}`;

    // Find git repository root for this README
    const repositoryPath = await this.findGitRoot(readme.dirPath);

    return this.createJob(jobId, {
      readmePath: readme.fullPath,
      relativePath: readme.relativePath,
      repositoryPath, // Add repository path for git workflow
      repository: repositoryPath ? path.basename(repositoryPath) : null,
      context,
      type: 'schema-enhancement',
    });
  }

  /**
   * Generate commit message for schema enhancement
   * @override
   * @protected
   */
  async _generateCommitMessage(job) {
    const { relativePath, context } = job.data;
    const impact = job.result?.impact;

    const title = `docs: add Schema.org structured data to ${path.basename(relativePath)}`;

    const bodyParts = [
      'Added Schema.org JSON-LD markup to enhance SEO and enable rich results.',
      ''
    ];

    if (job.result?.schemaType) {
      bodyParts.push(`- Schema type: ${job.result.schemaType}`);
    }

    if (impact) {
      bodyParts.push(
        `- Impact score: ${impact.impactScore}/100 (${impact.rating})`,
        `- ${impact.seoImprovements.length} SEO improvements: ${impact.seoImprovements.slice(0, 3).join(', ')}${impact.seoImprovements.length > 3 ? '...' : ''}`
      );

      if (impact.richResultsEligibility.length > 0) {
        bodyParts.push(`- Eligible for ${impact.richResultsEligibility.join(', ')} rich results in search engines`);
      }
    }

    return {
      title,
      body: bodyParts.join('\n')
    };
  }

  /**
   * Generate PR context for schema enhancement
   * @override
   * @protected
   */
  async _generatePRContext(job) {
    const commitMessage = await this._generateCommitMessage(job);
    const { relativePath, repository } = job.data;
    const impact = job.result?.impact;
    const schema = job.result?.schema;

    const bodyParts = [
      '## Summary',
      '',
      `This PR adds Schema.org structured data to \`${relativePath}\` to improve SEO and enable rich search results.`,
      ''
    ];

    if (impact) {
      bodyParts.push(
        '## Impact Analysis',
        '',
        `- **Impact Score**: ${impact.impactScore}/100 (${impact.rating})`,
        `- **SEO Improvements**: ${impact.seoImprovements.length}`,
        ''
      );

      if (impact.seoImprovements.length > 0) {
        bodyParts.push(
          '### SEO Enhancements',
          ...impact.seoImprovements.map(imp => `- ${imp}`),
          ''
        );
      }

      if (impact.richResultsEligibility.length > 0) {
        bodyParts.push(
          '### Rich Results Eligibility',
          ...impact.richResultsEligibility.map(result => `- ${result}`),
          ''
        );
      }
    }

    if (schema) {
      bodyParts.push(
        '## Schema Details',
        '',
        `- **Type**: \`${schema['@type']}\``,
        `- **Properties**: ${Object.keys(schema).length} properties defined`,
        ''
      );
    }

    bodyParts.push(
      '## Testing',
      '',
      '- [ ] Validate schema markup using [Google Rich Results Test](https://search.google.com/test/rich-results)',
      '- [ ] Verify structured data using [Schema.org validator](https://validator.schema.org/)',
      '',
      '---',
      '',
      'ü§ñ Generated with [Claude Code](https://claude.com/claude-code)'
    );

    return {
      branchName: job.git.branchName,
      title: commitMessage.title,
      body: bodyParts.join('\n'),
      labels: ['documentation', 'seo', 'schema-org', 'automated']
    };
  }

  /**
   * Get enhancement statistics
   */
  getEnhancementStats() {
    return {
      ...this.stats,
      total: this.stats.enhanced + this.stats.skipped + this.stats.failed,
      successRate: this.stats.enhanced > 0
        ? ((this.stats.enhanced / (this.stats.enhanced + this.stats.failed)) * 100).toFixed(2)
        : 0,
    };
  }

  /**
   * Generate enhancement summary report
   */
  async generateSummaryReport() {
    const stats = this.getEnhancementStats();
    const jobStats = this.getStats();

    const summary = {
      timestamp: new Date().toISOString(),
      enhancement: stats,
      jobs: jobStats,
      outputDirectory: this.outputBaseDir,
    };

    const summaryPath = path.join(
      this.outputBaseDir,
      `enhancement-summary-${Date.now()}.json`
    );

    await fs.mkdir(this.outputBaseDir, { recursive: true });
    await fs.writeFile(summaryPath, JSON.stringify(summary, null, 2));

    return summary;
  }
}
</file>

<file path="workers/test-refactor-worker.ts">
/**
 * Test Refactoring Worker
 *
 * Extends SidequestServer to handle test suite refactoring jobs
 * within the AlephAuto framework.
 *
 * Features:
 * - Analyzes test files for duplication patterns
 * - Generates modular utility files (assertions, validators, helpers)
 * - Extracts hardcoded strings to constants
 * - Creates E2E fixtures for Playwright
 * - Optional PR creation for refactoring suggestions
 * - Comprehensive metrics tracking
 */

import { SidequestServer } from '../core/server.js';
import { createComponentLogger } from '../utils/logger.js';
import { glob } from 'glob';
import path from 'path';
import fs from 'fs/promises';
import * as Sentry from '@sentry/node';

const logger = createComponentLogger('TestRefactorWorker');

// Type definitions
interface TestRefactorWorkerOptions {
  maxConcurrent?: number;
  logDir?: string;
  gitWorkflowEnabled?: boolean;
  gitBranchPrefix?: string;
  testsDir?: string;
  utilsDir?: string;
  e2eDir?: string;
  framework?: 'vitest' | 'jest' | 'playwright';
  dryRun?: boolean;
  sentryDsn?: string;
}

interface JobData {
  repositoryPath: string;
  repository: string;
  testsDir: string;
  utilsDir: string;
  e2eDir: string;
  framework: string;
  dryRun: boolean;
}

interface Job {
  id: string;
  data: JobData;
  result?: JobResult;
}

interface AnalysisPatterns {
  renderWaitFor: number;
  linkValidation: number;
  semanticChecks: number;
  formInteractions: number;
  hardcodedStrings: string[];
  duplicateAssertions: string[];
}

interface Analysis {
  testFiles: string[];
  patterns: AnalysisPatterns;
  recommendations: string[];
}

interface JobResult {
  status: string;
  reason?: string;
  analysis?: Analysis;
  testFiles?: number;
  generatedFiles?: string[];
  recommendations?: string[];
}

interface Metrics {
  totalProjects: number;
  successfulRefactors: number;
  failedRefactors: number;
  filesGenerated: number;
  patternsDetected: number;
  stringsExtracted: number;
  recommendationsGenerated: number;
}

interface Stats {
  total: number;
  queued: number;
  active: number;
  completed: number;
  failed: number;
}

/**
 * TestRefactorWorker
 *
 * Handles test suite refactoring jobs with pattern detection,
 * utility generation, and comprehensive reporting.
 */
export class TestRefactorWorker extends SidequestServer {
  testsDir: string;
  utilsDir: string;
  e2eDir: string;
  framework: string;
  dryRun: boolean;
  metrics: Metrics;
  declare queue: string[];

  constructor(options: TestRefactorWorkerOptions = {}) {
    super({
      maxConcurrent: options.maxConcurrent || 3,
      logDir: path.join(process.cwd(), 'logs', 'test-refactor'),
      gitWorkflowEnabled: options.gitWorkflowEnabled ?? false,
      gitBranchPrefix: options.gitBranchPrefix || 'test-refactor',
      jobType: 'test-refactor',
      ...options
    });

    this.testsDir = options.testsDir || 'tests';
    this.utilsDir = options.utilsDir || 'tests/utils';
    this.e2eDir = options.e2eDir || 'tests/e2e';
    this.framework = options.framework || 'vitest';
    this.dryRun = options.dryRun ?? false;
    this.queue = [];

    this.metrics = {
      totalProjects: 0,
      successfulRefactors: 0,
      failedRefactors: 0,
      filesGenerated: 0,
      patternsDetected: 0,
      stringsExtracted: 0,
      recommendationsGenerated: 0
    };
  }

  /**
   * Queue a project for test refactoring
   */
  queueProject(projectPath: string, options: Partial<JobData> = {}): Job {
    const jobId = `refactor-${path.basename(projectPath)}-${Date.now()}`;

    return this.createJob(jobId, {
      repositoryPath: projectPath,
      repository: path.basename(projectPath),
      testsDir: options.testsDir || this.testsDir,
      utilsDir: options.utilsDir || this.utilsDir,
      e2eDir: options.e2eDir || this.e2eDir,
      framework: options.framework || this.detectFramework(projectPath),
      dryRun: options.dryRun ?? this.dryRun
    });
  }

  /**
   * Detect test framework from package.json
   */
  detectFramework(projectPath: string): string {
    try {
      const packageJsonPath = path.join(projectPath, 'package.json');
      const packageJson = JSON.parse(require('fs').readFileSync(packageJsonPath, 'utf-8'));
      const deps = { ...packageJson.dependencies, ...packageJson.devDependencies };

      if (deps['vitest']) return 'vitest';
      if (deps['@playwright/test']) return 'playwright';
      return 'jest';
    } catch {
      return this.framework;
    }
  }

  /**
   * Execute the refactoring job
   */
  async runJobHandler(job: Job): Promise<JobResult> {
    const { repositoryPath, testsDir, utilsDir, e2eDir, framework, dryRun } = job.data;
    this.metrics.totalProjects++;

    logger.info({
      jobId: job.id,
      project: job.data.repository,
      framework
    }, 'Starting test refactoring');

    try {
      // Find test files
      const testFiles = await this.findTestFiles(repositoryPath, testsDir);

      if (testFiles.length === 0) {
        logger.warn({ jobId: job.id }, 'No test files found');
        return { status: 'skipped', reason: 'No test files found' };
      }

      // Analyze test files
      const analysis = await this.analyzeTestFiles(repositoryPath, testFiles);
      this.metrics.patternsDetected += Object.values(analysis.patterns).reduce(
        (a: number, b) => typeof b === 'number' ? a + b : a, 0);
      this.metrics.stringsExtracted += analysis.patterns.hardcodedStrings.length;
      this.metrics.recommendationsGenerated += analysis.recommendations.length;

      if (dryRun) {
        logger.info({ jobId: job.id, analysis }, 'Dry run - analysis complete');
        return { status: 'dry-run', analysis };
      }

      // Generate utility files
      const generatedFiles = await this.generateUtilityFiles(
        repositoryPath,
        utilsDir,
        e2eDir,
        framework,
        analysis
      );

      this.metrics.filesGenerated += generatedFiles.length;
      this.metrics.successfulRefactors++;

      logger.info({
        jobId: job.id,
        filesGenerated: generatedFiles.length,
        recommendations: analysis.recommendations.length
      }, 'Test refactoring completed');

      return {
        status: 'completed',
        testFiles: testFiles.length,
        analysis,
        generatedFiles,
        recommendations: analysis.recommendations
      };

    } catch (error) {
      this.metrics.failedRefactors++;
      throw error;
    }
  }

  /**
   * Find all test files in the project
   */
  async findTestFiles(projectPath: string, testsDir: string): Promise<string[]> {
    const patterns = [
      `${testsDir}/**/*.test.{ts,tsx,js,jsx}`,
      `${testsDir}/**/*.spec.{ts,tsx,js,jsx}`,
      `src/**/*.test.{ts,tsx,js,jsx}`,
      `src/**/*.spec.{ts,tsx,js,jsx}`
    ];

    const files: string[] = [];

    for (const pattern of patterns) {
      const matches = await glob(pattern, {
        cwd: projectPath,
        ignore: ['**/node_modules/**']
      });
      files.push(...matches);
    }

    return [...new Set(files)];
  }

  /**
   * Analyze test files for refactoring opportunities
   */
  async analyzeTestFiles(projectPath: string, testFiles: string[]): Promise<Analysis> {
    const result: Analysis = {
      testFiles,
      patterns: {
        renderWaitFor: 0,
        linkValidation: 0,
        semanticChecks: 0,
        formInteractions: 0,
        hardcodedStrings: [],
        duplicateAssertions: []
      },
      recommendations: []
    };

    const stringCounts = new Map<string, number>();
    const assertionCounts = new Map<string, number>();

    for (const file of testFiles) {
      const filePath = path.join(projectPath, file);
      const content = await fs.readFile(filePath, 'utf-8');

      // Count patterns
      const renderWaitMatches = content.match(/render\s*\([^)]+\);\s*await\s+waitFor/g);
      result.patterns.renderWaitFor += renderWaitMatches?.length || 0;

      const linkMatches = content.match(/toHaveAttribute\s*\(\s*['"]href['"]/g);
      result.patterns.linkValidation += linkMatches?.length || 0;

      const semanticMatches = content.match(/getByRole\s*\(\s*['"]heading['"]|querySelector\s*\(\s*['"]section#/g);
      result.patterns.semanticChecks += semanticMatches?.length || 0;

      const formMatches = content.match(/userEvent\.type|fireEvent\.click|getByRole\s*\(\s*['"]form['"]/g);
      result.patterns.formInteractions += formMatches?.length || 0;

      // Extract hardcoded strings
      const stringMatches = content.matchAll(/getByText\s*\(\s*['"]([^'"]+)['"]\)/g);
      for (const match of stringMatches) {
        if (match[1]) {
          const count = stringCounts.get(match[1]) || 0;
          stringCounts.set(match[1], count + 1);
        }
      }

      // Extract assertion patterns
      const assertionMatches = content.matchAll(/(expect\([^)]+\)\.[^;]+);/g);
      for (const match of assertionMatches) {
        const assertion = match[1].replace(/['"][^'"]+['"]/g, '""');
        const count = assertionCounts.get(assertion) || 0;
        assertionCounts.set(assertion, count + 1);
      }
    }

    // Find duplicated strings (3+ occurrences)
    for (const [str, count] of stringCounts) {
      if (count >= 3 && str.length > 5) {
        result.patterns.hardcodedStrings.push(str);
      }
    }

    // Find duplicated assertions (3+ occurrences)
    for (const [assertion, count] of assertionCounts) {
      if (count >= 3) {
        result.patterns.duplicateAssertions.push(assertion);
      }
    }

    // Generate recommendations
    if (result.patterns.renderWaitFor > 5) {
      result.recommendations.push('Create renderAndWait helper to reduce render + waitFor boilerplate');
    }
    if (result.patterns.linkValidation > 5) {
      result.recommendations.push('Create link assertion helpers (expectExternalLink, expectInternalLink, etc.)');
    }
    if (result.patterns.semanticChecks > 5) {
      result.recommendations.push('Create semantic validators (expectSectionWithId, expectHeadingLevel, etc.)');
    }
    if (result.patterns.formInteractions > 5) {
      result.recommendations.push('Create form helpers (fillContactForm, expectFormAccessibility, etc.)');
    }
    if (result.patterns.hardcodedStrings.length > 10) {
      result.recommendations.push('Extract hardcoded strings to test-constants.ts');
    }

    return result;
  }

  /**
   * Generate utility files based on analysis
   */
  async generateUtilityFiles(
    projectPath: string,
    utilsDir: string,
    e2eDir: string,
    framework: string,
    analysis: Analysis
  ): Promise<string[]> {
    const utilsPath = path.join(projectPath, utilsDir);
    const generatedFiles: string[] = [];

    // Ensure utils directory exists
    await fs.mkdir(utilsPath, { recursive: true });

    // Generate assertions.ts
    if (analysis.patterns.linkValidation > 0) {
      const assertionsPath = path.join(utilsPath, 'assertions.ts');
      if (!await this.fileExists(assertionsPath)) {
        await fs.writeFile(assertionsPath, this.generateAssertionsContent(framework));
        generatedFiles.push('assertions.ts');
      }
    }

    // Generate semantic-validators.ts
    if (analysis.patterns.semanticChecks > 0) {
      const validatorsPath = path.join(utilsPath, 'semantic-validators.ts');
      if (!await this.fileExists(validatorsPath)) {
        await fs.writeFile(validatorsPath, this.generateSemanticValidatorsContent(framework));
        generatedFiles.push('semantic-validators.ts');
      }
    }

    // Generate form-helpers.ts
    if (analysis.patterns.formInteractions > 0) {
      const formHelpersPath = path.join(utilsPath, 'form-helpers.ts');
      if (!await this.fileExists(formHelpersPath)) {
        await fs.writeFile(formHelpersPath, this.generateFormHelpersContent());
        generatedFiles.push('form-helpers.ts');
      }
    }

    // Generate test-constants.ts
    if (analysis.patterns.hardcodedStrings.length > 0) {
      const constantsPath = path.join(utilsPath, 'test-constants.ts');
      if (!await this.fileExists(constantsPath)) {
        await fs.writeFile(constantsPath, this.generateConstantsContent(analysis.patterns.hardcodedStrings));
        generatedFiles.push('test-constants.ts');
      }
    }

    // Generate index.ts
    const indexPath = path.join(utilsPath, 'index.ts');
    if (!await this.fileExists(indexPath) && generatedFiles.length > 0) {
      await fs.writeFile(indexPath, this.generateIndexContent(generatedFiles));
      generatedFiles.push('index.ts');
    }

    // Generate E2E fixtures if e2e directory exists
    const e2ePath = path.join(projectPath, e2eDir);
    if (await this.fileExists(e2ePath)) {
      const fixturesPath = path.join(e2ePath, 'fixtures');
      await fs.mkdir(fixturesPath, { recursive: true });

      const navFixturesPath = path.join(fixturesPath, 'navigation.ts');
      if (!await this.fileExists(navFixturesPath)) {
        await fs.writeFile(navFixturesPath, this.generateE2EFixturesContent());
        generatedFiles.push('e2e/fixtures/navigation.ts');
      }
    }

    return generatedFiles;
  }

  /**
   * Check if file exists
   */
  async fileExists(filePath: string): Promise<boolean> {
    try {
      await fs.access(filePath);
      return true;
    } catch {
      return false;
    }
  }

  /**
   * Generate assertions.ts content
   */
  generateAssertionsContent(framework: string): string {
    const importStatement = framework === 'vitest'
      ? "import { expect } from 'vitest';"
      : "import { expect } from '@jest/globals';";

    return `/**
 * Test Assertions Utilities
 * Generated by AlephAuto TestRefactorWorker
 */

${importStatement}

export function expectExternalLink(element: HTMLElement, href: string) {
  const link = element.closest('a');
  expect(link).toHaveAttribute('href', href);
  expect(link).toHaveAttribute('target', '_blank');
  expect(link).toHaveAttribute('rel', 'noopener noreferrer');
}

export function expectInternalLink(element: HTMLElement, href: string) {
  const link = element.closest('a');
  expect(link).toHaveAttribute('href', href);
}

export function expectMailtoLink(element: HTMLElement, email: string) {
  const link = element.closest('a');
  expect(link).toHaveAttribute('href', \`mailto:\${email}\`);
}

export function expectSectionLink(element: HTMLElement, sectionId: string) {
  expectInternalLink(element, \`#\${sectionId}\`);
}

export function expectAriaLabel(element: HTMLElement, label: string) {
  expect(element).toHaveAttribute('aria-label', label);
}
`;
  }

  /**
   * Generate semantic-validators.ts content
   */
  generateSemanticValidatorsContent(framework: string): string {
    const importStatement = framework === 'vitest'
      ? "import { expect } from 'vitest';"
      : "import { expect } from '@jest/globals';";

    return `/**
 * Semantic Validation Utilities
 * Generated by AlephAuto TestRefactorWorker
 */

import { screen } from '@testing-library/react';
${importStatement}

export function expectSectionWithId(id: string) {
  const section = document.querySelector(\`section#\${id}\`);
  expect(section).toBeInTheDocument();
  return section;
}

export function expectHeadingLevel(level: 1 | 2 | 3 | 4 | 5 | 6, name: string) {
  const heading = screen.getByRole('heading', { level, name });
  expect(heading).toBeInTheDocument();
  return heading;
}

export function expectArticleCount(count: number) {
  const articles = screen.getAllByRole('article');
  expect(articles).toHaveLength(count);
  return articles;
}

export function expectContentInfo() {
  const footer = screen.getByRole('contentinfo');
  expect(footer).toBeInTheDocument();
  return footer;
}

export function expectHeadingCount(level: 1 | 2 | 3 | 4 | 5 | 6, count: number) {
  const headings = screen.getAllByRole('heading', { level });
  expect(headings).toHaveLength(count);
  return headings;
}

export function expectListCount(minCount: number) {
  const lists = screen.getAllByRole('list');
  expect(lists.length).toBeGreaterThanOrEqual(minCount);
  return lists;
}

export function expectImageWithAlt(altText: string) {
  const img = screen.getByAltText(altText);
  expect(img).toBeInTheDocument();
  return img;
}
`;
  }

  /**
   * Generate form-helpers.ts content
   */
  generateFormHelpersContent(): string {
    return `/**
 * Form Testing Utilities
 * Generated by AlephAuto TestRefactorWorker
 */

import { screen, within, waitFor } from '@testing-library/react';
import { expect } from 'vitest';
import type { UserEvent } from '@testing-library/user-event';

export function getForm() {
  return screen.getByRole('form');
}

export function getFormInputs(form: HTMLElement) {
  return within(form).getAllByRole('textbox');
}

export function getSubmitButton(form: HTMLElement) {
  return within(form).getByRole('button', { name: /send|submit/i });
}

export async function fillContactForm(
  user: UserEvent,
  data: { name: string; email: string; organization?: string; message?: string }
) {
  const form = getForm();
  const inputs = getFormInputs(form);

  await user.type(inputs[0], data.name);
  await user.type(inputs[1], data.email);

  if (data.organization && inputs[2]) {
    await user.type(inputs[2], data.organization);
  }

  return { form, inputs };
}

export function expectFormAccessibility(form: HTMLElement) {
  expect(form).toHaveAttribute('noValidate');
  expect(form).toHaveAttribute('aria-describedby');
}

export async function waitForForm() {
  await waitFor(() => {
    expect(screen.getByRole('form')).toBeInTheDocument();
  });
  return getForm();
}
`;
  }

  /**
   * Generate test-constants.ts content
   */
  generateConstantsContent(hardcodedStrings: string[]): string {
    const uniqueStrings = [...new Set(hardcodedStrings)].slice(0, 20);

    return `/**
 * Test Constants
 * Generated by AlephAuto TestRefactorWorker
 *
 * TODO: Organize these strings into meaningful groups
 */

export const EXTRACTED_STRINGS = [
${uniqueStrings.map(s => `  '${s.replace(/'/g, "\\'")}',`).join('\n')}
] as const;

// Example constant groups - customize for your project:

export const NAV_LINKS = [
  { label: 'Home', href: '/' },
  { label: 'About', href: '#about' },
] as const;

export const APP_INFO = {
  name: 'Your App Name',
  email: 'contact@example.com',
} as const;

export const A11Y_LABELS = {
  openMenu: 'Open menu',
  closeMenu: 'Close menu',
} as const;
`;
  }

  /**
   * Generate index.ts content
   */
  generateIndexContent(generatedFiles: string[]): string {
    const exports = generatedFiles
      .filter(f => f !== 'index.ts' && !f.includes('/'))
      .map(f => `export * from './${f.replace('.ts', '')}';`)
      .join('\n');

    return `/**
 * Test Utilities Index
 * Generated by AlephAuto TestRefactorWorker
 */

${exports}
`;
  }

  /**
   * Generate E2E fixtures content
   */
  generateE2EFixturesContent(): string {
    return `/**
 * E2E Navigation Fixtures
 * Generated by AlephAuto TestRefactorWorker
 */

import { Page, expect } from '@playwright/test';

export async function navigateToSection(page: Page, section: string) {
  await page.click(\`text=\${section}\`);
  await expect(page.url()).toContain(\`#\${section.toLowerCase()}\`);
}

export async function goToHomepage(page: Page) {
  await page.goto('/');
}

export async function setMobileViewport(page: Page) {
  await page.setViewportSize({ width: 375, height: 667 });
}

export async function setTabletViewport(page: Page) {
  await page.setViewportSize({ width: 768, height: 1024 });
}

export async function setDesktopViewport(page: Page) {
  await page.setViewportSize({ width: 1280, height: 720 });
}

export async function openMobileMenu(page: Page) {
  await page.click('[aria-label="Open menu"]');
  await expect(page.locator('[aria-label="Close menu"]')).toBeVisible();
}

export async function expectTextVisible(page: Page, text: string) {
  await expect(page.locator(\`text=\${text}\`)).toBeVisible();
}

export async function expectAllTextVisible(page: Page, texts: string[]) {
  for (const text of texts) {
    await expect(page.locator(\`text=\${text}\`)).toBeVisible();
  }
}

export async function clickExternalLink(page: Page, linkText: string) {
  const [popup] = await Promise.all([
    page.waitForEvent('popup'),
    page.click(\`text=\${linkText}\`)
  ]);
  return popup;
}
`;
  }

  /**
   * Override commit message generation
   */
  async _generateCommitMessage(job: Job): Promise<{ title: string; body: string }> {
    const result = job.result;
    return {
      title: `refactor(tests): add modular test utilities`,
      body: `Automated test refactoring to reduce duplication and improve maintainability.

## Changes
- Generated ${result?.generatedFiles?.length || 0} utility files
- Analyzed ${result?.testFiles || 0} test files
- Detected ${this.metrics.patternsDetected} refactoring opportunities

## Recommendations
${result?.recommendations?.map(r => `- ${r}`).join('\n') || 'None'}

Files generated: ${result?.generatedFiles?.join(', ') || 'None'}`
    };
  }

  /**
   * Get metrics
   */
  getMetrics(): Metrics {
    return { ...this.metrics };
  }

  /**
   * Get stats - inherited from SidequestServer
   */
  getStats(): Stats {
    return super.getStats();
  }
}

export default TestRefactorWorker;
</file>

<file path=".env.example">
# AlephAuto Configuration
# Copy this file to .env and customize for your environment

# ============================================
# Base Directories
# ============================================

# Base directory containing all code repositories to process
# Default: ~/code
CODE_BASE_DIR=/Users/username/code

# Output directory for repomix condensed code
# Default: ./output/condense (relative to project root)
OUTPUT_BASE_DIR=./output/condense

# Directory for application logs
# Default: ./logs
LOG_DIR=./logs

# Directory for directory scan reports
# Default: ./output/directory-scan-reports
SCAN_REPORTS_DIR=./output/directory-scan-reports

# ============================================
# Job Processing
# ============================================

# Maximum number of concurrent jobs (1-50)
# Default: 5
MAX_CONCURRENT=5

# ============================================
# Monitoring & Error Tracking
# ============================================

# Sentry DSN for error tracking (optional)
# Get from: https://sentry.io/settings/projects/
SENTRY_DSN=https://your-sentry-dsn@sentry.io/project-id

# Node environment (development, production)
# Default: production
NODE_ENV=production

# ============================================
# Scheduling
# ============================================

# Cron schedule for repomix pipeline (default: 2 AM daily)
# Format: minute hour day month weekday
# Examples:
#   "0 2 * * *"  - 2 AM daily
#   "0 */6 * * *" - Every 6 hours
#   "0 0 * * 0"  - Midnight every Sunday
CRON_SCHEDULE="0 2 * * *"

# Cron schedule for documentation enhancement pipeline (default: 3 AM daily)
DOC_CRON_SCHEDULE="0 3 * * *"

# Run jobs immediately on startup (true/false)
# Default: false
RUN_ON_STARTUP=false

# ============================================
# Repomix Settings
# ============================================

# Timeout for repomix command in milliseconds (default: 10 minutes)
REPOMIX_TIMEOUT=600000

# Maximum buffer size for repomix output in bytes (default: 50MB)
REPOMIX_MAX_BUFFER=52428800

# ============================================
# Schema.org Integration
# ============================================

# URL for Schema.org MCP server (optional)
SCHEMA_MCP_URL=http://localhost:3001

# Force schema enhancement even if schema already exists
# Default: false
FORCE_ENHANCEMENT=false

# ============================================
# Logging
# ============================================

# Log level: debug, info, warn, error
# Default: info
LOG_LEVEL=info

# ============================================
# Health Check
# ============================================

# Port for health check HTTP server
# Default: 3000
HEALTH_CHECK_PORT=3000
</file>

<file path=".gitignore">
# Dependencies
node_modules/

# Logs
logs/
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# Output artifacts
output/
directory-scan-reports/
gitignore-update-report-*.json
repomix-output.xml
doc-enhancement/repomix-output.xml

# Environment
.env
.env.local
.env.*.local

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# OS
.DS_Store
Thumbs.db

# Sentry
.sentryclirc

# Testing
coverage/
.vitest/

# Build
dist/
build/
*.tsbuildinfo

# Temporary files
*.tmp
*.temp
</file>

<file path="git-report-config.json">
{
  "description": "Configuration for automated git activity reports",
  "version": "1.0.0",

  "scanning": {
    "code_directory": "~/code",
    "max_depth": 2,
    "exclude_patterns": [
      "vim/bundle",
      "node_modules",
      ".git",
      "venv",
      ".venv",
      "vendor",
      "target",
      "build",
      "dist"
    ]
  },

  "reports": {
    "weekly": {
      "enabled": true,
      "schedule": "Sunday 20:00",
      "days_back": 7,
      "auto_commit": false,
      "output_format": "markdown"
    },
    "monthly": {
      "enabled": true,
      "schedule": "1st of month 08:00",
      "days_back": 30,
      "auto_commit": false,
      "output_format": "markdown"
    },
    "quarterly": {
      "enabled": false,
      "schedule": "First Monday of quarter",
      "days_back": 90,
      "auto_commit": false,
      "output_format": "markdown"
    }
  },

  "output": {
    "personalsite_dir": "~/code/PersonalSite",
    "work_collection": "_work",
    "visualization_dir": "assets/images/git-activity-{year}",
    "json_cache_dir": "/tmp",
    "log_dir": "~/code/jobs/sidequest/logs"
  },

  "project_categories": {
    "Data & Analytics": {
      "keywords": ["scraper", "analytics", "bot", "data"],
      "description": "Data collection, scraping, analytics pipelines"
    },
    "Personal Sites": {
      "keywords": ["personalsite", "github.io", "portfolio", "blog"],
      "description": "Personal websites and blogs"
    },
    "Infrastructure": {
      "keywords": ["integrity", "studio", "visualizer", "tool", "dotfiles"],
      "description": "DevOps, tooling, development infrastructure"
    },
    "MCP Servers": {
      "keywords": ["mcp", "server", "context", "protocol"],
      "description": "Model Context Protocol server implementations"
    },
    "Client Work": {
      "keywords": ["client", "leora", "jobs"],
      "description": "Client projects and career tracking"
    },
    "Business Apps": {
      "keywords": ["inventory", "financial", "business"],
      "description": "Business applications and SaaS products"
    },
    "Legacy": {
      "keywords": ["old", "archive", "deprecated"],
      "description": "Archived or minimal-maintenance projects",
      "min_commits": 0,
      "max_commits": 5
    }
  },

  "visualizations": {
    "enabled": true,
    "formats": ["svg"],
    "charts": {
      "monthly_commits": {
        "enabled": true,
        "type": "pie",
        "filename": "monthly-commits.svg",
        "width": 800,
        "height": 600
      },
      "project_categories": {
        "enabled": true,
        "type": "pie",
        "filename": "project-categories.svg",
        "width": 800,
        "height": 600
      },
      "top_repositories": {
        "enabled": true,
        "type": "bar",
        "filename": "top-10-repos.svg",
        "width": 800,
        "height": 500,
        "count": 10
      },
      "language_distribution": {
        "enabled": true,
        "type": "pie",
        "filename": "language-distribution.svg",
        "width": 900,
        "height": 600
      }
    },
    "color_schemes": {
      "default": ["#0066cc", "#4da6ff", "#99ccff", "#00994d", "#ffcc00", "#ff6600", "#cc0000", "#9966cc", "#66cc99", "#ff6699"]
    }
  },

  "language_mapping": {
    "Python": [".py", ".pyw"],
    "JavaScript": [".js", ".mjs", ".cjs"],
    "TypeScript": [".ts", ".tsx"],
    "Ruby": [".rb", ".rake", ".gemspec"],
    "HTML": [".html", ".htm"],
    "CSS/SCSS": [".css", ".scss", ".sass", ".less"],
    "Markdown": [".md", ".markdown"],
    "JSON": [".json"],
    "YAML": [".yml", ".yaml"],
    "Shell": [".sh", ".bash", ".zsh"],
    "SQL": [".sql"],
    "Go": [".go"],
    "Rust": [".rs"],
    "C/C++": [".c", ".cpp", ".cc", ".h", ".hpp"],
    "Java": [".java"],
    "PHP": [".php"],
    "Lock Files": [".lock", "package-lock.json", "Gemfile.lock", "yarn.lock", "pnpm-lock.yaml"],
    "SVG": [".svg"],
    "Images": [".png", ".jpg", ".jpeg", ".gif", ".webp", ".ico", ".bmp"],
    "Data Files": [".csv", ".xml", ".tsv", ".parquet"],
    "Text Files": [".txt", ".log", ".ini", ".conf"]
  },

  "notifications": {
    "enabled": false,
    "methods": {
      "email": {
        "enabled": false,
        "to": "your-email@example.com",
        "subject": "Weekly Git Activity Report"
      },
      "slack": {
        "enabled": false,
        "webhook_url": "",
        "channel": "#dev-updates"
      },
      "discord": {
        "enabled": false,
        "webhook_url": ""
      }
    }
  },

  "git": {
    "auto_commit": false,
    "commit_message_template": "Add {period} git activity report: {start_date} to {end_date}\n\nü§ñ Generated with [Claude Code](https://claude.com/claude-code)\n\nCo-Authored-By: Claude <noreply@anthropic.com>",
    "auto_push": false
  },

  "cron": {
    "weekly": "0 20 * * 0",
    "monthly": "0 8 1 * *",
    "quarterly": "0 8 1 1,4,7,10 *"
  },

  "notes": [
    "This configuration file controls the automated git activity report generation.",
    "Edit the values above to customize the behavior for your workflow.",
    "The git activity reporter is now integrated with AlephAuto job queue framework.",
    "Usage: npm run git:weekly (weekly), npm run git:monthly (monthly), npm run git:schedule (scheduled mode)",
    "For production: pm2 start git-activity-pipeline.js --name git-activity",
    "Environment variable: GIT_CRON_SCHEDULE='0 20 * * 0' for custom schedule"
  ]
}
</file>

<file path="plugin-management-audit.sh">
#!/usr/bin/env bash
#
# Plugin Management Audit Script
# Analyzes enabled plugins and identifies potential cleanup opportunities
#
# Usage: ./plugin-management-audit.sh [--json] [--detailed]
#
# Note: Requires bash 4+ for associative arrays
#       macOS users: brew install bash

set -euo pipefail

# Check bash version
if [[ "${BASH_VERSINFO[0]}" -lt 4 ]]; then
    echo "Error: This script requires bash 4 or higher (current: $BASH_VERSION)"
    echo "macOS users can install with: brew install bash"
    echo "Then run with: /usr/local/bin/bash $0 $@"
    exit 1
fi

CLAUDE_CONFIG="${HOME}/.claude/settings.json"
OUTPUT_FORMAT="human"
DETAILED=false

# Parse arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        --json)
            OUTPUT_FORMAT="json"
            shift
            ;;
        --detailed)
            DETAILED=true
            shift
            ;;
        *)
            echo "Unknown option: $1"
            echo "Usage: $0 [--json] [--detailed]"
            exit 1
            ;;
    esac
done

# Check if config exists
if [[ ! -f "$CLAUDE_CONFIG" ]]; then
    echo "Error: Claude config not found at $CLAUDE_CONFIG"
    exit 1
fi

# Get enabled plugins
ENABLED_PLUGINS=$(jq -r '.enabledPlugins | keys[]' "$CLAUDE_CONFIG" 2>/dev/null || echo "")
PLUGIN_COUNT=$(echo "$ENABLED_PLUGINS" | grep -v '^$' | wc -l | tr -d ' ')

# Identify potential duplicates by category
declare -A categories
while IFS= read -r plugin; do
    [[ -z "$plugin" ]] && continue

    # Extract category hints from plugin names
    if [[ "$plugin" =~ documentation|document ]]; then
        categories["documentation"]+="$plugin "
    elif [[ "$plugin" =~ git|github ]]; then
        categories["git"]+="$plugin "
    elif [[ "$plugin" =~ test|testing ]]; then
        categories["testing"]+="$plugin "
    elif [[ "$plugin" =~ deploy|deployment ]]; then
        categories["deployment"]+="$plugin "
    elif [[ "$plugin" =~ lint|format ]]; then
        categories["linting"]+="$plugin "
    elif [[ "$plugin" =~ docker|container ]]; then
        categories["containers"]+="$plugin "
    elif [[ "$plugin" =~ api|rest|graphql ]]; then
        categories["api"]+="$plugin "
    elif [[ "$plugin" =~ db|database|sql ]]; then
        categories["database"]+="$plugin "
    fi
done <<< "$ENABLED_PLUGINS"

# Generate output
if [[ "$OUTPUT_FORMAT" == "json" ]]; then
    # JSON output
    echo "{"
    echo "  \"total_enabled\": $PLUGIN_COUNT,"
    echo "  \"enabled_plugins\": ["
    first=true
    while IFS= read -r plugin; do
        [[ -z "$plugin" ]] && continue
        [[ "$first" == false ]] && echo ","
        echo -n "    \"$plugin\""
        first=false
    done <<< "$ENABLED_PLUGINS"
    echo ""
    echo "  ],"
    echo "  \"potential_duplicates\": {"
    first=true
    for category in "${!categories[@]}"; do
        count=$(echo "${categories[$category]}" | wc -w | tr -d ' ')
        if [[ $count -gt 1 ]]; then
            [[ "$first" == false ]] && echo ","
            echo -n "    \"$category\": ["
            inner_first=true
            for plugin in ${categories[$category]}; do
                [[ "$inner_first" == false ]] && echo -n ", "
                echo -n "\"$plugin\""
                inner_first=false
            done
            echo -n "]"
            first=false
        fi
    done
    echo ""
    echo "  }"
    echo "}"
else
    # Human-readable output
    echo "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó"
    echo "‚ïë          Claude Code Plugin Management Audit                   ‚ïë"
    echo "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù"
    echo ""
    echo "Total Enabled Plugins: $PLUGIN_COUNT"
    echo ""

    if [[ "$DETAILED" == true ]]; then
        echo "Enabled Plugins:"
        echo "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ"
        while IFS= read -r plugin; do
            [[ -z "$plugin" ]] && continue
            echo "  ‚Ä¢ $plugin"
        done <<< "$ENABLED_PLUGINS"
        echo ""
    fi

    # Show potential duplicates
    has_duplicates=false
    for category in "${!categories[@]}"; do
        count=$(echo "${categories[$category]}" | wc -w | tr -d ' ')
        if [[ $count -gt 1 ]]; then
            has_duplicates=true
        fi
    done

    if [[ "$has_duplicates" == true ]]; then
        echo "‚ö†Ô∏è  Potential Duplicate Categories:"
        echo "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ"
        for category in "${!categories[@]}"; do
            count=$(echo "${categories[$category]}" | wc -w | tr -d ' ')
            if [[ $count -gt 1 ]]; then
                echo ""
                echo "  Category: $category ($count plugins)"
                for plugin in ${categories[$category]}; do
                    echo "    ‚Ä¢ $plugin"
                done
            fi
        done
        echo ""
        echo "üí° Consider reviewing these categories for consolidation."
    else
        echo "‚úÖ No obvious duplicate categories detected."
    fi

    echo ""
    echo "Recommendations:"
    echo "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ"

    if [[ $PLUGIN_COUNT -gt 30 ]]; then
        echo "  ‚Ä¢ High plugin count ($PLUGIN_COUNT). Consider disabling unused plugins."
    fi

    if [[ "$has_duplicates" == true ]]; then
        echo "  ‚Ä¢ Review duplicate categories above."
        echo "  ‚Ä¢ Keep only the plugins you actively use in each category."
    fi

    echo "  ‚Ä¢ Run: npm run status to see plugin usage statistics"
    echo "  ‚Ä¢ Backup before changes: npm run backup"
    echo ""
fi

# Exit with status
if [[ "$has_duplicates" == true ]] || [[ $PLUGIN_COUNT -gt 30 ]]; then
    exit 1
else
    exit 0
fi
</file>

<file path="repomix.config.json">
{
  "$schema": "https://repomix.com/schemas/latest/schema.json",
  "input": {
    "maxFileSize": 52428800
  },
  "output": {
    "filePath": "repomix-output.xml",
    "style": "xml",
    "parsableStyle": false,
    "fileSummary": true,
    "directoryStructure": true,
    "files": true,
    "removeComments": false,
    "removeEmptyLines": false,
    "compress": false,
    "topFilesLength": 5,
    "showLineNumbers": false,
    "truncateBase64": false,
    "copyToClipboard": false,
    "includeFullDirectoryStructure": false,
    "tokenCountTree": false,
    "git": {
      "sortByChanges": true,
      "sortByChangesMaxCommits": 100,
      "includeDiffs": false,
      "includeLogs": false,
      "includeLogsCount": 50
    }
  },
  "include": [],
  "ignore": {
    "useGitignore": true,
    "useDefaultPatterns": true,
    "customPatterns": [
      "logs/**",
      "node_modules/**",
      "*.log",
      "directory-scan-reports/**",
      "document-enhancement-impact-measurement/**",
      "**/go/pkg/mod/**",
      "**/pyenv/**",
      "**/python/pyenv/**",
      "**/vim/bundle/**",
      "**/vim/autoload/**",
      "repomix-output.xml",
      "repomix-output.txt"
    ]
  },
  "security": {
    "enableSecurityCheck": true
  },
  "tokenCount": {
    "encoding": "o200k_base"
  }
}
</file>

</files>
