This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where comments have been removed, empty lines have been removed, content has been compressed (code blocks are separated by ⋮---- delimiter).

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: logs/**, node_modules/**, *.log, directory-scan-reports/**, document-enhancement-impact-measurement/**, **/go/pkg/mod/**, **/pyenv/**, **/python/pyenv/**, **/vim/bundle/**, **/vim/autoload/**, repomix-output.xml, repomix-output.txt, **/README.md, **/README.MD, **/*.md
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Code comments have been removed from supported file types
- Empty lines have been removed from all files
- Content has been compressed - code blocks are separated by ⋮---- delimiter
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.claude/
  settings.local.json
core/
  config.ts
  constants.ts
  database.ts
  git-workflow-manager.ts
  index.ts
  job-repository.ts
  server.ts
pipeline-core/
  .claude/
    settings.local.json
  annotators/
    __init__.py
    __init__.pyi
    semantic_annotator.py
    semantic_annotator.pyi
    test_semantic_annotator.py
  cache/
    cached-scanner.js
    git-tracker.js
    scan-cache.js
  config/
    repository-config-loader.js
  errors/
    error-classifier.ts
  extractors/
    extract_blocks.py
    test_extract_blocks.py
  git/
    branch-manager.ts
    migration-transformer.ts
    pr-creator.ts
  models/
    __init__.py
    __init__.pyi
    code_block.py
    code_block.pyi
    consolidation_suggestion.py
    consolidation_suggestion.pyi
    duplicate_group.py
    duplicate_group.pyi
    scan_report.py
    scan_report.pyi
    test_models.py
  reports/
    html-report-generator.js
    json-report-generator.js
    markdown-report-generator.js
    report-coordinator.js
  scanners/
    ast-grep-detector.js
    codebase-health-scanner.js
    repository-scanner.js
    root-directory-analyzer.js
    timeout_detector.py
    timeout-pattern-detector.js
  similarity/
    __init__.py
    __init__.pyi
    config.py
    config.pyi
    grouping.py
    grouping.pyi
    semantic.py
    semantic.pyi
    structural.py
    structural.pyi
    test_grouping_layer3.py
  types/
    scan-orchestrator-types.ts
  utils/
    __init__.py
    __init__.pyi
    error-helpers.ts
    fs-helpers.ts
    index.ts
    process-helpers.ts
    timing-helpers.ts
    timing.py
    timing.pyi
  doppler-health-monitor.js
  inter-project-scanner.js
  py.typed
  scan-orchestrator.ts
pipeline-runners/
  bugfix-audit-pipeline.js
  claude-health-pipeline.js
  collect_git_activity.py
  dashboard-populate-pipeline.js
  duplicate-detection-pipeline.ts
  git-activity-pipeline.js
  gitignore-pipeline.js
  plugin-management-audit.sh
  plugin-management-pipeline.js
  repo-cleanup-pipeline.js
  schema-enhancement-pipeline.js
  test-refactor-pipeline.ts
  universal-repo-cleanup.sh
types/
  duplicate-detection-types.ts
utils/
  dependency-validator.js
  directory-scanner.js
  doppler-resilience.example.js
  doppler-resilience.js
  gitignore-repomix-updater.js
  logger.ts
  pipeline-names.ts
  plugin-manager.js
  refactor-test-suite.ts
  report-generator.js
  schema-mcp-tools.js
  time-helpers.ts
workers/
  bugfix-audit-worker.js
  claude-health-worker.js
  dashboard-populate-worker.js
  duplicate-detection-worker.js
  git-activity-worker.js
  gitignore-worker.js
  repo-cleanup-worker.js
  repomix-worker.js
  schema-enhancement-worker.js
  test-refactor-worker.ts
.env.example
.gitignore
git-report-config.json
repomix.config.json
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "Bash(node gitignore-repomix-updater.js:*)",
      "Bash(mv:*)",
      "Bash(chmod:*)",
      "Bash(doppler run:*)",
      "Bash(npm run cleanup:dryrun:*)",
      "Bash(node:*)",
      "Bash(npm run cleanup:once:*)",
      "Bash(find:*)",
      "Bash(npm run typecheck:*)",
      "Bash(curl:*)",
      "Bash(pm2 logs:*)",
      "Bash(pm2 info:*)",
      "Bash(npm test:*)",
      "Bash(npm run test:integration:*)"
    ],
    "deny": [],
    "ask": []
  }
}
</file>

<file path="core/config.ts">
import path from 'path';
import os from 'os';
import { fileURLToPath } from 'url';
import { dirname } from 'path';
⋮----
function safeParseInt(value: string | undefined, defaultValue: number, min?: number, max?: number): number
function safeParseFloat(value: string | undefined, defaultValue: number, min?: number, max?: number): number
⋮----
function validateConfig(): void
</file>

<file path="core/constants.ts">

</file>

<file path="core/database.ts">
import Database from 'better-sqlite3';
type DatabaseType = InstanceType<typeof Database>;
import path from 'path';
import fs from 'fs';
import { fileURLToPath } from 'url';
import { createComponentLogger, logMetrics } from '../utils/logger.ts';
import { isValidJobStatus } from '../../api/types/job-status.ts';
import { VALIDATION } from './constants.ts';
⋮----
export interface JobRow {
  id: string;
  pipeline_id: string;
  status: string;
  data: string | null;
  result: string | null;
  error: string | null;
  git: string | null;
  created_at: string;
  started_at: string | null;
  completed_at: string | null;
}
export interface ParsedJobError {
  message: string;
  stack?: string;
  code?: string;
  cancelled?: boolean;
}
function isParsedJobError(value: unknown): value is ParsedJobError
export interface ParsedJob {
  id: string;
  pipelineId: string;
  status: string;
  data: unknown;
  result: unknown;
  error: ParsedJobError | null;
  git: unknown;
  createdAt: string;
  startedAt: string | null;
  completedAt: string | null;
}
export interface JobQueryOptions {
  status?: string;
  limit?: number;
  offset?: number;
  tab?: string;
  includeTotal?: boolean;
}
export interface AllJobsQueryOptions {
  status?: string;
  limit?: number;
  offset?: number;
}
export interface JobCounts {
  total: number;
  completed: number;
  failed: number;
  running: number;
  queued: number;
}
export interface PipelineStats {
  pipelineId: string;
  total: number;
  completed: number;
  failed: number;
  running: number;
  queued: number;
  lastRun: string | null;
}
export interface BulkImportResult {
  imported: number;
  skipped: number;
  errors: string[];
}
export interface HealthStatus {
  initialized: boolean;
  degradedMode: boolean;
  persistenceWorking: boolean;
  persistFailureCount: number;
  recoveryAttempts: number;
  queuedWrites: number;
  queueStalenessMs: number;
  dbPath: string;
  dbSizeBytes: number;
  memoryPressure: string;
  status: string;
  message: string;
}
export interface SaveJobInput {
  id: string;
  pipelineId?: string;
  status: string;
  createdAt?: string | null;
  startedAt?: string | null;
  completedAt?: string | null;
  data?: unknown;
  result?: unknown;
  error?: unknown;
  git?: unknown;
}
export interface BulkImportJob {
  id: string;
  pipeline_id?: string;
  pipelineId?: string;
  status: string;
  created_at?: string;
  createdAt?: string;
  started_at?: string | null;
  startedAt?: string | null;
  completed_at?: string | null;
  completedAt?: string | null;
  data?: unknown;
  result?: unknown;
  error?: unknown;
  git?: unknown;
}
function safeJsonParse(str: string | null, fallback: unknown = null): unknown
function rowToParsedJob(row: JobRow): ParsedJob
export async function initDatabase(dbPath?: string): Promise<DatabaseType>
export function getDatabase(): DatabaseType
export function isDatabaseReady(): boolean
export function saveJob(job: SaveJobInput): void
function queryAll(sql: string, params: (string | number)[] = []): JobRow[]
function queryOne<T = Record<string, unknown>>(sql: string, params: (string | number)[] = []): T | null
export function getJobs(pipelineId: string, options: JobQueryOptions =
export function getJobById(id: string): ParsedJob | null
export function getJobCount(options:
export function getAllJobs(options: AllJobsQueryOptions =
export function getJobCounts(pipelineId: string): JobCounts | null
export function getLastJob(pipelineId: string): ParsedJob | null
export function getAllPipelineStats(): PipelineStats[]
export async function importReportsToDatabase(reportsDir: string): Promise<number>
⋮----
// Check if already imported
⋮----
// Import as completed job
⋮----
export async function importLogsToDatabase(logsDir: string): Promise<number>
⋮----
// Check if already imported
⋮----
// Determine status from content
⋮----
export function getHealthStatus(): HealthStatus
export function closeDatabase(): void
function isValidJobId(id: string): boolean
export function bulkImportJobs(jobs: BulkImportJob[]): BulkImportResult
</file>

<file path="core/git-workflow-manager.ts">
import { BranchManager } from '../pipeline-core/git/branch-manager.ts';
import type { BranchManagerOptions, JobBranchContext, BranchResult, PRContext } from '../pipeline-core/git/branch-manager.ts';
import { createComponentLogger, logError, logWarn } from '../utils/logger.ts';
⋮----
export interface CommitMessage {
  title: string;
  body: string;
}
export interface MessageGenerator {
  generateCommitMessage(): Promise<CommitMessage>;
  generatePRContext(): Promise<PRContext>;
}
⋮----
generateCommitMessage(): Promise<CommitMessage>;
generatePRContext(): Promise<PRContext>;
⋮----
export interface GitInfo {
  branchName: string;
  originalBranch: string;
}
export interface CommitContext {
  message: string;
  description?: string;
  jobId?: string;
}
export interface WorkflowResult {
  changedFiles: string[];
  commitSha: string | null;
  prUrl: string | null;
}
export class GitWorkflowManager
⋮----
constructor(options: BranchManagerOptions =
async createJobBranch(repositoryPath: string, jobInfo: JobBranchContext): Promise<BranchResult | null>
async hasChanges(repositoryPath: string): Promise<boolean>
async getChangedFiles(repositoryPath: string): Promise<string[]>
async commitChanges(repositoryPath: string, commitContext: CommitContext): Promise<string | null>
async pushBranch(repositoryPath: string, branchName: string): Promise<boolean>
async createPullRequest(repositoryPath: string, prContext: PRContext): Promise<string | null>
async cleanupBranch(repositoryPath: string, branchName: string, originalBranch: string): Promise<void>
async executeWorkflow(repositoryPath: string, gitInfo: GitInfo, messageGenerator: MessageGenerator): Promise<WorkflowResult>
</file>

<file path="core/index.ts">
import cron from 'node-cron';
import { RepomixWorker } from '../workers/repomix-worker.js';
import { DirectoryScanner } from '../utils/directory-scanner.js';
import { config } from './config.ts';
import { TIMEOUTS, TIME } from './constants.ts';
import path from 'path';
import fs from 'fs/promises';
import { createComponentLogger, logError, logStart } from '../utils/logger.ts';
import type { Job, JobStats } from './server.ts';
⋮----
interface ScanDirectory {
  fullPath: string;
  relativePath: string;
}
interface TypedDirectoryScanner {
  baseDir: string;
  scanDirectories(): Promise<ScanDirectory[]>;
  generateAndSaveScanResults(dirs: ScanDirectory[]): Promise<{
    reportPath: string;
    treePath: string;
    summaryPath: string;
    summary: {
      maxDepth: number;
      stats: { topDirectoryNames: Array<{ name: string }> };
    };
  }>;
}
⋮----
scanDirectories(): Promise<ScanDirectory[]>;
generateAndSaveScanResults(dirs: ScanDirectory[]): Promise<
⋮----
interface TypedRepomixWorker {
  logDir: string;
  outputBaseDir: string;
  createRepomixJob(fullPath: string, relativePath: string): void;
  getStats(): JobStats;
  on(event: string, listener: (job: Job) => void): this;
}
⋮----
createRepomixJob(fullPath: string, relativePath: string): void;
getStats(): JobStats;
on(event: string, listener: (job: Job)
⋮----
class RepomixCronApp
⋮----
constructor()
private setupEventListeners(): void
async runRepomixOnAllDirectories(): Promise<void>
private async waitForCompletion(): Promise<void>
private async saveRunSummary(stats: JobStats, duration: number): Promise<void>
private setupCronJob(schedule: string = '0 2 * * *'): void
async start(): Promise<void>
</file>

<file path="core/job-repository.ts">
import {
  initDatabase,
  saveJob as dbSaveJob,
  getJobById as dbGetJobById,
  getJobCount as dbGetJobCount,
  getJobs as dbGetJobs,
  getAllJobs as dbGetAllJobs,
  getJobCounts as dbGetJobCounts,
  getLastJob as dbGetLastJob,
  getAllPipelineStats as dbGetAllPipelineStats,
  bulkImportJobs as dbBulkImportJobs,
  closeDatabase as dbCloseDatabase,
} from './database.ts';
import type {
  ParsedJob,
  ParsedJobError,
  SaveJobInput,
  JobQueryOptions,
  AllJobsQueryOptions,
  JobCounts,
  PipelineStats,
  BulkImportResult,
  BulkImportJob,
} from './database.ts';
import { createComponentLogger, logError } from '../utils/logger.ts';
⋮----
interface JobRepositoryOptions {
  autoInitialize?: boolean;
}
class JobRepository
⋮----
constructor(options: JobRepositoryOptions =
async initialize(): Promise<void>
saveJob(job: SaveJobInput): void
getJob(id: string): ParsedJob | null
getJobs(pipelineId: string, filters: JobQueryOptions =
getAllJobs(filters: AllJobsQueryOptions =
getJobCount(filters:
getJobCounts(pipelineId: string): JobCounts | null
getLastJob(pipelineId: string): ParsedJob | null
getAllPipelineStats(): PipelineStats[]
bulkImport(jobs: BulkImportJob[]): BulkImportResult
close(): void
reset(): void
⋮----
export function createJobRepository(options: JobRepositoryOptions =
</file>

<file path="core/server.ts">
import { EventEmitter } from 'events';
⋮----
import { config } from './config.ts';
import fs from 'fs/promises';
import path from 'path';
import { createComponentLogger, logError, logWarn } from '../utils/logger.ts';
import { safeErrorMessage } from '../pipeline-core/utils/error-helpers.ts';
import { GitWorkflowManager } from './git-workflow-manager.ts';
import { jobRepository } from './job-repository.ts';
import { CONCURRENCY, RETRY } from './constants.ts';
import { isRetryable, classifyError } from '../pipeline-core/errors/error-classifier.ts';
import { toISOString } from '../utils/time-helpers.ts';
import { JOB_STATUS, TERMINAL_STATUSES, isValidJobStatus } from '../../api/types/job-status.ts';
import type { JobStatus } from '../../api/types/job-status.ts';
⋮----
export interface JobGitMetadata {
  branchName: string | null;
  originalBranch: string | null;
  commitSha: string | null;
  prUrl: string | null;
  changedFiles: string[];
}
export interface Job {
  id: string;
  status: JobStatus;
  data: Record<string, unknown>;
  createdAt: Date;
  startedAt: Date | null;
  completedAt: Date | null;
  error: { message: string; stack?: string; code?: string; cancelled?: boolean } | null;
  result: unknown;
  retryCount: number;
  retryPending?: boolean;
  pausedAt?: Date;
  resumedAt?: Date;
  git: JobGitMetadata;
}
export interface JobStats {
  total: number;
  queued: number;
  active: number;
  completed: number;
  failed: number;
}
export interface SidequestServerOptions {
  maxConcurrent?: number;
  maxRetries?: number;
  logDir?: string;
  autoStart?: boolean;
  gitWorkflowEnabled?: boolean;
  gitBranchPrefix?: string;
  gitBaseBranch?: string;
  gitDryRun?: boolean;
  jobType?: string;
  sentryDsn?: string;
}
interface JobActionResult {
  success: boolean;
  message: string;
  job?: Job;
}
export class SidequestServer extends EventEmitter
⋮----
constructor(options: SidequestServerOptions =
createJob(jobId: string, jobData: Record<string, unknown>): Job
async processQueue(): Promise<void>
async executeJob(jobId: string): Promise<void>
private _persistJob(job: Job): void
private _prepareJobForExecution(job: Job): void
private async _setupGitBranchIfEnabled(job: Job): Promise<boolean>
private async _finalizeJobSuccess(job: Job, result: unknown, branchCreated: boolean): Promise<void>
private async _finalizeJobFailure(job: Job, error: unknown, branchCreated: boolean): Promise<void>
private async _handleGitWorkflowSuccess(job: Job): Promise<void>
async _generateCommitMessage(job: Job): Promise<
async _generatePRContext(job: Job, commitMessage?:
async runJobHandler(_job: Job): Promise<unknown>
async start(): Promise<void>
stop(): void
set handleJob(handler: (job: Job) => Promise<unknown>)
private async _writeJobLog(job: Job, suffix: string, extra?: Record<string, unknown>): Promise<void>
async logJobCompletion(job: Job): Promise<void>
async logJobFailure(job: Job, error: Error): Promise<void>
getJob(jobId: string): Job | undefined
getAllJobs(): Job[]
getStats(): JobStats
private _executeJobAction(jobId: string, config: {
    action: string;
statusGuard: (status: JobStatus)
cancelJob(jobId: string): JobActionResult
pauseJob(jobId: string): JobActionResult
resumeJob(jobId: string): JobActionResult
</file>

<file path="pipeline-core/.claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "Bash(rm:*)"
    ],
    "deny": [],
    "ask": []
  }
}
</file>

<file path="pipeline-core/annotators/__init__.py">
__all__ = ['SemanticAnnotation', 'SemanticAnnotator']
</file>

<file path="pipeline-core/annotators/__init__.pyi">
"""Type stubs for annotators package."""

from .semantic_annotator import (
    SemanticAnnotation as SemanticAnnotation,
    SemanticAnnotator as SemanticAnnotator,
)

__all__: list[str]
</file>

<file path="pipeline-core/annotators/semantic_annotator.py">
@dataclass
class SemanticAnnotation
⋮----
category: str
operations: set[str] = field(default_factory=set)
domains: set[str] = field(default_factory=set)
patterns: set[str] = field(default_factory=set)
data_types: set[str] = field(default_factory=set)
intent: str = ""
def all_tags(self) -> set[str]
⋮----
"""Return all semantic tags as a single set."""
⋮----
def to_dict(self) -> dict
⋮----
"""Convert to dictionary for serialization."""
⋮----
# ---------------------------------------------------------------------------
# Pattern Libraries
⋮----
# NOTE: All regex patterns use bounded quantifiers (\s{0,20} instead of \s*)
# to prevent ReDoS (Regular Expression Denial of Service) attacks. (H3 fix)
# Max 20 whitespace chars is sufficient for all valid code patterns.
# Array/collection operations
ARRAY_OPERATION_PATTERNS: dict[str, str] = {
# CRUD/data operations
CRUD_OPERATION_PATTERNS: dict[str, str] = {
# Transformation operations
TRANSFORM_OPERATION_PATTERNS: dict[str, str] = {
# Validation operations
VALIDATION_OPERATION_PATTERNS: dict[str, str] = {
# Domain patterns
DOMAIN_PATTERNS: dict[str, str] = {
# Code patterns
CODE_PATTERN_PATTERNS: dict[str, str] = {
⋮----
# Guard clause / early return
⋮----
# Null/undefined checks
⋮----
# Error handling
⋮----
# Retry logic
⋮----
# Timeout handling
⋮----
# Async patterns
⋮----
# Caching patterns
⋮----
# Pagination
⋮----
# Batching
⋮----
# Streaming
⋮----
# Locking/mutex
⋮----
# Rate limiting
⋮----
# Data type patterns
DATA_TYPE_PATTERNS: dict[str, str] = {
⋮----
# Array
⋮----
# Object
⋮----
# String
⋮----
_COMPILED_ARRAY_OPS = _compile_patterns(ARRAY_OPERATION_PATTERNS, re.IGNORECASE)
_COMPILED_CRUD_OPS = _compile_patterns(CRUD_OPERATION_PATTERNS, re.IGNORECASE)
_COMPILED_TRANSFORM_OPS = _compile_patterns(TRANSFORM_OPERATION_PATTERNS, re.IGNORECASE)
_COMPILED_VALIDATION_OPS = _compile_patterns(VALIDATION_OPERATION_PATTERNS, re.IGNORECASE)
_COMPILED_DOMAIN = _compile_patterns(DOMAIN_PATTERNS, re.IGNORECASE)
_COMPILED_CODE_PATTERNS = _compile_patterns(CODE_PATTERN_PATTERNS, re.IGNORECASE)
_COMPILED_DATA_TYPES = _compile_patterns(DATA_TYPE_PATTERNS)
_COMPILED_ALL_OPERATIONS = (
class SemanticAnnotator
⋮----
def __init__(self, collect_timing: bool = False) -> None
def get_timing_report(self) -> dict[str, dict]
def extract_annotation(self, block: CodeBlock) -> SemanticAnnotation
⋮----
code = block.source_code
tags = block.tags if hasattr(block, 'tags') else []
category = block.category if hasattr(block, 'category') else 'unknown'
⋮----
category = category.value
⋮----
operations = self._extract_operations(code)
⋮----
domains = self._extract_domains(code, tags)
⋮----
patterns = self._extract_patterns(code)
⋮----
data_types = self._extract_data_types(code)
⋮----
intent = self._infer_intent(operations, domains, patterns)
⋮----
def _extract_operations(self, code: str) -> set[str]
⋮----
operations: set[str] = set()
⋮----
def _extract_domains(self, code: str, tags: list[str]) -> set[str]
⋮----
domains: set[str] = set()
text = code + ' ' + ' '.join(tags)
⋮----
def _extract_patterns(self, code: str) -> set[str]
⋮----
patterns: set[str] = set()
⋮----
def _extract_data_types(self, code: str) -> set[str]
⋮----
data_types: set[str] = set()
⋮----
parts: list[str] = []
</file>

<file path="pipeline-core/annotators/semantic_annotator.pyi">
"""Type stubs for semantic_annotator module."""

from dataclasses import dataclass

from ..models.code_block import CodeBlock
from ..utils.timing import TimingMetrics


@dataclass
class SemanticAnnotation:
    """Rich semantic metadata for a code block."""

    category: str
    operations: set[str]
    domains: set[str]
    patterns: set[str]
    data_types: set[str]
    intent: str

    def all_tags(self) -> set[str]: ...
    def to_dict(self) -> dict: ...


class SemanticAnnotator:
    """Stage 4: Full semantic annotation of code blocks."""

    collect_timing: bool
    timing: dict[str, TimingMetrics]

    def __init__(self, collect_timing: bool = ...) -> None: ...
    def get_timing_report(self) -> dict[str, dict]: ...
    def extract_annotation(self, block: CodeBlock) -> SemanticAnnotation: ...


# Pattern dictionaries (module-level)
ARRAY_OPERATION_PATTERNS: dict[str, str]
CRUD_OPERATION_PATTERNS: dict[str, str]
TRANSFORM_OPERATION_PATTERNS: dict[str, str]
VALIDATION_OPERATION_PATTERNS: dict[str, str]
DOMAIN_PATTERNS: dict[str, str]
CODE_PATTERN_PATTERNS: dict[str, str]
DATA_TYPE_PATTERNS: dict[str, str]
</file>

<file path="pipeline-core/annotators/test_semantic_annotator.py">
@dataclass
class MockCodeBlock
⋮----
source_code: str
category: str = 'utility'
tags: list = None
def __post_init__(self)
class TestSemanticAnnotation
⋮----
def test_all_tags_combines_sets(self)
⋮----
ann = SemanticAnnotation(
all_tags = ann.all_tags()
⋮----
def test_to_dict_serialization(self)
⋮----
d = ann.to_dict()
⋮----
class TestOperationExtraction
⋮----
def test_array_filter_operation(self)
⋮----
annotator = SemanticAnnotator()
block = MockCodeBlock(source_code="users.filter(u => u.active)")
ann = annotator.extract_annotation(block)
⋮----
def test_array_map_operation(self)
⋮----
block = MockCodeBlock(source_code="items.map(i => i.name)")
⋮----
def test_array_reduce_operation(self)
⋮----
block = MockCodeBlock(source_code="nums.reduce((a, b) => a + b, 0)")
⋮----
def test_multiple_array_operations(self)
⋮----
block = MockCodeBlock(
⋮----
def test_for_loop_iterate(self)
⋮----
block = MockCodeBlock(source_code="for (const item of items) { }")
⋮----
def test_crud_read_operation(self)
⋮----
block = MockCodeBlock(source_code="await api.get('/users')")
⋮----
def test_crud_create_operation(self)
⋮----
block = MockCodeBlock(source_code="await db.create({ name })")
⋮----
def test_fetch_api(self)
⋮----
block = MockCodeBlock(source_code="await fetch('/api/data')")
⋮----
def test_json_parse(self)
⋮----
block = MockCodeBlock(source_code="const data = JSON.parse(text)")
⋮----
def test_json_stringify(self)
⋮----
block = MockCodeBlock(source_code="JSON.stringify(data, null, 2)")
⋮----
def test_validate_function(self)
⋮----
block = MockCodeBlock(source_code="if (!isValid(email)) return")
⋮----
class TestDomainExtraction
⋮----
def test_user_domain(self)
⋮----
block = MockCodeBlock(source_code="const user = await getUser(userId)")
⋮----
def test_auth_domain(self)
⋮----
block = MockCodeBlock(source_code="const token = await authenticate(credentials)")
⋮----
def test_payment_domain(self)
⋮----
block = MockCodeBlock(source_code="await stripe.createCharge(payment)")
⋮----
def test_database_domain(self)
⋮----
block = MockCodeBlock(source_code="await prisma.user.findMany()")
⋮----
def test_api_domain(self)
⋮----
block = MockCodeBlock(source_code="router.get('/endpoint', handler)")
⋮----
def test_multiple_domains(self)
def test_domain_from_tags(self)
class TestPatternExtraction
⋮----
def test_guard_clause(self)
⋮----
block = MockCodeBlock(source_code="if (!user) return null")
⋮----
def test_null_check_equality(self)
⋮----
block = MockCodeBlock(source_code="if (value === null) throw new Error()")
⋮----
def test_null_check_nullish_coalescing(self)
⋮----
block = MockCodeBlock(source_code="const name = user ?? 'anonymous'")
⋮----
def test_null_check_optional_chaining(self)
⋮----
block = MockCodeBlock(source_code="const name = user?.profile?.name")
⋮----
def test_error_handling_try_catch(self)
⋮----
block = MockCodeBlock(source_code="try { doWork() } catch (e) { }")
⋮----
def test_error_handling_promise_catch(self)
⋮----
block = MockCodeBlock(source_code="fetch(url).catch(handleError)")
⋮----
def test_async_await_pattern(self)
⋮----
block = MockCodeBlock(source_code="async function getData() { await fetch() }")
⋮----
def test_promise_chain_pattern(self)
⋮----
block = MockCodeBlock(source_code="fetch(url).then(r => r.json())")
⋮----
def test_promise_all(self)
⋮----
block = MockCodeBlock(source_code="await Promise.all([p1, p2])")
⋮----
def test_pagination_pattern(self)
⋮----
block = MockCodeBlock(source_code="const { page, limit } = query")
⋮----
class TestDataTypeExtraction
⋮----
def test_array_type(self)
⋮----
block = MockCodeBlock(source_code="const items = []")
⋮----
def test_object_type(self)
⋮----
block = MockCodeBlock(source_code="const config = {}")
⋮----
def test_string_type(self)
⋮----
block = MockCodeBlock(source_code="const name = 'test'")
⋮----
def test_promise_type(self)
⋮----
block = MockCodeBlock(source_code="new Promise((resolve) => resolve())")
⋮----
def test_date_type(self)
⋮----
block = MockCodeBlock(source_code="const now = new Date()")
⋮----
class TestIntentInference
⋮----
def test_simple_operation_intent(self)
⋮----
block = MockCodeBlock(source_code="items.filter(i => i.active)")
⋮----
def test_operation_with_domain_intent(self)
def test_full_intent(self)
class TestEdgeCases
⋮----
def test_empty_code(self)
⋮----
block = MockCodeBlock(source_code="")
⋮----
def test_whitespace_only(self)
⋮----
"""Test annotation of whitespace-only code."""
⋮----
block = MockCodeBlock(source_code="   \n\t  ")
⋮----
def test_category_enum_handling(self)
⋮----
"""Test handling of enum category values."""
⋮----
class MockCategory(Enum)
⋮----
UTILITY = 'utility'
⋮----
block = MockCodeBlock(source_code="test()")
⋮----
def run_tests()
⋮----
"""Run all tests without pytest."""
⋮----
test_classes = [
passed = 0
failed = 0
⋮----
instance = test_class()
⋮----
method = getattr(instance, name)
</file>

<file path="pipeline-core/cache/cached-scanner.js">
export class CachedScanner
⋮----
initializeCache(redisClient, cacheOptions =
async scanRepository(repoPath, options =
async _shouldUseCache(repoPath, repoStatus, options)
async _getCachedResult(repoPath, commitHash)
async _cacheResult(repoPath, commitHash, scanResult)
async invalidateCache(repoPath)
async getCacheStatus(repoPath)
async getStats()
async warmCache(repoPaths, options =
</file>

<file path="pipeline-core/cache/git-tracker.js">
export class GitCommitTracker
⋮----
async getRepositoryCommit(repoPath)
async getShortCommit(repoPath)
async hasChanged(repoPath, lastCommit)
async getChangedFiles(repoPath, fromCommit)
async getCommitMetadata(repoPath, commitHash = 'HEAD')
async getBranchName(repoPath)
async hasUncommittedChanges(repoPath)
async getRemoteUrl(repoPath, remoteName = 'origin')
async getCommitCount(repoPath)
async isGitRepository(repoPath)
async getRepositoryStatus(repoPath)
async getCommitHistory(repoPath, limit = 10)
</file>

<file path="pipeline-core/cache/scan-cache.js">
export class ScanResultCache
⋮----
_generateCacheKey(repoPath, commitHash)
async getCachedScan(repoPath, commitHash)
async cacheScan(repoPath, commitHash, scanResult, options =
async _addToIndex(cacheKey, repoPath, commitHash)
async invalidateCache(repoPath)
async getStats()
async listCachedScans(limit = 10)
async getCacheMetadata(repoPath, commitHash)
async clearAll()
async isCached(repoPath, commitHash)
async getCacheAge(repoPath, commitHash)
</file>

<file path="pipeline-core/config/repository-config-loader.js">
export class RepositoryConfigLoader
⋮----
async load()
async reload()
getScanConfig()
getAllRepositories()
getEnabledRepositories()
getRepositoriesByPriority(priority)
getRepositoriesByFrequency(frequency)
getRepositoriesByTag(tag)
getRepository(name)
getAllGroups()
getEnabledGroups()
getGroup(name)
getGroupRepositories(groupName)
getRepositoriesToScanTonight(maxRepos = null)
getScanDefaults()
getNotificationSettings()
async updateLastScanned(repoName, timestamp = null)
async addScanHistory(repoName, historyEntry)
async save()
validate()
getStats()
_ensureLoaded()
_expandPaths()
</file>

<file path="pipeline-core/errors/error-classifier.ts">
import { RETRY } from '../../core/constants.ts';
export interface HTTPError extends Error {
  statusCode?: number;
  status?: number;
}
export interface ExtendedError extends Error {
  code?: string | number;
  errno?: number | string;
  syscall?: string;
  path?: string;
  stdout?: string;
  stderr?: string;
  statusCode?: number;
  status?: number;
  response?: {
    status?: number;
    statusText?: string;
    data?: unknown;
  };
}
export interface ClassifiedError extends Error {
  retryable?: boolean;
  classification?: {
    category: 'network' | 'file_system' | 'http' | 'database' | 'unknown';
    retryable: boolean;
    severity: 'low' | 'medium' | 'high';
  };
  statusCode?: number;
  status?: number;
}
export interface ErrorClassification {
  category: string;
  reason: string;
  suggestedDelay: number;
}
interface ErrorCodeConfig {
  retryable: boolean;
  description: string;
  delay: number;
  group: 'filesystem' | 'network' | 'application' | 'http';
}
interface MessagePatternConfig {
  pattern: string;
  retryable: boolean;
  delay: number;
}
⋮----
function classifyByErrorCode(errorCode: string): ErrorClassification | null
function classifyByHttpStatus(statusCode: number): ErrorClassification | null
function classifyByMessagePattern(message: string): ErrorClassification | null
export function classifyError(error: Error | null | undefined): ErrorClassification
⋮----
// Priority 4: Default classification
⋮----
/**
 * Type guard: check if an error is an HTTPError.
 */
export function isHTTPError(error: Error): error is HTTPError
export function isClassifiedError(error: Error): error is ClassifiedError
export function isRetryable(error: Error): boolean
export interface ErrorInfo {
  name: string;
  message: string;
  code: string | number | undefined;
  statusCode: number | undefined;
  category: string;
  reason: string;
  suggestedDelay: number;
  retryable: boolean;
  stack: string | undefined;
  cause: unknown;
}
export function getErrorInfo(error: Error): ErrorInfo
function getErrorCategoryGroup(error: Error): 'network' | 'file_system' | 'http' | 'database' | 'unknown'
export function createScanError(message: string, cause: Error): Error
</file>

<file path="pipeline-core/extractors/extract_blocks.py">
class PatternMatchInput(BaseModel)
⋮----
file_path: str = Field(..., max_length=500, description="Relative file path")
rule_id: str = Field(..., max_length=100, description="ast-grep rule ID")
matched_text: str = Field(..., max_length=100_000, description="Matched source code")
line_start: int = Field(..., ge=1, le=1_000_000, description="Start line number")
line_end: int = Field(..., ge=1, le=1_000_000, description="End line number")
column_start: Optional[int] = Field(None, ge=0, le=10_000)
column_end: Optional[int] = Field(None, ge=0, le=10_000)
severity: Optional[str] = Field(None, max_length=20)
confidence: Optional[float] = Field(None, ge=0.0, le=1.0)
⋮----
@field_validator('file_path')
@classmethod
    def validate_file_path(cls, v: str) -> str
⋮----
@field_validator('line_end')
@classmethod
    def validate_line_range(cls, v: int, info) -> int
⋮----
line_start = info.data.get('line_start')
⋮----
class RepositoryInfoInput(BaseModel)
⋮----
path: str = Field(..., max_length=1000, description="Repository path")
name: Optional[str] = Field(None, max_length=200)
git_remote: Optional[str] = Field(None, max_length=500)
git_branch: Optional[str] = Field(None, max_length=200)
git_commit: Optional[str] = Field(None, max_length=50)
class PipelineInput(BaseModel)
⋮----
repository_info: RepositoryInfoInput
pattern_matches: List[PatternMatchInput] = Field(
model_config = {
⋮----
DEBUG = SimilarityConfig.DEBUG
def _debug(msg: str) -> None
⋮----
# ---------------------------------------------------------------------------
# Language Detection
⋮----
LANGUAGE_MAP: dict[str, str] = {
⋮----
# Pattern to Category Mapping (H4 fix: extract constant to reduce function length)
⋮----
PATTERN_CATEGORY_MAP: dict[str, str] = {
⋮----
# New categories for previously miscategorized patterns
⋮----
def detect_language(file_path: str) -> str
⋮----
"""Detect programming language from file extension.
    Args:
        file_path: Path to the source file (can be absolute or relative)
    Returns:
        Language identifier string (e.g., 'javascript', 'typescript', 'python')
        Returns 'unknown' if extension is not recognized.
    """
ext = Path(file_path).suffix.lower()
⋮----
# Function Name Extraction Patterns
⋮----
FUNCTION_NAME_PATTERNS: tuple[str, ...] = (
⋮----
r'function\s+(\w+)\s*\(',              # function name(
r'const\s+(\w+)\s*=\s*(?:async\s+)?function',  # const name = function
r'const\s+(\w+)\s*=\s*(?:async\s+)?\(',        # const name = ( or const name = async (
r'let\s+(\w+)\s*=\s*(?:async\s+)?function',    # let name = function
r'let\s+(\w+)\s*=\s*(?:async\s+)?\(',          # let name = (
r'var\s+(\w+)\s*=\s*(?:async\s+)?function',    # var name = function
r'var\s+(\w+)\s*=\s*(?:async\s+)?\(',          # var name = (
r'async\s+function\s+(\w+)\s*\(',      # async function name(
r'(\w+)\s*:\s*function',               # name: function
r'(\w+)\s*:\s*async\s+function',       # name: async function
r'export\s+function\s+(\w+)',          # export function name
r'export\s+const\s+(\w+)\s*=',         # export const name =
⋮----
def _match_function_pattern(text: str, multiline: bool = False) -> str | None
⋮----
"""Try to match function name patterns against text."""
flags = re.MULTILINE if multiline else 0
⋮----
match = re.search(pattern, text, flags)
⋮----
"""Search backwards in file to find function declaration."""
full_path = Path(repo_path) / file_path
⋮----
lines = full_path.read_text(encoding='utf-8').splitlines()
⋮----
# Search backwards from match line (up to 10 lines before)
search_start = max(0, line_start - 11)
⋮----
func_name = _match_function_pattern(lines[i])
⋮----
"""Extract function name from source code using regex patterns.
    Priority 1: Function-Level Extraction
    This enables proper matching between detected and expected duplicates.
    If function name can't be found in source_code, reads the actual file
    to get more context (lines before the match).
    """
⋮----
# Try to find function name in the matched source code
func_name = _match_function_pattern(source_code, multiline=True)
⋮----
# Fall back to reading file context if we have location info
⋮----
def _get_function_name_from_tags(tags: list[str]) -> str | None
⋮----
"""Extract function name from block tags."""
⋮----
return tag[9:]  # Remove 'function:' prefix
⋮----
"""Try to add block using function-based deduplication.
    Returns True if block was processed (added or skipped as duplicate).
    """
function_key = f"{block.location.file_path}:{function_name}"
⋮----
# Already seen this function - keep the earlier occurrence
existing_block = seen_functions[function_key]
⋮----
"""Add block using location-based deduplication."""
location_key = f"{block.location.file_path}:{block.location.line_start}"
⋮----
def deduplicate_blocks(blocks: list[CodeBlock]) -> list[CodeBlock]
⋮----
"""Remove duplicate code blocks from the same location and function.
    Priority 4: Deduplicate Pattern Matches
    ast-grep patterns can match the same code multiple times within a function.
    This removes duplicates based on file:function_name, keeping only the earliest match.
    """
seen_locations: set[str] = set()
seen_functions: dict[str, CodeBlock] = {}
unique_blocks: list[CodeBlock] = []
⋮----
function_name = _get_function_name_from_tags(block.tags)
⋮----
removed = len(blocks) - len(unique_blocks)
⋮----
def _create_code_block(match: Dict, repository_info: Dict) -> CodeBlock
⋮----
"""Create a CodeBlock from a pattern match (H4 fix: extracted helper)."""
# Generate unique block ID
block_key = f"{match['file_path']}:{match['line_start']}"
block_hash = hashlib.sha256(block_key.encode()).hexdigest()[:12]
block_id = f"cb_{block_hash}"
# Map pattern_id to category
category = PATTERN_CATEGORY_MAP.get(match['rule_id'], 'utility')
# Extract function name from source code
source_code = match.get('matched_text', '')
function_name = extract_function_name(
⋮----
def extract_code_blocks(pattern_matches: List[Dict], repository_info: Dict) -> List[CodeBlock]
⋮----
"""Extract CodeBlock models from pattern matches."""
⋮----
blocks = []
⋮----
block = _create_code_block(match, repository_info)
⋮----
def group_duplicates(blocks: List[CodeBlock]) -> List[DuplicateGroup]
⋮----
"""
    Group similar code blocks using multi-layer similarity algorithm.
    Priority 2: Structural Similarity
    Uses the enhanced grouping algorithm that combines:
    - Layer 1: Exact matching (hash-based)
    - Layer 2: Structural similarity (AST-based)
    - Layer 3: Semantic equivalence (TODO)
    """
# Use the multi-layer grouping algorithm
groups = group_by_similarity(blocks, similarity_threshold=0.85)
⋮----
def generate_suggestions(groups: List[DuplicateGroup]) -> List[ConsolidationSuggestion]
⋮----
"""
    Generate consolidation suggestions with enhanced strategy logic
    """
suggestions = []
⋮----
# Determine strategy based on multiple factors
⋮----
# Generate migration steps
migration_steps = _generate_migration_steps(group, strategy)
# Generate code example
code_example = _generate_code_example(group, strategy)
# Calculate ROI score (higher for simpler, lower-risk refactoring)
roi_score = _calculate_roi(group, complexity, risk)
# Determine if this is a breaking change
breaking_changes = _is_breaking_change(group, strategy)
suggestion = ConsolidationSuggestion(
⋮----
# Strategy Determination Rules (Data-Driven)
⋮----
# Each category has rules with occurrence thresholds defining the strategy.
# Format: (max_occurrences, strategy, rationale_template, complexity, risk)
# Rules are evaluated in order; first matching rule wins.
⋮----
@dataclass
class StrategyRule
⋮----
"""Rule for determining consolidation strategy."""
max_occurrences: int | None  # None means unlimited
strategy: str
rationale_template: str
complexity: str
risk: str
# Category-specific strategy rules
CATEGORY_STRATEGY_RULES: dict[str, list[StrategyRule]] = {
DEFAULT_STRATEGY_RULES: list[StrategyRule] = [
⋮----
rationale = rule.rationale_template.format(occ=occurrences, files=files)
⋮----
last_rule = rules[-1]
⋮----
def _determine_strategy(group: DuplicateGroup) -> tuple[str, str, str, str]
⋮----
occurrences = group.occurrence_count
files = len(group.affected_files)
category = group.category
⋮----
rules = CATEGORY_STRATEGY_RULES.get(category, DEFAULT_STRATEGY_RULES)
⋮----
def _generate_migration_steps(group: DuplicateGroup, strategy: str) -> List[MigrationStep]
⋮----
steps = [
⋮----
def _generate_code_example(group: DuplicateGroup, strategy: str) -> str
⋮----
pattern = group.pattern_id
⋮----
def _calculate_roi(group: DuplicateGroup, complexity: str, risk: str) -> float
⋮----
roi = group.impact_score
complexity_multipliers = {
⋮----
risk_multipliers = {
⋮----
def _is_breaking_change(group: DuplicateGroup, strategy: str) -> bool
def _suggest_target_location(group: DuplicateGroup, strategy: str) -> str
⋮----
first_file = group.affected_files[0] if group.affected_files else ''
⋮----
dir_path = '/'.join(first_file.split('/')[:-1])
⋮----
else:  # autonomous_agent
⋮----
def _estimate_effort(group: DuplicateGroup, complexity: str) -> float
⋮----
"""Estimate effort in hours"""
base_hours = {
hours = base_hours.get(complexity, 2.0)
# Add time per affected file (more files = more refactoring)
⋮----
# Add time for testing
⋮----
"""Calculate comprehensive duplication metrics.
    Args:
        blocks: All extracted code blocks
        groups: All duplicate groups found
        suggestions: Generated consolidation suggestions
        total_repo_lines: Total lines in repository (for percentage calc)
    Returns:
        Dict with all metrics
    """
# Count by similarity method
exact_groups = [g for g in groups if g.similarity_method == 'exact_match']
structural_groups = [g for g in groups if g.similarity_method == 'structural']
semantic_groups = [g for g in groups if g.similarity_method == 'semantic']
total_duplicated_lines = sum(g.total_lines for g in groups)
# Calculate potential LOC reduction (keep one copy of each group)
potential_loc_reduction = sum(
# Calculate duplication percentage
# If total_repo_lines not provided, estimate from blocks
⋮----
total_repo_lines = sum(b.line_count for b in blocks)
duplication_percentage = (
# Identify quick wins (simple to fix)
quick_wins = [
# Identify high-impact suggestions (significant refactoring value)
high_impact = [
# Semantic annotation metrics
blocks_with_tags = sum(1 for b in blocks if b.tags)
total_tags = sum(len(b.tags) for b in blocks)
blocks_with_tags_percentage = (
avg_tags_per_block = round(total_tags / len(blocks), 2) if blocks else 0.0
⋮----
# Block counts
⋮----
# By similarity method
⋮----
# Line metrics
⋮----
# Suggestion metrics
⋮----
# Detailed by complexity
⋮----
# High priority (by impact score)
⋮----
# Semantic annotation coverage
⋮----
def main()
⋮----
"""
    Main pipeline execution
    """
⋮----
# Read and validate input from stdin (C2 security fix)
raw_input = json.load(sys.stdin)
⋮----
validated_input = PipelineInput(**raw_input)
⋮----
# Include available context for debugging
repo_hint = raw_input.get('repository_info', {}).get('path', 'unknown') if isinstance(raw_input, dict) else 'invalid_input'
⋮----
sys.exit(2)  # Distinct exit code for validation errors
# Convert validated models to dicts for compatibility
repository_info = validated_input.repository_info.model_dump()
pattern_matches = [m.model_dump() for m in validated_input.pattern_matches]
⋮----
# Stage 3: Extract code blocks
blocks = extract_code_blocks(pattern_matches, repository_info)
# Stage 3.5: Deduplicate blocks (Priority 4)
blocks = deduplicate_blocks(blocks)
# Stage 4: Semantic annotation
# Note: Full semantic annotation happens in Layer 3 grouping
# Blocks have basic category from extraction; rich annotation in grouping.py
# Stage 5: Group duplicates (Layers 1-3)
groups = group_duplicates(blocks)
# Stage 6: Generate suggestions
suggestions = generate_suggestions(groups)
# Stage 7: Calculate metrics
metrics = calculate_metrics(blocks, groups, suggestions)
# Output result as JSON (use mode='json' to serialize datetime objects)
result = {
⋮----
repo_path = repository_info.get('path', 'unknown') if 'repository_info' in dir() else 'unknown'
match_count = len(pattern_matches) if 'pattern_matches' in dir() else 0
</file>

<file path="pipeline-core/extractors/test_extract_blocks.py">
@dataclass
class MockLocation
⋮----
file_path: str = '/test/file.js'
line_start: int = 1
line_end: int = 5
⋮----
@dataclass
class MockCodeBlock
⋮----
block_id: str
line_count: int = 5
location: MockLocation = field(default_factory=MockLocation)
tags: list = field(default_factory=list)
⋮----
@dataclass
class MockDuplicateGroup
⋮----
group_id: str
similarity_method: str
occurrence_count: int
total_lines: int
affected_files: list = field(default_factory=list)
def __post_init__(self)
⋮----
@dataclass
class MockSuggestion
⋮----
suggestion_id: str
complexity: str
impact_score: float
def test_detect_language_javascript()
def test_detect_language_typescript()
def test_detect_language_python()
def test_detect_language_other_languages()
def test_detect_language_case_insensitive()
def test_detect_language_unknown()
def test_detect_language_with_paths()
def test_language_map_has_expected_entries()
⋮----
expected = {'.js', '.jsx', '.ts', '.tsx', '.py', '.go', '.rs', '.java'}
⋮----
def test_calculate_metrics_empty()
⋮----
metrics = calculate_metrics([], [], [])
⋮----
def test_calculate_metrics_counts_by_method()
⋮----
blocks = [MockCodeBlock(f'b{i}', line_count=5) for i in range(10)]
groups = [
suggestions = []
metrics = calculate_metrics(blocks, groups, suggestions)
⋮----
def test_calculate_metrics_line_totals()
⋮----
blocks = [MockCodeBlock('b1', line_count=100)]
⋮----
metrics = calculate_metrics(blocks, groups, [])
⋮----
def test_calculate_metrics_loc_reduction()
def test_calculate_metrics_duplication_percentage()
⋮----
blocks = [MockCodeBlock(f'b{i}', line_count=10) for i in range(10)]
# 25 lines duplicated
groups = [MockDuplicateGroup('g1', 'exact_match', 5, 25)]
⋮----
def test_calculate_metrics_duplication_percentage_with_total()
⋮----
blocks = [MockCodeBlock('b1', line_count=10)]
groups = [MockDuplicateGroup('g1', 'exact_match', 2, 50)]
metrics = calculate_metrics(blocks, groups, [], total_repo_lines=1000)
⋮----
def test_calculate_metrics_quick_wins()
⋮----
blocks = [MockCodeBlock('b1')]
⋮----
def test_calculate_metrics_high_impact()
def test_calculate_metrics_suggestion_complexity()
⋮----
suggestions = [
metrics = calculate_metrics(blocks, [], suggestions)
⋮----
def test_calculate_metrics_high_priority()
def test_calculate_metrics_semantic_annotation_coverage()
⋮----
blocks = [
metrics = calculate_metrics(blocks, [], [])
⋮----
def test_calculate_metrics_empty_semantic_annotation()
def main()
⋮----
tests = [
passed = 0
failed = 0
</file>

<file path="pipeline-core/git/branch-manager.ts">
import { runCommand } from '@shared/process-io';
import { createComponentLogger, logError } from '../../utils/logger.ts';
⋮----
export interface BranchManagerOptions {
  baseBranch?: string;
  branchPrefix?: string;
  dryRun?: boolean;
}
export interface JobBranchContext {
  jobId: string;
  jobType: string;
  description?: string;
}
export interface BranchResult {
  branchName: string;
  originalBranch: string;
}
export interface CommitContext {
  message: string;
  jobId: string;
  description?: string;
}
export interface PRContext {
  branchName: string;
  title: string;
  body: string;
  labels?: string[];
}
export class BranchManager
⋮----
constructor(options: BranchManagerOptions =
async hasChanges(repositoryPath: string): Promise<boolean>
async getChangedFiles(repositoryPath: string): Promise<string[]>
async getCurrentBranch(repositoryPath: string): Promise<string>
async isGitRepository(repositoryPath: string): Promise<boolean>
async createJobBranch(repositoryPath: string, jobContext: JobBranchContext): Promise<BranchResult>
async commitChanges(repositoryPath: string, commitContext: CommitContext): Promise<string>
async pushBranch(repositoryPath: string, branchName: string): Promise<boolean>
async createPullRequest(repositoryPath: string, prContext: PRContext): Promise<string | null>
async cleanupBranch(repositoryPath: string, branchName: string, originalBranch: string): Promise<void>
private _generateBranchName(jobContext: JobBranchContext): string
private _generateCommitMessage(commitContext: CommitContext, changedFiles: string[]): string
private async _runGitCommand(cwd: string, args: string[]): Promise<string>
private async _runCommand(cwd: string, command: string, args: string[]): Promise<string>
</file>

<file path="pipeline-core/git/migration-transformer.ts">
import { parse } from '@babel/parser';
import type { ParseResult, ParserPlugin } from '@babel/parser';
import _traverse from '@babel/traverse';
import type { NodePath, TraverseOptions } from '@babel/traverse';
import _generate from '@babel/generator';
import type { GeneratorOptions, GeneratorResult } from '@babel/generator';
⋮----
import type { File as BabelFile, Expression, Identifier, MemberExpression } from '@babel/types';
import fs from 'fs/promises';
import path from 'path';
import { glob } from 'glob';
import { runCommand } from '@shared/process-io';
import { createComponentLogger, logError } from '../../utils/logger.ts';
import { config } from '../../core/config.ts';
⋮----
type ParsedMigrationStep =
  | { type: 'update-import'; oldPath: string; newPath: string }
  | { type: 'replace-call'; oldName: string; newName: string }
  | { type: 'remove-declaration'; name: string }
  | { type: 'add-import'; imported: string; source: string };
interface MigrationStep {
  description: string;
  code_example?: string;
  step_number?: number;
}
interface MigrationSuggestion {
  suggestion_id?: string;
  migration_steps?: MigrationStep[];
}
interface ParsedStep extends MigrationStep {
  parsed: ParsedMigrationStep | null;
  index: number;
}
interface TransformResult {
  modified: boolean;
  transformations?: Array<Record<string, string>>;
  originalLength?: number;
  newLength?: number;
  reason?: string;
  error?: string;
}
interface MigrationResult {
  filesModified: string[];
  transformations: Array<Record<string, unknown>>;
  errors: Array<{ file: string; error: string }>;
  backupPath: string | null;
}
export interface MigrationTransformerOptions {
  dryRun?: boolean;
}
function escapeRegExp(str: string): string
function parseMigrationStep(description: string): ParsedMigrationStep | null
⋮----
export class MigrationTransformer
⋮----
constructor(options: MigrationTransformerOptions =
async applyMigrationSteps(suggestion: MigrationSuggestion, repositoryPath: string): Promise<MigrationResult>
private async _transformFile(filePath: string, steps: ParsedStep[], _suggestion: MigrationSuggestion): Promise<TransformResult>
private _updateImport(ast: ParseResult<BabelFile>, oldPath: string, newPath: string): boolean
⋮----
ImportDeclaration(path: NodePath<t.ImportDeclaration>)
⋮----
private _addImport(ast: ParseResult<BabelFile>, imported: string, source: string): boolean
private _replaceCallExpression(ast: ParseResult<BabelFile>, oldName: string, newName: string): boolean
⋮----
CallExpression(path: NodePath<t.CallExpression>)
⋮----
private _removeDeclaration(ast: ParseResult<BabelFile>, name: string): boolean
⋮----
FunctionDeclaration(path: NodePath<t.FunctionDeclaration>)
ClassDeclaration(path: NodePath<t.ClassDeclaration>)
VariableDeclarator(path: NodePath<t.VariableDeclarator>)
⋮----
private async _groupStepsByFile(
    parsedSteps: ParsedStep[],
    _suggestion: MigrationSuggestion,
    repositoryPath: string
): Promise<Record<string, ParsedStep[]>>
private async _findRepositoryFiles(repositoryPath: string): Promise<string[]>
private _contentMatchesStep(content: string, parsed: ParsedMigrationStep): boolean
private async _resolveAffectedFiles(
    unresolvedSteps: ParsedStep[],
    repositoryPath: string
): Promise<Map<number, string[]>>
private async _matchFileAgainstSteps(
    relPath: string,
    repositoryPath: string,
    unresolvedSteps: ParsedStep[],
    resolved: Map<number, string[]>
): Promise<void>
private _logResolvedFiles(resolved: Map<number, string[]>): void
private async _stashChanges(repositoryPath: string): Promise<string | null>
private async _unstashChanges(repositoryPath: string, stashRef: string, required = false): Promise<void>
async rollback(repositoryPath: string): Promise<void>
</file>

<file path="pipeline-core/git/pr-creator.ts">
import { runCommand } from '@shared/process-io';
import fs from 'fs/promises';
import path from 'path';
import { createComponentLogger, logError } from '../../utils/logger.ts';
⋮----
import { MigrationTransformer } from './migration-transformer.ts';
⋮----
interface MigrationStep {
  description: string;
  step_number: number;
  code_example?: string;
}
interface Suggestion {
  suggestion_id: string;
  automated_refactor_possible: boolean;
  impact_score: number;
  target_location?: string;
  target_name?: string;
  proposed_implementation?: string;
  strategy?: string;
  strategy_rationale: string;
  complexity?: string;
  migration_risk?: string;
  usage_example?: string;
  migration_steps: MigrationStep[];
}
interface ScanResult {
  suggestions?: Suggestion[];
}
export interface PRCreatorOptions {
  baseBranch?: string;
  branchPrefix?: string;
  dryRun?: boolean;
  maxSuggestionsPerPR?: number;
}
export interface PRCreationResults {
  prsCreated: number;
  prUrls: string[];
  errors: Array<{ batch: number; error: string; suggestions: string[] }>;
  skipped: number;
}
export class PRCreator
⋮----
constructor(options: PRCreatorOptions =
async createPRsForSuggestions(
    scanResult: ScanResult,
    repositoryPath: string,
    _options: Record<string, unknown> = {}
): Promise<PRCreationResults>
private async _createPRForBatch(
    suggestions: Suggestion[],
    repositoryPath: string,
    batchNumber: number,
    _options: Record<string, unknown>
): Promise<string | null>
private async _applySuggestions(suggestions: Suggestion[], repositoryPath: string): Promise<string[]>
private async _runGitCommand(cwd: string, args: string[]): Promise<string>
private async _createPR(cwd: string, branch: string, title: string, body: string): Promise<string>
private _batchSuggestions(suggestions: Suggestion[]): Suggestion[][]
private _generateBranchName(_suggestions: Suggestion[], batchNumber: number): string
private _generateCommitMessage(suggestions: Suggestion[], filesModified: string[]): string
private _generatePRTitle(suggestions: Suggestion[], batchNumber: number): string
private _generatePRDescription(suggestions: Suggestion[], filesModified: string[]): string
</file>

<file path="pipeline-core/models/__init__.py">
__all__ = [
__version__ = '1.0.0'
</file>

<file path="pipeline-core/models/__init__.pyi">
"""Type stubs for models package."""

from .code_block import (
    CodeBlock as CodeBlock,
    SourceLocation as SourceLocation,
    ASTNode as ASTNode,
    LanguageType as LanguageType,
    SemanticCategory as SemanticCategory,
)
from .duplicate_group import (
    DuplicateGroup as DuplicateGroup,
    SimilarityMethod as SimilarityMethod,
)
from .consolidation_suggestion import (
    ConsolidationSuggestion as ConsolidationSuggestion,
    ConsolidationStrategy as ConsolidationStrategy,
    ImplementationComplexity as ImplementationComplexity,
    MigrationRisk as MigrationRisk,
    MigrationStep as MigrationStep,
)
from .scan_report import (
    ScanReport as ScanReport,
    RepositoryInfo as RepositoryInfo,
    ScanConfiguration as ScanConfiguration,
    ScanMetrics as ScanMetrics,
)

__all__: list[str]
__version__: str
</file>

<file path="pipeline-core/models/code_block.py">
class LanguageType(str, Enum)
⋮----
JAVASCRIPT = "javascript"
TYPESCRIPT = "typescript"
PYTHON = "python"
JAVA = "java"
GO = "go"
RUST = "rust"
C = "c"
CPP = "cpp"
CSHARP = "csharp"
PHP = "php"
RUBY = "ruby"
class SemanticCategory(str, Enum)
⋮----
UTILITY = "utility"
HELPER = "helper"
VALIDATOR = "validator"
API_HANDLER = "api_handler"
AUTH_CHECK = "auth_check"
DATABASE_OPERATION = "database_operation"
ERROR_HANDLER = "error_handler"
LOGGER = "logger"
CONFIG_ACCESS = "config_access"
FILE_OPERATION = "file_operation"
ASYNC_PATTERN = "async_pattern"
PROCESS_IO = "process_io"
TIMING = "timing"
TRACING = "tracing"
UNKNOWN = "unknown"
class SourceLocation(BaseModel)
⋮----
file_path: str = Field(..., description="Absolute path to source file")
line_start: int = Field(..., ge=1, description="Starting line number (1-indexed)")
line_end: int = Field(..., ge=1, description="Ending line number (1-indexed)")
column_start: Optional[int] = Field(None, ge=0, description="Starting column (0-indexed)")
column_end: Optional[int] = Field(None, ge=0, description="Ending column (0-indexed)")
⋮----
@field_validator('line_end')
@classmethod
    def validate_line_range(cls, v, info)
def __str__(self) -> str
class ASTNode(BaseModel)
⋮----
"""Representation of AST node structure"""
node_type: str = Field(..., description="Type of AST node (e.g., 'CallExpression')")
children: List['ASTNode'] = Field(default_factory=list, description="Child nodes")
properties: Dict[str, Any] = Field(default_factory=dict, description="Node properties")
model_config = {
⋮----
class CodeBlock(BaseModel)
⋮----
block_id: str = Field(..., description="Unique identifier for this code block")
pattern_id: str = Field(..., description="ast-grep rule ID that matched this block")
location: SourceLocation = Field(..., description="Source code location")
relative_path: str = Field(..., description="Repository-relative path")
source_code: str = Field(..., description="Raw source code of the block")
normalized_code: Optional[str] = Field(None, description="Normalized/formatted code")
ast_structure: Optional[ASTNode] = Field(None, description="AST node tree")
ast_hash: Optional[str] = Field(None, description="Hash of AST structure")
language: LanguageType = Field(..., description="Programming language")
category: SemanticCategory = Field(..., description="Semantic category")
tags: List[str] = Field(default_factory=list, description="Additional semantic tags")
match_context: Dict[str, Any] = Field(
repository_path: str = Field(..., description="Absolute path to repository root")
repository_name: Optional[str] = Field(None, description="Repository name/identifier")
git_commit: Optional[str] = Field(None, description="Git commit hash when scanned")
detected_at: datetime = Field(
line_count: int = Field(..., ge=1, description="Number of lines in block")
complexity_score: Optional[float] = Field(
⋮----
@computed_field
@property
    def content_hash(self) -> str
⋮----
normalized = ' '.join(self.source_code.split())
⋮----
@computed_field
@property
    def structural_hash(self) -> str
def to_dict_for_comparison(self) -> Dict[str, Any]
def __hash__(self) -> int
def __eq__(self, other: object) -> bool
</file>

<file path="pipeline-core/models/code_block.pyi">
"""Type stubs for code_block module."""

from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional

from pydantic import BaseModel


class LanguageType(str, Enum):
    JAVASCRIPT = ...
    TYPESCRIPT = ...
    PYTHON = ...
    JAVA = ...
    GO = ...
    RUST = ...
    C = ...
    CPP = ...
    CSHARP = ...
    PHP = ...
    RUBY = ...


class SemanticCategory(str, Enum):
    UTILITY = ...
    HELPER = ...
    VALIDATOR = ...
    API_HANDLER = ...
    AUTH_CHECK = ...
    DATABASE_OPERATION = ...
    ERROR_HANDLER = ...
    LOGGER = ...
    CONFIG_ACCESS = ...
    FILE_OPERATION = ...
    ASYNC_PATTERN = ...
    UNKNOWN = ...


class SourceLocation(BaseModel):
    file_path: str
    line_start: int
    line_end: int
    column_start: Optional[int]
    column_end: Optional[int]

    def __str__(self) -> str: ...


class ASTNode(BaseModel):
    node_type: str
    children: List[ASTNode]
    properties: Dict[str, Any]


class CodeBlock(BaseModel):
    block_id: str
    pattern_id: str
    location: SourceLocation
    relative_path: str
    source_code: str
    normalized_code: Optional[str]
    ast_structure: Optional[ASTNode]
    ast_hash: Optional[str]
    language: LanguageType
    category: SemanticCategory
    tags: List[str]
    match_context: Dict[str, Any]
    repository_path: str
    repository_name: Optional[str]
    git_commit: Optional[str]
    detected_at: datetime
    line_count: int
    complexity_score: Optional[float]

    @property
    def content_hash(self) -> str: ...
    @property
    def structural_hash(self) -> str: ...

    def to_dict_for_comparison(self) -> Dict[str, Any]: ...
    def __hash__(self) -> int: ...
    def __eq__(self, other: object) -> bool: ...
</file>

<file path="pipeline-core/models/consolidation_suggestion.py">
class ConsolidationStrategy(str, Enum)
⋮----
LOCAL_UTIL = "local_util"
SHARED_PACKAGE = "shared_package"
MCP_SERVER = "mcp_server"
AUTONOMOUS_AGENT = "autonomous_agent"
NO_ACTION = "no_action"
class ImplementationComplexity(str, Enum)
⋮----
TRIVIAL = "trivial"
SIMPLE = "simple"
MODERATE = "moderate"
COMPLEX = "complex"
VERY_COMPLEX = "very_complex"
class MigrationRisk(str, Enum)
⋮----
MINIMAL = "minimal"
LOW = "low"
MEDIUM = "medium"
HIGH = "high"
CRITICAL = "critical"
class MigrationStep(BaseModel)
⋮----
step_number: int = Field(..., ge=1, description="Step order")
description: str = Field(..., description="What to do in this step")
code_example: Optional[str] = Field(None, description="Example code")
automated: bool = Field(False, description="Can this step be automated?")
estimated_time: Optional[str] = Field(None, description="Estimated time (e.g., '30min', '2h')")
class ConsolidationSuggestion(BaseModel)
⋮----
suggestion_id: str = Field(..., description="Unique identifier for this suggestion")
duplicate_group_id: str = Field(..., description="ID of DuplicateGroup being addressed")
strategy: ConsolidationStrategy = Field(..., description="Recommended consolidation tier")
strategy_rationale: str = Field(..., description="Why this strategy was chosen")
impact_score: float = Field(
complexity: ImplementationComplexity = Field(..., description="Implementation complexity")
migration_risk: MigrationRisk = Field(..., description="Migration risk level")
breaking_changes: bool = Field(..., description="Will this introduce breaking changes?")
migration_steps: List[MigrationStep] = Field(
target_location: Optional[str] = Field(
target_name: Optional[str] = Field(
proposed_implementation: Optional[str] = Field(
usage_example: Optional[str] = Field(
estimated_effort_hours: Optional[float] = Field(
loc_reduction: Optional[int] = Field(
affected_files_count: int = Field(..., ge=1, description="Number of files to modify")
affected_repositories_count: int = Field(..., ge=1, description="Number of repos affected")
dependencies: List[str] = Field(
prerequisite_suggestions: List[str] = Field(
test_strategy: Optional[str] = Field(
rollback_plan: Optional[str] = Field(
confidence: float = Field(
automated_refactor_possible: bool = Field(
requires_human_review: bool = Field(
benefits: List[str] = Field(
drawbacks: List[str] = Field(
notes: Optional[str] = Field(None, description="Additional notes")
created_at: datetime = Field(
model_config = {
⋮----
@field_validator('impact_score')
@classmethod
    def round_impact_score(cls, v)
⋮----
@computed_field
@property
    def priority(self) -> str
⋮----
@computed_field
@property
    def roi_score(self) -> float
⋮----
complexity_hours = {
effort = self.estimated_effort_hours or complexity_hours.get(self.complexity, 10)
⋮----
roi = (self.impact_score / effort) * 10
⋮----
@computed_field
@property
    def is_quick_win(self) -> bool
⋮----
step_number = len(self.migration_steps) + 1
step = MigrationStep(
⋮----
def add_benefit(self, benefit: str) -> None
def add_drawback(self, drawback: str) -> None
def to_markdown_summary(self) -> str
def __hash__(self) -> int
def __eq__(self, other: object) -> bool
</file>

<file path="pipeline-core/models/consolidation_suggestion.pyi">
"""Type stubs for consolidation_suggestion module."""

from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional

from pydantic import BaseModel


class ConsolidationStrategy(str, Enum):
    LOCAL_UTIL = ...
    SHARED_PACKAGE = ...
    MCP_SERVER = ...
    AUTONOMOUS_AGENT = ...
    NO_ACTION = ...


class ImplementationComplexity(str, Enum):
    TRIVIAL = ...
    SIMPLE = ...
    MODERATE = ...
    COMPLEX = ...
    VERY_COMPLEX = ...


class MigrationRisk(str, Enum):
    MINIMAL = ...
    LOW = ...
    MEDIUM = ...
    HIGH = ...
    CRITICAL = ...


class MigrationStep(BaseModel):
    step_number: int
    description: str
    code_example: Optional[str]
    automated: bool
    estimated_time: Optional[str]


class ConsolidationSuggestion(BaseModel):
    suggestion_id: str
    duplicate_group_id: str
    strategy: ConsolidationStrategy
    strategy_rationale: str
    impact_score: float
    complexity: ImplementationComplexity
    migration_risk: MigrationRisk
    breaking_changes: bool
    migration_steps: List[MigrationStep]
    target_location: Optional[str]
    target_name: Optional[str]
    proposed_implementation: Optional[str]
    usage_example: Optional[str]
    estimated_effort_hours: Optional[float]
    loc_reduction: Optional[int]
    affected_files_count: int
    affected_repositories_count: int
    dependencies: List[str]
    prerequisite_suggestions: List[str]
    test_strategy: Optional[str]
    rollback_plan: Optional[str]
    confidence: float
    automated_refactor_possible: bool
    requires_human_review: bool
    benefits: List[str]
    drawbacks: List[str]
    notes: Optional[str]
    created_at: datetime

    @property
    def priority(self) -> str: ...
    @property
    def roi_score(self) -> float: ...
    @property
    def is_quick_win(self) -> bool: ...

    def add_migration_step(
        self,
        description: str,
        code_example: Optional[str] = ...,
        automated: bool = ...,
        estimated_time: Optional[str] = ...,
    ) -> None: ...
    def add_benefit(self, benefit: str) -> None: ...
    def add_drawback(self, drawback: str) -> None: ...
    def to_markdown_summary(self) -> str: ...
    def __hash__(self) -> int: ...
    def __eq__(self, other: object) -> bool: ...
</file>

<file path="pipeline-core/models/duplicate_group.py">
class SimilarityMethod(str, Enum)
⋮----
EXACT_MATCH = "exact_match"
STRUCTURAL = "structural"
SEMANTIC = "semantic"
HYBRID = "hybrid"
class DuplicateGroup(BaseModel)
⋮----
group_id: str = Field(..., description="Unique identifier for this duplicate group")
pattern_id: str = Field(..., description="ast-grep pattern that matched these blocks")
member_block_ids: List[str] = Field(
similarity_score: float = Field(
similarity_method: SimilarityMethod = Field(
canonical_block_id: Optional[str] = Field(
category: str = Field(..., description="Semantic category of duplicates")
language: str = Field(..., description="Programming language")
occurrence_count: int = Field(..., ge=2, description="Number of occurrences")
total_lines: int = Field(..., ge=1, description="Total lines of duplicated code")
affected_files: List[str] = Field(
affected_repositories: List[str] = Field(
consolidation_complexity: Optional[str] = Field(
breaking_changes_risk: Optional[str] = Field(
created_at: datetime = Field(
updated_at: datetime = Field(
notes: Optional[str] = Field(None, description="Analysis notes")
metadata: Dict[str, Any] = Field(
model_config = {
⋮----
@field_validator('member_block_ids')
@classmethod
    def validate_min_members(cls, v)
⋮----
@field_validator('canonical_block_id')
@classmethod
    def validate_canonical_in_members(cls, v, info)
⋮----
@computed_field
@property
    def deduplication_potential(self) -> int
⋮----
avg_lines_per_instance = self.total_lines / self.occurrence_count
⋮----
@computed_field
@property
    def impact_score(self) -> float
⋮----
occurrence_factor = min(self.occurrence_count / 20.0, 1.0)
similarity_factor = self.similarity_score
loc_factor = min(self.total_lines / 100.0, 1.0)
score = (
⋮----
@computed_field
@property
    def is_cross_repository(self) -> bool
⋮----
@computed_field
@property
    def priority_level(self) -> str
def add_member(self, block_id: str) -> None
def remove_member(self, block_id: str) -> None
def set_canonical(self, block_id: str) -> None
def __hash__(self) -> int
def __eq__(self, other: object) -> bool
</file>

<file path="pipeline-core/models/duplicate_group.pyi">
"""Type stubs for duplicate_group module."""

from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional

from pydantic import BaseModel


class SimilarityMethod(str, Enum):
    EXACT_MATCH = ...
    STRUCTURAL = ...
    SEMANTIC = ...
    HYBRID = ...


class DuplicateGroup(BaseModel):
    group_id: str
    pattern_id: str
    member_block_ids: List[str]
    similarity_score: float
    similarity_method: SimilarityMethod
    canonical_block_id: Optional[str]
    category: str
    language: str
    occurrence_count: int
    total_lines: int
    affected_files: List[str]
    affected_repositories: List[str]
    consolidation_complexity: Optional[str]
    breaking_changes_risk: Optional[str]
    created_at: datetime
    updated_at: datetime
    notes: Optional[str]
    metadata: Dict[str, Any]

    @property
    def deduplication_potential(self) -> int: ...
    @property
    def impact_score(self) -> float: ...
    @property
    def is_cross_repository(self) -> bool: ...
    @property
    def priority_level(self) -> str: ...

    def add_member(self, block_id: str) -> None: ...
    def remove_member(self, block_id: str) -> None: ...
    def set_canonical(self, block_id: str) -> None: ...
    def __hash__(self) -> int: ...
    def __eq__(self, other: object) -> bool: ...
</file>

<file path="pipeline-core/models/scan_report.py">
class RepositoryInfo(BaseModel)
⋮----
repository_path: str = Field(..., description="Absolute path to repository")
repository_name: str = Field(..., description="Repository name/identifier")
git_remote: Optional[str] = Field(None, description="Git remote URL")
git_branch: Optional[str] = Field(None, description="Current branch")
git_commit: Optional[str] = Field(None, description="Current commit hash")
total_files: int = Field(..., ge=0, description="Total files scanned")
total_lines: int = Field(..., ge=0, description="Total lines of code scanned")
languages: List[str] = Field(default_factory=list, description="Languages detected")
class ScanConfiguration(BaseModel)
⋮----
rules_used: List[str] = Field(default_factory=list, description="ast-grep rules applied")
excluded_paths: List[str] = Field(default_factory=list, description="Paths excluded from scan")
min_similarity_threshold: float = Field(0.8, ge=0.0, le=1.0, description="Minimum similarity for grouping")
min_duplicate_size: int = Field(3, ge=1, description="Minimum lines for duplicate detection")
class ScanMetrics(BaseModel)
⋮----
total_code_blocks: int = Field(..., ge=0, description="Total code blocks detected")
code_blocks_by_category: Dict[str, int] = Field(
code_blocks_by_language: Dict[str, int] = Field(
total_duplicate_groups: int = Field(..., ge=0, description="Total duplicate groups found")
exact_duplicates: int = Field(..., ge=0, description="Groups with 100% similarity")
structural_duplicates: int = Field(..., ge=0, description="Groups with structural similarity")
semantic_duplicates: int = Field(..., ge=0, description="Groups with semantic similarity")
total_duplicated_lines: int = Field(..., ge=0, description="Total lines in duplicate groups")
potential_loc_reduction: int = Field(..., ge=0, description="Potential lines that could be removed")
duplication_percentage: float = Field(..., ge=0.0, le=100.0, description="Percentage of code that's duplicated")
total_suggestions: int = Field(..., ge=0, description="Total consolidation suggestions")
quick_wins: int = Field(..., ge=0, description="Number of quick win suggestions")
high_priority_suggestions: int = Field(..., ge=0, description="High priority suggestions")
cross_repository_duplicates: int = Field(0, ge=0, description="Duplicates spanning multiple repos")
class ScanReport(BaseModel)
⋮----
report_id: str = Field(..., description="Unique identifier for this report")
scan_name: Optional[str] = Field(None, description="Descriptive name for this scan")
scanned_at: datetime = Field(default_factory=datetime.utcnow, description="Scan timestamp")
scan_duration_seconds: Optional[float] = Field(None, ge=0, description="How long the scan took")
scanner_version: str = Field("1.0.0", description="Version of duplicate detection pipeline")
configuration: ScanConfiguration = Field(..., description="Scan configuration")
repositories: List[RepositoryInfo] = Field(..., description="Repositories scanned")
code_block_ids: List[str] = Field(default_factory=list, description="IDs of detected code blocks")
duplicate_group_ids: List[str] = Field(default_factory=list, description="IDs of duplicate groups")
suggestion_ids: List[str] = Field(default_factory=list, description="IDs of consolidation suggestions")
metrics: ScanMetrics = Field(..., description="Statistical metrics")
executive_summary: Optional[str] = Field(None, description="High-level summary of findings")
recommendations: List[str] = Field(default_factory=list, description="Top-level recommendations")
warnings: List[str] = Field(default_factory=list, description="Warnings or issues encountered")
output_directory: str = Field(..., description="Directory where detailed results are saved")
report_files: Dict[str, str] = Field(
tags: List[str] = Field(default_factory=list, description="Tags for categorization")
metadata: Dict[str, Any] = Field(default_factory=dict, description="Additional metadata")
model_config = {
⋮----
@computed_field
@property
    def is_multi_repository(self) -> bool
⋮----
@computed_field
@property
    def total_scanned_files(self) -> int
⋮----
@computed_field
@property
    def total_scanned_lines(self) -> int
⋮----
@computed_field
@property
    def duplication_severity(self) -> str
⋮----
dup_pct = self.metrics.duplication_percentage
⋮----
@computed_field
@property
    def consolidation_opportunity_score(self) -> float
⋮----
dup_factor = min(self.metrics.duplication_percentage / 40 * 100, 100)
quick_win_factor = min(self.metrics.quick_wins / 10 * 100, 100)
⋮----
loc_reduction_factor = (
⋮----
loc_reduction_factor = 0
score = (
⋮----
def add_repository(self, repo_info: RepositoryInfo) -> None
def add_code_block_id(self, block_id: str) -> None
def add_duplicate_group_id(self, group_id: str) -> None
def add_suggestion_id(self, suggestion_id: str) -> None
def generate_executive_summary(self) -> str
⋮----
repos_text = f"{len(self.repositories)} repository" if len(self.repositories) == 1 else f"{len(self.repositories)} repositories"
summary = f"""
⋮----
def to_summary_dict(self) -> Dict[str, Any]
def __hash__(self) -> int
def __eq__(self, other: object) -> bool
</file>

<file path="pipeline-core/models/scan_report.pyi">
"""Type stubs for scan_report module."""

from datetime import datetime
from typing import Any, Dict, List, Optional

from pydantic import BaseModel


class RepositoryInfo(BaseModel):
    repository_path: str
    repository_name: str
    git_remote: Optional[str]
    git_branch: Optional[str]
    git_commit: Optional[str]
    total_files: int
    total_lines: int
    languages: List[str]


class ScanConfiguration(BaseModel):
    rules_used: List[str]
    excluded_paths: List[str]
    min_similarity_threshold: float
    min_duplicate_size: int


class ScanMetrics(BaseModel):
    total_code_blocks: int
    code_blocks_by_category: Dict[str, int]
    code_blocks_by_language: Dict[str, int]
    total_duplicate_groups: int
    exact_duplicates: int
    structural_duplicates: int
    semantic_duplicates: int
    total_duplicated_lines: int
    potential_loc_reduction: int
    duplication_percentage: float
    total_suggestions: int
    quick_wins: int
    high_priority_suggestions: int
    cross_repository_duplicates: int


class ScanReport(BaseModel):
    report_id: str
    scan_name: Optional[str]
    scanned_at: datetime
    scan_duration_seconds: Optional[float]
    scanner_version: str
    configuration: ScanConfiguration
    repositories: List[RepositoryInfo]
    code_block_ids: List[str]
    duplicate_group_ids: List[str]
    suggestion_ids: List[str]
    metrics: ScanMetrics
    executive_summary: Optional[str]
    recommendations: List[str]
    warnings: List[str]
    output_directory: str
    report_files: Dict[str, str]
    tags: List[str]
    metadata: Dict[str, Any]

    @property
    def is_multi_repository(self) -> bool: ...
    @property
    def total_scanned_files(self) -> int: ...
    @property
    def total_scanned_lines(self) -> int: ...
    @property
    def duplication_severity(self) -> str: ...
    @property
    def consolidation_opportunity_score(self) -> float: ...

    def add_repository(self, repo_info: RepositoryInfo) -> None: ...
    def add_code_block_id(self, block_id: str) -> None: ...
    def add_duplicate_group_id(self, group_id: str) -> None: ...
    def add_suggestion_id(self, suggestion_id: str) -> None: ...
    def generate_executive_summary(self) -> str: ...
    def to_summary_dict(self) -> Dict[str, Any]: ...
    def __hash__(self) -> int: ...
    def __eq__(self, other: object) -> bool: ...
</file>

<file path="pipeline-core/models/test_models.py">
def test_model_structure()
⋮----
models_dir = Path(__file__).parent
models = [
⋮----
model_path = models_dir / f"{model_name}.py"
⋮----
content = f.read()
checks = {
all_passed = all(checks.values())
symbol = "✅" if all_passed else "⚠️"
⋮----
status = "✓" if passed else "✗"
⋮----
def test_sample_data()
⋮----
"""Test with sample data structure (without pydantic)"""
⋮----
# Sample CodeBlock data
code_block_data = {
# Sample DuplicateGroup data
duplicate_group_data = {
# Sample ConsolidationSuggestion data
consolidation_data = {
# Sample ScanReport data
scan_report_data = {
samples = [
⋮----
required_fields = len(data)
⋮----
def test_computed_fields_logic()
⋮----
"""Test the logic of computed fields (without pydantic)"""
⋮----
# Test DuplicateGroup impact_score calculation
def calculate_impact_score(occurrence_count, similarity_score, total_lines)
⋮----
occurrence_factor = min(occurrence_count / 20.0, 1.0)
similarity_factor = similarity_score
loc_factor = min(total_lines / 100.0, 1.0)
score = (
⋮----
test_cases = [
⋮----
score = calculate_impact_score(occurrence, similarity, lines)
⋮----
def calculate_roi_score(impact_score, estimated_hours)
⋮----
roi = (impact_score / estimated_hours) * 10
⋮----
roi_tests = [
⋮----
roi = calculate_roi_score(impact, hours)
⋮----
def main()
</file>

<file path="pipeline-core/reports/html-report-generator.js">
export class HTMLReportGenerator
⋮----
static generateReport(scanResult, options =
static async saveReport(scanResult, outputPath, options =
static _generateHeader(scanResult, title, isInterProject)
static _generateMetrics(scanResult, isInterProject)
static _generateSummaryCharts(scanResult, isInterProject)
static _generateCrossRepoSection(scanResult)
static _generateDuplicateGroups(scanResult, isInterProject)
/**
   * Generate suggestions section
   * @private
   */
static _generateSuggestions(scanResult, isInterProject)
/**
   * Generate footer
   * @private
   */
static _generateFooter(scanResult)
/**
   * Get CSS styles
   * @private
   */
static _getStyles()
/**
   * Get JavaScript
   * @private
   */
static _getScripts()
static _getImpactClass(score)
static _getROIClass(score)
static _escapeHtml(text)
</file>

<file path="pipeline-core/reports/json-report-generator.js">
export class JSONReportGenerator
⋮----
static generateReport(scanResult, options =
static generateSummary(scanResult)
static async saveReport(scanResult, outputPath, options =
static async saveSummary(scanResult, outputPath)
static _generateMetadata(scanResult, isInterProject)
static _generateSummary(scanResult, isInterProject)
static _formatDuplicateGroups(scanResult, isInterProject, maxGroups, includeSourceCode)
static _formatSuggestions(scanResult, isInterProject, maxSuggestions)
static _formatRepositoryScans(scanResult, includeCodeBlocks, includeSourceCode)
static _formatCodeBlocks(scanResult, includeSourceCode)
static _calculateStrategyDistribution(suggestions)
static _calculateComplexityDistribution(suggestions)
</file>

<file path="pipeline-core/reports/markdown-report-generator.js">
export class MarkdownReportGenerator
⋮----
static generateReport(scanResult, options =
⋮----
// Header
⋮----
static generateSummary(scanResult)
static async saveReport(scanResult, outputPath, options =
static async saveSummary(scanResult, outputPath)
static _generateHeader(scanResult, isInterProject)
static _generateMetrics(scanResult, isInterProject)
static _generateRepositoryInfo(scanResult)
static _generateDuplicateGroups(scanResult, isInterProject, maxGroups, includeDetails)
static _generateSuggestions(scanResult, isInterProject, maxSuggestions, includeDetails)
static _generateFooter(scanResult)
static _formatScore(score)
static _formatStrategy(strategy)
static _formatComplexity(complexity)
static _formatRisk(risk)
</file>

<file path="pipeline-core/reports/report-coordinator.js">
export class ReportCoordinator
⋮----
async generateAllReports(scanResult, options =
async generateHTMLReport(scanResult, baseFilename = null, options =
async generateMarkdownReport(scanResult, baseFilename = null, options =
async generateJSONReport(scanResult, baseFilename = null, options =
async generateJSONSummary(scanResult, baseFilename = null)
async generateMarkdownSummary(scanResult, baseFilename = null)
_generateBaseFilename(scanResult, isInterProject = null)
_generateTitle(scanResult)
static generateQuickSummary(scanResult)
static printQuickSummary(scanResult)
</file>

<file path="pipeline-core/scanners/ast-grep-detector.js">
export class AstGrepPatternDetector
⋮----
async detectPatterns(repoPath, detectConfig =
async runAstGrepScan(repoPath, config)
normalizeMatch(match, repoPath)
async loadRules(rulesDir)
⋮----
async function walkRules(dir)
⋮----
async detectInFile(filePath, rules = [])
⋮----
export class PatternDetectionError extends Error
</file>

<file path="pipeline-core/scanners/codebase-health-scanner.js">
async function main()
function generateMarkdownReport(results)
⋮----
// Timeout scan report
⋮----
// Root directory analysis report
⋮----
// Summary
⋮----
export async function runHealthScan(repoPath, options =
</file>

<file path="pipeline-core/scanners/repository-scanner.js">
export class RepositoryScanner
⋮----
async scanRepository(repoPath, scanConfig =
async validateRepository(repoPath)
async getRepositoryInfo(repoPath)
async runRepomixScan(repoPath)
async parseRepomixOutput(outputFile)
async getFileMetadata(repoPath, scanConfig)
async listFiles(repoPath, filter =
⋮----
async function walk(dir)
⋮----
detectLanguages(files)
⋮----
export class RepositoryScanError extends Error
</file>

<file path="pipeline-core/scanners/root-directory-analyzer.js">
export class RootDirectoryAnalyzer
⋮----
async analyze(repoPath, options =
async getRootFiles(repoPath)
categorizeFiles(files)
async analyzeImportDependencies(repoPath, categorized)
async analyzePythonImports(filePath, repoPath)
isLocalModule(moduleName, repoPath)
generateRecommendations(categorized, dependencies)
⋮----
// Identify as library if:
// - Has "lib", "utils", "helper", "middleware", "config" in name
// - Is imported by other root files
// - Is not a main entry point (server, app, main)
⋮----
calculateImportChanges(files, targetDir, dependencies)
⋮----
// This module's imports from other root files need updating
⋮----
generateMoveCommands(files, targetDir)
generateReport(analysis)
⋮----
// Category breakdown
⋮----
// Recommendations
⋮----
export function createRootDirectoryAnalyzer(options)
</file>

<file path="pipeline-core/scanners/timeout_detector.py">
@dataclass
class Finding
⋮----
file_path: str
line_number: int
severity: str
category: str
message: str
code_snippet: str
recommendation: str
⋮----
@dataclass
class FileContext
⋮----
file_path: Path
content: str
lines: list[str]
def has_text_in_range(self, text: str, start_line: int, num_lines: int) -> bool
⋮----
end = min(start_line + num_lines, len(self.lines))
⋮----
has_try = ctx.has_pattern_in_range(r'\btry\b', line_num, 50)
has_catch = ctx.has_pattern_in_range(r'\bcatch\b', line_num, 50)
⋮----
PATTERN_DETECTORS: list[Callable[[FileContext, int, str], Finding | None]] = [
EXCLUDED_DIRS = frozenset({'node_modules', '.git', 'dist', 'build'})
def _should_include_file(file_path: Path) -> bool
class TimeoutDetector
⋮----
def __init__(self, logger: Callable[[str], None] | None = None) -> None
def scan_directory(self, repo_path: str) -> dict[str, Any]
⋮----
path = Path(repo_path)
⋮----
files = list(self._find_files(path))
⋮----
def _find_files(self, repo_path: Path) -> Iterator[Path]
⋮----
extensions = {'.ts', '.tsx', '.js', '.jsx', '.py'}
⋮----
def _scan_file(self, file_path: Path) -> None
⋮----
"""Scan a single file for patterns."""
⋮----
content = file_path.read_text(encoding='utf-8')
⋮----
ctx = FileContext(file_path=file_path, content=content, lines=content.split('\n'))
⋮----
def _check_line(self, ctx: FileContext, line_num: int, line: str) -> None
⋮----
"""Check a line against all pattern detectors."""
⋮----
finding = detector(ctx, line_num, line)
⋮----
def _calculate_statistics(self) -> dict[str, Any]
⋮----
"""Calculate statistics from findings."""
⋮----
severity_counts = Counter(f.severity for f in self.findings)
category_counts = Counter(f.category for f in self.findings)
files_affected = {f.file_path for f in self.findings}
⋮----
def generate_report(self, results: dict[str, Any]) -> str
⋮----
"""Generate markdown report."""
stats = results['statistics']
lines = [
severity_emoji = {'high': '🔴', 'medium': '🟡', 'low': '🟢'}
⋮----
emoji = severity_emoji.get(severity, '⚪')
⋮----
def _add_findings_by_severity(self, lines: list[str], max_per_severity: int = 10) -> None
⋮----
"""Add findings grouped by severity to report lines."""
⋮----
severity_findings = [f for f in self.findings if f.severity == severity]
⋮----
remaining = len(severity_findings) - max_per_severity
⋮----
def main()
⋮----
repo_path = sys.argv[1]
output_file = None
⋮----
output_index = sys.argv.index('--output')
⋮----
output_file = sys.argv[output_index + 1]
detector = TimeoutDetector()
results = detector.scan_directory(repo_path)
⋮----
output = json.dumps(results, indent=2)
⋮----
output = detector.generate_report(results)
⋮----
high_severity_count = results['statistics']['severity_breakdown'].get('high', 0)
</file>

<file path="pipeline-core/scanners/timeout-pattern-detector.js">
export class TimeoutPatternDetector
⋮----
async scan(repoPath, options =
async findPromiseRaceWithoutTimeout(repoPath)
async findLoadingWithoutFinally(repoPath)
async findAsyncWithoutErrorHandling(repoPath)
async findMissingTimeoutConstants(repoPath)
async findSetLoadingWithoutReset(repoPath)
async searchPattern(repoPath, pattern, options =
async readFile(filePath)
/**
   * Group matches by file
   */
groupByFile(matches)
/**
   * Calculate severity breakdown
   */
calculateSeverityBreakdown(findings)
generateReport(findings)
⋮----
export function createTimeoutPatternDetector(options)
</file>

<file path="pipeline-core/similarity/__init__.py">
__all__ = [
</file>

<file path="pipeline-core/similarity/__init__.pyi">
"""Type stubs for similarity package."""

from typing import List, Tuple

from .structural import (
    calculate_structural_similarity as calculate_structural_similarity,
    normalize_code as normalize_code,
)
from .grouping import group_by_similarity as group_by_similarity

__all__: list[str]
</file>

<file path="pipeline-core/similarity/config.py">
class SimilarityConfig
⋮----
DEBUG = os.environ.get('PIPELINE_DEBUG', '').lower() in ('1', 'true', 'yes')
MIN_LINE_COUNT = int(os.getenv('MIN_LINE_COUNT', '1'))
MIN_UNIQUE_TOKENS = int(os.getenv('MIN_UNIQUE_TOKENS', '5'))
STRUCTURAL_THRESHOLD = float(os.getenv('STRUCTURAL_THRESHOLD', '0.90'))
OPPOSITE_LOGIC_PENALTY = float(os.getenv('OPPOSITE_LOGIC_PENALTY', '0.8'))
STATUS_CODE_PENALTY = float(os.getenv('STATUS_CODE_PENALTY', '0.7'))
SEMANTIC_METHOD_PENALTY = float(os.getenv('SEMANTIC_METHOD_PENALTY', '0.85'))
CHAIN_WEIGHT_LEVENSHTEIN = float(os.getenv('CHAIN_WEIGHT_LEVENSHTEIN', '0.7'))
CHAIN_WEIGHT_CHAIN = float(os.getenv('CHAIN_WEIGHT_CHAIN', '0.3'))
MIN_COMPLEXITY_RATIO = float(os.getenv('MIN_COMPLEXITY_RATIO', '0.5'))
SEMANTIC_SIMILARITY_THRESHOLD = float(os.getenv('SEMANTIC_SIMILARITY_THRESHOLD', '0.70'))
MIN_GROUP_QUALITY = float(os.getenv('MIN_GROUP_QUALITY', '0.70'))
QUALITY_WEIGHT_SIMILARITY = float(os.getenv('QUALITY_WEIGHT_SIMILARITY', '0.4'))
QUALITY_WEIGHT_SIZE = float(os.getenv('QUALITY_WEIGHT_SIZE', '0.2'))
QUALITY_WEIGHT_COMPLEXITY = float(os.getenv('QUALITY_WEIGHT_COMPLEXITY', '0.2'))
QUALITY_WEIGHT_SEMANTIC = float(os.getenv('QUALITY_WEIGHT_SEMANTIC', '0.2'))
SIZE_NORMALIZATION = float(os.getenv('SIZE_NORMALIZATION', '4.0'))
COMPLEXITY_NORMALIZATION = float(os.getenv('COMPLEXITY_NORMALIZATION', '10.0'))
SEMANTIC_PERFECT_CONSISTENCY = 1.0
SEMANTIC_SAME_CATEGORY = 0.7
SEMANTIC_SAME_PATTERN = 0.5
SEMANTIC_MIXED = 0.3
⋮----
@classmethod
    def to_dict(cls) -> dict
⋮----
@classmethod
    def print_config(cls)
config = SimilarityConfig()
</file>

<file path="pipeline-core/similarity/config.pyi">
"""Type stubs for similarity config module."""


class SimilarityConfig:
    """Configuration for multi-layer similarity algorithm."""

    # Debug mode
    DEBUG: bool

    # Layer 0: Complexity filtering
    MIN_LINE_COUNT: int
    MIN_UNIQUE_TOKENS: int

    # Layer 2: Structural similarity
    STRUCTURAL_THRESHOLD: float

    # Penalties
    OPPOSITE_LOGIC_PENALTY: float
    STATUS_CODE_PENALTY: float
    SEMANTIC_METHOD_PENALTY: float

    # Method chain validation
    CHAIN_WEIGHT_LEVENSHTEIN: float
    CHAIN_WEIGHT_CHAIN: float

    # Layer 3: Semantic validation
    MIN_COMPLEXITY_RATIO: float
    SEMANTIC_SIMILARITY_THRESHOLD: float

    # Layer 4: Quality filtering
    MIN_GROUP_QUALITY: float

    # Quality score weights
    QUALITY_WEIGHT_SIMILARITY: float
    QUALITY_WEIGHT_SIZE: float
    QUALITY_WEIGHT_COMPLEXITY: float
    QUALITY_WEIGHT_SEMANTIC: float

    # Quality normalization factors
    SIZE_NORMALIZATION: float
    COMPLEXITY_NORMALIZATION: float

    # Semantic consistency scores
    SEMANTIC_PERFECT_CONSISTENCY: float
    SEMANTIC_SAME_CATEGORY: float
    SEMANTIC_SAME_PATTERN: float
    SEMANTIC_MIXED: float

    @classmethod
    def to_dict(cls) -> dict: ...
    @classmethod
    def print_config(cls) -> None: ...


config: SimilarityConfig
</file>

<file path="pipeline-core/similarity/grouping.py">
DEBUG = SimilarityConfig.DEBUG
⋮----
MIN_COMPLEXITY_THRESHOLD = {
MIN_GROUP_QUALITY = SimilarityConfig.MIN_GROUP_QUALITY
OPPOSITE_OPERATOR_PAIRS: list[tuple[set[str], set[str]]] = [
⋮----
@dataclass
class SemanticCheckResult
⋮----
is_valid: bool
reason: str
details: tuple[Any, Any] | None = None
def _check_method_chain(code1: str, code2: str) -> SemanticCheckResult
⋮----
chain1 = extract_method_chain(code1)
chain2 = extract_method_chain(code2)
⋮----
def _check_http_status_codes(code1: str, code2: str) -> SemanticCheckResult
⋮----
status1 = extract_http_status_codes(code1)
status2 = extract_http_status_codes(code2)
⋮----
def _check_logical_operators(code1: str, code2: str) -> SemanticCheckResult
⋮----
ops1 = extract_logical_operators(code1)
ops2 = extract_logical_operators(code2)
⋮----
has_opposite = (
⋮----
def _check_semantic_methods(code1: str, code2: str) -> SemanticCheckResult
⋮----
methods1 = extract_semantic_methods(code1)
methods2 = extract_semantic_methods(code2)
⋮----
SEMANTIC_CHECKS: list[Callable[[str, str], SemanticCheckResult]] = [
def _extract_function_names(blocks: List['CodeBlock']) -> list[str]
⋮----
func_names = []
⋮----
def _run_semantic_checks(code1: str, code2: str) -> SemanticCheckResult
⋮----
result = check(code1, code2)
⋮----
def calculate_code_complexity(source_code: str) -> dict
⋮----
lines = [line.strip() for line in source_code.split('\n') if line.strip()]
line_count = len(lines)
tokens = re.findall(r'\b\w+\b', source_code)
unique_tokens = len(set(tokens))
control_flow_keywords = ['if', 'else', 'for', 'while', 'switch', 'case', 'try', 'catch']
has_control_flow = any(keyword in source_code for keyword in control_flow_keywords)
⋮----
def is_complex_enough(block: 'CodeBlock') -> bool
⋮----
complexity = calculate_code_complexity(block.source_code)
⋮----
def calculate_group_quality_score(group_blocks: List['CodeBlock'], similarity_score: float) -> float
⋮----
w_sim = SimilarityConfig.QUALITY_WEIGHT_SIMILARITY
w_size = SimilarityConfig.QUALITY_WEIGHT_SIZE
w_complexity = SimilarityConfig.QUALITY_WEIGHT_COMPLEXITY
w_semantic = SimilarityConfig.QUALITY_WEIGHT_SEMANTIC
similarity_factor = similarity_score * w_sim
size_norm = SimilarityConfig.SIZE_NORMALIZATION
size_factor = min(len(group_blocks) / size_norm, 1.0) * w_size
avg_line_count = sum(b.line_count for b in group_blocks) / len(group_blocks)
complexity_norm = SimilarityConfig.COMPLEXITY_NORMALIZATION
complexity_factor = min(avg_line_count / complexity_norm, 1.0) * w_complexity
categories = set(b.category for b in group_blocks)
pattern_ids = set(b.pattern_id for b in group_blocks)
⋮----
semantic_score = SimilarityConfig.SEMANTIC_PERFECT_CONSISTENCY
⋮----
semantic_score = SimilarityConfig.SEMANTIC_SAME_CATEGORY
⋮----
semantic_score = SimilarityConfig.SEMANTIC_SAME_PATTERN
⋮----
semantic_score = SimilarityConfig.SEMANTIC_MIXED
semantic_factor = semantic_score * w_semantic
total_quality = similarity_factor + size_factor + complexity_factor + semantic_factor
⋮----
def validate_exact_group_semantics(group_blocks: List['CodeBlock']) -> tuple
⋮----
func_names = _extract_function_names(group_blocks)
⋮----
result = _run_semantic_checks(
⋮----
# Check group quality
quality_score = calculate_group_quality_score(group_blocks, similarity_score)
⋮----
# Accept group
group = _create_duplicate_group(group_blocks, similarity_score, similarity_method)
⋮----
# Mark blocks as grouped
⋮----
"""
    Group code blocks using multi-layer similarity algorithm with complexity filtering.
    Implements Layer 0 (complexity), Layer 1 (exact), Layer 2 (structural), and Layer 3 (semantic).
    Algorithm:
    0. Layer 0: Filter trivial blocks (below complexity threshold)
    1. Layer 1: Group by exact content hash (O(n))
    2. Layer 2: Group remaining by structural similarity (O(n*k))
    3. Layer 3: Semantic validation (pattern, category, tags)
    Returns:
        List of DuplicateGroup objects with similarity scores
    """
# Layer 0: Filter out trivial blocks before grouping
complex_blocks = [b for b in blocks if is_complex_enough(b)]
trivial_count = len(blocks) - len(complex_blocks)
⋮----
groups = []
grouped_block_ids = set()
# Layer 1: Exact matching (hash-based)
⋮----
exact_groups = _group_by_exact_hash(complex_blocks)
⋮----
ungrouped_blocks = [b for b in complex_blocks if b.block_id not in grouped_block_ids]
⋮----
structural_groups = _group_by_structural_similarity(ungrouped_blocks, similarity_threshold)
layer1_count = len(groups)
⋮----
# Annotate all ungrouped blocks with optional timing
annotator = SemanticAnnotator(collect_timing=DEBUG)
layer3_start = time.perf_counter()
annotations = {
annotation_time_ms = (time.perf_counter() - layer3_start) * 1000
layer2_count = len(groups)
grouping_start = time.perf_counter()
semantic_groups = _group_by_semantic_similarity(
grouping_time_ms = (time.perf_counter() - grouping_start) * 1000
⋮----
# Print timing summary when DEBUG
⋮----
timing_report = annotator.get_timing_report()
⋮----
def _group_by_exact_hash(blocks: List['CodeBlock']) -> Dict[str, List['CodeBlock']]
⋮----
hash_groups = defaultdict(list)
⋮----
hash_val = block.content_hash
⋮----
func_names = _extract_function_names([block])
func_name = func_names[0] if func_names else 'unknown'
⋮----
"""
    Group blocks by structural similarity using clustering.
    Returns:
        List of (group_blocks, similarity_score) tuples
    """
⋮----
# Build similarity matrix
n = len(blocks)
⋮----
used = set()
# For each block, find all structurally similar blocks
⋮----
# Start a new group with this block
group = [block1]
similarities = []
# Compare with remaining blocks
⋮----
block2 = blocks[j]
# Pre-check semantic compatibility
⋮----
continue  # Skip incompatible blocks
# Calculate structural similarity
⋮----
# If we found similar blocks, validate and create a group
⋮----
# Validate complete group
⋮----
# Average similarity score for the group
avg_similarity = sum(similarities) / len(similarities) if similarities else 1.0
⋮----
# Group failed semantic validation
⋮----
# ---------------------------------------------------------------------------
# Layer 3: Semantic Similarity
⋮----
# Weights for semantic similarity calculation
SEMANTIC_WEIGHTS = {
⋮----
'operations': 0.40,  # What the code does
'domains': 0.25,     # What domain it operates on
'patterns': 0.20,    # What patterns it uses
'data_types': 0.15,  # What data types it processes
⋮----
# Threshold for semantic similarity matching (from config)
SEMANTIC_SIMILARITY_THRESHOLD = SimilarityConfig.SEMANTIC_SIMILARITY_THRESHOLD
def _calculate_jaccard_similarity(set1: set[str], set2: set[str]) -> float
⋮----
"""Calculate Jaccard similarity between two sets.
    Args:
        set1: First set of strings
        set2: Second set of strings
    Returns:
        Jaccard similarity coefficient (0.0 - 1.0)
    """
⋮----
return 1.0  # Both empty = compatible
⋮----
return 0.5  # One empty = partial match
intersection = len(set1 & set2)
union = len(set1 | set2)
⋮----
"""Calculate weighted semantic similarity between two annotations.
    Uses Jaccard similarity on each tag category with predefined weights:
    - Operations: 40%
    - Domains: 25%
    - Patterns: 20%
    - Data types: 15%
    Args:
        ann1: First semantic annotation
        ann2: Second semantic annotation
    Returns:
        Weighted similarity score (0.0 - 1.0)
    """
op_sim = _calculate_jaccard_similarity(ann1.operations, ann2.operations)
domain_sim = _calculate_jaccard_similarity(ann1.domains, ann2.domains)
pattern_sim = _calculate_jaccard_similarity(ann1.patterns, ann2.patterns)
type_sim = _calculate_jaccard_similarity(ann1.data_types, ann2.data_types)
⋮----
def _intents_compatible(intent1: str, intent2: str) -> bool
⋮----
intent2: Second intent string
</file>

<file path="pipeline-core/similarity/grouping.pyi">
"""Type stubs for grouping module."""

from dataclasses import dataclass
from typing import Any, Callable, Dict, List, Set, Tuple

from ..models.code_block import CodeBlock
from ..models.duplicate_group import DuplicateGroup
from ..annotators.semantic_annotator import SemanticAnnotation


@dataclass
class SemanticCheckResult:
    """Result of a semantic compatibility check."""
    is_valid: bool
    reason: str
    details: Tuple[Any, Any] | None = ...


# Module constants
MIN_COMPLEXITY_THRESHOLD: Dict[str, int]
MIN_GROUP_QUALITY: float
OPPOSITE_OPERATOR_PAIRS: List[Tuple[Set[str], Set[str]]]
SEMANTIC_CHECKS: List[Callable[[str, str], SemanticCheckResult]]
SEMANTIC_WEIGHTS: Dict[str, float]
SEMANTIC_SIMILARITY_THRESHOLD: float
DEBUG: bool


def calculate_code_complexity(source_code: str) -> dict: ...
def is_complex_enough(block: CodeBlock) -> bool: ...
def calculate_group_quality_score(
    group_blocks: List[CodeBlock], similarity_score: float
) -> float: ...
def validate_exact_group_semantics(group_blocks: List[CodeBlock]) -> tuple: ...
def group_by_similarity(
    blocks: List[CodeBlock], similarity_threshold: float = ...
) -> List[DuplicateGroup]: ...
</file>

<file path="pipeline-core/similarity/semantic.py">
def are_semantically_compatible(block1: 'CodeBlock', block2: 'CodeBlock') -> bool
⋮----
tags1 = set(block1.tags)
tags2 = set(block2.tags)
func1 = _extract_function_tag(tags1)
func2 = _extract_function_tag(tags2)
⋮----
line_ratio = min(block1.line_count, block2.line_count) / max(block1.line_count, block2.line_count)
⋮----
def calculate_tag_overlap(block1: 'CodeBlock', block2: 'CodeBlock') -> float
⋮----
intersection = tags1 & tags2
union = tags1 | tags2
⋮----
def _extract_function_tag(tags: set) -> Optional[str]
def validate_duplicate_group(blocks: List['CodeBlock']) -> bool
⋮----
pattern_ids = set(b.pattern_id for b in blocks)
⋮----
categories = set(b.category for b in blocks)
</file>

<file path="pipeline-core/similarity/semantic.pyi">
"""Type stubs for semantic similarity module."""

from typing import List, Optional

from ..models.code_block import CodeBlock


def are_semantically_compatible(block1: CodeBlock, block2: CodeBlock) -> bool: ...
def calculate_tag_overlap(block1: CodeBlock, block2: CodeBlock) -> float: ...
def validate_duplicate_group(blocks: List[CodeBlock]) -> bool: ...
</file>

<file path="pipeline-core/similarity/structural.py">
@dataclass
class SemanticFeatures
⋮----
http_status_codes: Set[int] = field(default_factory=set)
logical_operators: Set[str] = field(default_factory=set)
semantic_methods: Set[str] = field(default_factory=set)
def extract_semantic_features(source_code: str) -> SemanticFeatures
⋮----
features = SemanticFeatures()
⋮----
status_pattern = r'\.status\((\d{3})\)'
⋮----
status_code = int(match.group(1))
⋮----
# Extract logical operators (===, !==, ==, !=, !, &&, ||)
# Order matters: match longer operators first to avoid partial matches
operator_patterns = [
⋮----
(r'!==', '!=='),   # Strict inequality
(r'===', '==='),   # Strict equality
(r'!=', '!='),     # Loose inequality
(r'==', '=='),     # Loose equality
(r'!\s*[^=]', '!'), # Logical NOT (followed by non-=)
(r'&&', '&&'),     # Logical AND
(r'\|\|', '||'),   # Logical OR
⋮----
# Extract semantic methods (Math.max, Math.min, console.log, etc.)
semantic_patterns = {
⋮----
def normalize_code(source_code: str) -> str
⋮----
"""
    Normalize code by removing variable-specific information.
    This allows structural comparison by focusing on code structure
    rather than specific names or values.
    """
⋮----
# Remove comments
normalized = re.sub(r'//.*?$', '', source_code, flags=re.MULTILINE)  # Single-line comments
normalized = re.sub(r'/\*.*?\*/', '', normalized, flags=re.DOTALL)   # Multi-line comments
# Normalize whitespace (collapse to single spaces)
normalized = re.sub(r'\s+', ' ', normalized)
# Normalize string literals (replace with placeholder)
normalized = re.sub(r"'[^']*'", "'STR'", normalized)
normalized = re.sub(r'"[^"]*"', '"STR"', normalized)
normalized = re.sub(r'`[^`]*`', '`STR`', normalized)
# Normalize numbers (replace with placeholder)
normalized = re.sub(r'\b\d+\b', 'NUM', normalized)
SEMANTIC_METHODS = {
SEMANTIC_OBJECTS = {
⋮----
normalized = re.sub(rf'\b{obj}\b', f'__PRESERVE_OBJ_{obj.upper()}__', normalized)
⋮----
normalized = re.sub(rf'\b{method}\b', f'__PRESERVE_{method.upper()}__', normalized)
normalized = re.sub(r'\b[a-z][a-zA-Z0-9_]*\b', 'var', normalized)
normalized = re.sub(r'\b[A-Z][A-Z0-9_]*\b', 'CONST', normalized)
⋮----
normalized = normalized.replace(f'__PRESERVE_OBJ_{obj.upper()}__', obj)
⋮----
normalized = normalized.replace(f'__PRESERVE_{method.upper()}__', method)
normalized = re.sub(r'\s*([(){}[\];,.])\s*', r'\1', normalized)
normalized = re.sub(r'\s*(=>|===?|!==?|[+\-*/%<>=&|])\s*', r' \1 ', normalized)
# Collapse multiple spaces
⋮----
# Trim
normalized = normalized.strip()
⋮----
def calculate_ast_hash(source_code: str) -> str
⋮----
Penalties are multiplicative - each mismatch reduces similarity:
- Logical operators: 0.80x (20% penalty)
- Semantic methods: 0.75x (25% penalty)
Args:
features2: Semantic features from second code block
Returns:
</file>

<file path="pipeline-core/similarity/structural.pyi">
"""Type stubs for structural similarity module."""

from dataclasses import dataclass
from typing import Set, Tuple


@dataclass
class SemanticFeatures:
    """Semantic features extracted from original code before normalization."""
    http_status_codes: Set[int]
    logical_operators: Set[str]
    semantic_methods: Set[str]


def extract_semantic_features(source_code: str) -> SemanticFeatures: ...
def normalize_code(source_code: str) -> str: ...
def calculate_ast_hash(source_code: str) -> str: ...
def calculate_levenshtein_similarity(str1: str, str2: str) -> float: ...
def extract_logical_operators(source_code: str) -> set: ...
def extract_http_status_codes(source_code: str) -> set: ...
def extract_semantic_methods(source_code: str) -> set: ...
def extract_method_chain(source_code: str) -> list: ...
def compare_method_chains(code1: str, code2: str) -> float: ...
def calculate_semantic_penalty(
    features1: SemanticFeatures, features2: SemanticFeatures
) -> float: ...
def calculate_structural_similarity(
    code1: str, code2: str, threshold: float = ...
) -> Tuple[float, str]: ...
def are_structurally_similar(
    code1: str, code2: str, threshold: float = ...
) -> bool: ...
</file>

<file path="pipeline-core/similarity/test_grouping_layer3.py">
@dataclass
class MockCodeBlock
⋮----
block_id: str
source_code: str
category: str = 'utility'
tags: list = field(default_factory=list)
pattern_id: str = 'test-pattern'
line_count: int = 5
location: object = None
repository_path: str = '/test'
language: str = 'javascript'
content_hash: str = 'abc123'
def __post_init__(self)
class TestJaccardSimilarity
⋮----
def test_identical_sets(self)
⋮----
result = _calculate_jaccard_similarity({'a', 'b', 'c'}, {'a', 'b', 'c'})
⋮----
def test_disjoint_sets(self)
⋮----
result = _calculate_jaccard_similarity({'a', 'b'}, {'c', 'd'})
⋮----
def test_partial_overlap(self)
⋮----
result = _calculate_jaccard_similarity({'a', 'b', 'c'}, {'b', 'c', 'd'})
⋮----
def test_both_empty(self)
⋮----
result = _calculate_jaccard_similarity(set(), set())
⋮----
def test_one_empty(self)
⋮----
result = _calculate_jaccard_similarity({'a', 'b'}, set())
⋮----
result = _calculate_jaccard_similarity(set(), {'a', 'b'})
⋮----
def test_subset(self)
⋮----
result = _calculate_jaccard_similarity({'a'}, {'a', 'b'})
⋮----
class TestSemanticSimilarity
⋮----
def test_identical_annotations(self)
⋮----
ann = SemanticAnnotation(
result = _calculate_semantic_similarity(ann, ann)
⋮----
def test_different_operations_major_impact(self)
⋮----
ann1 = SemanticAnnotation(
ann2 = SemanticAnnotation(
result = _calculate_semantic_similarity(ann1, ann2)
expected = 0.0 * 0.4 + 1.0 * 0.25 + 1.0 * 0.20 + 1.0 * 0.15
⋮----
def test_different_domains_moderate_impact(self)
⋮----
expected = 1.0 * 0.4 + 0.0 * 0.25 + 1.0 * 0.20 + 1.0 * 0.15
⋮----
def test_similar_operations_high_score(self)
def test_weights_sum_to_one(self)
⋮----
total = sum(SEMANTIC_WEIGHTS.values())
⋮----
class TestIntentCompatibility
⋮----
def test_identical_intents(self)
def test_shared_operation(self)
def test_no_shared_operation(self)
def test_unknown_intent(self)
def test_both_unknown(self)
def test_empty_operations(self)
class TestSemanticGrouping
⋮----
def test_groups_same_category_similar_operations(self)
⋮----
blocks = [
annotations = {
groups = _group_by_semantic_similarity(blocks, annotations, threshold=0.70)
⋮----
def test_different_categories_not_grouped(self)
def test_incompatible_intents_not_grouped(self)
def test_below_threshold_not_grouped(self)
⋮----
groups = _group_by_semantic_similarity(blocks, annotations, threshold=0.90)
⋮----
def test_empty_blocks_returns_empty(self)
⋮----
groups = _group_by_semantic_similarity([], {}, threshold=0.70)
⋮----
def test_single_block_returns_empty(self)
⋮----
blocks = [MockCodeBlock(block_id='b1', source_code='code1')]
⋮----
def test_multiple_groups_formed(self)
def test_missing_annotation_skipped(self)
def run_tests()
⋮----
test_classes = [
passed = 0
failed = 0
⋮----
instance = test_class()
⋮----
method = getattr(instance, name)
</file>

<file path="pipeline-core/types/scan-orchestrator-types.ts">
import { z } from 'zod';
⋮----
export type RepositoryInfo = z.infer<typeof RepositoryInfoSchema>;
export type PatternMatch = z.infer<typeof PatternMatchSchema>;
export type ScanConfig = z.infer<typeof ScanConfigSchema>;
export type PythonPipelineInput = z.infer<typeof PythonPipelineInputSchema>;
export type CodeBlock = z.infer<typeof CodeBlockSchema>;
export type DuplicateGroup = z.infer<typeof DuplicateGroupSchema>;
export type ConsolidationSuggestion = z.infer<typeof ConsolidationSuggestionSchema>;
export type ScanMetrics = z.infer<typeof ScanMetricsSchema>;
export type PythonPipelineOutput = z.infer<typeof PythonPipelineOutputSchema>;
export type ScanResult = z.infer<typeof ScanResultSchema>;
export type ReportOptions = z.infer<typeof ReportOptionsSchema>;
export type ScanOrchestratorOptions = z.infer<typeof ScanOrchestratorOptionsSchema>;
</file>

<file path="pipeline-core/utils/__init__.py">
__all__ = ['TimingMetrics', 'Timer']
</file>

<file path="pipeline-core/utils/__init__.pyi">
"""Type stubs for utils package."""

from .timing import TimingMetrics as TimingMetrics, Timer as Timer

__all__: list[str]
</file>

<file path="pipeline-core/utils/error-helpers.ts">
interface ToErrorObjectOptions {
  fallbackMessage?: string;
  metadata?: Record<string, unknown>;
}
interface ErrorObject {
  message: string;
  stack: string | undefined;
  type: string;
  isError: boolean;
  metadata: Record<string, unknown>;
  name?: string;
  code?: unknown;
}
interface SerializedError {
  message: string;
  type: string;
  name?: string;
  stack?: string;
  cause?: SerializedError;
  code?: unknown;
}
interface FormatErrorOptions {
  includeType?: boolean;
  includeCode?: boolean;
}
export function safeErrorMessage(error: unknown, fallback = 'Unknown error'): string
export function safeErrorStack(error: unknown): string | undefined
export function toErrorObject(error: unknown, options: ToErrorObjectOptions =
function getErrorType(error: unknown): string
export function isErrorLike(value: unknown): boolean
export function serializeError(error: unknown, includeStack = true): SerializedError
export function formatErrorMessage(error: unknown, options: FormatErrorOptions =
export function combineErrors(errors: unknown[], separator = '; '): string
</file>

<file path="pipeline-core/utils/fs-helpers.ts">
import fs from 'fs/promises';
import path from 'path';
export async function ensureDir(dirPath: string): Promise<void>
export async function ensureParentDir(filePath: string): Promise<void>
export async function writeFileWithDir(filePath: string, content: string | Buffer, options:
export async function readFileIfExists(filePath: string, encoding: BufferEncoding = 'utf-8'): Promise<string | null>
export async function pathExists(filePath: string): Promise<boolean>
export function joinLines(lines: string[]): string
</file>

<file path="pipeline-core/utils/index.ts">

</file>

<file path="pipeline-core/utils/process-helpers.ts">

</file>

<file path="pipeline-core/utils/timing-helpers.ts">
import { TIME } from '../../core/constants.ts';
interface Timer {
  elapsed: () => number;
  elapsedMs: () => number;
  elapsedFormatted: () => string;
}
export function createTimer(): Timer
export async function withTiming<T>(fn: () => Promise<T>, _label?: string): Promise<
export function measureSync<T>(fn: () => T):
</file>

<file path="pipeline-core/utils/timing.py">
@dataclass
class TimingMetrics
⋮----
stage_name: str
total_ms: float = 0.0
count: int = 0
min_ms: float = field(default=float('inf'))
max_ms: float = 0.0
def record(self, duration_ms: float) -> None
⋮----
@property
    def avg_ms(self) -> float
def to_dict(self) -> dict
class Timer
⋮----
def __init__(self, metrics: TimingMetrics | None = None) -> None
⋮----
@property
    def elapsed_ms(self) -> float
def __enter__(self) -> Timer
def __exit__(self, *args) -> None
⋮----
end_time = time.perf_counter()
</file>

<file path="pipeline-core/utils/timing.pyi">
"""Type stubs for timing module."""

from dataclasses import dataclass


@dataclass
class TimingMetrics:
    """Collected timing metrics for a processing stage."""

    stage_name: str
    total_ms: float
    count: int
    min_ms: float
    max_ms: float

    def record(self, duration_ms: float) -> None: ...
    @property
    def avg_ms(self) -> float: ...
    def to_dict(self) -> dict: ...


class Timer:
    """Context manager for timing code blocks."""

    def __init__(self, metrics: TimingMetrics | None = ...) -> None: ...
    @property
    def elapsed_ms(self) -> float: ...
    def __enter__(self) -> Timer: ...
    def __exit__(self, *args) -> None: ...
</file>

<file path="pipeline-core/doppler-health-monitor.js">
export class DopplerHealthMonitor
⋮----
async checkCacheHealth()
getSeverity(cacheAge)
async startMonitoring(intervalMinutes = 15)
stopMonitoring()
getCacheDirectoryPath()
</file>

<file path="pipeline-core/inter-project-scanner.js">
export class InterProjectScanner
⋮----
async scanRepositories(repoPaths, scanConfig =
_aggregateCodeBlocks(repositoryScans)
_detectCrossRepoDuplicates(allCodeBlocks)
_calculateCrossRepoImpactScore(group)
_generateCrossRepoSuggestions(crossRepoDuplicates, repositoryScans)
_determineCrossRepoStrategy(group)
_generateCrossRepoRationale(group)
_assessComplexity(group)
_assessRisk(group, strategy)
_suggestCrossRepoLocation(group, strategy)
_calculateCrossRepoROI(group, complexity, risk)
_calculateInterProjectMetrics(repositoryScans, crossRepoDuplicates, suggestions)
async saveResults(results, filename = 'inter-project-scan.json')
</file>

<file path="pipeline-core/py.typed">

</file>

<file path="pipeline-core/scan-orchestrator.ts">
import { RepositoryScanner } from './scanners/repository-scanner.js';
import { AstGrepPatternDetector } from './scanners/ast-grep-detector.js';
import { HTMLReportGenerator } from './reports/html-report-generator.js';
import { MarkdownReportGenerator } from './reports/markdown-report-generator.js';
import { InterProjectScanner } from './inter-project-scanner.js';
import { createComponentLogger, logStart, logStage } from '../utils/logger.ts';
import { DependencyValidator } from '../utils/dependency-validator.js';
import { spawn, ChildProcess, execSync } from 'child_process';
⋮----
import { existsSync } from 'fs';
⋮----
import { TIMEOUTS } from '../core/constants.ts';
⋮----
export interface RepositoryInfo {
  path: string;
  name: string;
  gitRemote?: string;
  gitBranch?: string;
  gitCommit?: string;
  totalFiles: number;
  totalLines: number;
  languages: string[];
}
export interface PatternMatch {
  file_path: string;
  rule_id: string;
  matched_text: string;
  line_start: number;
  line_end: number;
  column_start?: number;
  column_end?: number;
  severity?: string;
  confidence?: number;
}
export interface ScanConfig {
  scan_config?: {
    includeTests?: boolean;
    maxDepth?: number;
    excludePaths?: string[];
    [key: string]: any;
  };
  pattern_config?: {
    rulesDirectory?: string;
    configPath?: string;
    [key: string]: any;
  };
  generateReports?: boolean;
  [key: string]: any;
}
export interface PythonPipelineInput {
  repository_info: RepositoryInfo;
  pattern_matches: PatternMatch[];
  scan_config: ScanConfig;
}
export interface CodeBlock {
  block_id: string;
  file_path: string;
  line_start: number;
  line_end: number;
  source_code: string;
  language: string;
  semantic_category?: string;
  tags: string[];
  complexity_metrics?: {
    cyclomatic: number;
    cognitive: number;
    halstead: Record<string, number>;
  };
}
export interface DuplicateGroup {
  group_id: string;
  block_ids: string[];
  similarity_score: number;
  group_type: 'exact' | 'structural' | 'semantic';
  total_lines: number;
  potential_reduction: number;
  impact_score?: number;
}
export interface ConsolidationSuggestion {
  suggestion_id: string;
  group_id: string;
  suggestion_type: string;
  priority: 'critical' | 'high' | 'medium' | 'low';
  estimated_effort: 'minimal' | 'low' | 'medium' | 'high';
  potential_reduction: number;
  implementation_notes?: string;
  migration_steps?: Array<{
    order: number;
    description: string;
    code_snippet?: string;
  }>;
}
export interface ScanMetrics {
  total_code_blocks: number;
  code_blocks_by_category?: Record<string, number>;
  code_blocks_by_language?: Record<string, number>;
  total_duplicate_groups: number;
  exact_duplicates: number;
  structural_duplicates: number;
  semantic_duplicates: number;
  total_duplicated_lines: number;
  potential_loc_reduction: number;
  duplication_percentage: number;
  total_suggestions: number;
  quick_wins?: number;
  high_priority_suggestions?: number;
}
export interface PythonPipelineOutput {
  code_blocks: CodeBlock[];
  duplicate_groups: DuplicateGroup[];
  suggestions: ConsolidationSuggestion[];
  metrics: ScanMetrics;
  repository_info: RepositoryInfo;
  scan_type?: 'single-project' | 'inter-project';
  error?: string;
  warnings?: string[];
}
export interface ScanResult extends PythonPipelineOutput {
  scan_metadata: {
    duration_seconds: number;
    scanned_at: string;
    repository_path: string;
  };
  report_paths?: ReportPaths;
}
export interface ReportPaths {
  html?: string;
  markdown?: string;
  summary?: string;
  json?: string;
}
export interface ReportOptions {
  outputDir?: string;
  baseName?: string;
  title?: string;
  html?: boolean;
  markdown?: boolean;
  summary?: boolean;
  json?: boolean;
  includeDetails?: boolean;
  maxDuplicates?: number;
  maxSuggestions?: number;
}
export interface ScanOrchestratorOptions {
  scanner?: Record<string, any>;
  detector?: Record<string, any>;
  pythonPath?: string;
  extractorScript?: string;
  reports?: ReportOptions;
  outputDir?: string;
  autoGenerateReports?: boolean;
  config?: Record<string, any>;
}
export interface CrossRepositoryDuplicate {
  group_id: string;
  pattern_id: string;
  content_hash: string;
  member_blocks: CodeBlock[];
  occurrence_count: number;
  repository_count: number;
  affected_repositories: string[];
  affected_files: string[];
  category: string;
  language: string;
  total_lines: number;
  similarity_score: number;
  similarity_method: string;
  impact_score: number;
}
export interface CrossRepositorySuggestion {
  suggestion_id: string;
  group_id: string;
  suggestion_type: string;
  priority: 'critical' | 'high' | 'medium' | 'low';
  estimated_effort: 'minimal' | 'low' | 'medium' | 'high';
  potential_reduction: number;
  affected_repositories: string[];
  implementation_notes?: string;
  migration_steps?: Array<{
    order: number;
    description: string;
    code_snippet?: string;
  }>;
}
export interface MultiRepositoryScanResult {
  repositories: Array<ScanResult | { error: string; repository_path: string }>;
  total_scanned: number;
  successful: number;
  failed: number;
  cross_repository_duplicates?: CrossRepositoryDuplicate[];
  cross_repository_suggestions?: CrossRepositorySuggestion[];
  scan_type?: 'single-project' | 'inter-project';
  metrics?: ScanMetrics & {
    total_cross_repository_groups?: number;
    cross_repository_occurrences?: number;
    cross_repository_duplicated_lines?: number;
  };
}
interface RepositoryScanOutput {
  repository_info: RepositoryInfo;
  metadata?: {
    totalFiles: number;
    totalLines: number;
    languages: string[];
  };
  repomix_output?: any;
}
interface PatternDetectionOutput {
  matches: PatternMatch[];
  statistics: {
    total_matches: number;
    rules_applied: number;
    files_scanned: number;
    scan_duration_ms: number;
  };
}
export class ScanOrchestrator
⋮----
constructor(options: ScanOrchestratorOptions =
private _detectPythonPath(): void
async scanRepository(repoPath: string, scanConfig: ScanConfig =
private _handleProcessClose(
    code: number | null,
    signal: string | null,
    stdout: string,
    stderr: string
):
private _handleSuccess(
    stdout: string,
    stderr: string
):
private _handleSignalTermination(
    signal: string | null,
    stdout: string,
    stderr: string
):
/**
   * Handle error exit (non-zero code)
   */
private _handleError(
    code: number,
    stderr: string
):
private _calculateTimeout(data: PythonPipelineInput): number
private async runPythonPipeline(data: PythonPipelineInput): Promise<PythonPipelineOutput>
⋮----
// Send input data via stdin
⋮----
async generateReports(scanResult: ScanResult, options: ReportOptions =
async scanMultipleRepositories(
    repoPaths: string[],
    scanConfig: ScanConfig = {}
): Promise<MultiRepositoryScanResult>
⋮----
export class ScanError extends Error
⋮----
constructor(message: string, options?:
</file>

<file path="pipeline-runners/bugfix-audit-pipeline.js">
class BugfixAuditPipeline
⋮----
setupEventListeners()
async runBugfixAudit()
async waitForCompletion()
async saveRunSummary(stats, duration, jobs)
scheduleAudits(cronSchedule = '0 1 * * *')
scheduleTonight()
</file>

<file path="pipeline-runners/claude-health-pipeline.js">
class ClaudeHealthPipeline
⋮----
setupEventListeners()
displayResults(result)
async runHealthCheck(options =
async waitForCompletion()
scheduleHealthChecks(cronSchedule)
getStats()
⋮----
function printSummary(result)
⋮----
// Environment
⋮----
// Summary statistics
⋮----
function printRecommendations(recommendations)
function printDetailedChecks(checks)
⋮----
// Hooks
⋮----
// Plugins
⋮----
// Performance
⋮----
function getScoreColor(score)
function formatBytes(bytes)
</file>

<file path="pipeline-runners/collect_git_activity.py">
CODE_DIR = Path.home() / 'code'
REPORTS_DIR = Path.home() / 'reports'
HOME_DIR = Path.home()
ADDITIONAL_REPOS = [
EXCLUDE_PATTERNS = ['vim/bundle', 'node_modules', '.git', 'venv', '.venv']
DEFAULT_MAX_DEPTH = 3
LANGUAGE_EXTENSIONS = {
def find_git_repos(max_depth=DEFAULT_MAX_DEPTH, include_dotfiles=True)
⋮----
repos = []
⋮----
cmd = f"find {CODE_DIR} -maxdepth {max_depth} -name .git -type d"
result = subprocess.run(cmd.split(), capture_output=True, text=True)
⋮----
repo_path = Path(line).parent
⋮----
cmd = f"find {REPORTS_DIR} -maxdepth {max_depth} -name .git -type d"
⋮----
# Check if directory itself is a git repo
⋮----
# Also check subdirectories (depth 1 within dotfile dirs)
⋮----
# Skip directories we can't access (e.g., .Trash)
⋮----
# Add explicit additional repositories
⋮----
def get_repo_stats(repo_path, since_date, until_date=None)
⋮----
git_cmd = f"git log --since={since_date} --all --oneline"
⋮----
result = subprocess.run(git_cmd.split(), capture_output=True, text=True)
commits = len(result.stdout.strip().split('\n')) if result.stdout.strip() else 0
# Get file changes for language analysis
git_cmd = f"git log --since={since_date} --all --name-only --pretty=format:"
⋮----
files = [f for f in result.stdout.strip().split('\n') if f]
# Get parent directory (for organization/grouping)
⋮----
parent = None
⋮----
parent = '~'  # Dotfile at home root
⋮----
parent = repo_path.parent.name  # Inside a dotfile directory
⋮----
parent = repo_path.parent.name
⋮----
def analyze_languages(all_files)
⋮----
"""Analyze file changes by programming language"""
language_stats = defaultdict(int)
⋮----
file_ext = Path(file_path).suffix.lower()
file_name = Path(file_path).name
# Map to language
found = False
⋮----
found = True
⋮----
def find_project_websites(repositories)
⋮----
"""Scan for CNAME files to discover GitHub Pages websites"""
websites = {}
⋮----
repo_path = Path(repo['path'])
cname_file = repo_path / 'CNAME'
⋮----
website = cname_file.read_text().strip()
if website and '.' in website:  # Basic validation
⋮----
def categorize_repositories(repositories)
⋮----
"""Categorize repositories by project type"""
categories = {
⋮----
name = repo['name'].lower()
commits = repo['commits']
# Categorization logic (customize based on your projects)
⋮----
def create_pie_chart_svg(data, title, output_file, width=800, height=600)
⋮----
"""Create SVG pie chart without matplotlib dependency"""
⋮----
radius = min(width, height) / 3
colors = [
total = sum(data.values())
⋮----
svg_parts = [
start_angle = 0
legend_y = 50
⋮----
percent = (value / total) * 100
angle = (value / total) * 360
end_angle = start_angle + angle
# Convert to radians
start_rad = math.radians(start_angle - 90)
end_rad = math.radians(end_angle - 90)
# Calculate arc path
x1 = cx + radius * math.cos(start_rad)
y1 = cy + radius * math.sin(start_rad)
x2 = cx + radius * math.cos(end_rad)
y2 = cy + radius * math.sin(end_rad)
large_arc = 1 if angle > 180 else 0
# Create pie slice
path = f'M {cx},{cy} L {x1},{y1} A {radius},{radius} 0 {large_arc},1 {x2},{y2} Z'
color = colors[i % len(colors)]
⋮----
# Add legend
legend_x = width - 200
⋮----
start_angle = end_angle
⋮----
# Write to file
⋮----
def create_bar_chart_svg(data, title, output_file, width=800, height=600)
⋮----
"""Create SVG horizontal bar chart"""
max_value = max(data.values()) if data else 1
bar_height = 30
spacing = 10
chart_height = len(data) * (bar_height + spacing)
margin_left = 250
margin_top = 50
actual_height = chart_height + margin_top + 50
⋮----
y = margin_top + i * (bar_height + spacing)
bar_width = ((width - margin_left - 100) * value / max_value)
# Bar
⋮----
# Label
⋮----
# Value
⋮----
# ---------------------------------------------------------------------------
# Helper Functions for Main
⋮----
def _calculate_date_range(args) -> tuple[str, str | None] | None
⋮----
"""Calculate date range from command line arguments.
    Returns:
        (since_date, until_date) tuple, or None if invalid arguments
    """
# Handle shorthand flags
⋮----
end_date = datetime.now()
start_date = end_date - timedelta(days=args.days)
⋮----
def _resolve_output_dir(args) -> Path
⋮----
"""Resolve output directory from arguments or use default."""
⋮----
year = datetime.now().year
⋮----
def _collect_repository_stats(repos: list[Path], since_date: str, until_date: str | None) -> tuple[list, list]
⋮----
"""Collect statistics from all repositories.
    Returns:
        (repositories, all_files) tuple
    """
⋮----
repositories = []
all_files = []
⋮----
stats = get_repo_stats(repo, since_date, until_date)
⋮----
"""Compile all activity data into a single dictionary."""
⋮----
language_stats = analyze_languages(all_files)
⋮----
websites = find_project_websites(repositories)
⋮----
categories = categorize_repositories(repositories)
⋮----
def _print_summary(data: dict, output_dir: Path) -> None
⋮----
"""Print activity summary to console."""
repositories = data['repositories']
language_stats = data['languages']
websites = data['websites']
⋮----
sorted_langs = sorted(language_stats.items(), key=lambda x: x[1], reverse=True)
⋮----
def generate_jekyll_report(data: dict, output_file: Path) -> None
⋮----
date_range = data['date_range']
start_date = date_range['start']
end_date = date_range['end']
start = datetime.strptime(start_date, '%Y-%m-%d')
end = datetime.strptime(end_date, '%Y-%m-%d')
days = (end - start).days
⋮----
report_type = "Weekly"
⋮----
report_type = "Monthly"
⋮----
report_type = f"{days}-Day"
report_date = datetime.now().strftime('%Y-%m-%d')
frontmatter = f"""---
content = f"""
⋮----
parent_prefix = f"{repo['parent']}/" if repo['parent'] else ""
⋮----
sorted_langs = sorted(data['languages'].items(), key=lambda x: x[1], reverse=True)
⋮----
"""Generate all SVG visualizations"""
⋮----
# Monthly commits (if available in data)
⋮----
monthly_data = {month: count for month, count in data['monthly'].items()}
⋮----
# Top 10 repositories
top_10 = {}
⋮----
name = repo['name']
⋮----
name = f"{repo['parent']}/{name}"
⋮----
# Project categories
⋮----
category_data = {cat: len(repos) for cat, repos in data['categories'].items() if repos}
⋮----
# Language distribution
⋮----
language_data = data['languages']
⋮----
def main()
⋮----
parser = argparse.ArgumentParser(description='Generate comprehensive git activity report')
⋮----
args = parser.parse_args()
# Calculate date range
date_range = _calculate_date_range(args)
⋮----
repos = find_git_repos(args.max_depth)
⋮----
data = _compile_activity_data(repositories, all_files, since_date, until_date)
default_report_dir = Path.home() / 'code' / 'PersonalSite' / '_reports'
⋮----
report_file = default_report_dir / f'{report_date}-git-activity-report.md'
⋮----
output_dir = _resolve_output_dir(args)
</file>

<file path="pipeline-runners/dashboard-populate-pipeline.js">
async function main()
</file>

<file path="pipeline-runners/duplicate-detection-pipeline.ts">
import { SidequestServer } from '../core/server.ts';
import { RepositoryConfigLoader } from '../pipeline-core/config/repository-config-loader.js';
import { InterProjectScanner } from '../pipeline-core/inter-project-scanner.js';
import { ScanOrchestrator } from '../pipeline-core/scan-orchestrator.ts';
import { ReportCoordinator } from '../pipeline-core/reports/report-coordinator.js';
import { PRCreator, PRCreationResults } from '../pipeline-core/git/pr-creator.ts';
import { createComponentLogger, logStart, logRetry } from '../utils/logger.ts';
import { config } from '../core/config.ts';
import { TIMEOUTS, RETRY } from '../core/constants.ts';
import { isRetryable, getErrorInfo } from '../pipeline-core/errors/error-classifier.ts';
⋮----
import type { Logger } from 'pino';
export enum JobStatus {
  QUEUED = 'queued',
  RUNNING = 'running',
  COMPLETED = 'completed',
  FAILED = 'failed'
}
export enum ScanType {
  INTER_PROJECT = 'inter-project',
  INTRA_PROJECT = 'intra-project'
}
export interface JobData {
  scanType: ScanType | string;
  repositories?: RepositoryConfig[];
  groupName?: string | null;
  type?: string;
}
export interface Job {
  id: string;
  status: JobStatus;
  data: JobData;
  createdAt: Date;
  startedAt: Date | null;
  completedAt: Date | null;
  error: Error | null;
  result: any;
}
export interface RepositoryConfig {
  name: string;
  path: string;
  enabled?: boolean;
  frequency?: string;
  lastScanned?: string | null;
  priority?: number;
  groups?: string[];
  scanHistory?: ScanHistoryEntry[];
}
export interface ScanHistoryEntry {
  date: string;
  status: 'success' | 'failure';
  duration: number;
  duplicatesFound: number;
}
export interface RetryInfo {
  attempts: number;
  lastAttempt: number;
  maxAttempts: number;
  delay: number;
}
export interface ScanMetrics {
  totalScans: number;
  successfulScans: number;
  failedScans: number;
  totalDuplicatesFound: number;
  totalSuggestionsGenerated: number;
  highImpactDuplicates: number;
  prsCreated: number;
  prCreationErrors: number;
}
export interface RetryMetrics {
  activeRetries: number;
  totalRetryAttempts: number;
  jobsBeingRetried: Array<{
    jobId: string;
    attempts: number;
    maxAttempts: number;
    lastAttempt: string;
  }>;
  retryDistribution: {
    attempt1: number;
    attempt2: number;
    attempt3Plus: number;
    nearingLimit: number;
  };
}
export interface ScanResult {
  scan_type: 'single-project' | 'inter-project' | 'intra-project';
  scan_metadata?: {
    duration_seconds: number;
    [key: string]: any;
  };
  metrics: {
    total_duplicate_groups?: number;
    total_cross_repository_groups?: number;
    total_suggestions?: number;
    [key: string]: any;
  };
  duplicate_groups?: DuplicateGroup[];
  cross_repository_duplicates?: DuplicateGroup[];
  suggestions?: Suggestion[];
  [key: string]: any;
}
export interface DuplicateGroup {
  id: string;
  impact_score: number;
  files: Array<{
    path: string;
    repository?: string;
  }>;
  [key: string]: any;
}
export interface Suggestion {
  id: string;
  type: string;
  impact: number;
  files: string[];
  [key: string]: any;
}
export interface PRCreationResult {
  prsCreated: number;
  prUrls: string[];
  errors: Array<{
    message: string;
    [key: string]: any;
  }>;
}
export interface DuplicateDetectionWorkerOptions {
  maxConcurrentScans?: number;
  logDir?: string;
  sentryDsn?: string;
  configPath?: string;
  baseBranch?: string;
  branchPrefix?: string;
  dryRun?: boolean;
  maxSuggestionsPerPR?: number;
  enablePRCreation?: boolean;
}
export interface InterProjectScanJobResult {
  scanType: 'inter-project';
  repositories: number;
  crossRepoDuplicates: number;
  suggestions: number;
  duration: number;
}
export interface IntraProjectScanJobResult {
  scanType: 'intra-project';
  repository: string;
  duplicates: number;
  suggestions: number;
  duration: number;
  prResults: {
    prsCreated: number;
    prUrls: string[];
    errors: number;
  } | null;
}
export type JobResult = InterProjectScanJobResult | IntraProjectScanJobResult;
⋮----
class DuplicateDetectionWorker extends SidequestServer
⋮----
constructor(options: DuplicateDetectionWorkerOptions =
async initialize(): Promise<void>
async runJobHandler(baseJob: import('../core/server.ts').Job): Promise<unknown>
private _getOriginalJobId(jobId: string): string
/**
   * Handle retry logic with exponential backoff
   */
private async _handleRetry(job: Job, error: Error): Promise<boolean>
⋮----
// Get original job ID to track retries correctly
⋮----
// Classify error to determine if retry is appropriate
⋮----
private async _runInterProjectScan(job: Job, repositoryConfigs: RepositoryConfig[]): Promise<InterProjectScanJobResult>
private async _runIntraProjectScan(job: Job, repositoryConfig: RepositoryConfig): Promise<IntraProjectScanJobResult>
private _updateMetrics(scanResult: ScanResult): void
private async _updateRepositoryConfigs(repositoryConfigs: RepositoryConfig[], scanResult: ScanResult): Promise<void>
private async _checkForHighImpactDuplicates(scanResult: ScanResult): Promise<void>
public scheduleScan(scanType: ScanType | string, repositories: RepositoryConfig[], groupName: string | null = null): Job
public async runNightlyScan(): Promise<void>
public getRetryMetrics(): RetryMetrics
public getScanMetrics(): ScanMetrics &
⋮----
async function main(): Promise<void>
</file>

<file path="pipeline-runners/git-activity-pipeline.js">
class GitActivityPipeline
⋮----
setupEventListeners()
async runReport(options =
async waitForCompletion()
scheduleWeeklyReports(cronSchedule = '0 20 * * 0')
scheduleMonthlyReports(cronSchedule = '0 0 1 * *')
</file>

<file path="pipeline-runners/gitignore-pipeline.js">
async function main()
</file>

<file path="pipeline-runners/plugin-management-audit.sh">
set -euo pipefail
if [[ "${BASH_VERSINFO[0]}" -lt 4 ]]; then
    echo "Error: This script requires bash 4 or higher (current: $BASH_VERSION)"
    echo "macOS users can install with: brew install bash"
    echo "Then run with: /usr/local/bin/bash $0 $@"
    exit 1
fi
CLAUDE_CONFIG="${HOME}/.claude/settings.json"
OUTPUT_FORMAT="human"
DETAILED=false
while [[ $
    case $1 in
        --json)
            OUTPUT_FORMAT="json"
            shift
            ;;
        --detailed)
            DETAILED=true
            shift
            ;;
        *)
            echo "Unknown option: $1"
            echo "Usage: $0 [--json] [--detailed]"
            exit 1
            ;;
    esac
done
if [[ ! -f "$CLAUDE_CONFIG" ]]; then
    echo "Error: Claude config not found at $CLAUDE_CONFIG"
    exit 1
fi
ENABLED_PLUGINS=$(jq -r '.enabledPlugins | keys[]' "$CLAUDE_CONFIG" 2>/dev/null || echo "")
PLUGIN_COUNT=$(echo "$ENABLED_PLUGINS" | grep -v '^$' | wc -l | tr -d ' ')
declare -A categories
while IFS= read -r plugin; do
    [[ -z "$plugin" ]] && continue
    if [[ "$plugin" =~ documentation|document ]]; then
        categories["documentation"]+="$plugin "
    elif [[ "$plugin" =~ git|github ]]; then
        categories["git"]+="$plugin "
    elif [[ "$plugin" =~ test|testing ]]; then
        categories["testing"]+="$plugin "
    elif [[ "$plugin" =~ deploy|deployment ]]; then
        categories["deployment"]+="$plugin "
    elif [[ "$plugin" =~ lint|format ]]; then
        categories["linting"]+="$plugin "
    elif [[ "$plugin" =~ docker|container ]]; then
        categories["containers"]+="$plugin "
    elif [[ "$plugin" =~ api|rest|graphql ]]; then
        categories["api"]+="$plugin "
    elif [[ "$plugin" =~ db|database|sql ]]; then
        categories["database"]+="$plugin "
    fi
done <<< "$ENABLED_PLUGINS"
if [[ "$OUTPUT_FORMAT" == "json" ]]; then
    echo "{"
    echo "  \"total_enabled\": $PLUGIN_COUNT,"
    echo "  \"enabled_plugins\": ["
    first=true
    while IFS= read -r plugin; do
        [[ -z "$plugin" ]] && continue
        [[ "$first" == false ]] && echo ","
        echo -n "    \"$plugin\""
        first=false
    done <<< "$ENABLED_PLUGINS"
    echo ""
    echo "  ],"
    echo "  \"potential_duplicates\": {"
    first=true
    for category in "${!categories[@]}"; do
        count=$(echo "${categories[$category]}" | wc -w | tr -d ' ')
        if [[ $count -gt 1 ]]; then
            [[ "$first" == false ]] && echo ","
            echo -n "    \"$category\": ["
            inner_first=true
            for plugin in ${categories[$category]}; do
                [[ "$inner_first" == false ]] && echo -n ", "
                echo -n "\"$plugin\""
                inner_first=false
            done
            echo -n "]"
            first=false
        fi
    done
    echo ""
    echo "  }"
    echo "}"
else
    # Human-readable output
    echo "╔════════════════════════════════════════════════════════════════╗"
    echo "║          Claude Code Plugin Management Audit                   ║"
    echo "╚════════════════════════════════════════════════════════════════╝"
    echo ""
    echo "Total Enabled Plugins: $PLUGIN_COUNT"
    echo ""
    if [[ "$DETAILED" == true ]]; then
        echo "Enabled Plugins:"
        echo "────────────────────────────────────────────────────────────────"
        while IFS= read -r plugin; do
            [[ -z "$plugin" ]] && continue
            echo "  • $plugin"
        done <<< "$ENABLED_PLUGINS"
        echo ""
    fi
    # Show potential duplicates
    has_duplicates=false
    for category in "${!categories[@]}"; do
        count=$(echo "${categories[$category]}" | wc -w | tr -d ' ')
        if [[ $count -gt 1 ]]; then
            has_duplicates=true
        fi
    done
    if [[ "$has_duplicates" == true ]]; then
        echo "⚠️  Potential Duplicate Categories:"
        echo "────────────────────────────────────────────────────────────────"
        for category in "${!categories[@]}"; do
            count=$(echo "${categories[$category]}" | wc -w | tr -d ' ')
            if [[ $count -gt 1 ]]; then
                echo ""
                echo "  Category: $category ($count plugins)"
                for plugin in ${categories[$category]}; do
                    echo "    • $plugin"
                done
            fi
        done
        echo ""
        echo "💡 Consider reviewing these categories for consolidation."
    else
        echo "✅ No obvious duplicate categories detected."
    fi
    echo ""
    echo "Recommendations:"
    echo "────────────────────────────────────────────────────────────────"
    if [[ $PLUGIN_COUNT -gt 30 ]]; then
        echo "  • High plugin count ($PLUGIN_COUNT). Consider disabling unused plugins."
    fi
    if [[ "$has_duplicates" == true ]]; then
        echo "  • Review duplicate categories above."
        echo "  • Keep only the plugins you actively use in each category."
    fi
    echo "  • Run: npm run status to see plugin usage statistics"
    echo "  • Backup before changes: npm run backup"
    echo ""
fi
# Exit with status
if [[ "$has_duplicates" == true ]] || [[ $PLUGIN_COUNT -gt 30 ]]; then
    exit 1
else
    exit 0
fi
</file>

<file path="pipeline-runners/plugin-management-pipeline.js">
class PluginManagementPipeline
⋮----
setupEventListeners()
displayRecommendations(result)
/**
   * Run a single audit
   * @param {Object} options - Audit options
   */
async runAudit(options =
async waitForCompletion()
scheduleAudits(cronSchedule = '0 9 * * 1')
</file>

<file path="pipeline-runners/repo-cleanup-pipeline.js">
async function main()
</file>

<file path="pipeline-runners/schema-enhancement-pipeline.js">
class SchemaEnhancementPipeline
⋮----
setupEventListeners()
async scanForReadmes(dir, baseDir = dir, results = [])
async scanDirectory(directory)
async createEnhancementJobs(readmeFiles)
async runEnhancement(directory = this.baseDir)
async waitForCompletion()
scheduleEnhancements(cronSchedule = '0 3 * * 0')
</file>

<file path="pipeline-runners/test-refactor-pipeline.ts">
import { TestRefactorWorker } from '../workers/test-refactor-worker.js';
import { DirectoryScanner } from '../utils/directory-scanner.js';
import { createComponentLogger } from '../utils/logger.ts';
import { config } from '../core/config.ts';
import { TIMEOUTS } from '../core/constants.ts';
import cron from 'node-cron';
import path from 'path';
⋮----
interface DirectoryInfo {
  path: string;
  name: string;
}
interface PipelineMetrics {
  totalProjects: number;
  successfulRefactors: number;
  failedRefactors: number;
  filesGenerated: number;
  patternsDetected: number;
  stringsExtracted: number;
  recommendationsGenerated: number;
}
interface PipelineStats {
  total: number;
  queued: number;
  active: number;
  completed: number;
  failed: number;
}
interface PipelineResult {
  metrics: PipelineMetrics;
  stats: PipelineStats;
}
async function runPipeline(targetPath: string | null = null): Promise<PipelineResult>
async function hasTestDirectory(dirPath: string): Promise<boolean>
function waitForCompletion(worker: TestRefactorWorker): Promise<void>
⋮----
const checkCompletion = () =>
⋮----
async function main(): Promise<void>
</file>

<file path="pipeline-runners/universal-repo-cleanup.sh">
set -e
set -u
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'
TARGET_DIR="${1:-$(pwd)}"
TARGET_DIR="$(cd "$TARGET_DIR" 2>/dev/null && pwd)" || {
    echo -e "${RED}✗ Error: Directory '$1' does not exist${NC}"
    exit 1
}
################################################################################
# Configuration - Customize what gets cleaned
################################################################################
# Python virtual environment directory names (common patterns)
VENV_PATTERNS=(
    "venv"
    ".venv"
    "env"
    ".env"
    "virtualenv"
    "*.venv"
    "personal_site"
)
BUILD_ARTIFACTS=(
    ".jekyll-cache"
    ".sass-cache"
    ".bundle"
    "node_modules/.cache"
    "dist"
    "build"
    ".next"
    ".nuxt"
    "out"
    ".output"
    "target"
    ".gradle"
)
TEMP_FILE_PATTERNS=(
    ".DS_Store"
    "*.pyc"
    "*.pyo"
    "__pycache__"
    "*.swp"
    "*.swo"
    "*~"
    ".*.swp"
    "Thumbs.db"
    "desktop.ini"
)
OUTPUT_FILE_PATTERNS=(
    "repomix-output.xml"
    "*.log"
    "npm-debug.log*"
    "yarn-debug.log*"
    "yarn-error.log*"
)
REDUNDANT_DIRS=(
    "drafts"
    "temp"
    "tmp"
    "backup"
    "backups"
    "old"
    "archive"
    "deprecated"
)
print_header() {
    echo -e "\n${BLUE}========================================${NC}"
    echo -e "${BLUE}$1${NC}"
    echo -e "${BLUE}========================================${NC}\n"
}
print_success() {
    echo -e "${GREEN}✓ $1${NC}"
}
print_warning() {
    echo -e "${YELLOW}⚠ $1${NC}"
}
print_error() {
    echo -e "${RED}✗ $1${NC}"
}
print_info() {
    echo -e "${BLUE}→ $1${NC}"
}
get_size() {
    local path="$1"
    if [ -e "$path" ]; then
        du -sh "$path" 2>/dev/null | cut -f1
    else
        echo "N/A"
    fi
}
count_files() {
    local path="$1"
    if [ -d "$path" ]; then
        find "$path" -type f 2>/dev/null | wc -l | tr -d ' '
    else
        echo "0"
    fi
}
scan_venvs() {
    print_info "Scanning for Python virtual environments..." >&2
    local found=()
    for pattern in "${VENV_PATTERNS[@]}"; do
        while IFS= read -r -d '' dir; do
            # Skip if inside node_modules
            if [[ ! "$dir" =~ node_modules ]]; then
                found+=("$dir")
            fi
        done < <(find "$TARGET_DIR" -maxdepth 3 -type d -name "$pattern" -print0 2>/dev/null)
    done
    # Remove duplicates (handle empty array safely)
    [ ${#found[@]} -gt 0 ] && printf '%s\n' "${found[@]}" | sort -u
}
scan_temp_files() {
    print_info "Scanning for temporary/cache files..." >&2
    local found=()
    for pattern in "${TEMP_FILE_PATTERNS[@]}"; do
        while IFS= read -r -d '' file; do
            found+=("$file")
        done < <(find "$TARGET_DIR" -name "$pattern" -print0 2>/dev/null)
    done
    [ ${#found[@]} -gt 0 ] && printf '%s\n' "${found[@]}" | sort -u
}
scan_output_files() {
    print_info "Scanning for output/generated files..." >&2
    local found=()
    for pattern in "${OUTPUT_FILE_PATTERNS[@]}"; do
        while IFS= read -r -d '' file; do
            # Keep root-level repomix-output.xml
            if [[ "$file" != "$TARGET_DIR/repomix-output.xml" ]]; then
                found+=("$file")
            fi
        done < <(find "$TARGET_DIR" -name "$pattern" -print0 2>/dev/null)
    done
    [ ${#found[@]} -gt 0 ] && printf '%s\n' "${found[@]}" | sort -u
}
scan_build_artifacts() {
    print_info "Scanning for build artifacts..." >&2
    local found=()
    for artifact in "${BUILD_ARTIFACTS[@]}"; do
        local path="$TARGET_DIR/$artifact"
        if [ -d "$path" ]; then
            found+=("$path")
        fi
    done
    [ ${
}
scan_redundant_dirs() {
    print_info "Scanning for redundant directories..." >&2
    local found=()
    for dir_name in "${REDUNDANT_DIRS[@]}"; do
        local path="$TARGET_DIR/$dir_name"
        if [ -d "$path" ]; then
            if [ -f "$TARGET_DIR/.gitignore" ] && grep -q "^${dir_name}/\?$" "$TARGET_DIR/.gitignore" 2>/dev/null; then
                found+=("$path (in .gitignore)")
            else
                found+=("$path")
            fi
        fi
    done
    [ ${
}
show_preview() {
    print_header "Repository Cleanup Preview"
    echo "Target Directory: $TARGET_DIR"
    echo "Current Size: $(get_size "$TARGET_DIR")"
    echo ""
    # Scan all categories
    local venvs=($(scan_venvs))
    local temp_files=($(scan_temp_files))
    local output_files=($(scan_output_files))
    local build_artifacts=($(scan_build_artifacts))
    local redundant_dirs=($(scan_redundant_dirs))
    local total_items=0
    # Python venvs
    if [ ${#venvs[@]} -gt 0 ]; then
        echo -e "${YELLOW}Python Virtual Environments (${
        for venv in "${venvs[@]}"; do
            [ -n "$venv" ] && echo "  - $venv ($(get_size "$venv"))" && ((total_items++))
        done
        echo ""
    fi
    # Temporary files
    if [ ${#temp_files[@]} -gt 0 ]; then
        echo -e "${YELLOW}Temporary/Cache Files (${
        local count=0
        for file in "${temp_files[@]}"; do
            [ -n "$file" ] && ((count++)) && ((total_items++))
        done
        echo "  - $count files (.DS_Store, __pycache__, .swp, etc.)"
        echo ""
    fi
    # Output files
    if [ ${#output_files[@]} -gt 0 ]; then
        echo -e "${YELLOW}Output/Generated Files (${
        for file in "${output_files[@]}"; do
            [ -n "$file" ] && echo "  - $file" && ((total_items++))
        done
        echo ""
    fi
    # Build artifacts
    if [ ${#build_artifacts[@]} -gt 0 ]; then
        echo -e "${YELLOW}Build Artifacts (${
        for artifact in "${build_artifacts[@]}"; do
            [ -n "$artifact" ] && echo "  - $artifact ($(get_size "$artifact"))" && ((total_items++))
        done
        echo ""
    fi
    # Redundant directories
    if [ ${#redundant_dirs[@]} -gt 0 ]; then
        echo -e "${YELLOW}Redundant Directories (${
        for dir in "${redundant_dirs[@]}"; do
            [ -n "$dir" ] && echo "  - $dir" && ((total_items++))
        done
        echo ""
    fi
    if [ $total_items -eq 0 ]; then
        print_success "No items found to clean - repository is already clean!"
        exit 0
    fi
    echo -e "${YELLOW}Total items to remove: $total_items${NC}"
    echo ""
    # Store for cleanup functions
    export FOUND_VENVS="${venvs[*]}"
    export FOUND_TEMP_FILES="${temp_files[*]}"
    export FOUND_OUTPUT_FILES="${output_files[*]}"
    export FOUND_BUILD_ARTIFACTS="${build_artifacts[*]}"
    export FOUND_REDUNDANT_DIRS="${redundant_dirs[*]}"
}
confirm_cleanup() {
    read -p "Do you want to proceed with cleanup? (yes/no): " response
    if [[ ! "$response" =~ ^[Yy][Ee][Ss]$ ]]; then
        print_warning "Cleanup cancelled by user"
        exit 0
    fi
    echo ""
}
################################################################################
# Cleanup Functions
################################################################################
cleanup_venvs() {
    print_header "Step 1: Removing Python Virtual Environments"
    local venvs=($FOUND_VENVS)
    local removed=0
    if [ ${
        print_info "No virtual environments to remove"
        return
    fi
    for venv in "${venvs[@]}"; do
        if [ -n "$venv" ] && [ -d "$venv" ]; then
            local size=$(get_size "$venv")
            print_info "Removing $(basename "$venv") ($size)..."
            rm -rf "$venv"
            print_success "Removed $venv"
            ((removed++))
        fi
    done
    print_success "Removed $removed virtual environment(s)"
}
cleanup_temp_files() {
    print_header "Step 2: Removing Temporary/Cache Files"
    local temp_files=($FOUND_TEMP_FILES)
    local removed=0
    if [ ${
        print_info "No temporary files to remove"
        return
    fi
    for file in "${temp_files[@]}"; do
        if [ -n "$file" ] && [ -e "$file" ]; then
            rm -rf "$file" 2>/dev/null && ((removed++))
        fi
    done
    print_success "Removed $removed temporary file(s)"
}
cleanup_output_files() {
    print_header "Step 3: Removing Output/Generated Files"
    local output_files=($FOUND_OUTPUT_FILES)
    local removed=0
    if [ ${
        print_info "No output files to remove"
        return
    fi
    for file in "${output_files[@]}"; do
        if [ -n "$file" ] && [ -f "$file" ]; then
            print_info "Removing $(basename "$file")..."
            rm -f "$file"
            print_success "Removed $file"
            ((removed++))
        fi
    done
    print_success "Removed $removed output file(s)"
}
cleanup_build_artifacts() {
    print_header "Step 4: Removing Build Artifacts"
    local build_artifacts=($FOUND_BUILD_ARTIFACTS)
    local removed=0
    if [ ${
        print_info "No build artifacts to remove"
        return
    fi
    for artifact in "${build_artifacts[@]}"; do
        if [ -n "$artifact" ] && [ -d "$artifact" ]; then
            local size=$(get_size "$artifact")
            print_info "Removing $(basename "$artifact") ($size)..."
            rm -rf "$artifact"
            print_success "Removed $artifact"
            ((removed++))
        fi
    done
    print_success "Removed $removed build artifact(s)"
}
cleanup_redundant_dirs() {
    print_header "Step 5: Removing Redundant Directories"
    local redundant_dirs=($FOUND_REDUNDANT_DIRS)
    local removed=0
    if [ ${
        print_info "No redundant directories to remove"
        return
    fi
    for dir in "${redundant_dirs[@]}"; do
        dir="${dir% (in .gitignore)}"
        if [ -n "$dir" ] && [ -d "$dir" ]; then
            local size=$(get_size "$dir")
            print_info "Removing $(basename "$dir") ($size)..."
            rm -rf "$dir"
            print_success "Removed $dir"
            ((removed++))
        fi
    done
    print_success "Removed $removed redundant director(ies)"
}
print_summary() {
    print_header "Cleanup Summary"
    local final_size=$(get_size "$TARGET_DIR")
    echo "Cleanup completed successfully!"
    echo ""
    echo "Target Directory: $TARGET_DIR"
    echo "Final Size: $final_size"
    echo ""
    echo "Cleaned up:"
    echo "  ✓ Python virtual environments"
    echo "  ✓ Temporary/cache files (.DS_Store, __pycache__, etc.)"
    echo "  ✓ Output/generated files (logs, repomix files, etc.)"
    echo "  ✓ Build artifacts (.jekyll-cache, dist/, etc.)"
    echo "  ✓ Redundant directories (drafts/, temp/, backup/, etc.)"
    echo ""
    print_success "Repository cleanup completed!"
}
recommend_gitignore() {
    print_header "Recommendations"
    if [ ! -f "$TARGET_DIR/.gitignore" ]; then
        print_warning "No .gitignore found - consider creating one"
        return
    fi
    echo "Consider adding these patterns to .gitignore if not already present:"
    echo ""
    echo "
    echo "  venv/"
    echo "  .venv/"
    echo "  *.pyc"
    echo "  __pycache__/"
    echo ""
    echo "
    echo "  .DS_Store"
    echo "  Thumbs.db"
    echo ""
    echo "
    echo "  dist/"
    echo "  build/"
    echo "  *.log"
    echo ""
    echo "
    echo "  temp/"
    echo "  tmp/"
    echo "  *.swp"
    echo ""
}
################################################################################
# Main Execution
################################################################################
main() {
    print_header "Universal Repository Cleanup Script"
    echo "Target: $TARGET_DIR"
    echo ""
    # Show preview and get confirmation
    show_preview
    confirm_cleanup
    # Execute cleanup tasks
    cleanup_venvs
    cleanup_temp_files
    cleanup_output_files
    cleanup_build_artifacts
    cleanup_redundant_dirs
    # Print summary
    print_summary
    # Print recommendations
    recommend_gitignore
    print_success "All cleanup tasks completed successfully!"
}
# Run main function
main "$@"
</file>

<file path="types/duplicate-detection-types.ts">
import { z } from 'zod';
⋮----
export type ScanType = z.infer<typeof ScanTypeSchema>;
⋮----
export type ScanFrequency = z.infer<typeof ScanFrequencySchema>;
⋮----
export type PriorityLevel = z.infer<typeof PriorityLevelSchema>;
export interface RepositoryConfig {
  name: string;
  path: string;
  enabled: boolean;
  priority: PriorityLevel;
  scanFrequency: ScanFrequency;
  lastScanned?: string;
  scanHistory?: ScanHistoryEntry[];
}
export interface ScanHistoryEntry {
  timestamp: string;
  status: 'success' | 'failure';
  duration: number;
  duplicatesFound: number;
}
export interface RepositoryGroupConfig {
  name: string;
  description?: string;
  enabled: boolean;
  scanType: 'inter-project';
  repositories: string[];
}
⋮----
export type DuplicateDetectionJobData = z.infer<typeof DuplicateDetectionJobDataSchema>;
export interface ScanMetadata {
  scan_started: string;
  scan_completed: string;
  duration_seconds: number;
  total_files_scanned: number;
  scan_type: ScanType;
}
export interface ScanMetrics {
  total_duplicate_groups?: number;
  total_cross_repository_groups?: number;
  total_suggestions: number;
  high_impact_duplicates: number;
}
export interface DuplicateGroup {
  group_id: string;
  instances: Array<{
    file_path: string;
    start_line: number;
    end_line: number;
    repository?: string;
  }>;
  impact_score: number;
  similarity_score: number;
  suggestion?: string;
}
export interface IntraProjectScanResult {
  scan_type: 'intra-project';
  scan_metadata: ScanMetadata;
  metrics: ScanMetrics;
  duplicate_groups: DuplicateGroup[];
  suggestions?: string[];
}
export interface InterProjectScanResult {
  scan_type: 'inter-project';
  scan_metadata: ScanMetadata;
  metrics: ScanMetrics;
  cross_repository_duplicates: DuplicateGroup[];
  suggestions?: string[];
}
export type ScanResult = IntraProjectScanResult | InterProjectScanResult;
export interface PRCreationResult {
  prsCreated: number;
  prUrls: string[];
  errors: Array<{
    repository?: string;
    error: string;
  }>;
}
export interface IntraProjectJobResult {
  scanType: 'intra-project';
  repository: string;
  duplicates: number;
  suggestions: number;
  duration: number;
  prResults?: {
    prsCreated: number;
    prUrls: string[];
    errors: number;
  } | null;
}
export interface InterProjectJobResult {
  scanType: 'inter-project';
  repositories: number;
  crossRepoDuplicates: number;
  suggestions: number;
  duration: number;
}
export type JobResult = IntraProjectJobResult | InterProjectJobResult;
export interface WorkerScanMetrics {
  totalScans: number;
  successfulScans: number;
  failedScans: number;
  totalDuplicatesFound: number;
  totalSuggestionsGenerated: number;
  highImpactDuplicates: number;
  prsCreated: number;
  prCreationErrors: number;
}
export interface RetryInfo {
  attempts: number;
  lastAttempt: number;
  maxAttempts: number;
  delay: number;
}
export interface RetryMetrics {
  activeRetries: number;
  totalRetryAttempts: number;
  jobsBeingRetried: Array<{
    jobId: string;
    attempts: number;
    maxAttempts: number;
    lastAttempt: string;
  }>;
  retryDistribution: {
    attempt1: number;
    attempt2: number;
    attempt3Plus: number;
    nearingLimit: number;
  };
}
export interface CompleteScanMetrics extends WorkerScanMetrics {
  queueStats: {
    queued: number;
    active: number;
    completed: number;
    failed: number;
  };
  retryMetrics: RetryMetrics;
}
export interface DuplicateDetectionWorkerOptions {
  maxConcurrentScans?: number;
  maxConcurrent?: number;
  logDir?: string;
  configPath?: string;
  enablePRCreation?: boolean;
  baseBranch?: string;
  branchPrefix?: string;
  dryRun?: boolean;
  maxSuggestionsPerPR?: number;
  sentryDsn?: string;
}
export interface InitializedEventPayload {
  totalRepositories: number;
  enabledRepositories: number;
  groups: number;
  byPriority: Record<PriorityLevel, number>;
  byFrequency: Record<ScanFrequency, number>;
}
export type PipelineStatusEventPayload =
  | { status: 'initialized'; stats: InitializedEventPayload }
  | { status: 'scanning'; jobId: string; scanType: ScanType; repositories: number }
  | { status: 'failed'; jobId: string; error: string }
  | { status: 'completed'; jobId: string; result: JobResult };
export interface ScanCompletedEventPayload {
  jobId: string;
  scanType: ScanType;
  metrics: {
    duplicates: number;
    suggestions: number;
    duration: number;
  };
}
export interface PRCreatedEventPayload {
  repository: string;
  prsCreated: number;
  prUrls: string[];
}
export interface PRFailedEventPayload {
  repository: string;
  error: string;
}
export interface HighImpactDetectedEventPayload {
  count: number;
  threshold: number;
  topImpactScore: number;
}
export interface RetryScheduledEventPayload {
  jobId: string;
  attempt: number;
  delay: number;
}
export interface RetryWarningEventPayload {
  jobId: string;
  attempts: number;
  maxAttempts: number;
}
export interface RetryCircuitBreakerEventPayload {
  jobId: string;
  attempts: number;
  maxAbsolute: number;
}
export interface MetricsUpdatedEventPayload {
  metrics: CompleteScanMetrics;
}
export interface ScanConfig {
  enabled: boolean;
  retryAttempts: number;
  retryDelay: number;
  maxConcurrentScans: number;
}
export interface NotificationSettings {
  enabled: boolean;
  onHighImpactDuplicates: boolean;
  highImpactThreshold: number;
}
export function isIntraProjectScanResult(result: ScanResult): result is IntraProjectScanResult
export function isInterProjectScanResult(result: ScanResult): result is InterProjectScanResult
export function isIntraProjectJobResult(result: JobResult): result is IntraProjectJobResult
export function isInterProjectJobResult(result: JobResult): result is InterProjectJobResult
</file>

<file path="utils/dependency-validator.js">
export class DependencyValidationError extends Error
export class DependencyValidator
⋮----
static async validateAll()
static async validateRepomix()
static async validateAstGrep()
static async validatePython()
</file>

<file path="utils/directory-scanner.js">
export class DirectoryScanner
⋮----
async scanDirectories()
/**
   * Check if a directory is a git repository root
   */
async isGitRepository(dirPath)
async scanRecursive(currentPath, relativePath, depth, results)
async shouldProcess(dirPath)
async getDirectoryInfo(dirPath)
generateScanStats(directories)
async saveScanReport(directories, stats)
generateDirectoryTree(directories)
async saveDirectoryTree(directories)
async generateAndSaveScanResults(directories)
</file>

<file path="utils/doppler-resilience.example.js">
class DopplerConfigManager extends DopplerResilience
⋮----
async fetchFromDoppler()
async getSecret(key, defaultValue = null)
⋮----
async function exampleUsage()
export function createDopplerHealthMiddleware(dopplerConfig)
export class IntegratedDopplerMonitor
⋮----
async getComprehensiveHealth()
generateRecommendations(health, cacheAge)
async triggerRecovery()
⋮----
export class DopplerHealthService
⋮----
start()
stop()
async getHealth()
</file>

<file path="utils/doppler-resilience.js">
export class DopplerResilience
⋮----
async getSecrets()
async fetchFromDoppler()
async getFallbackSecrets()
isCacheStale()
handleSuccess()
handleFailure(error)
getHealth()
reset()
getState()
</file>

<file path="utils/gitignore-repomix-updater.js">
export class GitignoreRepomixUpdater
⋮----
async findGitRepositories()
async scanForGitRepos(currentPath, depth, results)
async gitignoreContainsEntry(gitignorePath)
async addToGitignore(repoPath)
async processRepositories()
generateSummary(results)
async saveResults(results, outputPath)
⋮----
export async function main()
</file>

<file path="utils/logger.ts">
import type { Logger } from 'pino';
import { createLogger as createBaseLogger, createChildLogger as createBaseChildLogger, createComponentLogger as createBaseComponentLogger } from '@shared/logging';
import { config } from '../core/config.ts';
⋮----
export function createChildLogger(bindings: Record<string, unknown>): Logger
export function createComponentLogger(component: string): Logger
export function logStart(log: Logger, operation: string, context: Record<string, unknown> =
export function logComplete(log: Logger, operation: string, startTime: number, context: Record<string, unknown> =
export function logError(log: Logger, error: Error | unknown, message: string, context: Record<string, unknown> =
export function logWarn(log: Logger, error: Error | null, message: string, context: Record<string, unknown> =
export function logSkip(log: Logger, what: string, reason: string, context: Record<string, unknown> =
export function logStage(log: Logger, stage: string, context: Record<string, unknown> =
export function logMetrics(log: Logger, operation: string, metrics: Record<string, unknown> =
export function logRetry(log: Logger, operation: string, attempt: number, maxAttempts: number, context: Record<string, unknown> =
</file>

<file path="utils/pipeline-names.ts">
export function getPipelineName(id: string): string
export function getAllKnownPipelineIds(): string[]
export function isKnownPipeline(id: string): boolean
</file>

<file path="utils/plugin-manager.js">
class PluginManagerWorker extends SidequestServer
⋮----
async runJobHandler(job)
async runAuditScript(detailed = false)
async analyzeResults(auditResults)
async loadPluginMetadata()
generateRecommendations(analysis)
addJob(options =
</file>

<file path="utils/refactor-test-suite.ts">
import { glob } from 'glob';
interface RefactorConfig {
  projectPath: string;
  testsDir: string;
  utilsDir: string;
  e2eDir: string;
  framework: 'vitest' | 'jest' | 'playwright';
}
interface AnalysisResult {
  testFiles: string[];
  patterns: {
    renderWaitFor: number;
    linkValidation: number;
    semanticChecks: number;
    formInteractions: number;
    hardcodedStrings: string[];
    duplicateAssertions: string[];
  };
  recommendations: string[];
}
⋮----
function detectFramework(projectPath: string): 'vitest' | 'jest' | 'playwright'
async function findTestFiles(config: RefactorConfig): Promise<string[]>
async function analyzeTestFiles(config: RefactorConfig, testFiles: string[]): Promise<AnalysisResult>
⋮----
// Count semantic checks
⋮----
// Count form interactions
⋮----
// Extract hardcoded strings (quoted strings in expect statements)
⋮----
// Extract assertion patterns
⋮----
// Find duplicated strings (appearing 3+ times)
⋮----
// Find duplicated assertions (appearing 3+ times)
⋮----
// Generate recommendations
⋮----
/**
 * Generate the assertions utility file
 */
function generateAssertionsFile(framework: string): string
/**
 * Generate the semantic validators utility file
 */
function generateSemanticValidatorsFile(framework: string): string
/**
 * Generate the form helpers utility file
 */
function generateFormHelpersFile(): string
interface CategorizedStrings {
  navigation: string[];
  actions: string[];
  accessibility: string[];
  messages: string[];
  headings: string[];
  labels: string[];
  misc: string[];
}
/**
 * Categorize strings into meaningful groups based on content patterns
 */
function categorizeStrings(strings: string[]): CategorizedStrings
/**
 * Generate a constant export for an array of strings
 */
function generateArrayExport(name: string, strings: string[], comment: string): string
function generateConstantsTemplate(hardcodedStrings: string[]): string
function generateRenderHelpers(): string
function generateE2EFixtures(): string
function generateIndexFile(): string
async function main()
</file>

<file path="utils/report-generator.js">
export async function generateReport(options)
function generateHTMLReport(data)
function generateJSONReport(data)
function generateHTMLHeader(title, jobId, status, statusClass, timestamp, duration)
function generateHTMLParameters(parameters)
/**
 * Generate HTML metadata section
 *
 * @private
 */
function generateHTMLMetadata(metadata)
/**
 * Generate HTML results section
 *
 * @private
 */
function generateHTMLResults(result, jobType)
⋮----
// Try to detect metrics/stats in result
⋮----
/**
 * Generate metrics cards
 *
 * @private
 */
function generateMetricsSection(metrics)
/**
 * Generate details section
 *
 * @private
 */
function generateDetailsSection(details, jobType)
/**
 * Generate HTML footer
 *
 * @private
 */
function generateHTMLFooter(timestamp)
/**
 * Extract metrics from result object
 *
 * @private
 */
function extractMetrics(result)
⋮----
// Look for common metric field names
⋮----
function extractDetails(result)
function getJobTypeTitle(jobType)
function formatValue(value)
export async function pruneOldReports(outputDir = DEFAULT_OUTPUT_DIR, maxAgeMs = 30 * TIME.DAY)
function escapeHtml(text)
function getHTMLStyles()
</file>

<file path="utils/schema-mcp-tools.js">
export class SchemaMCPTools
⋮----
async getSchemaType(readmePath, content, context)
async generateSchema(readmePath, content, context, schemaType)
extractDescription(content)
⋮----
// Skip title
⋮----
// Skip empty lines and code blocks
⋮----
if (description) break; // Stop at first empty line after description
⋮----
// Found description
⋮----
// Limit description length
⋮----
async validateSchema(schema)
async analyzeSchemaImpact(originalContent, enhancedContent, schema)
getRating(score)
createJSONLDScript(schema)
injectSchema(content, schema)
</file>

<file path="utils/time-helpers.ts">
import { TIME } from '../core/constants.ts';
export function toISOString(val: Date | string | null | undefined): string | null
export function calculateDurationSeconds(startTime: Date | string | null, endTime: Date | string | null): number | null
export function formatDuration(seconds: number | null | undefined): string
</file>

<file path="workers/bugfix-audit-worker.js">
export class BugfixAuditWorker extends SidequestServer
⋮----
async findMarkdownFiles()
⋮----
async function scanDirectory(dir)
⋮----
getRepositoryFromPath(markdownPath)
/**
   * Run Claude Code agent via CLI
   */
async runClaudeAgent(agentType, prompt, cwd)
async runSlashCommand(command, cwd)
async runJobHandler(job)
createBugfixJob(markdownFile, projectName, repoPath)
async createJobsForAllMarkdownFiles()
</file>

<file path="workers/claude-health-worker.js">
class ClaudeHealthWorker extends SidequestServer
⋮----
async runJobHandler(job)
async runAllChecks(options =
async checkEnvironment()
async checkDirectories()
async checkConfiguration()
async checkHooks()
async checkComponents(detailed = false)
async checkPlugins()
async checkPerformance()
analyzeResults(checks)
calculateHealthScore(issues, warnings, successes)
generateRecommendations(analysis)
generateSummary(analysis, recommendations)
getStatusMessage(healthScore, critical, warnings)
addJob(options =
</file>

<file path="workers/dashboard-populate-worker.js">
export class DashboardPopulateWorker extends SidequestServer
⋮----
createPopulateJob(options =
async runJobHandler(job)
</file>

<file path="workers/duplicate-detection-worker.js">
export class DuplicateDetectionWorker extends SidequestServer
⋮----
async initialize()
async runJobHandler(job)
_getOriginalJobId(jobId)
/**
   * Handle retry logic with exponential backoff
   */
async _handleRetry(job, error)
⋮----
// Get original job ID to track retries correctly
⋮----
// Classify error to determine if retry is appropriate
⋮----
async _runInterProjectScan(job, repositoryConfigs)
async _runIntraProjectScan(job, repositoryConfig)
_updateMetrics(scanResult)
async _updateRepositoryConfigs(repositoryConfigs, scanResult)
async _checkForHighImpactDuplicates(scanResult)
scheduleScan(scanType, repositories, groupName = null)
async runNightlyScan()
getRetryMetrics()
getScanMetrics()
</file>

<file path="workers/git-activity-worker.js">
export class GitActivityWorker extends SidequestServer
⋮----
async runJobHandler(job)
⋮----
createWeeklyReportJob()
createMonthlyReportJob()
createCustomReportJob(sinceDate, untilDate)
createReportJob(options =
</file>

<file path="workers/gitignore-worker.js">
export class GitignoreWorker extends SidequestServer
⋮----
async runJobHandler(job)
⋮----
createUpdateAllJob(options =
createUpdateRepositoriesJob(repositories, options =
createDryRunJob(options =
</file>

<file path="workers/repo-cleanup-worker.js">
export class RepoCleanupWorker extends SidequestServer
⋮----
async runJobHandler(job)
⋮----
createCleanupJob(targetDir, options =
createDryRunJob(targetDir, options =
</file>

<file path="workers/repomix-worker.js">
export class RepomixWorker extends SidequestServer
⋮----
async runJobHandler(job)
⋮----
createRepomixJob(sourceDir, relativePath)
</file>

<file path="workers/schema-enhancement-worker.js">
async runJobHandler(job)
async saveImpactReport(relativePath, schema, impact)
async saveEnhancedCopy(relativePath, enhancedContent)
async findGitRoot(startPath)
async createEnhancementJob(readme, context)
async _generateCommitMessage(job)
</file>

<file path="workers/test-refactor-worker.ts">
import { SidequestServer, Job as BaseJob } from '../core/server.ts';
import { createComponentLogger } from '../utils/logger.ts';
import { glob } from 'glob';
import path from 'path';
import fs from 'fs/promises';
⋮----
interface TestRefactorWorkerOptions {
  maxConcurrent?: number;
  logDir?: string;
  gitWorkflowEnabled?: boolean;
  gitBranchPrefix?: string;
  testsDir?: string;
  utilsDir?: string;
  e2eDir?: string;
  framework?: 'vitest' | 'jest' | 'playwright';
  dryRun?: boolean;
  sentryDsn?: string;
}
interface JobData {
  repositoryPath: string;
  repository: string;
  testsDir: string;
  utilsDir: string;
  e2eDir: string;
  framework: string;
  dryRun: boolean;
}
interface RefactorJob {
  id: string;
  data: JobData;
  result?: JobResult;
}
interface AnalysisPatterns {
  renderWaitFor: number;
  linkValidation: number;
  semanticChecks: number;
  formInteractions: number;
  hardcodedStrings: string[];
  duplicateAssertions: string[];
}
interface Analysis {
  testFiles: string[];
  patterns: AnalysisPatterns;
  recommendations: string[];
}
interface JobResult {
  status: string;
  reason?: string;
  analysis?: Analysis;
  testFiles?: number;
  generatedFiles?: string[];
  recommendations?: string[];
}
interface Metrics {
  totalProjects: number;
  successfulRefactors: number;
  failedRefactors: number;
  filesGenerated: number;
  patternsDetected: number;
  stringsExtracted: number;
  recommendationsGenerated: number;
}
interface Stats {
  total: number;
  queued: number;
  active: number;
  completed: number;
  failed: number;
}
⋮----
constructor(options: TestRefactorWorkerOptions =
queueProject(projectPath: string, options: Partial<JobData> =
detectFramework(projectPath: string): string
async runJobHandler(job: BaseJob): Promise<unknown>
async findTestFiles(projectPath: string, testsDir: string): Promise<string[]>
async analyzeTestFiles(projectPath: string, testFiles: string[]): Promise<Analysis>
⋮----
// Extract hardcoded strings
⋮----
// Extract assertion patterns
⋮----
// Find duplicated strings (3+ occurrences)
⋮----
// Find duplicated assertions (3+ occurrences)
⋮----
// Generate recommendations
⋮----
/**
   * Generate utility files based on analysis
   */
async generateUtilityFiles(
    projectPath: string,
    utilsDir: string,
    e2eDir: string,
    framework: string,
    analysis: Analysis
): Promise<string[]>
⋮----
// Ensure utils directory exists
⋮----
// Generate assertions.ts
⋮----
// Generate semantic-validators.ts
⋮----
// Generate form-helpers.ts
⋮----
// Generate test-constants.ts
⋮----
// Generate index.ts
⋮----
// Generate E2E fixtures if e2e directory exists
⋮----
/**
   * Check if file exists
   */
async fileExists(filePath: string): Promise<boolean>
/**
   * Generate assertions.ts content
   */
generateAssertionsContent(framework: string): string
/**
   * Generate semantic-validators.ts content
   */
generateSemanticValidatorsContent(framework: string): string
/**
   * Generate form-helpers.ts content
   */
generateFormHelpersContent(): string
/**
   * Categorize strings into meaningful groups based on content patterns
   */
categorizeStrings(strings: string[]):
/**
   * Generate a constant export for an array of strings
   */
generateArrayExport(name: string, strings: string[], comment: string): string
⋮----
return `/**
 * Test Constants
 * Generated by AlephAuto TestRefactorWorker
 *
 * Auto-categorized by string content patterns.
 */
⋮----
// Combined export for convenience
⋮----
.join('\n');
</file>

<file path=".env.example">
# AlephAuto Configuration
# Copy this file to .env and customize for your environment

# ============================================
# Base Directories
# ============================================

# Base directory containing all code repositories to process
# Default: ~/code
CODE_BASE_DIR=/Users/username/code

# Output directory for repomix condensed code
# Default: ./output/condense (relative to project root)
OUTPUT_BASE_DIR=./output/condense

# Directory for application logs
# Default: ./logs
LOG_DIR=./logs

# Directory for directory scan reports
# Default: ./output/directory-scan-reports
SCAN_REPORTS_DIR=./output/directory-scan-reports

# ============================================
# Job Processing
# ============================================

# Maximum number of concurrent jobs (1-50)
# Default: 5
MAX_CONCURRENT=5

# ============================================
# Monitoring & Error Tracking
# ============================================

# Sentry DSN for error tracking (optional)
# Get from: https://sentry.io/settings/projects/
SENTRY_DSN=https://your-sentry-dsn@sentry.io/project-id

# Node environment (development, production)
# Default: production
NODE_ENV=production

# ============================================
# Scheduling
# ============================================

# Cron schedule for repomix pipeline (default: 2 AM daily)
# Format: minute hour day month weekday
# Examples:
#   "0 2 * * *"  - 2 AM daily
#   "0 */6 * * *" - Every 6 hours
#   "0 0 * * 0"  - Midnight every Sunday
CRON_SCHEDULE="0 2 * * *"

# Cron schedule for documentation enhancement pipeline (default: 3 AM daily)
DOC_CRON_SCHEDULE="0 3 * * *"

# Run jobs immediately on startup (true/false)
# Default: false
RUN_ON_STARTUP=false

# ============================================
# Repomix Settings
# ============================================

# Timeout for repomix command in milliseconds (default: 10 minutes)
REPOMIX_TIMEOUT=600000

# Maximum buffer size for repomix output in bytes (default: 50MB)
REPOMIX_MAX_BUFFER=52428800

# ============================================
# Schema.org Integration
# ============================================

# URL for Schema.org MCP server (optional)
SCHEMA_MCP_URL=http://localhost:3001

# Force schema enhancement even if schema already exists
# Default: false
FORCE_ENHANCEMENT=false

# ============================================
# Logging
# ============================================

# Log level: debug, info, warn, error
# Default: info
LOG_LEVEL=info

# ============================================
# Health Check
# ============================================

# Port for health check HTTP server
# Default: 3000
HEALTH_CHECK_PORT=3000
</file>

<file path=".gitignore">
# Dependencies
node_modules/

# Logs
logs/
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# Output artifacts
output/
directory-scan-reports/
gitignore-update-report-*.json
repomix-output.xml
doc-enhancement/repomix-output.xml

# Environment
.env
.env.local
.env.*.local

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# OS
.DS_Store
Thumbs.db

# Sentry
.sentryclirc

# Testing
coverage/
.vitest/

# Build
dist/
build/
*.tsbuildinfo

# Temporary files
*.tmp
*.temp
</file>

<file path="git-report-config.json">
{
  "description": "Configuration for automated git activity reports",
  "version": "1.0.0",

  "scanning": {
    "code_directory": "~/code",
    "reports_directory": "~/reports",
    "additional_repositories": [
      "~/schema-org-file-system",
      "~/claude-tool-use",
      "~/dotfiles"
    ],
    "include_dotfiles": true,
    "max_depth": 2,
    "exclude_patterns": [
      "vim/bundle",
      "node_modules",
      ".git",
      "venv",
      ".venv",
      "vendor",
      "target",
      "build",
      "dist"
    ]
  },

  "reports": {
    "weekly": {
      "enabled": true,
      "schedule": "Sunday 20:00",
      "days_back": 7,
      "auto_commit": false,
      "output_format": "markdown"
    },
    "monthly": {
      "enabled": true,
      "schedule": "1st of month 08:00",
      "days_back": 30,
      "auto_commit": false,
      "output_format": "markdown"
    },
    "quarterly": {
      "enabled": false,
      "schedule": "First Monday of quarter",
      "days_back": 90,
      "auto_commit": false,
      "output_format": "markdown"
    }
  },

  "output": {
    "personalsite_dir": "~/code/PersonalSite",
    "work_collection": "_work",
    "visualization_dir": "assets/images/git-activity-{year}",
    "json_cache_dir": "/tmp",
    "log_dir": "~/code/jobs/sidequest/logs"
  },

  "project_categories": {
    "Data & Analytics": {
      "keywords": ["scraper", "analytics", "bot", "data"],
      "description": "Data collection, scraping, analytics pipelines"
    },
    "Personal Sites": {
      "keywords": ["personalsite", "github.io", "portfolio", "blog"],
      "description": "Personal websites and blogs"
    },
    "Infrastructure": {
      "keywords": ["integrity", "studio", "visualizer", "tool", "dotfiles"],
      "description": "DevOps, tooling, development infrastructure"
    },
    "MCP Servers": {
      "keywords": ["mcp", "server", "context", "protocol"],
      "description": "Model Context Protocol server implementations"
    },
    "Client Work": {
      "keywords": ["client", "leora", "jobs"],
      "description": "Client projects and career tracking"
    },
    "Business Apps": {
      "keywords": ["inventory", "financial", "business"],
      "description": "Business applications and SaaS products"
    },
    "Legacy": {
      "keywords": ["old", "archive", "deprecated"],
      "description": "Archived or minimal-maintenance projects",
      "min_commits": 0,
      "max_commits": 5
    }
  },

  "visualizations": {
    "enabled": true,
    "formats": ["svg"],
    "charts": {
      "monthly_commits": {
        "enabled": true,
        "type": "pie",
        "filename": "monthly-commits.svg",
        "width": 800,
        "height": 600
      },
      "project_categories": {
        "enabled": true,
        "type": "pie",
        "filename": "project-categories.svg",
        "width": 800,
        "height": 600
      },
      "top_repositories": {
        "enabled": true,
        "type": "bar",
        "filename": "top-10-repos.svg",
        "width": 800,
        "height": 500,
        "count": 10
      },
      "language_distribution": {
        "enabled": true,
        "type": "pie",
        "filename": "language-distribution.svg",
        "width": 900,
        "height": 600
      }
    },
    "color_schemes": {
      "default": ["#0066cc", "#4da6ff", "#99ccff", "#00994d", "#ffcc00", "#ff6600", "#cc0000", "#9966cc", "#66cc99", "#ff6699"]
    }
  },

  "language_mapping": {
    "Python": [".py", ".pyw"],
    "JavaScript": [".js", ".mjs", ".cjs"],
    "TypeScript": [".ts", ".tsx"],
    "Ruby": [".rb", ".rake", ".gemspec"],
    "HTML": [".html", ".htm"],
    "CSS/SCSS": [".css", ".scss", ".sass", ".less"],
    "Markdown": [".md", ".markdown"],
    "JSON": [".json"],
    "YAML": [".yml", ".yaml"],
    "Shell": [".sh", ".bash", ".zsh"],
    "SQL": [".sql"],
    "Go": [".go"],
    "Rust": [".rs"],
    "C/C++": [".c", ".cpp", ".cc", ".h", ".hpp"],
    "Java": [".java"],
    "PHP": [".php"],
    "Lock Files": [".lock", "package-lock.json", "Gemfile.lock", "yarn.lock", "pnpm-lock.yaml"],
    "SVG": [".svg"],
    "Images": [".png", ".jpg", ".jpeg", ".gif", ".webp", ".ico", ".bmp"],
    "Data Files": [".csv", ".xml", ".tsv", ".parquet"],
    "Text Files": [".txt", ".log", ".ini", ".conf"]
  },

  "notifications": {
    "enabled": false,
    "methods": {
      "email": {
        "enabled": false,
        "to": "your-email@example.com",
        "subject": "Weekly Git Activity Report"
      },
      "slack": {
        "enabled": false,
        "webhook_url": "",
        "channel": "#dev-updates"
      },
      "discord": {
        "enabled": false,
        "webhook_url": ""
      }
    }
  },

  "git": {
    "auto_commit": false,
    "commit_message_template": "Add {period} git activity report: {start_date} to {end_date}\n\n🤖 Generated with [Claude Code](https://claude.com/claude-code)\n\nCo-Authored-By: Claude <noreply@anthropic.com>",
    "auto_push": false
  },

  "cron": {
    "weekly": "0 20 * * 0",
    "monthly": "0 8 1 * *",
    "quarterly": "0 8 1 1,4,7,10 *"
  },

  "notes": [
    "This configuration file controls the automated git activity report generation.",
    "Edit the values above to customize the behavior for your workflow.",
    "The git activity reporter is now integrated with AlephAuto job queue framework.",
    "Usage: npm run git:weekly (weekly), npm run git:monthly (monthly), npm run git:schedule (scheduled mode)",
    "For production: pm2 start git-activity-pipeline.js --name git-activity",
    "Environment variable: GIT_CRON_SCHEDULE='0 20 * * 0' for custom schedule"
  ]
}
</file>

<file path="repomix.config.json">
{
  "$schema": "https://repomix.com/schemas/latest/schema.json",
  "input": {
    "maxFileSize": 52428800
  },
  "output": {
    "filePath": "repomix-output.xml",
    "style": "xml",
    "parsableStyle": false,
    "directoryStructure": true,
    "files": true,
    "removeComments": true,
    "removeEmptyLines": true,
    "compress": true,
    "topFilesLength": 5,
    "showLineNumbers": false,
    "truncateBase64": false,
    "copyToClipboard": false,
    "includeFullDirectoryStructure": false,
    "tokenCountTree": false,
    "includeEmptyDirectories": false,
    "git": {
      "sortByChanges": true,
      "sortByChangesMaxCommits": 100,
      "includeDiffs": false,
      "includeLogs": false,
      "includeLogsCount": 50
    }
  },
  "include": [],
  "ignore": {
    "useGitignore": true,
    "useDefaultPatterns": true,
    "customPatterns": [
      "logs/**",
      "node_modules/**",
      "*.log",
      "directory-scan-reports/**",
      "document-enhancement-impact-measurement/**",
      "**/go/pkg/mod/**",
      "**/pyenv/**",
      "**/python/pyenv/**",
      "**/vim/bundle/**",
      "**/vim/autoload/**",
      "repomix-output.xml",
      "repomix-output.txt"
    ]
  },
  "security": {
    "enableSecurityCheck": true
  },
  "tokenCount": {
    "encoding": "o200k_base"
  }
}
</file>

</files>
