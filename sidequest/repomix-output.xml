This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where comments have been removed, empty lines have been removed, content has been compressed (code blocks are separated by ‚ãÆ---- delimiter).

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: logs/**, node_modules/**, *.log, directory-scan-reports/**, document-enhancement-impact-measurement/**, **/go/pkg/mod/**, **/pyenv/**, **/python/pyenv/**, **/vim/bundle/**, **/vim/autoload/**, repomix-output.xml, repomix-output.txt, **/README.md, **/README.MD, **/*.md
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Code comments have been removed from supported file types
- Empty lines have been removed from all files
- Content has been compressed - code blocks are separated by ‚ãÆ---- delimiter
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.claude/
  settings.local.json
bug-fixes/
  bugfix-audit-worker.js
  index.js
  launch-tonight.sh
  package.json
core/
  config.js
  constants.js
  database.js
  git-workflow-manager.js
  index.js
  job-repository.js
  server.d.ts
  server.js
pipeline-core/
  .claude/
    settings.local.json
  cache/
    cached-scanner.js
    git-tracker.js
    scan-cache.js
  config/
    repository-config-loader.js
  errors/
    error-classifier.js
    error-types.d.ts
    types.d.ts
  extractors/
    extract_blocks.py
  git/
    branch-manager.js
    migration-transformer.js
    pr-creator.js
  models/
    __init__.py
    code_block.py
    consolidation_suggestion.py
    duplicate_group.py
    scan_report.py
    test_models.py
  reports/
    html-report-generator.js
    json-report-generator.js
    markdown-report-generator.js
    report-coordinator.js
  scanners/
    ast-grep-detector.js
    codebase-health-scanner.js
    repository-scanner.js
    root-directory-analyzer.js
    timeout_detector.py
    timeout-pattern-detector.js
  similarity/
    __init__.py
    config.py
    grouping.py
    semantic.py
    structural.py
  types/
    scan-orchestrator-types.js
    scan-orchestrator-types.ts
  utils/
    error-helpers.js
  doppler-health-monitor.js
  inter-project-scanner.js
  scan-orchestrator.d.ts
  scan-orchestrator.js
  scan-orchestrator.ts
pipeline-runners/
  claude-health-pipeline.js
  collect_git_activity.py
  duplicate-detection-pipeline.js
  duplicate-detection-pipeline.ts
  git-activity-pipeline.js
  gitignore-pipeline.js
  plugin-management-pipeline.js
  repo-cleanup-pipeline.js
  schema-enhancement-pipeline.js
  test-refactor-pipeline.ts
  universal-repo-cleanup.sh
types/
  duplicate-detection-types.js
  duplicate-detection-types.ts
utils/
  dependency-validator.js
  directory-scanner.js
  doppler-resilience.example.js
  doppler-resilience.js
  gitignore-repomix-updater.js
  logger.d.ts
  logger.js
  pipeline-names.js
  plugin-manager.js
  refactor-test-suite.ts
  report-generator.js
  schema-mcp-tools.js
  time-helpers.js
workers/
  claude-health-worker.js
  duplicate-detection-worker.js
  git-activity-worker.js
  gitignore-worker.js
  repo-cleanup-worker.js
  repomix-worker.js
  schema-enhancement-worker.js
  test-refactor-worker.ts
.env.example
.gitignore
git-report-config.json
plugin-management-audit.sh
repomix.config.json
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "Bash(node gitignore-repomix-updater.js:*)",
      "Bash(mv:*)",
      "Bash(chmod:*)",
      "Bash(doppler run:*)",
      "Bash(npm run cleanup:dryrun:*)",
      "Bash(node:*)",
      "Bash(npm run cleanup:once:*)",
      "Bash(find:*)",
      "Bash(npm run typecheck:*)",
      "Bash(curl:*)",
      "Bash(pm2 logs:*)",
      "Bash(pm2 info:*)",
      "Bash(npm test:*)",
      "Bash(npm run test:integration:*)"
    ],
    "deny": [],
    "ask": []
  }
}
</file>

<file path="bug-fixes/bugfix-audit-worker.js">
export class BugfixAuditWorker extends SidequestServer
‚ãÆ----
async findMarkdownFiles()
‚ãÆ----
async function scanDirectory(dir)
‚ãÆ----
getRepositoryFromPath(markdownPath)
/**
   * Run Claude Code agent via CLI
   */
async runClaudeAgent(agentType, prompt, cwd)
async runSlashCommand(command, cwd)
async createGitBranch(repoPath, branchName)
async commitChanges(repoPath, message)
async createPullRequest(repoPath, title, body)
async runJobHandler(job)
async createJobsForAllMarkdownFiles()
</file>

<file path="bug-fixes/index.js">
class BugfixAuditApp
‚ãÆ----
setupEventListeners()
async runBugfixAudit()
async waitForCompletion()
async saveRunSummary(stats, duration, jobs)
setupCronJob(schedule = '0 1 * * *')
setupOneTimeJob()
async start(options =
</file>

<file path="bug-fixes/launch-tonight.sh">
set -e
SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
cd "$SCRIPT_DIR"
echo "ü§ñ BugfixAudit - Automated Bug Detection & Fixing"
echo "=================================================="
echo ""
# Check if npm packages are installed
if [ ! -d "node_modules" ]; then
    echo "üì¶ Installing dependencies..."
    npm install
    echo "‚úÖ Dependencies installed"
    echo ""
fi
# Check for required tools
echo "üîç Checking required tools..."
if ! command -v git &> /dev/null; then
    echo "‚ùå git is not installed"
    exit 1
fi
echo "  ‚úÖ git"
if ! command -v gh &> /dev/null; then
    echo "‚ùå gh CLI is not installed"
    echo "   Install with: brew install gh"
    exit 1
fi
echo "  ‚úÖ gh"
if ! command -v doppler &> /dev/null; then
    echo "‚ùå doppler is not installed"
    echo "   Install with: brew install dopplerhq/cli/doppler"
    exit 1
fi
echo "  ‚úÖ doppler"
if ! command -v node &> /dev/null; then
    echo "‚ùå node is not installed"
    exit 1
fi
echo "  ‚úÖ node $(node --version)"
echo ""
# Calculate time until 1 AM
current_hour=$(date +%H)
current_minute=$(date +%M)
if [ "$current_hour" -lt 1 ]; then
    hours_until=$((1 - current_hour))
    minutes_until=$((60 - current_minute))
else
    hours_until=$((25 - current_hour))
    minutes_until=$((60 - current_minute))
fi
echo "‚è∞ Scheduling execution for tonight at 1:00 AM"
echo "   Current time: $(date '+%H:%M')"
echo "   Time until execution: ${hours_until}h ${minutes_until}m"
echo ""
echo "üìã Workflow:"
echo "   1. Scan ~/dev/active for markdown files"
echo "   2. Create git branches for each project"
echo "   3. Run analysis: bugfix-planner, bug-detective, audit, quality-controller"
echo "   4. Implement fixes with refractor"
echo "   5. Commit after each stage"
echo "   6. Create pull requests"
echo ""
echo "üìÇ Output will be saved to:"
echo "   ~/code/jobs/sidequest/bug-fixes/output/"
echo ""
echo "üöÄ Starting background process..."
echo ""
# Run in background and detach
nohup npm run start:once > logs/bugfix-audit-$(date +%Y%m%d-%H%M%S).log 2>&1 &
PID=$!
echo "‚úÖ Process started with PID: $PID"
echo ""
echo "üìä Monitor progress:"
echo "   tail -f logs/bugfix-audit-*.log"
echo ""
echo "üõë Stop execution:"
echo "   kill $PID"
echo ""
echo "üí§ The process will run at 1 AM and then exit."
echo "   You can close this terminal."
echo ""
</file>

<file path="bug-fixes/package.json">
{
  "name": "@alephauto/bugfix-audit",
  "version": "1.0.0",
  "description": "Automated bug detection and fixing using Claude Code agents and plugins",
  "type": "module",
  "main": "index.js",
  "scripts": {
    "start": "doppler run -- node index.js",
    "start:now": "doppler run -- node index.js --now",
    "start:once": "doppler run -- node index.js --once",
    "start:recurring": "doppler run -- node index.js --recurring",
    "dev": "doppler run -- node --watch index.js --now",
    "test": "echo \"No tests yet\" && exit 0"
  },
  "keywords": [
    "automation",
    "bugfix",
    "audit",
    "claude-code",
    "alephauto"
  ],
  "author": "Alyshia Ledlie",
  "license": "MIT",
  "dependencies": {
    "node-cron": "^3.0.3",
    "pino": "^8.17.2",
    "pino-pretty": "^10.3.1"
  },
  "engines": {
    "node": ">=18.0.0"
  }
}
</file>

<file path="core/config.js">
function safeParseInt(value, defaultValue, min, max)
function safeParseFloat(value, defaultValue, min, max)
‚ãÆ----
function validateConfig()
</file>

<file path="core/constants.js">

</file>

<file path="core/database.js">
function safeJsonParse(str, fallback = null)
export async function initDatabase()
function _enterDegradedMode(initialError)
function _scheduleRecovery()
function _attemptRecovery()
‚ãÆ----
function _processWriteQueue()
function persistDatabase()
export function getDatabase()
export function isDatabaseReady()
export function saveJob(job)
function queryAll(query, params = [])
function queryOne(query, params = [])
export function getJobs(pipelineId, options =
export function getAllJobs(options =
export function getJobCounts(pipelineId)
export function getLastJob(pipelineId)
export function getAllPipelineStats()
export async function importReportsToDatabase(reportsDir)
‚ãÆ----
// Check if already imported
‚ãÆ----
// Import as completed job
‚ãÆ----
export async function importLogsToDatabase(logsDir)
‚ãÆ----
// Check if already imported
‚ãÆ----
// Determine status from content
‚ãÆ----
export function getHealthStatus()
export function closeDatabase()
function isValidJobId(id)
export function bulkImportJobs(jobs)
</file>

<file path="core/git-workflow-manager.js">
export class GitWorkflowManager
‚ãÆ----
async createJobBranch(repositoryPath, jobInfo)
async hasChanges(repositoryPath)
async getChangedFiles(repositoryPath)
async commitChanges(repositoryPath, commitContext)
async pushBranch(repositoryPath, branchName)
async createPullRequest(repositoryPath, prContext)
async cleanupBranch(repositoryPath, branchName, originalBranch)
async executeWorkflow(repositoryPath, gitInfo, messageGenerator)
</file>

<file path="core/index.js">
class RepomixCronApp
‚ãÆ----
setupEventListeners()
async runRepomixOnAllDirectories()
async waitForCompletion()
async saveRunSummary(stats, duration)
setupCronJob(schedule = '0 2 * * *')
async start()
</file>

<file path="core/job-repository.js">
class JobRepository
‚ãÆ----
async initialize()
saveJob(job)
getJob(id)
getJobs(pipelineId, filters =
getAllJobs(filters =
getJobCounts(pipelineId)
getLastJob(pipelineId)
getAllPipelineStats()
bulkImport(jobs)
close()
reset()
‚ãÆ----
export function createJobRepository(options =
</file>

<file path="core/server.d.ts">
import { EventEmitter } from 'events';
export type JobStatus = 'queued' | 'running' | 'completed' | 'failed' | 'cancelled' | 'paused';
export interface JobGitMetadata {
  branchName: string | null;
  originalBranch: string | null;
  commitSha: string | null;
  prUrl: string | null;
  changedFiles: string[];
}
export interface JobError {
  message: string;
  stack?: string;
  code?: string;
}
export interface Job {
  id: string;
  status: JobStatus;
  data: Record<string, any>;
  createdAt: Date;
  startedAt: Date | null;
  completedAt: Date | null;
  error: JobError | null;
  result: any;
  git: JobGitMetadata;
}
export interface JobStats {
  total: number;
  queued: number;
  active: number;
  completed: number;
  failed: number;
}
export interface SidequestServerOptions {
  maxConcurrent?: number;
  logDir?: string;
  autoStart?: boolean;
  gitWorkflowEnabled?: boolean;
  gitBranchPrefix?: string;
  gitBaseBranch?: string;
  gitDryRun?: boolean;
  jobType?: string;
  sentryDsn?: string;
}
export class SidequestServer extends EventEmitter
‚ãÆ----
constructor(options?: SidequestServerOptions);
createJob(jobId: string, jobData: Record<string, any>): Job;
processQueue(): Promise<void>;
executeJob(jobId: string): Promise<void>;
runJobHandler(job: unknown): Promise<unknown>;
start(): void;
stop(): void;
logJobCompletion(job: Job): Promise<void>;
logJobFailure(job: Job, error: Error): Promise<void>;
getJob(jobId: string): Job | undefined;
getAllJobs(): Job[];
getStats(): JobStats;
cancelJob(jobId: string):
pauseJob(jobId: string):
resumeJob(jobId: string):
pause?(): void | Promise<void>;
resume?(): void | Promise<void>;
setPaused?(paused: boolean): void;
scheduleScan?(scanType: string, repositories: unknown[], groupName?: string | null): void;
scheduleJob?(options: Record<string, unknown>): Job | Promise<Job>;
addJob?(options: Record<string, unknown>): Job | Promise<Job>;
protected _generateCommitMessage?(job: unknown): Promise<
protected _generatePRContext?(job: unknown): Promise<
on(event: 'job:created', listener: (job: Job)
on(event: 'job:started', listener: (job: Job)
on(event: 'job:completed', listener: (job: Job)
on(event: 'job:failed', listener: (job: Job, error: Error)
on(event: 'job:cancelled', listener: (job: Job)
on(event: 'job:paused', listener: (job: Job)
on(event: 'job:resumed', listener: (job: Job)
on(event: string, listener: (...args: unknown[])
emit(event: 'job:created', job: Job): boolean;
emit(event: 'job:started', job: Job): boolean;
emit(event: 'job:completed', job: Job): boolean;
emit(event: 'job:failed', job: Job, error: Error): boolean;
emit(event: 'job:cancelled', job: Job): boolean;
emit(event: 'job:paused', job: Job): boolean;
emit(event: 'job:resumed', job: Job): boolean;
emit(event: string, ...args: unknown[]): boolean;
</file>

<file path="core/server.js">
export class SidequestServer extends EventEmitter
‚ãÆ----
createJob(jobId, jobData)
async processQueue()
async executeJob(jobId)
_persistJob(job)
_prepareJobForExecution(job)
async _setupGitBranchIfEnabled(job)
async _finalizeJobSuccess(job, result, branchCreated)
async _finalizeJobFailure(job, error, branchCreated)
async _handleGitWorkflowSuccess(job)
async _generateCommitMessage(job)
async _generatePRContext(job)
async runJobHandler(job)
start()
stop()
set handleJob(handler)
async logJobCompletion(job)
async logJobFailure(job, error)
getJob(jobId)
getAllJobs()
getStats()
cancelJob(jobId)
pauseJob(jobId)
resumeJob(jobId)
</file>

<file path="pipeline-core/.claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "Bash(rm:*)"
    ],
    "deny": [],
    "ask": []
  }
}
</file>

<file path="pipeline-core/cache/cached-scanner.js">
export class CachedScanner
‚ãÆ----
initializeCache(redisClient, cacheOptions =
async scanRepository(repoPath, options =
async _shouldUseCache(repoPath, repoStatus, options)
async _getCachedResult(repoPath, commitHash)
async _cacheResult(repoPath, commitHash, scanResult)
async invalidateCache(repoPath)
async getCacheStatus(repoPath)
async getStats()
async warmCache(repoPaths, options =
</file>

<file path="pipeline-core/cache/git-tracker.js">
export class GitCommitTracker
‚ãÆ----
async getRepositoryCommit(repoPath)
async getShortCommit(repoPath)
async hasChanged(repoPath, lastCommit)
async getChangedFiles(repoPath, fromCommit)
async getCommitMetadata(repoPath, commitHash = 'HEAD')
async getBranchName(repoPath)
async hasUncommittedChanges(repoPath)
async getRemoteUrl(repoPath, remoteName = 'origin')
async getCommitCount(repoPath)
async isGitRepository(repoPath)
async getRepositoryStatus(repoPath)
async getCommitHistory(repoPath, limit = 10)
</file>

<file path="pipeline-core/cache/scan-cache.js">
export class ScanResultCache
‚ãÆ----
_generateCacheKey(repoPath, commitHash)
async getCachedScan(repoPath, commitHash)
async cacheScan(repoPath, commitHash, scanResult, options =
async _addToIndex(cacheKey, repoPath, commitHash)
async invalidateCache(repoPath)
async getStats()
async listCachedScans(limit = 10)
async getCacheMetadata(repoPath, commitHash)
async clearAll()
async isCached(repoPath, commitHash)
async getCacheAge(repoPath, commitHash)
</file>

<file path="pipeline-core/config/repository-config-loader.js">
export class RepositoryConfigLoader
‚ãÆ----
async load()
async reload()
getScanConfig()
getAllRepositories()
getEnabledRepositories()
getRepositoriesByPriority(priority)
getRepositoriesByFrequency(frequency)
getRepositoriesByTag(tag)
getRepository(name)
getAllGroups()
getEnabledGroups()
getGroup(name)
getGroupRepositories(groupName)
getRepositoriesToScanTonight(maxRepos = null)
getScanDefaults()
getNotificationSettings()
async updateLastScanned(repoName, timestamp = null)
async addScanHistory(repoName, historyEntry)
async save()
validate()
getStats()
_ensureLoaded()
_expandPaths()
</file>

<file path="pipeline-core/errors/error-classifier.js">
function classifyByErrorCode(errorCode)
function classifyByHttpStatus(statusCode)
function classifyByMessagePattern(message)
export function classifyError(error)
‚ãÆ----
// Priority 4: Default classification
‚ãÆ----
/**
 * Check if an error is retryable.
 *
 * @param {Error} error - The error to check
 * @returns {boolean} True if the error should be retried
 */
export function isRetryable(error)
/**
 * Get detailed error information including classification.
 *
 * @param {Error} error - The error to analyze
 * @returns {Object} Detailed error information
 */
export function getErrorInfo(error)
‚ãÆ----
const extError = /** @type {ExtendedError} */ (error);
const httpError = /** @type {HTTPError} */ (error);
‚ãÆ----
function getErrorCategoryGroup(error)
export function createScanError(message, cause)
</file>

<file path="pipeline-core/errors/error-types.d.ts">
export interface NodeError extends Error {
  code?: string;
  errno?: number | string;
  syscall?: string;
  path?: string;
}
export interface ProcessError extends Error {
  code?: number | string;
  stdout?: string;
  stderr?: string;
}
export interface HTTPError extends Error {
  statusCode?: number;
  status?: number;
  response?: {
    status?: number;
    statusText?: string;
    data?: any;
  };
}
export interface ExtendedError extends Error {
  code?: string | number;
  errno?: number | string;
  syscall?: string;
  path?: string;
  stdout?: string;
  stderr?: string;
  statusCode?: number;
  status?: number;
  response?: {
    status?: number;
    statusText?: string;
    data?: any;
  };
  cause?: Error;
}
</file>

<file path="pipeline-core/errors/types.d.ts">
export interface HTTPError extends Error {
  statusCode?: number;
  status?: number;
}
export interface ClassifiedError extends Error {
  retryable?: boolean;
  classification?: {
    category: 'network' | 'file_system' | 'http' | 'database' | 'unknown';
    retryable: boolean;
    severity: 'low' | 'medium' | 'high';
  };
  statusCode?: number;
  status?: number;
}
export function isHTTPError(error: Error): error is HTTPError;
export function isClassifiedError(error: Error): error is ClassifiedError;
</file>

<file path="pipeline-core/extractors/extract_blocks.py">
DEBUG = os.environ.get('PIPELINE_DEBUG', '').lower() in ('1', 'true', 'yes')
def _debug(msg: str) -> None
# Add lib/models and lib/similarity to Python path
‚ãÆ----
# ---------------------------------------------------------------------------
# Function Name Extraction Patterns
‚ãÆ----
FUNCTION_NAME_PATTERNS: tuple[str, ...] = (
‚ãÆ----
r'function\s+(\w+)\s*\(',              # function name(
r'const\s+(\w+)\s*=\s*(?:async\s+)?function',  # const name = function
r'const\s+(\w+)\s*=\s*(?:async\s+)?\(',        # const name = ( or const name = async (
r'let\s+(\w+)\s*=\s*(?:async\s+)?function',    # let name = function
r'let\s+(\w+)\s*=\s*(?:async\s+)?\(',          # let name = (
r'var\s+(\w+)\s*=\s*(?:async\s+)?function',    # var name = function
r'var\s+(\w+)\s*=\s*(?:async\s+)?\(',          # var name = (
r'async\s+function\s+(\w+)\s*\(',      # async function name(
r'(\w+)\s*:\s*function',               # name: function
r'(\w+)\s*:\s*async\s+function',       # name: async function
r'export\s+function\s+(\w+)',          # export function name
r'export\s+const\s+(\w+)\s*=',         # export const name =
‚ãÆ----
def _match_function_pattern(text: str, multiline: bool = False) -> str | None
‚ãÆ----
"""Try to match function name patterns against text."""
flags = re.MULTILINE if multiline else 0
‚ãÆ----
match = re.search(pattern, text, flags)
‚ãÆ----
"""Search backwards in file to find function declaration."""
full_path = Path(repo_path) / file_path
‚ãÆ----
lines = full_path.read_text(encoding='utf-8').splitlines()
‚ãÆ----
# Search backwards from match line (up to 10 lines before)
search_start = max(0, line_start - 11)
‚ãÆ----
func_name = _match_function_pattern(lines[i])
‚ãÆ----
"""Extract function name from source code using regex patterns.
    Priority 1: Function-Level Extraction
    This enables proper matching between detected and expected duplicates.
    If function name can't be found in source_code, reads the actual file
    to get more context (lines before the match).
    """
‚ãÆ----
# Try to find function name in the matched source code
func_name = _match_function_pattern(source_code, multiline=True)
‚ãÆ----
# Fall back to reading file context if we have location info
‚ãÆ----
def _get_function_name_from_tags(tags: list[str]) -> str | None
‚ãÆ----
"""Extract function name from block tags."""
‚ãÆ----
return tag[9:]  # Remove 'function:' prefix
‚ãÆ----
"""Try to add block using function-based deduplication.
    Returns True if block was processed (added or skipped as duplicate).
    """
function_key = f"{block.location.file_path}:{function_name}"
‚ãÆ----
# Already seen this function - keep the earlier occurrence
existing_block = seen_functions[function_key]
‚ãÆ----
"""Add block using location-based deduplication."""
location_key = f"{block.location.file_path}:{block.location.line_start}"
‚ãÆ----
def deduplicate_blocks(blocks: list[CodeBlock]) -> list[CodeBlock]
‚ãÆ----
"""Remove duplicate code blocks from the same location and function.
    Priority 4: Deduplicate Pattern Matches
    ast-grep patterns can match the same code multiple times within a function.
    This removes duplicates based on file:function_name, keeping only the earliest match.
    """
seen_locations: set[str] = set()
seen_functions: dict[str, CodeBlock] = {}
unique_blocks: list[CodeBlock] = []
‚ãÆ----
function_name = _get_function_name_from_tags(block.tags)
‚ãÆ----
removed = len(blocks) - len(unique_blocks)
‚ãÆ----
def extract_code_blocks(pattern_matches: List[Dict], repository_info: Dict) -> List[CodeBlock]
‚ãÆ----
"""
    Extract CodeBlock models from pattern matches
    """
‚ãÆ----
blocks = []
‚ãÆ----
# Generate unique block ID
# Python 3.11 doesn't support nested f-strings, so construct the string first
block_key = f"{match['file_path']}:{match['line_start']}"
block_hash = hashlib.sha256(block_key.encode()).hexdigest()[:12]
block_id = f"cb_{block_hash}"
# Map pattern_id to category (must match SemanticCategory enum)
category_map = {
category = category_map.get(match['rule_id'], 'utility')
# Extract function name from source code (Priority 1: Function-Level Extraction)
source_code = match.get('matched_text', '')
function_name = extract_function_name(
# Create CodeBlock
# Note: file_path from ast-grep is already relative to repository root
block = CodeBlock(
‚ãÆ----
relative_path=match['file_path'],  # Already relative from ast-grep
‚ãÆ----
language='javascript',  # TODO: Detect from file extension
‚ãÆ----
# Store function name in tags field
‚ãÆ----
# Debug: confirm tags are set
if DEBUG and i < 3:  # Only log first 3 blocks
‚ãÆ----
def group_duplicates(blocks: List[CodeBlock]) -> List[DuplicateGroup]
‚ãÆ----
"""
    Group similar code blocks using multi-layer similarity algorithm.
    Priority 2: Structural Similarity
    Uses the enhanced grouping algorithm that combines:
    - Layer 1: Exact matching (hash-based)
    - Layer 2: Structural similarity (AST-based)
    - Layer 3: Semantic equivalence (TODO)
    """
# Use the multi-layer grouping algorithm
groups = group_by_similarity(blocks, similarity_threshold=0.85)
‚ãÆ----
def generate_suggestions(groups: List[DuplicateGroup]) -> List[ConsolidationSuggestion]
‚ãÆ----
"""
    Generate consolidation suggestions with enhanced strategy logic
    """
suggestions = []
‚ãÆ----
# Determine strategy based on multiple factors
‚ãÆ----
# Generate migration steps
migration_steps = _generate_migration_steps(group, strategy)
# Generate code example
code_example = _generate_code_example(group, strategy)
# Calculate ROI score (higher for simpler, lower-risk refactoring)
roi_score = _calculate_roi(group, complexity, risk)
# Determine if this is a breaking change
breaking_changes = _is_breaking_change(group, strategy)
suggestion = ConsolidationSuggestion(
‚ãÆ----
# Strategy Determination Rules (Data-Driven)
‚ãÆ----
# Each category has rules with occurrence thresholds defining the strategy.
# Format: (max_occurrences, strategy, rationale_template, complexity, risk)
# Rules are evaluated in order; first matching rule wins.
‚ãÆ----
@dataclass
class StrategyRule
‚ãÆ----
"""Rule for determining consolidation strategy."""
max_occurrences: int | None  # None means unlimited
strategy: str
rationale_template: str
complexity: str
risk: str
# Category-specific strategy rules
CATEGORY_STRATEGY_RULES: dict[str, list[StrategyRule]] = {
DEFAULT_STRATEGY_RULES: list[StrategyRule] = [
‚ãÆ----
rationale = rule.rationale_template.format(occ=occurrences, files=files)
‚ãÆ----
last_rule = rules[-1]
‚ãÆ----
def _determine_strategy(group: DuplicateGroup) -> tuple[str, str, str, str]
‚ãÆ----
occurrences = group.occurrence_count
files = len(group.affected_files)
category = group.category
‚ãÆ----
rules = CATEGORY_STRATEGY_RULES.get(category, DEFAULT_STRATEGY_RULES)
‚ãÆ----
def _generate_migration_steps(group: DuplicateGroup, strategy: str) -> List[MigrationStep]
‚ãÆ----
steps = [
‚ãÆ----
def _generate_code_example(group: DuplicateGroup, strategy: str) -> str
‚ãÆ----
pattern = group.pattern_id
‚ãÆ----
def _calculate_roi(group: DuplicateGroup, complexity: str, risk: str) -> float
‚ãÆ----
roi = group.impact_score
complexity_multipliers = {
‚ãÆ----
risk_multipliers = {
‚ãÆ----
def _is_breaking_change(group: DuplicateGroup, strategy: str) -> bool
def _suggest_target_location(group: DuplicateGroup, strategy: str) -> str
‚ãÆ----
first_file = group.affected_files[0] if group.affected_files else ''
‚ãÆ----
dir_path = '/'.join(first_file.split('/')[:-1])
‚ãÆ----
else:  # autonomous_agent
‚ãÆ----
def _estimate_effort(group: DuplicateGroup, complexity: str) -> float
‚ãÆ----
"""Estimate effort in hours"""
base_hours = {
hours = base_hours.get(complexity, 2.0)
# Add time per affected file (more files = more refactoring)
‚ãÆ----
# Add time for testing
‚ãÆ----
def main()
‚ãÆ----
"""
    Main pipeline execution
    """
‚ãÆ----
# Read input from stdin
input_data = json.load(sys.stdin)
repository_info = input_data['repository_info']
pattern_matches = input_data['pattern_matches']
# Stage 3: Extract code blocks
blocks = extract_code_blocks(pattern_matches, repository_info)
# Stage 3.5: Deduplicate blocks (Priority 4)
blocks = deduplicate_blocks(blocks)
# Stage 4: Semantic annotation (TODO: Implement full annotator)
# For now, blocks already have basic category from extraction
# Stage 5: Group duplicates
groups = group_duplicates(blocks)
# Stage 6: Generate suggestions
suggestions = generate_suggestions(groups)
# Stage 7: Calculate metrics
metrics = {
‚ãÆ----
'semantic_duplicates': 0,  # TODO: Implement semantic grouping
‚ãÆ----
'duplication_percentage': 0.0,  # TODO: Calculate properly
‚ãÆ----
# Output result as JSON (use mode='json' to serialize datetime objects)
result = {
</file>

<file path="pipeline-core/git/branch-manager.js">
export class BranchManager
‚ãÆ----
async hasChanges(repositoryPath)
async getChangedFiles(repositoryPath)
async getCurrentBranch(repositoryPath)
/**
   * Check if repository is a git repository
   *
   * @param {string} repositoryPath - Path to check
   * @returns {Promise<boolean>} True if git repository
   */
async isGitRepository(repositoryPath)
async createJobBranch(repositoryPath, jobContext)
‚ãÆ----
// Get current branch
‚ãÆ----
async commitChanges(repositoryPath, commitContext)
‚ãÆ----
// Get changed files for logging
‚ãÆ----
async pushBranch(repositoryPath, branchName)
async createPullRequest(repositoryPath, prContext)
async cleanupBranch(repositoryPath, branchName, originalBranch)
_generateBranchName(jobContext)
/**
   * Generate commit message from context
   *
   * @param {Object} commitContext - Commit context
   * @param {string[]} changedFiles - Changed files
   * @returns {string} Commit message
   * @private
   */
_generateCommitMessage(commitContext, changedFiles)
async _runGitCommand(cwd, args)
async _runCommand(cwd, command, args)
</file>

<file path="pipeline-core/git/migration-transformer.js">
function parseMigrationStep(description)
export class MigrationTransformer
‚ãÆ----
async applyMigrationSteps(suggestion, repositoryPath)
async _transformFile(filePath, steps, suggestion)
_updateImport(ast, oldPath, newPath)
‚ãÆ----
ImportDeclaration(path)
‚ãÆ----
_addImport(ast, imported, source)
_replaceCallExpression(ast, oldName, newName)
‚ãÆ----
CallExpression(path)
‚ãÆ----
_removeDeclaration(ast, name)
‚ãÆ----
FunctionDeclaration(path)
ClassDeclaration(path)
VariableDeclarator(path)
‚ãÆ----
_groupStepsByFile(parsedSteps, suggestion)
async _createBackup(repositoryPath)
async _backupFile(filePath, backupPath)
async rollback(backupPath, repositoryPath)
</file>

<file path="pipeline-core/git/pr-creator.js">
export class PRCreator
‚ãÆ----
async createPRsForSuggestions(scanResult, repositoryPath, options =
async _createPRForBatch(suggestions, repositoryPath, batchNumber, options)
async _applySuggestions(suggestions, repositoryPath)
async _runGitCommand(cwd, args)
async _createPR(cwd, branch, title, body)
_batchSuggestions(suggestions)
_generateBranchName(suggestions, batchNumber)
_generateCommitMessage(suggestions, filesModified)
_generatePRTitle(suggestions, batchNumber)
_generatePRDescription(suggestions, filesModified)
</file>

<file path="pipeline-core/models/__init__.py">
__all__ = [
__version__ = '1.0.0'
</file>

<file path="pipeline-core/models/code_block.py">
class LanguageType(str, Enum)
‚ãÆ----
JAVASCRIPT = "javascript"
TYPESCRIPT = "typescript"
PYTHON = "python"
JAVA = "java"
GO = "go"
RUST = "rust"
C = "c"
CPP = "cpp"
CSHARP = "csharp"
PHP = "php"
RUBY = "ruby"
class SemanticCategory(str, Enum)
‚ãÆ----
UTILITY = "utility"
HELPER = "helper"
VALIDATOR = "validator"
API_HANDLER = "api_handler"
AUTH_CHECK = "auth_check"
DATABASE_OPERATION = "database_operation"
ERROR_HANDLER = "error_handler"
LOGGER = "logger"
CONFIG_ACCESS = "config_access"
FILE_OPERATION = "file_operation"
ASYNC_PATTERN = "async_pattern"
UNKNOWN = "unknown"
class SourceLocation(BaseModel)
‚ãÆ----
file_path: str = Field(..., description="Absolute path to source file")
line_start: int = Field(..., ge=1, description="Starting line number (1-indexed)")
line_end: int = Field(..., ge=1, description="Ending line number (1-indexed)")
column_start: Optional[int] = Field(None, ge=0, description="Starting column (0-indexed)")
column_end: Optional[int] = Field(None, ge=0, description="Ending column (0-indexed)")
‚ãÆ----
@field_validator('line_end')
@classmethod
    def validate_line_range(cls, v, info)
def __str__(self) -> str
class ASTNode(BaseModel)
‚ãÆ----
"""Representation of AST node structure"""
node_type: str = Field(..., description="Type of AST node (e.g., 'CallExpression')")
children: List['ASTNode'] = Field(default_factory=list, description="Child nodes")
properties: Dict[str, Any] = Field(default_factory=dict, description="Node properties")
model_config = {
‚ãÆ----
class CodeBlock(BaseModel)
‚ãÆ----
block_id: str = Field(..., description="Unique identifier for this code block")
pattern_id: str = Field(..., description="ast-grep rule ID that matched this block")
location: SourceLocation = Field(..., description="Source code location")
relative_path: str = Field(..., description="Repository-relative path")
source_code: str = Field(..., description="Raw source code of the block")
normalized_code: Optional[str] = Field(None, description="Normalized/formatted code")
ast_structure: Optional[ASTNode] = Field(None, description="AST node tree")
ast_hash: Optional[str] = Field(None, description="Hash of AST structure")
language: LanguageType = Field(..., description="Programming language")
category: SemanticCategory = Field(..., description="Semantic category")
tags: List[str] = Field(default_factory=list, description="Additional semantic tags")
match_context: Dict[str, Any] = Field(
repository_path: str = Field(..., description="Absolute path to repository root")
repository_name: Optional[str] = Field(None, description="Repository name/identifier")
git_commit: Optional[str] = Field(None, description="Git commit hash when scanned")
detected_at: datetime = Field(
line_count: int = Field(..., ge=1, description="Number of lines in block")
complexity_score: Optional[float] = Field(
‚ãÆ----
@computed_field
@property
    def content_hash(self) -> str
‚ãÆ----
normalized = ' '.join(self.source_code.split())
‚ãÆ----
@computed_field
@property
    def structural_hash(self) -> str
def to_dict_for_comparison(self) -> Dict[str, Any]
def __hash__(self) -> int
def __eq__(self, other: object) -> bool
</file>

<file path="pipeline-core/models/consolidation_suggestion.py">
class ConsolidationStrategy(str, Enum)
‚ãÆ----
LOCAL_UTIL = "local_util"
SHARED_PACKAGE = "shared_package"
MCP_SERVER = "mcp_server"
AUTONOMOUS_AGENT = "autonomous_agent"
NO_ACTION = "no_action"
class ImplementationComplexity(str, Enum)
‚ãÆ----
TRIVIAL = "trivial"
SIMPLE = "simple"
MODERATE = "moderate"
COMPLEX = "complex"
VERY_COMPLEX = "very_complex"
class MigrationRisk(str, Enum)
‚ãÆ----
MINIMAL = "minimal"
LOW = "low"
MEDIUM = "medium"
HIGH = "high"
CRITICAL = "critical"
class MigrationStep(BaseModel)
‚ãÆ----
step_number: int = Field(..., ge=1, description="Step order")
description: str = Field(..., description="What to do in this step")
code_example: Optional[str] = Field(None, description="Example code")
automated: bool = Field(False, description="Can this step be automated?")
estimated_time: Optional[str] = Field(None, description="Estimated time (e.g., '30min', '2h')")
class ConsolidationSuggestion(BaseModel)
‚ãÆ----
suggestion_id: str = Field(..., description="Unique identifier for this suggestion")
duplicate_group_id: str = Field(..., description="ID of DuplicateGroup being addressed")
strategy: ConsolidationStrategy = Field(..., description="Recommended consolidation tier")
strategy_rationale: str = Field(..., description="Why this strategy was chosen")
impact_score: float = Field(
complexity: ImplementationComplexity = Field(..., description="Implementation complexity")
migration_risk: MigrationRisk = Field(..., description="Migration risk level")
breaking_changes: bool = Field(..., description="Will this introduce breaking changes?")
migration_steps: List[MigrationStep] = Field(
target_location: Optional[str] = Field(
target_name: Optional[str] = Field(
proposed_implementation: Optional[str] = Field(
usage_example: Optional[str] = Field(
estimated_effort_hours: Optional[float] = Field(
loc_reduction: Optional[int] = Field(
affected_files_count: int = Field(..., ge=1, description="Number of files to modify")
affected_repositories_count: int = Field(..., ge=1, description="Number of repos affected")
dependencies: List[str] = Field(
prerequisite_suggestions: List[str] = Field(
test_strategy: Optional[str] = Field(
rollback_plan: Optional[str] = Field(
confidence: float = Field(
automated_refactor_possible: bool = Field(
requires_human_review: bool = Field(
benefits: List[str] = Field(
drawbacks: List[str] = Field(
notes: Optional[str] = Field(None, description="Additional notes")
created_at: datetime = Field(
model_config = {
‚ãÆ----
@field_validator('impact_score')
@classmethod
    def round_impact_score(cls, v)
‚ãÆ----
@computed_field
@property
    def priority(self) -> str
‚ãÆ----
@computed_field
@property
    def roi_score(self) -> float
‚ãÆ----
complexity_hours = {
effort = self.estimated_effort_hours or complexity_hours.get(self.complexity, 10)
‚ãÆ----
roi = (self.impact_score / effort) * 10
‚ãÆ----
@computed_field
@property
    def is_quick_win(self) -> bool
‚ãÆ----
step_number = len(self.migration_steps) + 1
step = MigrationStep(
‚ãÆ----
def add_benefit(self, benefit: str) -> None
def add_drawback(self, drawback: str) -> None
def to_markdown_summary(self) -> str
def __hash__(self) -> int
def __eq__(self, other: object) -> bool
</file>

<file path="pipeline-core/models/duplicate_group.py">
class SimilarityMethod(str, Enum)
‚ãÆ----
EXACT_MATCH = "exact_match"
STRUCTURAL = "structural"
SEMANTIC = "semantic"
HYBRID = "hybrid"
class DuplicateGroup(BaseModel)
‚ãÆ----
group_id: str = Field(..., description="Unique identifier for this duplicate group")
pattern_id: str = Field(..., description="ast-grep pattern that matched these blocks")
member_block_ids: List[str] = Field(
similarity_score: float = Field(
similarity_method: SimilarityMethod = Field(
canonical_block_id: Optional[str] = Field(
category: str = Field(..., description="Semantic category of duplicates")
language: str = Field(..., description="Programming language")
occurrence_count: int = Field(..., ge=2, description="Number of occurrences")
total_lines: int = Field(..., ge=1, description="Total lines of duplicated code")
affected_files: List[str] = Field(
affected_repositories: List[str] = Field(
consolidation_complexity: Optional[str] = Field(
breaking_changes_risk: Optional[str] = Field(
created_at: datetime = Field(
updated_at: datetime = Field(
notes: Optional[str] = Field(None, description="Analysis notes")
metadata: Dict[str, Any] = Field(
model_config = {
‚ãÆ----
@field_validator('member_block_ids')
@classmethod
    def validate_min_members(cls, v)
‚ãÆ----
@field_validator('canonical_block_id')
@classmethod
    def validate_canonical_in_members(cls, v, info)
‚ãÆ----
@computed_field
@property
    def deduplication_potential(self) -> int
‚ãÆ----
avg_lines_per_instance = self.total_lines / self.occurrence_count
‚ãÆ----
@computed_field
@property
    def impact_score(self) -> float
‚ãÆ----
occurrence_factor = min(self.occurrence_count / 20.0, 1.0)
similarity_factor = self.similarity_score
loc_factor = min(self.total_lines / 100.0, 1.0)
score = (
‚ãÆ----
@computed_field
@property
    def is_cross_repository(self) -> bool
‚ãÆ----
@computed_field
@property
    def priority_level(self) -> str
def add_member(self, block_id: str) -> None
def remove_member(self, block_id: str) -> None
def set_canonical(self, block_id: str) -> None
def __hash__(self) -> int
def __eq__(self, other: object) -> bool
</file>

<file path="pipeline-core/models/scan_report.py">
class RepositoryInfo(BaseModel)
‚ãÆ----
repository_path: str = Field(..., description="Absolute path to repository")
repository_name: str = Field(..., description="Repository name/identifier")
git_remote: Optional[str] = Field(None, description="Git remote URL")
git_branch: Optional[str] = Field(None, description="Current branch")
git_commit: Optional[str] = Field(None, description="Current commit hash")
total_files: int = Field(..., ge=0, description="Total files scanned")
total_lines: int = Field(..., ge=0, description="Total lines of code scanned")
languages: List[str] = Field(default_factory=list, description="Languages detected")
class ScanConfiguration(BaseModel)
‚ãÆ----
rules_used: List[str] = Field(default_factory=list, description="ast-grep rules applied")
excluded_paths: List[str] = Field(default_factory=list, description="Paths excluded from scan")
min_similarity_threshold: float = Field(0.8, ge=0.0, le=1.0, description="Minimum similarity for grouping")
min_duplicate_size: int = Field(3, ge=1, description="Minimum lines for duplicate detection")
class ScanMetrics(BaseModel)
‚ãÆ----
total_code_blocks: int = Field(..., ge=0, description="Total code blocks detected")
code_blocks_by_category: Dict[str, int] = Field(
code_blocks_by_language: Dict[str, int] = Field(
total_duplicate_groups: int = Field(..., ge=0, description="Total duplicate groups found")
exact_duplicates: int = Field(..., ge=0, description="Groups with 100% similarity")
structural_duplicates: int = Field(..., ge=0, description="Groups with structural similarity")
semantic_duplicates: int = Field(..., ge=0, description="Groups with semantic similarity")
total_duplicated_lines: int = Field(..., ge=0, description="Total lines in duplicate groups")
potential_loc_reduction: int = Field(..., ge=0, description="Potential lines that could be removed")
duplication_percentage: float = Field(..., ge=0.0, le=100.0, description="Percentage of code that's duplicated")
total_suggestions: int = Field(..., ge=0, description="Total consolidation suggestions")
quick_wins: int = Field(..., ge=0, description="Number of quick win suggestions")
high_priority_suggestions: int = Field(..., ge=0, description="High priority suggestions")
cross_repository_duplicates: int = Field(0, ge=0, description="Duplicates spanning multiple repos")
class ScanReport(BaseModel)
‚ãÆ----
report_id: str = Field(..., description="Unique identifier for this report")
scan_name: Optional[str] = Field(None, description="Descriptive name for this scan")
scanned_at: datetime = Field(default_factory=datetime.utcnow, description="Scan timestamp")
scan_duration_seconds: Optional[float] = Field(None, ge=0, description="How long the scan took")
scanner_version: str = Field("1.0.0", description="Version of duplicate detection pipeline")
configuration: ScanConfiguration = Field(..., description="Scan configuration")
repositories: List[RepositoryInfo] = Field(..., description="Repositories scanned")
code_block_ids: List[str] = Field(default_factory=list, description="IDs of detected code blocks")
duplicate_group_ids: List[str] = Field(default_factory=list, description="IDs of duplicate groups")
suggestion_ids: List[str] = Field(default_factory=list, description="IDs of consolidation suggestions")
metrics: ScanMetrics = Field(..., description="Statistical metrics")
executive_summary: Optional[str] = Field(None, description="High-level summary of findings")
recommendations: List[str] = Field(default_factory=list, description="Top-level recommendations")
warnings: List[str] = Field(default_factory=list, description="Warnings or issues encountered")
output_directory: str = Field(..., description="Directory where detailed results are saved")
report_files: Dict[str, str] = Field(
tags: List[str] = Field(default_factory=list, description="Tags for categorization")
metadata: Dict[str, Any] = Field(default_factory=dict, description="Additional metadata")
model_config = {
‚ãÆ----
@computed_field
@property
    def is_multi_repository(self) -> bool
‚ãÆ----
@computed_field
@property
    def total_scanned_files(self) -> int
‚ãÆ----
@computed_field
@property
    def total_scanned_lines(self) -> int
‚ãÆ----
@computed_field
@property
    def duplication_severity(self) -> str
‚ãÆ----
dup_pct = self.metrics.duplication_percentage
‚ãÆ----
@computed_field
@property
    def consolidation_opportunity_score(self) -> float
‚ãÆ----
dup_factor = min(self.metrics.duplication_percentage / 40 * 100, 100)
quick_win_factor = min(self.metrics.quick_wins / 10 * 100, 100)
‚ãÆ----
loc_reduction_factor = (
‚ãÆ----
loc_reduction_factor = 0
score = (
‚ãÆ----
def add_repository(self, repo_info: RepositoryInfo) -> None
def add_code_block_id(self, block_id: str) -> None
def add_duplicate_group_id(self, group_id: str) -> None
def add_suggestion_id(self, suggestion_id: str) -> None
def generate_executive_summary(self) -> str
‚ãÆ----
repos_text = f"{len(self.repositories)} repository" if len(self.repositories) == 1 else f"{len(self.repositories)} repositories"
summary = f"""
‚ãÆ----
def to_summary_dict(self) -> Dict[str, Any]
def __hash__(self) -> int
def __eq__(self, other: object) -> bool
</file>

<file path="pipeline-core/models/test_models.py">
def test_model_structure()
‚ãÆ----
models_dir = Path(__file__).parent
models = [
‚ãÆ----
model_path = models_dir / f"{model_name}.py"
‚ãÆ----
content = f.read()
checks = {
all_passed = all(checks.values())
symbol = "‚úÖ" if all_passed else "‚ö†Ô∏è"
‚ãÆ----
status = "‚úì" if passed else "‚úó"
‚ãÆ----
def test_sample_data()
‚ãÆ----
"""Test with sample data structure (without pydantic)"""
‚ãÆ----
# Sample CodeBlock data
code_block_data = {
# Sample DuplicateGroup data
duplicate_group_data = {
# Sample ConsolidationSuggestion data
consolidation_data = {
# Sample ScanReport data
scan_report_data = {
samples = [
‚ãÆ----
required_fields = len(data)
‚ãÆ----
def test_computed_fields_logic()
‚ãÆ----
"""Test the logic of computed fields (without pydantic)"""
‚ãÆ----
# Test DuplicateGroup impact_score calculation
def calculate_impact_score(occurrence_count, similarity_score, total_lines)
‚ãÆ----
occurrence_factor = min(occurrence_count / 20.0, 1.0)
similarity_factor = similarity_score
loc_factor = min(total_lines / 100.0, 1.0)
score = (
‚ãÆ----
test_cases = [
‚ãÆ----
score = calculate_impact_score(occurrence, similarity, lines)
‚ãÆ----
def calculate_roi_score(impact_score, estimated_hours)
‚ãÆ----
roi = (impact_score / estimated_hours) * 10
‚ãÆ----
roi_tests = [
‚ãÆ----
roi = calculate_roi_score(impact, hours)
‚ãÆ----
def main()
</file>

<file path="pipeline-core/reports/html-report-generator.js">
export class HTMLReportGenerator
‚ãÆ----
static generateReport(scanResult, options =
static async saveReport(scanResult, outputPath, options =
static _generateHeader(scanResult, title, isInterProject)
static _generateMetrics(scanResult, isInterProject)
static _generateSummaryCharts(scanResult, isInterProject)
static _generateCrossRepoSection(scanResult)
static _generateDuplicateGroups(scanResult, isInterProject)
/**
   * Generate suggestions section
   * @private
   */
static _generateSuggestions(scanResult, isInterProject)
/**
   * Generate footer
   * @private
   */
static _generateFooter(scanResult)
/**
   * Get CSS styles
   * @private
   */
static _getStyles()
/**
   * Get JavaScript
   * @private
   */
static _getScripts()
static _getImpactClass(score)
static _getROIClass(score)
static _escapeHtml(text)
</file>

<file path="pipeline-core/reports/json-report-generator.js">
export class JSONReportGenerator
‚ãÆ----
static generateReport(scanResult, options =
static generateSummary(scanResult)
static async saveReport(scanResult, outputPath, options =
static async saveSummary(scanResult, outputPath)
static _generateMetadata(scanResult, isInterProject)
static _generateSummary(scanResult, isInterProject)
static _formatDuplicateGroups(scanResult, isInterProject, maxGroups, includeSourceCode)
static _formatSuggestions(scanResult, isInterProject, maxSuggestions)
static _formatRepositoryScans(scanResult, includeCodeBlocks, includeSourceCode)
static _formatCodeBlocks(scanResult, includeSourceCode)
static _calculateStrategyDistribution(suggestions)
static _calculateComplexityDistribution(suggestions)
</file>

<file path="pipeline-core/reports/markdown-report-generator.js">
export class MarkdownReportGenerator
‚ãÆ----
static generateReport(scanResult, options =
‚ãÆ----
// Header
‚ãÆ----
static generateSummary(scanResult)
static async saveReport(scanResult, outputPath, options =
static async saveSummary(scanResult, outputPath)
static _generateHeader(scanResult, isInterProject)
static _generateMetrics(scanResult, isInterProject)
static _generateRepositoryInfo(scanResult)
static _generateDuplicateGroups(scanResult, isInterProject, maxGroups, includeDetails)
static _generateSuggestions(scanResult, isInterProject, maxSuggestions, includeDetails)
static _generateFooter(scanResult)
static _formatScore(score)
static _formatStrategy(strategy)
static _formatComplexity(complexity)
static _formatRisk(risk)
</file>

<file path="pipeline-core/reports/report-coordinator.js">
export class ReportCoordinator
‚ãÆ----
async generateAllReports(scanResult, options =
async generateHTMLReport(scanResult, baseFilename = null, options =
async generateMarkdownReport(scanResult, baseFilename = null, options =
async generateJSONReport(scanResult, baseFilename = null, options =
async generateJSONSummary(scanResult, baseFilename = null)
async generateMarkdownSummary(scanResult, baseFilename = null)
_generateBaseFilename(scanResult, isInterProject = null)
_generateTitle(scanResult)
static generateQuickSummary(scanResult)
static printQuickSummary(scanResult)
</file>

<file path="pipeline-core/scanners/ast-grep-detector.js">
export class AstGrepPatternDetector
‚ãÆ----
async detectPatterns(repoPath, detectConfig =
async runAstGrepScan(repoPath, config)
normalizeMatch(match, repoPath)
async loadRules(rulesDir)
‚ãÆ----
async function walkRules(dir)
‚ãÆ----
async detectInFile(filePath, rules = [])
‚ãÆ----
export class PatternDetectionError extends Error
</file>

<file path="pipeline-core/scanners/codebase-health-scanner.js">
info: (...args)
warn: (...args)
error: (...args)
‚ãÆ----
async function main()
function generateMarkdownReport(results)
‚ãÆ----
// Timeout scan report
‚ãÆ----
// Root directory analysis report
‚ãÆ----
// Summary
‚ãÆ----
export async function runHealthScan(repoPath, options =
</file>

<file path="pipeline-core/scanners/repository-scanner.js">
export class RepositoryScanner
‚ãÆ----
async scanRepository(repoPath, scanConfig =
async validateRepository(repoPath)
async getRepositoryInfo(repoPath)
async runRepomixScan(repoPath)
async parseRepomixOutput(outputFile)
async getFileMetadata(repoPath, scanConfig)
async listFiles(repoPath, filter =
‚ãÆ----
async function walk(dir)
‚ãÆ----
detectLanguages(files)
‚ãÆ----
export class RepositoryScanError extends Error
</file>

<file path="pipeline-core/scanners/root-directory-analyzer.js">
export class RootDirectoryAnalyzer
‚ãÆ----
async analyze(repoPath, options =
async getRootFiles(repoPath)
categorizeFiles(files)
async analyzeImportDependencies(repoPath, categorized)
async analyzePythonImports(filePath, repoPath)
isLocalModule(moduleName, repoPath)
generateRecommendations(categorized, dependencies)
‚ãÆ----
// Identify as library if:
// - Has "lib", "utils", "helper", "middleware", "config" in name
// - Is imported by other root files
// - Is not a main entry point (server, app, main)
‚ãÆ----
calculateImportChanges(files, targetDir, dependencies)
‚ãÆ----
// This module's imports from other root files need updating
‚ãÆ----
generateMoveCommands(files, targetDir)
generateReport(analysis)
‚ãÆ----
// Category breakdown
‚ãÆ----
// Recommendations
‚ãÆ----
export function createRootDirectoryAnalyzer(options)
</file>

<file path="pipeline-core/scanners/timeout_detector.py">
@dataclass
class Finding
‚ãÆ----
file_path: str
line_number: int
severity: str
category: str
message: str
code_snippet: str
recommendation: str
‚ãÆ----
@dataclass
class FileContext
‚ãÆ----
file_path: Path
content: str
lines: list[str]
def has_text_in_range(self, text: str, start_line: int, num_lines: int) -> bool
‚ãÆ----
end = min(start_line + num_lines, len(self.lines))
‚ãÆ----
has_try = ctx.has_pattern_in_range(r'\btry\b', line_num, 50)
has_catch = ctx.has_pattern_in_range(r'\bcatch\b', line_num, 50)
‚ãÆ----
PATTERN_DETECTORS: list[Callable[[FileContext, int, str], Finding | None]] = [
EXCLUDED_DIRS = frozenset({'node_modules', '.git', 'dist', 'build'})
def _should_include_file(file_path: Path) -> bool
class TimeoutDetector
‚ãÆ----
def __init__(self, logger: Callable[[str], None] | None = None) -> None
def scan_directory(self, repo_path: str) -> dict[str, Any]
‚ãÆ----
path = Path(repo_path)
‚ãÆ----
files = list(self._find_files(path))
‚ãÆ----
def _find_files(self, repo_path: Path) -> Iterator[Path]
‚ãÆ----
extensions = {'.ts', '.tsx', '.js', '.jsx', '.py'}
‚ãÆ----
def _scan_file(self, file_path: Path) -> None
‚ãÆ----
"""Scan a single file for patterns."""
‚ãÆ----
content = file_path.read_text(encoding='utf-8')
‚ãÆ----
ctx = FileContext(file_path=file_path, content=content, lines=content.split('\n'))
‚ãÆ----
def _check_line(self, ctx: FileContext, line_num: int, line: str) -> None
‚ãÆ----
"""Check a line against all pattern detectors."""
‚ãÆ----
finding = detector(ctx, line_num, line)
‚ãÆ----
def _calculate_statistics(self) -> dict[str, Any]
‚ãÆ----
"""Calculate statistics from findings."""
‚ãÆ----
severity_counts = Counter(f.severity for f in self.findings)
category_counts = Counter(f.category for f in self.findings)
files_affected = {f.file_path for f in self.findings}
‚ãÆ----
def generate_report(self, results: dict[str, Any]) -> str
‚ãÆ----
"""Generate markdown report."""
stats = results['statistics']
lines = [
severity_emoji = {'high': 'üî¥', 'medium': 'üü°', 'low': 'üü¢'}
‚ãÆ----
emoji = severity_emoji.get(severity, '‚ö™')
‚ãÆ----
def _add_findings_by_severity(self, lines: list[str], max_per_severity: int = 10) -> None
‚ãÆ----
"""Add findings grouped by severity to report lines."""
‚ãÆ----
severity_findings = [f for f in self.findings if f.severity == severity]
‚ãÆ----
remaining = len(severity_findings) - max_per_severity
‚ãÆ----
def main()
‚ãÆ----
repo_path = sys.argv[1]
output_file = None
‚ãÆ----
output_index = sys.argv.index('--output')
‚ãÆ----
output_file = sys.argv[output_index + 1]
detector = TimeoutDetector()
results = detector.scan_directory(repo_path)
‚ãÆ----
output = json.dumps(results, indent=2)
‚ãÆ----
output = detector.generate_report(results)
‚ãÆ----
high_severity_count = results['statistics']['severity_breakdown'].get('high', 0)
</file>

<file path="pipeline-core/scanners/timeout-pattern-detector.js">
export class TimeoutPatternDetector
‚ãÆ----
async scan(repoPath, options =
async findPromiseRaceWithoutTimeout(repoPath)
async findLoadingWithoutFinally(repoPath)
async findAsyncWithoutErrorHandling(repoPath)
async findMissingTimeoutConstants(repoPath)
async findSetLoadingWithoutReset(repoPath)
async searchPattern(repoPath, pattern, options =
async readFile(filePath)
/**
   * Group matches by file
   */
groupByFile(matches)
/**
   * Calculate severity breakdown
   */
calculateSeverityBreakdown(findings)
generateReport(findings)
‚ãÆ----
export function createTimeoutPatternDetector(options)
</file>

<file path="pipeline-core/similarity/__init__.py">
__all__ = [
</file>

<file path="pipeline-core/similarity/config.py">
class SimilarityConfig
‚ãÆ----
MIN_LINE_COUNT = int(os.getenv('MIN_LINE_COUNT', '1'))
MIN_UNIQUE_TOKENS = int(os.getenv('MIN_UNIQUE_TOKENS', '3'))
STRUCTURAL_THRESHOLD = float(os.getenv('STRUCTURAL_THRESHOLD', '0.90'))
OPPOSITE_LOGIC_PENALTY = float(os.getenv('OPPOSITE_LOGIC_PENALTY', '0.8'))
STATUS_CODE_PENALTY = float(os.getenv('STATUS_CODE_PENALTY', '0.7'))
SEMANTIC_METHOD_PENALTY = float(os.getenv('SEMANTIC_METHOD_PENALTY', '0.85'))
CHAIN_WEIGHT_LEVENSHTEIN = float(os.getenv('CHAIN_WEIGHT_LEVENSHTEIN', '0.7'))
CHAIN_WEIGHT_CHAIN = float(os.getenv('CHAIN_WEIGHT_CHAIN', '0.3'))
MIN_COMPLEXITY_RATIO = float(os.getenv('MIN_COMPLEXITY_RATIO', '0.5'))
MIN_GROUP_QUALITY = float(os.getenv('MIN_GROUP_QUALITY', '0.70'))
QUALITY_WEIGHT_SIMILARITY = float(os.getenv('QUALITY_WEIGHT_SIMILARITY', '0.4'))
QUALITY_WEIGHT_SIZE = float(os.getenv('QUALITY_WEIGHT_SIZE', '0.2'))
QUALITY_WEIGHT_COMPLEXITY = float(os.getenv('QUALITY_WEIGHT_COMPLEXITY', '0.2'))
QUALITY_WEIGHT_SEMANTIC = float(os.getenv('QUALITY_WEIGHT_SEMANTIC', '0.2'))
‚ãÆ----
@classmethod
    def to_dict(cls) -> dict
‚ãÆ----
@classmethod
    def print_config(cls)
config = SimilarityConfig()
</file>

<file path="pipeline-core/similarity/grouping.py">
DEBUG = os.environ.get('PIPELINE_DEBUG', '').lower() in ('1', 'true', 'yes')
‚ãÆ----
MIN_COMPLEXITY_THRESHOLD = {
MIN_GROUP_QUALITY = float(os.getenv('MIN_GROUP_QUALITY', '0.70'))
OPPOSITE_OPERATOR_PAIRS: list[tuple[set[str], set[str]]] = [
‚ãÆ----
@dataclass
class SemanticCheckResult
‚ãÆ----
is_valid: bool
reason: str
details: tuple[Any, Any] | None = None
def _check_method_chain(code1: str, code2: str) -> SemanticCheckResult
‚ãÆ----
chain1 = extract_method_chain(code1)
chain2 = extract_method_chain(code2)
‚ãÆ----
def _check_http_status_codes(code1: str, code2: str) -> SemanticCheckResult
‚ãÆ----
status1 = extract_http_status_codes(code1)
status2 = extract_http_status_codes(code2)
‚ãÆ----
def _check_logical_operators(code1: str, code2: str) -> SemanticCheckResult
‚ãÆ----
ops1 = extract_logical_operators(code1)
ops2 = extract_logical_operators(code2)
‚ãÆ----
has_opposite = (
‚ãÆ----
def _check_semantic_methods(code1: str, code2: str) -> SemanticCheckResult
‚ãÆ----
methods1 = extract_semantic_methods(code1)
methods2 = extract_semantic_methods(code2)
‚ãÆ----
SEMANTIC_CHECKS: list[Callable[[str, str], SemanticCheckResult]] = [
def _extract_function_names(blocks: list) -> list[str]
‚ãÆ----
func_names = []
‚ãÆ----
def _run_semantic_checks(code1: str, code2: str) -> SemanticCheckResult
‚ãÆ----
result = check(code1, code2)
‚ãÆ----
def calculate_code_complexity(source_code: str) -> dict
‚ãÆ----
lines = [line.strip() for line in source_code.split('\n') if line.strip()]
line_count = len(lines)
tokens = re.findall(r'\b\w+\b', source_code)
unique_tokens = len(set(tokens))
control_flow_keywords = ['if', 'else', 'for', 'while', 'switch', 'case', 'try', 'catch']
has_control_flow = any(keyword in source_code for keyword in control_flow_keywords)
‚ãÆ----
def is_complex_enough(block: 'CodeBlock') -> bool
‚ãÆ----
complexity = calculate_code_complexity(block.source_code)
‚ãÆ----
def calculate_group_quality_score(group_blocks: List['CodeBlock'], similarity_score: float) -> float
‚ãÆ----
similarity_factor = similarity_score * 0.4
size_factor = min(len(group_blocks) / 4.0, 1.0) * 0.2
avg_line_count = sum(b.line_count for b in group_blocks) / len(group_blocks)
complexity_factor = min(avg_line_count / 10.0, 1.0) * 0.2
categories = set(b.category for b in group_blocks)
pattern_ids = set(b.pattern_id for b in group_blocks)
semantic_factor = 0.0
‚ãÆ----
semantic_factor = 1.0 * 0.2
‚ãÆ----
semantic_factor = 0.7 * 0.2
‚ãÆ----
semantic_factor = 0.5 * 0.2
‚ãÆ----
semantic_factor = 0.3 * 0.2
total_quality = similarity_factor + size_factor + complexity_factor + semantic_factor
‚ãÆ----
def validate_exact_group_semantics(group_blocks: List['CodeBlock']) -> tuple
‚ãÆ----
func_names = _extract_function_names(group_blocks)
‚ãÆ----
result = _run_semantic_checks(
‚ãÆ----
# Check group quality
quality_score = calculate_group_quality_score(group_blocks, similarity_score)
‚ãÆ----
# Accept group
group = _create_duplicate_group(group_blocks, similarity_score, similarity_method)
‚ãÆ----
# Mark blocks as grouped
‚ãÆ----
"""
    Group code blocks using multi-layer similarity algorithm with complexity filtering.
    Implements Layer 0 (complexity), Layer 1 (exact), Layer 2 (structural), and Layer 3 (semantic).
    Algorithm:
    0. Layer 0: Filter trivial blocks (below complexity threshold)
    1. Layer 1: Group by exact content hash (O(n))
    2. Layer 2: Group remaining by structural similarity (O(n*k))
    3. Layer 3: Semantic validation (pattern, category, tags)
    Returns:
        List of DuplicateGroup objects with similarity scores
    """
# Layer 0: Filter out trivial blocks before grouping
complex_blocks = [b for b in blocks if is_complex_enough(b)]
trivial_count = len(blocks) - len(complex_blocks)
‚ãÆ----
groups = []
grouped_block_ids = set()
# Layer 1: Exact matching (hash-based)
‚ãÆ----
exact_groups = _group_by_exact_hash(complex_blocks)
‚ãÆ----
ungrouped_blocks = [b for b in complex_blocks if b.block_id not in grouped_block_ids]
‚ãÆ----
structural_groups = _group_by_structural_similarity(ungrouped_blocks, similarity_threshold)
layer1_count = len(groups)
‚ãÆ----
def _group_by_exact_hash(blocks: List['CodeBlock']) -> Dict[str, List['CodeBlock']]
‚ãÆ----
hash_groups = defaultdict(list)
‚ãÆ----
hash_val = block.content_hash
‚ãÆ----
func_names = _extract_function_names([block])
func_name = func_names[0] if func_names else 'unknown'
‚ãÆ----
"""
    Group blocks by structural similarity using clustering.
    Returns:
        List of (group_blocks, similarity_score) tuples
    """
‚ãÆ----
# Build similarity matrix
n = len(blocks)
‚ãÆ----
used = set()
# For each block, find all structurally similar blocks
‚ãÆ----
# Start a new group with this block
group = [block1]
similarities = []
# Compare with remaining blocks
‚ãÆ----
block2 = blocks[j]
# Pre-check semantic compatibility
‚ãÆ----
continue  # Skip incompatible blocks
# Calculate structural similarity
‚ãÆ----
# If we found similar blocks, validate and create a group
‚ãÆ----
# Validate complete group
‚ãÆ----
# Average similarity score for the group
avg_similarity = sum(similarities) / len(similarities) if similarities else 1.0
‚ãÆ----
# Group failed semantic validation
‚ãÆ----
"""Create a DuplicateGroup from a list of similar blocks."""
</file>

<file path="pipeline-core/similarity/semantic.py">
def are_semantically_compatible(block1: 'CodeBlock', block2: 'CodeBlock') -> bool
‚ãÆ----
tags1 = set(block1.tags)
tags2 = set(block2.tags)
func1 = _extract_function_tag(tags1)
func2 = _extract_function_tag(tags2)
‚ãÆ----
line_ratio = min(block1.line_count, block2.line_count) / max(block1.line_count, block2.line_count)
‚ãÆ----
def calculate_tag_overlap(block1: 'CodeBlock', block2: 'CodeBlock') -> float
‚ãÆ----
intersection = tags1 & tags2
union = tags1 | tags2
‚ãÆ----
def _extract_function_tag(tags: set) -> Optional[str]
def validate_duplicate_group(blocks: List['CodeBlock']) -> bool
‚ãÆ----
pattern_ids = set(b.pattern_id for b in blocks)
‚ãÆ----
categories = set(b.category for b in blocks)
</file>

<file path="pipeline-core/similarity/structural.py">
@dataclass
class SemanticFeatures
‚ãÆ----
http_status_codes: Set[int] = field(default_factory=set)
logical_operators: Set[str] = field(default_factory=set)
semantic_methods: Set[str] = field(default_factory=set)
def extract_semantic_features(source_code: str) -> SemanticFeatures
‚ãÆ----
features = SemanticFeatures()
‚ãÆ----
status_pattern = r'\.status\((\d{3})\)'
‚ãÆ----
status_code = int(match.group(1))
‚ãÆ----
# Extract logical operators (===, !==, ==, !=, !, &&, ||)
# Order matters: match longer operators first to avoid partial matches
operator_patterns = [
‚ãÆ----
(r'!==', '!=='),   # Strict inequality
(r'===', '==='),   # Strict equality
(r'!=', '!='),     # Loose inequality
(r'==', '=='),     # Loose equality
(r'!\s*[^=]', '!'), # Logical NOT (followed by non-=)
(r'&&', '&&'),     # Logical AND
(r'\|\|', '||'),   # Logical OR
‚ãÆ----
# Extract semantic methods (Math.max, Math.min, console.log, etc.)
semantic_patterns = {
‚ãÆ----
def normalize_code(source_code: str) -> str
‚ãÆ----
"""
    Normalize code by removing variable-specific information.
    This allows structural comparison by focusing on code structure
    rather than specific names or values.
    """
‚ãÆ----
# Remove comments
normalized = re.sub(r'//.*?$', '', source_code, flags=re.MULTILINE)  # Single-line comments
normalized = re.sub(r'/\*.*?\*/', '', normalized, flags=re.DOTALL)   # Multi-line comments
# Normalize whitespace (collapse to single spaces)
normalized = re.sub(r'\s+', ' ', normalized)
# Normalize string literals (replace with placeholder)
normalized = re.sub(r"'[^']*'", "'STR'", normalized)
normalized = re.sub(r'"[^"]*"', '"STR"', normalized)
normalized = re.sub(r'`[^`]*`', '`STR`', normalized)
# Normalize numbers (replace with placeholder)
normalized = re.sub(r'\b\d+\b', 'NUM', normalized)
SEMANTIC_METHODS = {
SEMANTIC_OBJECTS = {
‚ãÆ----
normalized = re.sub(rf'\b{obj}\b', f'__PRESERVE_OBJ_{obj.upper()}__', normalized)
‚ãÆ----
normalized = re.sub(rf'\b{method}\b', f'__PRESERVE_{method.upper()}__', normalized)
normalized = re.sub(r'\b[a-z][a-zA-Z0-9_]*\b', 'var', normalized)
normalized = re.sub(r'\b[A-Z][A-Z0-9_]*\b', 'CONST', normalized)
‚ãÆ----
normalized = normalized.replace(f'__PRESERVE_OBJ_{obj.upper()}__', obj)
‚ãÆ----
normalized = normalized.replace(f'__PRESERVE_{method.upper()}__', method)
normalized = re.sub(r'\s*([(){}[\];,.])\s*', r'\1', normalized)
normalized = re.sub(r'\s*(=>|===?|!==?|[+\-*/%<>=&|])\s*', r' \1 ', normalized)
# Collapse multiple spaces
‚ãÆ----
# Trim
normalized = normalized.strip()
‚ãÆ----
def calculate_ast_hash(source_code: str) -> str
‚ãÆ----
Penalties are multiplicative - each mismatch reduces similarity:
- Logical operators: 0.80x (20% penalty)
- Semantic methods: 0.75x (25% penalty)
Args:
features2: Semantic features from second code block
Returns:
</file>

<file path="pipeline-core/types/scan-orchestrator-types.js">

</file>

<file path="pipeline-core/types/scan-orchestrator-types.ts">
import { z } from 'zod';
‚ãÆ----
export type RepositoryInfo = z.infer<typeof RepositoryInfoSchema>;
export type PatternMatch = z.infer<typeof PatternMatchSchema>;
export type ScanConfig = z.infer<typeof ScanConfigSchema>;
export type PythonPipelineInput = z.infer<typeof PythonPipelineInputSchema>;
export type CodeBlock = z.infer<typeof CodeBlockSchema>;
export type DuplicateGroup = z.infer<typeof DuplicateGroupSchema>;
export type ConsolidationSuggestion = z.infer<typeof ConsolidationSuggestionSchema>;
export type ScanMetrics = z.infer<typeof ScanMetricsSchema>;
export type PythonPipelineOutput = z.infer<typeof PythonPipelineOutputSchema>;
export type ScanResult = z.infer<typeof ScanResultSchema>;
export type ReportOptions = z.infer<typeof ReportOptionsSchema>;
export type ScanOrchestratorOptions = z.infer<typeof ScanOrchestratorOptionsSchema>;
</file>

<file path="pipeline-core/utils/error-helpers.js">
export function safeErrorMessage(error, fallback = 'Unknown error')
export function safeErrorStack(error)
export function toErrorObject(error, options =
function getErrorType(error)
export function isErrorLike(value)
export function serializeError(error, includeStack = true)
export function formatErrorMessage(error, options =
export function combineErrors(errors, separator = '; ')
</file>

<file path="pipeline-core/doppler-health-monitor.js">
export class DopplerHealthMonitor
‚ãÆ----
async checkCacheHealth()
getSeverity(cacheAge)
async startMonitoring(intervalMinutes = 15)
stopMonitoring()
getCacheDirectoryPath()
</file>

<file path="pipeline-core/inter-project-scanner.js">
export class InterProjectScanner
‚ãÆ----
async scanRepositories(repoPaths, scanConfig =
_aggregateCodeBlocks(repositoryScans)
_detectCrossRepoDuplicates(allCodeBlocks)
_calculateCrossRepoImpactScore(group)
_generateCrossRepoSuggestions(crossRepoDuplicates, repositoryScans)
_determineCrossRepoStrategy(group)
_generateCrossRepoRationale(group)
_assessComplexity(group)
_assessRisk(group, strategy)
_suggestCrossRepoLocation(group, strategy)
_calculateCrossRepoROI(group, complexity, risk)
_calculateInterProjectMetrics(repositoryScans, crossRepoDuplicates, suggestions)
async saveResults(results, filename = 'inter-project-scan.json')
</file>

<file path="pipeline-core/scan-orchestrator.d.ts">
export interface RepositoryInfo {
    path: string;
    name: string;
    gitRemote?: string;
    gitBranch?: string;
    gitCommit?: string;
    totalFiles: number;
    totalLines: number;
    languages: string[];
}
export interface PatternMatch {
    file_path: string;
    rule_id: string;
    matched_text: string;
    line_start: number;
    line_end: number;
    column_start?: number;
    column_end?: number;
    severity?: string;
    confidence?: number;
}
export interface ScanConfig {
    scan_config?: {
        includeTests?: boolean;
        maxDepth?: number;
        excludePaths?: string[];
        [key: string]: any;
    };
    pattern_config?: {
        rulesDirectory?: string;
        configPath?: string;
        [key: string]: any;
    };
    generateReports?: boolean;
    [key: string]: any;
}
export interface PythonPipelineInput {
    repository_info: RepositoryInfo;
    pattern_matches: PatternMatch[];
    scan_config: ScanConfig;
}
export interface CodeBlock {
    block_id: string;
    file_path: string;
    line_start: number;
    line_end: number;
    source_code: string;
    language: string;
    semantic_category?: string;
    tags: string[];
    complexity_metrics?: {
        cyclomatic: number;
        cognitive: number;
        halstead: Record<string, number>;
    };
}
export interface DuplicateGroup {
    group_id: string;
    block_ids: string[];
    similarity_score: number;
    group_type: 'exact' | 'structural' | 'semantic';
    total_lines: number;
    potential_reduction: number;
    impact_score?: number;
}
export interface ConsolidationSuggestion {
    suggestion_id: string;
    group_id: string;
    suggestion_type: string;
    priority: 'critical' | 'high' | 'medium' | 'low';
    estimated_effort: 'minimal' | 'low' | 'medium' | 'high';
    potential_reduction: number;
    implementation_notes?: string;
    migration_steps?: Array<{
        order: number;
        description: string;
        code_snippet?: string;
    }>;
}
export interface ScanMetrics {
    total_code_blocks: number;
    code_blocks_by_category?: Record<string, number>;
    code_blocks_by_language?: Record<string, number>;
    total_duplicate_groups: number;
    exact_duplicates: number;
    structural_duplicates: number;
    semantic_duplicates: number;
    total_duplicated_lines: number;
    potential_loc_reduction: number;
    duplication_percentage: number;
    total_suggestions: number;
    quick_wins?: number;
    high_priority_suggestions?: number;
}
export interface PythonPipelineOutput {
    code_blocks: CodeBlock[];
    duplicate_groups: DuplicateGroup[];
    suggestions: ConsolidationSuggestion[];
    metrics: ScanMetrics;
    repository_info: RepositoryInfo;
    scan_type?: 'single-project' | 'inter-project';
    error?: string;
    warnings?: string[];
}
export interface ScanResult extends PythonPipelineOutput {
    scan_metadata: {
        duration_seconds: number;
        scanned_at: string;
        repository_path: string;
    };
    report_paths?: ReportPaths;
}
export interface ReportPaths {
    html?: string;
    markdown?: string;
    summary?: string;
    json?: string;
}
export interface ReportOptions {
    outputDir?: string;
    baseName?: string;
    title?: string;
    html?: boolean;
    markdown?: boolean;
    summary?: boolean;
    json?: boolean;
    includeDetails?: boolean;
    maxDuplicates?: number;
    maxSuggestions?: number;
}
export interface ScanOrchestratorOptions {
    scanner?: Record<string, any>;
    detector?: Record<string, any>;
    pythonPath?: string;
    extractorScript?: string;
    reports?: ReportOptions;
    outputDir?: string;
    autoGenerateReports?: boolean;
    config?: Record<string, any>;
}
export interface CrossRepositoryDuplicate {
    group_id: string;
    pattern_id: string;
    content_hash: string;
    member_blocks: CodeBlock[];
    occurrence_count: number;
    repository_count: number;
    affected_repositories: string[];
    affected_files: string[];
    category: string;
    language: string;
    total_lines: number;
    similarity_score: number;
    similarity_method: string;
    impact_score: number;
}
export interface CrossRepositorySuggestion {
    suggestion_id: string;
    group_id: string;
    suggestion_type: string;
    priority: 'critical' | 'high' | 'medium' | 'low';
    estimated_effort: 'minimal' | 'low' | 'medium' | 'high';
    potential_reduction: number;
    affected_repositories: string[];
    implementation_notes?: string;
    migration_steps?: Array<{
        order: number;
        description: string;
        code_snippet?: string;
    }>;
}
export interface MultiRepositoryScanResult {
    repositories: Array<ScanResult | {
        error: string;
        repository_path: string;
    }>;
    total_scanned: number;
    successful: number;
    failed: number;
    cross_repository_duplicates?: CrossRepositoryDuplicate[];
    cross_repository_suggestions?: CrossRepositorySuggestion[];
    scan_type?: 'single-project' | 'inter-project';
    metrics?: ScanMetrics & {
        total_cross_repository_groups?: number;
        cross_repository_occurrences?: number;
        cross_repository_duplicated_lines?: number;
    };
}
export declare class ScanOrchestrator
‚ãÆ----
constructor(options?: ScanOrchestratorOptions);
scanRepository(repoPath: string, scanConfig?: ScanConfig): Promise<ScanResult>;
‚ãÆ----
generateReports(scanResult: ScanResult, options?: ReportOptions): Promise<ReportPaths>;
scanMultipleRepositories(repoPaths: string[], scanConfig?: ScanConfig): Promise<MultiRepositoryScanResult>;
‚ãÆ----
export declare class ScanError extends Error
‚ãÆ----
constructor(message: string, options?: {
        cause?: unknown;
    });
</file>

<file path="pipeline-core/scan-orchestrator.js">
export class ScanOrchestrator
‚ãÆ----
_detectPythonPath()
async scanRepository(repoPath, scanConfig =
_handleProcessClose(code, signal, stdout, stderr)
_handleSuccess(stdout, stderr)
_handleSignalTermination(signal, stdout, stderr)
/**
     * Handle error exit (non-zero code)
     */
_handleError(code, stderr)
_calculateTimeout(data)
async runPythonPipeline(data)
‚ãÆ----
// Send input data via stdin
‚ãÆ----
async generateReports(scanResult, options =
async scanMultipleRepositories(repoPaths, scanConfig =
‚ãÆ----
export class ScanError extends Error
</file>

<file path="pipeline-core/scan-orchestrator.ts">
import { RepositoryScanner } from './scanners/repository-scanner.js';
import { AstGrepPatternDetector } from './scanners/ast-grep-detector.js';
import { HTMLReportGenerator } from './reports/html-report-generator.js';
import { MarkdownReportGenerator } from './reports/markdown-report-generator.js';
import { InterProjectScanner } from './inter-project-scanner.js';
import { createComponentLogger } from '../utils/logger.js';
import { DependencyValidator } from '../utils/dependency-validator.js';
import { spawn, ChildProcess, execSync } from 'child_process';
‚ãÆ----
import { existsSync } from 'fs';
‚ãÆ----
import { TIMEOUTS } from '../core/constants.js';
‚ãÆ----
export interface RepositoryInfo {
  path: string;
  name: string;
  gitRemote?: string;
  gitBranch?: string;
  gitCommit?: string;
  totalFiles: number;
  totalLines: number;
  languages: string[];
}
export interface PatternMatch {
  file_path: string;
  rule_id: string;
  matched_text: string;
  line_start: number;
  line_end: number;
  column_start?: number;
  column_end?: number;
  severity?: string;
  confidence?: number;
}
export interface ScanConfig {
  scan_config?: {
    includeTests?: boolean;
    maxDepth?: number;
    excludePaths?: string[];
    [key: string]: any;
  };
  pattern_config?: {
    rulesDirectory?: string;
    configPath?: string;
    [key: string]: any;
  };
  generateReports?: boolean;
  [key: string]: any;
}
export interface PythonPipelineInput {
  repository_info: RepositoryInfo;
  pattern_matches: PatternMatch[];
  scan_config: ScanConfig;
}
export interface CodeBlock {
  block_id: string;
  file_path: string;
  line_start: number;
  line_end: number;
  source_code: string;
  language: string;
  semantic_category?: string;
  tags: string[];
  complexity_metrics?: {
    cyclomatic: number;
    cognitive: number;
    halstead: Record<string, number>;
  };
}
export interface DuplicateGroup {
  group_id: string;
  block_ids: string[];
  similarity_score: number;
  group_type: 'exact' | 'structural' | 'semantic';
  total_lines: number;
  potential_reduction: number;
  impact_score?: number;
}
export interface ConsolidationSuggestion {
  suggestion_id: string;
  group_id: string;
  suggestion_type: string;
  priority: 'critical' | 'high' | 'medium' | 'low';
  estimated_effort: 'minimal' | 'low' | 'medium' | 'high';
  potential_reduction: number;
  implementation_notes?: string;
  migration_steps?: Array<{
    order: number;
    description: string;
    code_snippet?: string;
  }>;
}
export interface ScanMetrics {
  total_code_blocks: number;
  code_blocks_by_category?: Record<string, number>;
  code_blocks_by_language?: Record<string, number>;
  total_duplicate_groups: number;
  exact_duplicates: number;
  structural_duplicates: number;
  semantic_duplicates: number;
  total_duplicated_lines: number;
  potential_loc_reduction: number;
  duplication_percentage: number;
  total_suggestions: number;
  quick_wins?: number;
  high_priority_suggestions?: number;
}
export interface PythonPipelineOutput {
  code_blocks: CodeBlock[];
  duplicate_groups: DuplicateGroup[];
  suggestions: ConsolidationSuggestion[];
  metrics: ScanMetrics;
  repository_info: RepositoryInfo;
  scan_type?: 'single-project' | 'inter-project';
  error?: string;
  warnings?: string[];
}
export interface ScanResult extends PythonPipelineOutput {
  scan_metadata: {
    duration_seconds: number;
    scanned_at: string;
    repository_path: string;
  };
  report_paths?: ReportPaths;
}
export interface ReportPaths {
  html?: string;
  markdown?: string;
  summary?: string;
  json?: string;
}
export interface ReportOptions {
  outputDir?: string;
  baseName?: string;
  title?: string;
  html?: boolean;
  markdown?: boolean;
  summary?: boolean;
  json?: boolean;
  includeDetails?: boolean;
  maxDuplicates?: number;
  maxSuggestions?: number;
}
export interface ScanOrchestratorOptions {
  scanner?: Record<string, any>;
  detector?: Record<string, any>;
  pythonPath?: string;
  extractorScript?: string;
  reports?: ReportOptions;
  outputDir?: string;
  autoGenerateReports?: boolean;
  config?: Record<string, any>;
}
export interface CrossRepositoryDuplicate {
  group_id: string;
  pattern_id: string;
  content_hash: string;
  member_blocks: CodeBlock[];
  occurrence_count: number;
  repository_count: number;
  affected_repositories: string[];
  affected_files: string[];
  category: string;
  language: string;
  total_lines: number;
  similarity_score: number;
  similarity_method: string;
  impact_score: number;
}
export interface CrossRepositorySuggestion {
  suggestion_id: string;
  group_id: string;
  suggestion_type: string;
  priority: 'critical' | 'high' | 'medium' | 'low';
  estimated_effort: 'minimal' | 'low' | 'medium' | 'high';
  potential_reduction: number;
  affected_repositories: string[];
  implementation_notes?: string;
  migration_steps?: Array<{
    order: number;
    description: string;
    code_snippet?: string;
  }>;
}
export interface MultiRepositoryScanResult {
  repositories: Array<ScanResult | { error: string; repository_path: string }>;
  total_scanned: number;
  successful: number;
  failed: number;
  cross_repository_duplicates?: CrossRepositoryDuplicate[];
  cross_repository_suggestions?: CrossRepositorySuggestion[];
  scan_type?: 'single-project' | 'inter-project';
  metrics?: ScanMetrics & {
    total_cross_repository_groups?: number;
    cross_repository_occurrences?: number;
    cross_repository_duplicated_lines?: number;
  };
}
interface RepositoryScanOutput {
  repository_info: RepositoryInfo;
  metadata?: {
    totalFiles: number;
    totalLines: number;
    languages: string[];
  };
  repomix_output?: any;
}
interface PatternDetectionOutput {
  matches: PatternMatch[];
  statistics: {
    total_matches: number;
    rules_applied: number;
    files_scanned: number;
    scan_duration_ms: number;
  };
}
export class ScanOrchestrator
‚ãÆ----
constructor(options: ScanOrchestratorOptions =
private _detectPythonPath(): void
async scanRepository(repoPath: string, scanConfig: ScanConfig =
private _handleProcessClose(
    code: number | null,
    signal: string | null,
    stdout: string,
    stderr: string
):
private _handleSuccess(
    stdout: string,
    stderr: string
):
private _handleSignalTermination(
    signal: string | null,
    stdout: string,
    stderr: string
):
/**
   * Handle error exit (non-zero code)
   */
private _handleError(
    code: number,
    stderr: string
):
private _calculateTimeout(data: PythonPipelineInput): number
private async runPythonPipeline(data: PythonPipelineInput): Promise<PythonPipelineOutput>
‚ãÆ----
// Send input data via stdin
‚ãÆ----
async generateReports(scanResult: ScanResult, options: ReportOptions =
async scanMultipleRepositories(
    repoPaths: string[],
    scanConfig: ScanConfig = {}
): Promise<MultiRepositoryScanResult>
‚ãÆ----
export class ScanError extends Error
‚ãÆ----
constructor(message: string, options?:
</file>

<file path="pipeline-runners/claude-health-pipeline.js">
class ClaudeHealthPipeline
‚ãÆ----
setupEventListeners()
displayResults(result)
async runHealthCheck(options =
async waitForCompletion()
scheduleHealthChecks(cronSchedule)
getStats()
‚ãÆ----
function printSummary(result)
‚ãÆ----
// Environment
‚ãÆ----
// Summary statistics
‚ãÆ----
function printRecommendations(recommendations)
function printDetailedChecks(checks)
‚ãÆ----
// Hooks
‚ãÆ----
// Plugins
‚ãÆ----
// Performance
‚ãÆ----
function getScoreColor(score)
function formatBytes(bytes)
</file>

<file path="pipeline-runners/collect_git_activity.py">
CODE_DIR = Path.home() / 'code'
EXCLUDE_PATTERNS = ['vim/bundle', 'node_modules', '.git', 'venv', '.venv']
DEFAULT_MAX_DEPTH = 2
LANGUAGE_EXTENSIONS = {
def find_git_repos(max_depth=DEFAULT_MAX_DEPTH)
‚ãÆ----
cmd = f"find {CODE_DIR} -maxdepth {max_depth} -name .git -type d"
result = subprocess.run(cmd.split(), capture_output=True, text=True)
repos = []
‚ãÆ----
repo_path = Path(line).parent
‚ãÆ----
def get_repo_stats(repo_path, since_date, until_date=None)
‚ãÆ----
git_cmd = f"git log --since={since_date} --all --oneline"
‚ãÆ----
result = subprocess.run(git_cmd.split(), capture_output=True, text=True)
commits = len(result.stdout.strip().split('\n')) if result.stdout.strip() else 0
# Get file changes for language analysis
git_cmd = f"git log --since={since_date} --all --name-only --pretty=format:"
‚ãÆ----
files = [f for f in result.stdout.strip().split('\n') if f]
# Get parent directory (for organization/grouping)
parent = repo_path.parent.name if repo_path.parent != CODE_DIR else None
‚ãÆ----
def analyze_languages(all_files)
‚ãÆ----
"""Analyze file changes by programming language"""
language_stats = defaultdict(int)
‚ãÆ----
file_ext = Path(file_path).suffix.lower()
file_name = Path(file_path).name
# Map to language
found = False
‚ãÆ----
found = True
‚ãÆ----
def find_project_websites(repositories)
‚ãÆ----
"""Scan for CNAME files to discover GitHub Pages websites"""
websites = {}
‚ãÆ----
repo_path = Path(repo['path'])
cname_file = repo_path / 'CNAME'
‚ãÆ----
website = cname_file.read_text().strip()
if website and '.' in website:  # Basic validation
‚ãÆ----
def categorize_repositories(repositories)
‚ãÆ----
"""Categorize repositories by project type"""
categories = {
‚ãÆ----
name = repo['name'].lower()
commits = repo['commits']
# Categorization logic (customize based on your projects)
‚ãÆ----
def create_pie_chart_svg(data, title, output_file, width=800, height=600)
‚ãÆ----
"""Create SVG pie chart without matplotlib dependency"""
‚ãÆ----
radius = min(width, height) / 3
colors = [
total = sum(data.values())
‚ãÆ----
svg_parts = [
start_angle = 0
legend_y = 50
‚ãÆ----
percent = (value / total) * 100
angle = (value / total) * 360
end_angle = start_angle + angle
# Convert to radians
start_rad = math.radians(start_angle - 90)
end_rad = math.radians(end_angle - 90)
# Calculate arc path
x1 = cx + radius * math.cos(start_rad)
y1 = cy + radius * math.sin(start_rad)
x2 = cx + radius * math.cos(end_rad)
y2 = cy + radius * math.sin(end_rad)
large_arc = 1 if angle > 180 else 0
# Create pie slice
path = f'M {cx},{cy} L {x1},{y1} A {radius},{radius} 0 {large_arc},1 {x2},{y2} Z'
color = colors[i % len(colors)]
‚ãÆ----
# Add legend
legend_x = width - 200
‚ãÆ----
start_angle = end_angle
‚ãÆ----
# Write to file
‚ãÆ----
def create_bar_chart_svg(data, title, output_file, width=800, height=600)
‚ãÆ----
"""Create SVG horizontal bar chart"""
max_value = max(data.values()) if data else 1
bar_height = 30
spacing = 10
chart_height = len(data) * (bar_height + spacing)
margin_left = 250
margin_top = 50
actual_height = chart_height + margin_top + 50
‚ãÆ----
y = margin_top + i * (bar_height + spacing)
bar_width = ((width - margin_left - 100) * value / max_value)
# Bar
‚ãÆ----
# Label
‚ãÆ----
# Value
‚ãÆ----
# ---------------------------------------------------------------------------
# Helper Functions for Main
‚ãÆ----
def _calculate_date_range(args) -> tuple[str, str | None] | None
‚ãÆ----
"""Calculate date range from command line arguments.
    Returns:
        (since_date, until_date) tuple, or None if invalid arguments
    """
# Handle shorthand flags
‚ãÆ----
end_date = datetime.now()
start_date = end_date - timedelta(days=args.days)
‚ãÆ----
def _resolve_output_dir(args) -> Path
‚ãÆ----
"""Resolve output directory from arguments or use default."""
‚ãÆ----
year = datetime.now().year
‚ãÆ----
def _collect_repository_stats(repos: list[Path], since_date: str, until_date: str | None) -> tuple[list, list]
‚ãÆ----
"""Collect statistics from all repositories.
    Returns:
        (repositories, all_files) tuple
    """
‚ãÆ----
repositories = []
all_files = []
‚ãÆ----
stats = get_repo_stats(repo, since_date, until_date)
‚ãÆ----
"""Compile all activity data into a single dictionary."""
‚ãÆ----
language_stats = analyze_languages(all_files)
‚ãÆ----
websites = find_project_websites(repositories)
‚ãÆ----
categories = categorize_repositories(repositories)
‚ãÆ----
def _print_summary(data: dict, output_dir: Path) -> None
‚ãÆ----
"""Print activity summary to console."""
repositories = data['repositories']
language_stats = data['languages']
websites = data['websites']
‚ãÆ----
sorted_langs = sorted(language_stats.items(), key=lambda x: x[1], reverse=True)
‚ãÆ----
def generate_jekyll_report(data: dict, output_file: Path) -> None
‚ãÆ----
date_range = data['date_range']
start_date = date_range['start']
end_date = date_range['end']
start = datetime.strptime(start_date, '%Y-%m-%d')
end = datetime.strptime(end_date, '%Y-%m-%d')
days = (end - start).days
‚ãÆ----
report_type = "Weekly"
‚ãÆ----
report_type = "Monthly"
‚ãÆ----
report_type = f"{days}-Day"
report_date = datetime.now().strftime('%Y-%m-%d')
frontmatter = f"""---
content = f"""
‚ãÆ----
parent_prefix = f"{repo['parent']}/" if repo['parent'] else ""
‚ãÆ----
sorted_langs = sorted(data['languages'].items(), key=lambda x: x[1], reverse=True)
‚ãÆ----
"""Generate all SVG visualizations"""
‚ãÆ----
# Monthly commits (if available in data)
‚ãÆ----
monthly_data = {month: count for month, count in data['monthly'].items()}
‚ãÆ----
# Top 10 repositories
top_10 = {}
‚ãÆ----
name = repo['name']
‚ãÆ----
name = f"{repo['parent']}/{name}"
‚ãÆ----
# Project categories
‚ãÆ----
category_data = {cat: len(repos) for cat, repos in data['categories'].items() if repos}
‚ãÆ----
# Language distribution
‚ãÆ----
language_data = data['languages']
‚ãÆ----
def main()
‚ãÆ----
parser = argparse.ArgumentParser(description='Generate comprehensive git activity report')
‚ãÆ----
args = parser.parse_args()
# Calculate date range
date_range = _calculate_date_range(args)
‚ãÆ----
repos = find_git_repos(args.max_depth)
‚ãÆ----
data = _compile_activity_data(repositories, all_files, since_date, until_date)
default_report_dir = Path.home() / 'code' / 'PersonalSite' / '_reports'
‚ãÆ----
report_file = default_report_dir / f'{report_date}-git-activity-report.md'
‚ãÆ----
output_dir = _resolve_output_dir(args)
</file>

<file path="pipeline-runners/duplicate-detection-pipeline.js">
async function main()
‚ãÆ----
const waitForCompletion = () =>
</file>

<file path="pipeline-runners/duplicate-detection-pipeline.ts">
import { SidequestServer } from '../core/server.js';
import { RepositoryConfigLoader } from '../pipeline-core/config/repository-config-loader.js';
import { InterProjectScanner } from '../pipeline-core/inter-project-scanner.js';
import { ScanOrchestrator } from '../pipeline-core/scan-orchestrator.js';
import { ReportCoordinator } from '../pipeline-core/reports/report-coordinator.js';
import { PRCreator } from '../pipeline-core/git/pr-creator.js';
import { createComponentLogger } from '../utils/logger.js';
import { config } from '../core/config.js';
import { isRetryable, getErrorInfo } from '../pipeline-core/errors/error-classifier.js';
‚ãÆ----
import type { Logger } from 'pino';
export enum JobStatus {
  QUEUED = 'queued',
  RUNNING = 'running',
  COMPLETED = 'completed',
  FAILED = 'failed'
}
export enum ScanType {
  INTER_PROJECT = 'inter-project',
  INTRA_PROJECT = 'intra-project'
}
export interface JobData {
  scanType: ScanType | string;
  repositories?: RepositoryConfig[];
  groupName?: string | null;
  type?: string;
}
export interface Job {
  id: string;
  status: JobStatus;
  data: JobData;
  createdAt: Date;
  startedAt: Date | null;
  completedAt: Date | null;
  error: Error | null;
  result: any;
}
export interface RepositoryConfig {
  name: string;
  path: string;
  enabled?: boolean;
  frequency?: string;
  lastScanned?: string | null;
  priority?: number;
  groups?: string[];
  scanHistory?: ScanHistoryEntry[];
}
export interface ScanHistoryEntry {
  date: string;
  status: 'success' | 'failure';
  duration: number;
  duplicatesFound: number;
}
export interface RetryInfo {
  attempts: number;
  lastAttempt: number;
  maxAttempts: number;
  delay: number;
}
export interface ScanMetrics {
  totalScans: number;
  successfulScans: number;
  failedScans: number;
  totalDuplicatesFound: number;
  totalSuggestionsGenerated: number;
  highImpactDuplicates: number;
  prsCreated: number;
  prCreationErrors: number;
}
export interface RetryMetrics {
  activeRetries: number;
  totalRetryAttempts: number;
  jobsBeingRetried: Array<{
    jobId: string;
    attempts: number;
    maxAttempts: number;
    lastAttempt: string;
  }>;
  retryDistribution: {
    attempt1: number;
    attempt2: number;
    attempt3Plus: number;
    nearingLimit: number;
  };
}
export interface ScanResult {
  scan_type: 'single-project' | 'inter-project' | 'intra-project';
  scan_metadata?: {
    duration_seconds: number;
    [key: string]: any;
  };
  metrics: {
    total_duplicate_groups?: number;
    total_cross_repository_groups?: number;
    total_suggestions?: number;
    [key: string]: any;
  };
  duplicate_groups?: DuplicateGroup[];
  cross_repository_duplicates?: DuplicateGroup[];
  suggestions?: Suggestion[];
  [key: string]: any;
}
export interface DuplicateGroup {
  id: string;
  impact_score: number;
  files: Array<{
    path: string;
    repository?: string;
  }>;
  [key: string]: any;
}
export interface Suggestion {
  id: string;
  type: string;
  impact: number;
  files: string[];
  [key: string]: any;
}
export interface PRCreationResult {
  prsCreated: number;
  prUrls: string[];
  errors: Array<{
    message: string;
    [key: string]: any;
  }>;
}
export interface DuplicateDetectionWorkerOptions {
  maxConcurrentScans?: number;
  logDir?: string;
  sentryDsn?: string;
  configPath?: string;
  baseBranch?: string;
  branchPrefix?: string;
  dryRun?: boolean;
  maxSuggestionsPerPR?: number;
  enablePRCreation?: boolean;
}
export interface InterProjectScanJobResult {
  scanType: 'inter-project';
  repositories: number;
  crossRepoDuplicates: number;
  suggestions: number;
  duration: number;
}
export interface IntraProjectScanJobResult {
  scanType: 'intra-project';
  repository: string;
  duplicates: number;
  suggestions: number;
  duration: number;
  prResults: {
    prsCreated: number;
    prUrls: string[];
    errors: number;
  } | null;
}
export type JobResult = InterProjectScanJobResult | IntraProjectScanJobResult;
‚ãÆ----
class DuplicateDetectionWorker extends SidequestServer
‚ãÆ----
constructor(options: DuplicateDetectionWorkerOptions =
async initialize(): Promise<void>
async runJobHandler(job: Job): Promise<JobResult>
private _getOriginalJobId(jobId: string): string
/**
   * Handle retry logic with exponential backoff
   */
private async _handleRetry(job: Job, error: Error): Promise<boolean>
‚ãÆ----
// Get original job ID to track retries correctly
‚ãÆ----
// Classify error to determine if retry is appropriate
‚ãÆ----
private async _runInterProjectScan(job: Job, repositoryConfigs: RepositoryConfig[]): Promise<InterProjectScanJobResult>
private async _runIntraProjectScan(job: Job, repositoryConfig: RepositoryConfig): Promise<IntraProjectScanJobResult>
private _updateMetrics(scanResult: ScanResult): void
private async _updateRepositoryConfigs(repositoryConfigs: RepositoryConfig[], scanResult: ScanResult): Promise<void>
private async _checkForHighImpactDuplicates(scanResult: ScanResult): Promise<void>
public scheduleScan(scanType: ScanType | string, repositories: RepositoryConfig[], groupName: string | null = null): Job
public async runNightlyScan(): Promise<void>
public getRetryMetrics(): RetryMetrics
public getScanMetrics(): ScanMetrics &
‚ãÆ----
async function main(): Promise<void>
</file>

<file path="pipeline-runners/git-activity-pipeline.js">
class GitActivityPipeline
‚ãÆ----
setupEventListeners()
async runReport(options =
async waitForCompletion()
scheduleWeeklyReports(cronSchedule = '0 20 * * 0')
scheduleMonthlyReports(cronSchedule = '0 0 1 * *')
</file>

<file path="pipeline-runners/gitignore-pipeline.js">
async function main()
</file>

<file path="pipeline-runners/plugin-management-pipeline.js">
class PluginManagementPipeline
‚ãÆ----
setupEventListeners()
displayRecommendations(result)
/**
   * Run a single audit
   * @param {Object} options - Audit options
   */
async runAudit(options =
async waitForCompletion()
scheduleAudits(cronSchedule = '0 9 * * 1')
</file>

<file path="pipeline-runners/repo-cleanup-pipeline.js">
async function main()
</file>

<file path="pipeline-runners/schema-enhancement-pipeline.js">
class SchemaEnhancementPipeline
‚ãÆ----
setupEventListeners()
async scanForReadmes(dir, baseDir = dir, results = [])
async scanDirectory(directory)
async createEnhancementJobs(readmeFiles)
async runEnhancement(directory = this.baseDir)
async waitForCompletion()
scheduleEnhancements(cronSchedule = '0 3 * * 0')
</file>

<file path="pipeline-runners/test-refactor-pipeline.ts">
import { TestRefactorWorker } from '../workers/test-refactor-worker.js';
import { DirectoryScanner } from '../utils/directory-scanner.js';
import { createComponentLogger } from '../utils/logger.js';
import { config } from '../core/config.js';
import cron from 'node-cron';
import path from 'path';
‚ãÆ----
interface DirectoryInfo {
  path: string;
  name: string;
}
interface PipelineMetrics {
  totalProjects: number;
  successfulRefactors: number;
  failedRefactors: number;
  filesGenerated: number;
  patternsDetected: number;
  stringsExtracted: number;
  recommendationsGenerated: number;
}
interface PipelineStats {
  total: number;
  queued: number;
  active: number;
  completed: number;
  failed: number;
}
interface PipelineResult {
  metrics: PipelineMetrics;
  stats: PipelineStats;
}
async function runPipeline(targetPath: string | null = null): Promise<PipelineResult>
async function hasTestDirectory(dirPath: string): Promise<boolean>
function waitForCompletion(worker: TestRefactorWorker): Promise<void>
‚ãÆ----
const checkCompletion = () =>
‚ãÆ----
async function main(): Promise<void>
</file>

<file path="pipeline-runners/universal-repo-cleanup.sh">
set -e
set -u
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'
TARGET_DIR="${1:-$(pwd)}"
TARGET_DIR="$(cd "$TARGET_DIR" 2>/dev/null && pwd)" || {
    echo -e "${RED}‚úó Error: Directory '$1' does not exist${NC}"
    exit 1
}
################################################################################
# Configuration - Customize what gets cleaned
################################################################################
# Python virtual environment directory names (common patterns)
VENV_PATTERNS=(
    "venv"
    ".venv"
    "env"
    ".env"
    "virtualenv"
    "*.venv"
    "personal_site"
)
BUILD_ARTIFACTS=(
    ".jekyll-cache"
    ".sass-cache"
    ".bundle"
    "node_modules/.cache"
    "dist"
    "build"
    ".next"
    ".nuxt"
    "out"
    ".output"
    "target"
    ".gradle"
)
TEMP_FILE_PATTERNS=(
    ".DS_Store"
    "*.pyc"
    "*.pyo"
    "__pycache__"
    "*.swp"
    "*.swo"
    "*~"
    ".*.swp"
    "Thumbs.db"
    "desktop.ini"
)
OUTPUT_FILE_PATTERNS=(
    "repomix-output.xml"
    "*.log"
    "npm-debug.log*"
    "yarn-debug.log*"
    "yarn-error.log*"
)
REDUNDANT_DIRS=(
    "drafts"
    "temp"
    "tmp"
    "backup"
    "backups"
    "old"
    "archive"
    "deprecated"
)
print_header() {
    echo -e "\n${BLUE}========================================${NC}"
    echo -e "${BLUE}$1${NC}"
    echo -e "${BLUE}========================================${NC}\n"
}
print_success() {
    echo -e "${GREEN}‚úì $1${NC}"
}
print_warning() {
    echo -e "${YELLOW}‚ö† $1${NC}"
}
print_error() {
    echo -e "${RED}‚úó $1${NC}"
}
print_info() {
    echo -e "${BLUE}‚Üí $1${NC}"
}
get_size() {
    local path="$1"
    if [ -e "$path" ]; then
        du -sh "$path" 2>/dev/null | cut -f1
    else
        echo "N/A"
    fi
}
count_files() {
    local path="$1"
    if [ -d "$path" ]; then
        find "$path" -type f 2>/dev/null | wc -l | tr -d ' '
    else
        echo "0"
    fi
}
scan_venvs() {
    print_info "Scanning for Python virtual environments..."
    local found=()
    for pattern in "${VENV_PATTERNS[@]}"; do
        while IFS= read -r -d '' dir; do
            # Skip if inside node_modules
            if [[ ! "$dir" =~ node_modules ]]; then
                found+=("$dir")
            fi
        done < <(find "$TARGET_DIR" -maxdepth 3 -type d -name "$pattern" -print0 2>/dev/null)
    done
    # Remove duplicates
    printf '%s\n' "${found[@]}" | sort -u
}
scan_temp_files() {
    print_info "Scanning for temporary/cache files..."
    local found=()
    for pattern in "${TEMP_FILE_PATTERNS[@]}"; do
        while IFS= read -r -d '' file; do
            found+=("$file")
        done < <(find "$TARGET_DIR" -name "$pattern" -print0 2>/dev/null)
    done
    printf '%s\n' "${found[@]}" | sort -u
}
scan_output_files() {
    print_info "Scanning for output/generated files..."
    local found=()
    for pattern in "${OUTPUT_FILE_PATTERNS[@]}"; do
        while IFS= read -r -d '' file; do
            # Keep root-level repomix-output.xml
            if [[ "$file" != "$TARGET_DIR/repomix-output.xml" ]]; then
                found+=("$file")
            fi
        done < <(find "$TARGET_DIR" -name "$pattern" -print0 2>/dev/null)
    done
    printf '%s\n' "${found[@]}" | sort -u
}
scan_build_artifacts() {
    print_info "Scanning for build artifacts..."
    local found=()
    for artifact in "${BUILD_ARTIFACTS[@]}"; do
        local path="$TARGET_DIR/$artifact"
        if [ -d "$path" ]; then
            found+=("$path")
        fi
    done
    printf '%s\n' "${found[@]}" | sort -u
}
scan_redundant_dirs() {
    print_info "Scanning for redundant directories..."
    local found=()
    for dir_name in "${REDUNDANT_DIRS[@]}"; do
        local path="$TARGET_DIR/$dir_name"
        if [ -d "$path" ]; then
            if [ -f "$TARGET_DIR/.gitignore" ] && grep -q "^${dir_name}/\?$" "$TARGET_DIR/.gitignore" 2>/dev/null; then
                found+=("$path (in .gitignore)")
            else
                found+=("$path")
            fi
        fi
    done
    printf '%s\n' "${found[@]}" | sort -u
}
show_preview() {
    print_header "Repository Cleanup Preview"
    echo "Target Directory: $TARGET_DIR"
    echo "Current Size: $(get_size "$TARGET_DIR")"
    echo ""
    # Scan all categories
    local venvs=($(scan_venvs))
    local temp_files=($(scan_temp_files))
    local output_files=($(scan_output_files))
    local build_artifacts=($(scan_build_artifacts))
    local redundant_dirs=($(scan_redundant_dirs))
    local total_items=0
    # Python venvs
    if [ ${#venvs[@]} -gt 0 ]; then
        echo -e "${YELLOW}Python Virtual Environments (${
        for venv in "${venvs[@]}"; do
            [ -n "$venv" ] && echo "  - $venv ($(get_size "$venv"))" && ((total_items++))
        done
        echo ""
    fi
    # Temporary files
    if [ ${#temp_files[@]} -gt 0 ]; then
        echo -e "${YELLOW}Temporary/Cache Files (${
        local count=0
        for file in "${temp_files[@]}"; do
            [ -n "$file" ] && ((count++)) && ((total_items++))
        done
        echo "  - $count files (.DS_Store, __pycache__, .swp, etc.)"
        echo ""
    fi
    # Output files
    if [ ${#output_files[@]} -gt 0 ]; then
        echo -e "${YELLOW}Output/Generated Files (${
        for file in "${output_files[@]}"; do
            [ -n "$file" ] && echo "  - $file" && ((total_items++))
        done
        echo ""
    fi
    # Build artifacts
    if [ ${#build_artifacts[@]} -gt 0 ]; then
        echo -e "${YELLOW}Build Artifacts (${
        for artifact in "${build_artifacts[@]}"; do
            [ -n "$artifact" ] && echo "  - $artifact ($(get_size "$artifact"))" && ((total_items++))
        done
        echo ""
    fi
    # Redundant directories
    if [ ${#redundant_dirs[@]} -gt 0 ]; then
        echo -e "${YELLOW}Redundant Directories (${
        for dir in "${redundant_dirs[@]}"; do
            [ -n "$dir" ] && echo "  - $dir" && ((total_items++))
        done
        echo ""
    fi
    if [ $total_items -eq 0 ]; then
        print_success "No items found to clean - repository is already clean!"
        exit 0
    fi
    echo -e "${YELLOW}Total items to remove: $total_items${NC}"
    echo ""
    # Store for cleanup functions
    export FOUND_VENVS="${venvs[*]}"
    export FOUND_TEMP_FILES="${temp_files[*]}"
    export FOUND_OUTPUT_FILES="${output_files[*]}"
    export FOUND_BUILD_ARTIFACTS="${build_artifacts[*]}"
    export FOUND_REDUNDANT_DIRS="${redundant_dirs[*]}"
}
confirm_cleanup() {
    read -p "Do you want to proceed with cleanup? (yes/no): " response
    if [[ ! "$response" =~ ^[Yy][Ee][Ss]$ ]]; then
        print_warning "Cleanup cancelled by user"
        exit 0
    fi
    echo ""
}
################################################################################
# Cleanup Functions
################################################################################
cleanup_venvs() {
    print_header "Step 1: Removing Python Virtual Environments"
    local venvs=($FOUND_VENVS)
    local removed=0
    if [ ${
        print_info "No virtual environments to remove"
        return
    fi
    for venv in "${venvs[@]}"; do
        if [ -n "$venv" ] && [ -d "$venv" ]; then
            local size=$(get_size "$venv")
            print_info "Removing $(basename "$venv") ($size)..."
            rm -rf "$venv"
            print_success "Removed $venv"
            ((removed++))
        fi
    done
    print_success "Removed $removed virtual environment(s)"
}
cleanup_temp_files() {
    print_header "Step 2: Removing Temporary/Cache Files"
    local temp_files=($FOUND_TEMP_FILES)
    local removed=0
    if [ ${
        print_info "No temporary files to remove"
        return
    fi
    for file in "${temp_files[@]}"; do
        if [ -n "$file" ] && [ -e "$file" ]; then
            rm -rf "$file" 2>/dev/null && ((removed++))
        fi
    done
    print_success "Removed $removed temporary file(s)"
}
cleanup_output_files() {
    print_header "Step 3: Removing Output/Generated Files"
    local output_files=($FOUND_OUTPUT_FILES)
    local removed=0
    if [ ${
        print_info "No output files to remove"
        return
    fi
    for file in "${output_files[@]}"; do
        if [ -n "$file" ] && [ -f "$file" ]; then
            print_info "Removing $(basename "$file")..."
            rm -f "$file"
            print_success "Removed $file"
            ((removed++))
        fi
    done
    print_success "Removed $removed output file(s)"
}
cleanup_build_artifacts() {
    print_header "Step 4: Removing Build Artifacts"
    local build_artifacts=($FOUND_BUILD_ARTIFACTS)
    local removed=0
    if [ ${
        print_info "No build artifacts to remove"
        return
    fi
    for artifact in "${build_artifacts[@]}"; do
        if [ -n "$artifact" ] && [ -d "$artifact" ]; then
            local size=$(get_size "$artifact")
            print_info "Removing $(basename "$artifact") ($size)..."
            rm -rf "$artifact"
            print_success "Removed $artifact"
            ((removed++))
        fi
    done
    print_success "Removed $removed build artifact(s)"
}
cleanup_redundant_dirs() {
    print_header "Step 5: Removing Redundant Directories"
    local redundant_dirs=($FOUND_REDUNDANT_DIRS)
    local removed=0
    if [ ${
        print_info "No redundant directories to remove"
        return
    fi
    for dir in "${redundant_dirs[@]}"; do
        dir="${dir% (in .gitignore)}"
        if [ -n "$dir" ] && [ -d "$dir" ]; then
            local size=$(get_size "$dir")
            print_info "Removing $(basename "$dir") ($size)..."
            rm -rf "$dir"
            print_success "Removed $dir"
            ((removed++))
        fi
    done
    print_success "Removed $removed redundant director(ies)"
}
print_summary() {
    print_header "Cleanup Summary"
    local final_size=$(get_size "$TARGET_DIR")
    echo "Cleanup completed successfully!"
    echo ""
    echo "Target Directory: $TARGET_DIR"
    echo "Final Size: $final_size"
    echo ""
    echo "Cleaned up:"
    echo "  ‚úì Python virtual environments"
    echo "  ‚úì Temporary/cache files (.DS_Store, __pycache__, etc.)"
    echo "  ‚úì Output/generated files (logs, repomix files, etc.)"
    echo "  ‚úì Build artifacts (.jekyll-cache, dist/, etc.)"
    echo "  ‚úì Redundant directories (drafts/, temp/, backup/, etc.)"
    echo ""
    print_success "Repository cleanup completed!"
}
recommend_gitignore() {
    print_header "Recommendations"
    if [ ! -f "$TARGET_DIR/.gitignore" ]; then
        print_warning "No .gitignore found - consider creating one"
        return
    fi
    echo "Consider adding these patterns to .gitignore if not already present:"
    echo ""
    echo "
    echo "  venv/"
    echo "  .venv/"
    echo "  *.pyc"
    echo "  __pycache__/"
    echo ""
    echo "
    echo "  .DS_Store"
    echo "  Thumbs.db"
    echo ""
    echo "
    echo "  dist/"
    echo "  build/"
    echo "  *.log"
    echo ""
    echo "
    echo "  temp/"
    echo "  tmp/"
    echo "  *.swp"
    echo ""
}
################################################################################
# Main Execution
################################################################################
main() {
    print_header "Universal Repository Cleanup Script"
    echo "Target: $TARGET_DIR"
    echo ""
    # Show preview and get confirmation
    show_preview
    confirm_cleanup
    # Execute cleanup tasks
    cleanup_venvs
    cleanup_temp_files
    cleanup_output_files
    cleanup_build_artifacts
    cleanup_redundant_dirs
    # Print summary
    print_summary
    # Print recommendations
    recommend_gitignore
    print_success "All cleanup tasks completed successfully!"
}
# Run main function
main "$@"
</file>

<file path="types/duplicate-detection-types.js">
export function isIntraProjectScanResult(result)
export function isInterProjectScanResult(result)
export function isIntraProjectJobResult(result)
export function isInterProjectJobResult(result)
</file>

<file path="types/duplicate-detection-types.ts">
import { z } from 'zod';
‚ãÆ----
export type ScanType = z.infer<typeof ScanTypeSchema>;
‚ãÆ----
export type ScanFrequency = z.infer<typeof ScanFrequencySchema>;
‚ãÆ----
export type PriorityLevel = z.infer<typeof PriorityLevelSchema>;
export interface RepositoryConfig {
  name: string;
  path: string;
  enabled: boolean;
  priority: PriorityLevel;
  scanFrequency: ScanFrequency;
  lastScanned?: string;
  scanHistory?: ScanHistoryEntry[];
}
export interface ScanHistoryEntry {
  timestamp: string;
  status: 'success' | 'failure';
  duration: number;
  duplicatesFound: number;
}
export interface RepositoryGroupConfig {
  name: string;
  description?: string;
  enabled: boolean;
  scanType: 'inter-project';
  repositories: string[];
}
‚ãÆ----
export type DuplicateDetectionJobData = z.infer<typeof DuplicateDetectionJobDataSchema>;
export interface ScanMetadata {
  scan_started: string;
  scan_completed: string;
  duration_seconds: number;
  total_files_scanned: number;
  scan_type: ScanType;
}
export interface ScanMetrics {
  total_duplicate_groups?: number;
  total_cross_repository_groups?: number;
  total_suggestions: number;
  high_impact_duplicates: number;
}
export interface DuplicateGroup {
  group_id: string;
  instances: Array<{
    file_path: string;
    start_line: number;
    end_line: number;
    repository?: string;
  }>;
  impact_score: number;
  similarity_score: number;
  suggestion?: string;
}
export interface IntraProjectScanResult {
  scan_type: 'intra-project';
  scan_metadata: ScanMetadata;
  metrics: ScanMetrics;
  duplicate_groups: DuplicateGroup[];
  suggestions?: string[];
}
export interface InterProjectScanResult {
  scan_type: 'inter-project';
  scan_metadata: ScanMetadata;
  metrics: ScanMetrics;
  cross_repository_duplicates: DuplicateGroup[];
  suggestions?: string[];
}
export type ScanResult = IntraProjectScanResult | InterProjectScanResult;
export interface PRCreationResult {
  prsCreated: number;
  prUrls: string[];
  errors: Array<{
    repository?: string;
    error: string;
  }>;
}
export interface IntraProjectJobResult {
  scanType: 'intra-project';
  repository: string;
  duplicates: number;
  suggestions: number;
  duration: number;
  prResults?: {
    prsCreated: number;
    prUrls: string[];
    errors: number;
  } | null;
}
export interface InterProjectJobResult {
  scanType: 'inter-project';
  repositories: number;
  crossRepoDuplicates: number;
  suggestions: number;
  duration: number;
}
export type JobResult = IntraProjectJobResult | InterProjectJobResult;
export interface WorkerScanMetrics {
  totalScans: number;
  successfulScans: number;
  failedScans: number;
  totalDuplicatesFound: number;
  totalSuggestionsGenerated: number;
  highImpactDuplicates: number;
  prsCreated: number;
  prCreationErrors: number;
}
export interface RetryInfo {
  attempts: number;
  lastAttempt: number;
  maxAttempts: number;
  delay: number;
}
export interface RetryMetrics {
  activeRetries: number;
  totalRetryAttempts: number;
  jobsBeingRetried: Array<{
    jobId: string;
    attempts: number;
    maxAttempts: number;
    lastAttempt: string;
  }>;
  retryDistribution: {
    attempt1: number;
    attempt2: number;
    attempt3Plus: number;
    nearingLimit: number;
  };
}
export interface CompleteScanMetrics extends WorkerScanMetrics {
  queueStats: {
    queued: number;
    active: number;
    completed: number;
    failed: number;
  };
  retryMetrics: RetryMetrics;
}
export interface DuplicateDetectionWorkerOptions {
  maxConcurrentScans?: number;
  maxConcurrent?: number;
  logDir?: string;
  configPath?: string;
  enablePRCreation?: boolean;
  baseBranch?: string;
  branchPrefix?: string;
  dryRun?: boolean;
  maxSuggestionsPerPR?: number;
  sentryDsn?: string;
}
export interface InitializedEventPayload {
  totalRepositories: number;
  enabledRepositories: number;
  groups: number;
  byPriority: Record<PriorityLevel, number>;
  byFrequency: Record<ScanFrequency, number>;
}
export type PipelineStatusEventPayload =
  | { status: 'initialized'; stats: InitializedEventPayload }
  | { status: 'scanning'; jobId: string; scanType: ScanType; repositories: number }
  | { status: 'failed'; jobId: string; error: string }
  | { status: 'completed'; jobId: string; result: JobResult };
export interface ScanCompletedEventPayload {
  jobId: string;
  scanType: ScanType;
  metrics: {
    duplicates: number;
    suggestions: number;
    duration: number;
  };
}
export interface PRCreatedEventPayload {
  repository: string;
  prsCreated: number;
  prUrls: string[];
}
export interface PRFailedEventPayload {
  repository: string;
  error: string;
}
export interface HighImpactDetectedEventPayload {
  count: number;
  threshold: number;
  topImpactScore: number;
}
export interface RetryScheduledEventPayload {
  jobId: string;
  attempt: number;
  delay: number;
}
export interface RetryWarningEventPayload {
  jobId: string;
  attempts: number;
  maxAttempts: number;
}
export interface RetryCircuitBreakerEventPayload {
  jobId: string;
  attempts: number;
  maxAbsolute: number;
}
export interface MetricsUpdatedEventPayload {
  metrics: CompleteScanMetrics;
}
export interface ScanConfig {
  enabled: boolean;
  retryAttempts: number;
  retryDelay: number;
  maxConcurrentScans: number;
}
export interface NotificationSettings {
  enabled: boolean;
  onHighImpactDuplicates: boolean;
  highImpactThreshold: number;
}
export function isIntraProjectScanResult(result: ScanResult): result is IntraProjectScanResult
export function isInterProjectScanResult(result: ScanResult): result is InterProjectScanResult
export function isIntraProjectJobResult(result: JobResult): result is IntraProjectJobResult
export function isInterProjectJobResult(result: JobResult): result is InterProjectJobResult
</file>

<file path="utils/dependency-validator.js">
export class DependencyValidationError extends Error
export class DependencyValidator
‚ãÆ----
static async validateAll()
static async validateRepomix()
static async validateAstGrep()
static async validatePython()
</file>

<file path="utils/directory-scanner.js">
export class DirectoryScanner
‚ãÆ----
async scanDirectories()
/**
   * Check if a directory is a git repository root
   */
async isGitRepository(dirPath)
async scanRecursive(currentPath, relativePath, depth, results)
async shouldProcess(dirPath)
async getDirectoryInfo(dirPath)
generateScanStats(directories)
async saveScanReport(directories, stats)
generateDirectoryTree(directories)
async saveDirectoryTree(directories)
async generateAndSaveScanResults(directories)
</file>

<file path="utils/doppler-resilience.example.js">
class DopplerConfigManager extends DopplerResilience
‚ãÆ----
async fetchFromDoppler()
async getSecret(key, defaultValue = null)
‚ãÆ----
async function exampleUsage()
export function createDopplerHealthMiddleware(dopplerConfig)
export class IntegratedDopplerMonitor
‚ãÆ----
async getComprehensiveHealth()
generateRecommendations(health, cacheAge)
async triggerRecovery()
‚ãÆ----
export class DopplerHealthService
‚ãÆ----
start()
stop()
async getHealth()
</file>

<file path="utils/doppler-resilience.js">
export class DopplerResilience
‚ãÆ----
async getSecrets()
async fetchFromDoppler()
async getFallbackSecrets()
isCacheStale()
handleSuccess()
handleFailure(error)
getHealth()
reset()
getState()
</file>

<file path="utils/gitignore-repomix-updater.js">
export class GitignoreRepomixUpdater
‚ãÆ----
async findGitRepositories()
async scanForGitRepos(currentPath, depth, results)
async gitignoreContainsEntry(gitignorePath)
async addToGitignore(repoPath)
async processRepositories()
generateSummary(results)
async saveResults(results, outputPath)
‚ãÆ----
export async function main()
</file>

<file path="utils/logger.d.ts">
import type { Logger } from 'pino';
‚ãÆ----
export function createChildLogger(bindings: Record<string, any>): Logger;
export function createComponentLogger(component: string): Logger;
</file>

<file path="utils/logger.js">
export function createChildLogger(bindings)
export function createComponentLogger(component)
</file>

<file path="utils/pipeline-names.js">
export function getPipelineName(id)
export function getAllKnownPipelineIds()
export function isKnownPipeline(id)
</file>

<file path="utils/plugin-manager.js">
class PluginManagerWorker extends SidequestServer
‚ãÆ----
async runJobHandler(job)
async runAuditScript(detailed = false)
async analyzeResults(auditResults)
async loadPluginMetadata()
generateRecommendations(analysis)
addJob(options =
</file>

<file path="utils/refactor-test-suite.ts">
import { glob } from 'glob';
interface RefactorConfig {
  projectPath: string;
  testsDir: string;
  utilsDir: string;
  e2eDir: string;
  framework: 'vitest' | 'jest' | 'playwright';
}
interface AnalysisResult {
  testFiles: string[];
  patterns: {
    renderWaitFor: number;
    linkValidation: number;
    semanticChecks: number;
    formInteractions: number;
    hardcodedStrings: string[];
    duplicateAssertions: string[];
  };
  recommendations: string[];
}
‚ãÆ----
function detectFramework(projectPath: string): 'vitest' | 'jest' | 'playwright'
async function findTestFiles(config: RefactorConfig): Promise<string[]>
async function analyzeTestFiles(config: RefactorConfig, testFiles: string[]): Promise<AnalysisResult>
‚ãÆ----
// Count semantic checks
‚ãÆ----
// Count form interactions
‚ãÆ----
// Extract hardcoded strings (quoted strings in expect statements)
‚ãÆ----
// Extract assertion patterns
‚ãÆ----
// Find duplicated strings (appearing 3+ times)
‚ãÆ----
// Find duplicated assertions (appearing 3+ times)
‚ãÆ----
// Generate recommendations
‚ãÆ----
/**
 * Generate the assertions utility file
 */
function generateAssertionsFile(framework: string): string
/**
 * Generate the semantic validators utility file
 */
function generateSemanticValidatorsFile(framework: string): string
/**
 * Generate the form helpers utility file
 */
function generateFormHelpersFile(): string
/**
 * Generate the test constants file template
 */
function generateConstantsTemplate(hardcodedStrings: string[]): string
/**
 * Generate the render helpers to add to test-utils
 */
function generateRenderHelpers(): string
/**
 * Generate E2E navigation fixtures for Playwright
 */
function generateE2EFixtures(): string
/**
 * Generate index file for clean imports
 */
function generateIndexFile(): string
/**
 * Main execution
 */
async function main()
</file>

<file path="utils/report-generator.js">
export async function generateReport(options)
function generateHTMLReport(data)
function generateJSONReport(data)
function generateHTMLHeader(title, jobId, status, statusClass, timestamp, duration)
function generateHTMLParameters(parameters)
/**
 * Generate HTML metadata section
 *
 * @private
 */
function generateHTMLMetadata(metadata)
/**
 * Generate HTML results section
 *
 * @private
 */
function generateHTMLResults(result, jobType)
‚ãÆ----
// Try to detect metrics/stats in result
‚ãÆ----
/**
 * Generate metrics cards
 *
 * @private
 */
function generateMetricsSection(metrics)
/**
 * Generate details section
 *
 * @private
 */
function generateDetailsSection(details, jobType)
/**
 * Generate HTML footer
 *
 * @private
 */
function generateHTMLFooter(timestamp)
/**
 * Extract metrics from result object
 *
 * @private
 */
function extractMetrics(result)
‚ãÆ----
// Look for common metric field names
‚ãÆ----
function extractDetails(result)
function getJobTypeTitle(jobType)
function formatValue(value)
function escapeHtml(text)
function getHTMLStyles()
</file>

<file path="utils/schema-mcp-tools.js">
export class SchemaMCPTools
‚ãÆ----
async getSchemaType(readmePath, content, context)
async generateSchema(readmePath, content, context, schemaType)
extractDescription(content)
‚ãÆ----
// Skip title
‚ãÆ----
// Skip empty lines and code blocks
‚ãÆ----
if (description) break; // Stop at first empty line after description
‚ãÆ----
// Found description
‚ãÆ----
// Limit description length
‚ãÆ----
async validateSchema(schema)
async analyzeSchemaImpact(originalContent, enhancedContent, schema)
getRating(score)
createJSONLDScript(schema)
injectSchema(content, schema)
</file>

<file path="utils/time-helpers.js">
export function toISOString(val)
export function calculateDurationSeconds(startTime, endTime)
export function formatDuration(seconds)
</file>

<file path="workers/claude-health-worker.js">
class ClaudeHealthWorker extends SidequestServer
‚ãÆ----
async runJobHandler(job)
async runAllChecks(options =
async checkEnvironment()
async checkDirectories()
async checkConfiguration()
async checkHooks()
async checkComponents(detailed = false)
async checkPlugins()
async checkPerformance()
analyzeResults(checks)
calculateHealthScore(issues, warnings, successes)
generateRecommendations(analysis)
generateSummary(analysis, recommendations)
getStatusMessage(healthScore, critical, warnings)
addJob(options =
</file>

<file path="workers/duplicate-detection-worker.js">
export class DuplicateDetectionWorker extends SidequestServer
‚ãÆ----
async initialize()
async runJobHandler(job)
_getOriginalJobId(jobId)
/**
   * Handle retry logic with exponential backoff
   */
async _handleRetry(job, error)
‚ãÆ----
// Get original job ID to track retries correctly
‚ãÆ----
// Classify error to determine if retry is appropriate
‚ãÆ----
async _runInterProjectScan(job, repositoryConfigs)
async _runIntraProjectScan(job, repositoryConfig)
_updateMetrics(scanResult)
async _updateRepositoryConfigs(repositoryConfigs, scanResult)
async _checkForHighImpactDuplicates(scanResult)
scheduleScan(scanType, repositories, groupName = null)
async runNightlyScan()
getRetryMetrics()
getScanMetrics()
</file>

<file path="workers/git-activity-worker.js">
export class GitActivityWorker extends SidequestServer
‚ãÆ----
async runJobHandler(job)
‚ãÆ----
createWeeklyReportJob()
createMonthlyReportJob()
createCustomReportJob(sinceDate, untilDate)
createReportJob(options =
</file>

<file path="workers/gitignore-worker.js">
export class GitignoreWorker extends SidequestServer
‚ãÆ----
async runJobHandler(job)
‚ãÆ----
createUpdateAllJob(options =
createUpdateRepositoriesJob(repositories, options =
createDryRunJob(options =
</file>

<file path="workers/repo-cleanup-worker.js">
export class RepoCleanupWorker extends SidequestServer
‚ãÆ----
async runJobHandler(job)
‚ãÆ----
createCleanupJob(targetDir, options =
createDryRunJob(targetDir, options =
</file>

<file path="workers/repomix-worker.js">
export class RepomixWorker extends SidequestServer
‚ãÆ----
async runJobHandler(job)
‚ãÆ----
createRepomixJob(sourceDir, relativePath)
</file>

<file path="workers/schema-enhancement-worker.js">
async runJobHandler(job)
async saveImpactReport(relativePath, schema, impact)
async saveEnhancedCopy(relativePath, enhancedContent)
async findGitRoot(startPath)
async createEnhancementJob(readme, context)
async _generateCommitMessage(job)
</file>

<file path="workers/test-refactor-worker.ts">
import { SidequestServer, Job as BaseJob } from '../core/server.js';
import { createComponentLogger } from '../utils/logger.js';
import { glob } from 'glob';
import path from 'path';
import fs from 'fs/promises';
‚ãÆ----
interface TestRefactorWorkerOptions {
  maxConcurrent?: number;
  logDir?: string;
  gitWorkflowEnabled?: boolean;
  gitBranchPrefix?: string;
  testsDir?: string;
  utilsDir?: string;
  e2eDir?: string;
  framework?: 'vitest' | 'jest' | 'playwright';
  dryRun?: boolean;
  sentryDsn?: string;
}
interface JobData {
  repositoryPath: string;
  repository: string;
  testsDir: string;
  utilsDir: string;
  e2eDir: string;
  framework: string;
  dryRun: boolean;
}
interface RefactorJob {
  id: string;
  data: JobData;
  result?: JobResult;
}
interface AnalysisPatterns {
  renderWaitFor: number;
  linkValidation: number;
  semanticChecks: number;
  formInteractions: number;
  hardcodedStrings: string[];
  duplicateAssertions: string[];
}
interface Analysis {
  testFiles: string[];
  patterns: AnalysisPatterns;
  recommendations: string[];
}
interface JobResult {
  status: string;
  reason?: string;
  analysis?: Analysis;
  testFiles?: number;
  generatedFiles?: string[];
  recommendations?: string[];
}
interface Metrics {
  totalProjects: number;
  successfulRefactors: number;
  failedRefactors: number;
  filesGenerated: number;
  patternsDetected: number;
  stringsExtracted: number;
  recommendationsGenerated: number;
}
interface Stats {
  total: number;
  queued: number;
  active: number;
  completed: number;
  failed: number;
}
export class TestRefactorWorker extends SidequestServer
‚ãÆ----
constructor(options: TestRefactorWorkerOptions =
queueProject(projectPath: string, options: Partial<JobData> =
detectFramework(projectPath: string): string
async runJobHandler(job: RefactorJob): Promise<JobResult>
async findTestFiles(projectPath: string, testsDir: string): Promise<string[]>
async analyzeTestFiles(projectPath: string, testFiles: string[]): Promise<Analysis>
‚ãÆ----
// Extract hardcoded strings
‚ãÆ----
// Extract assertion patterns
‚ãÆ----
// Find duplicated strings (3+ occurrences)
‚ãÆ----
// Find duplicated assertions (3+ occurrences)
‚ãÆ----
// Generate recommendations
‚ãÆ----
/**
   * Generate utility files based on analysis
   */
async generateUtilityFiles(
    projectPath: string,
    utilsDir: string,
    e2eDir: string,
    framework: string,
    analysis: Analysis
): Promise<string[]>
‚ãÆ----
// Ensure utils directory exists
‚ãÆ----
// Generate assertions.ts
‚ãÆ----
// Generate semantic-validators.ts
‚ãÆ----
// Generate form-helpers.ts
‚ãÆ----
// Generate test-constants.ts
‚ãÆ----
// Generate index.ts
‚ãÆ----
// Generate E2E fixtures if e2e directory exists
‚ãÆ----
/**
   * Check if file exists
   */
async fileExists(filePath: string): Promise<boolean>
/**
   * Generate assertions.ts content
   */
generateAssertionsContent(framework: string): string
/**
   * Generate semantic-validators.ts content
   */
generateSemanticValidatorsContent(framework: string): string
/**
   * Generate form-helpers.ts content
   */
generateFormHelpersContent(): string
/**
   * Generate test-constants.ts content
   */
generateConstantsContent(hardcodedStrings: string[]): string
generateIndexContent(generatedFiles: string[]): string
generateE2EFixturesContent(): string
async _generateCommitMessage(job: RefactorJob): Promise<
getMetrics(): Metrics
getStats(): Stats
</file>

<file path=".env.example">
# AlephAuto Configuration
# Copy this file to .env and customize for your environment

# ============================================
# Base Directories
# ============================================

# Base directory containing all code repositories to process
# Default: ~/code
CODE_BASE_DIR=/Users/username/code

# Output directory for repomix condensed code
# Default: ./output/condense (relative to project root)
OUTPUT_BASE_DIR=./output/condense

# Directory for application logs
# Default: ./logs
LOG_DIR=./logs

# Directory for directory scan reports
# Default: ./output/directory-scan-reports
SCAN_REPORTS_DIR=./output/directory-scan-reports

# ============================================
# Job Processing
# ============================================

# Maximum number of concurrent jobs (1-50)
# Default: 5
MAX_CONCURRENT=5

# ============================================
# Monitoring & Error Tracking
# ============================================

# Sentry DSN for error tracking (optional)
# Get from: https://sentry.io/settings/projects/
SENTRY_DSN=https://your-sentry-dsn@sentry.io/project-id

# Node environment (development, production)
# Default: production
NODE_ENV=production

# ============================================
# Scheduling
# ============================================

# Cron schedule for repomix pipeline (default: 2 AM daily)
# Format: minute hour day month weekday
# Examples:
#   "0 2 * * *"  - 2 AM daily
#   "0 */6 * * *" - Every 6 hours
#   "0 0 * * 0"  - Midnight every Sunday
CRON_SCHEDULE="0 2 * * *"

# Cron schedule for documentation enhancement pipeline (default: 3 AM daily)
DOC_CRON_SCHEDULE="0 3 * * *"

# Run jobs immediately on startup (true/false)
# Default: false
RUN_ON_STARTUP=false

# ============================================
# Repomix Settings
# ============================================

# Timeout for repomix command in milliseconds (default: 10 minutes)
REPOMIX_TIMEOUT=600000

# Maximum buffer size for repomix output in bytes (default: 50MB)
REPOMIX_MAX_BUFFER=52428800

# ============================================
# Schema.org Integration
# ============================================

# URL for Schema.org MCP server (optional)
SCHEMA_MCP_URL=http://localhost:3001

# Force schema enhancement even if schema already exists
# Default: false
FORCE_ENHANCEMENT=false

# ============================================
# Logging
# ============================================

# Log level: debug, info, warn, error
# Default: info
LOG_LEVEL=info

# ============================================
# Health Check
# ============================================

# Port for health check HTTP server
# Default: 3000
HEALTH_CHECK_PORT=3000
</file>

<file path=".gitignore">
# Dependencies
node_modules/

# Logs
logs/
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# Output artifacts
output/
directory-scan-reports/
gitignore-update-report-*.json
repomix-output.xml
doc-enhancement/repomix-output.xml

# Environment
.env
.env.local
.env.*.local

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# OS
.DS_Store
Thumbs.db

# Sentry
.sentryclirc

# Testing
coverage/
.vitest/

# Build
dist/
build/
*.tsbuildinfo

# Temporary files
*.tmp
*.temp
</file>

<file path="git-report-config.json">
{
  "description": "Configuration for automated git activity reports",
  "version": "1.0.0",

  "scanning": {
    "code_directory": "~/code",
    "max_depth": 2,
    "exclude_patterns": [
      "vim/bundle",
      "node_modules",
      ".git",
      "venv",
      ".venv",
      "vendor",
      "target",
      "build",
      "dist"
    ]
  },

  "reports": {
    "weekly": {
      "enabled": true,
      "schedule": "Sunday 20:00",
      "days_back": 7,
      "auto_commit": false,
      "output_format": "markdown"
    },
    "monthly": {
      "enabled": true,
      "schedule": "1st of month 08:00",
      "days_back": 30,
      "auto_commit": false,
      "output_format": "markdown"
    },
    "quarterly": {
      "enabled": false,
      "schedule": "First Monday of quarter",
      "days_back": 90,
      "auto_commit": false,
      "output_format": "markdown"
    }
  },

  "output": {
    "personalsite_dir": "~/code/PersonalSite",
    "work_collection": "_work",
    "visualization_dir": "assets/images/git-activity-{year}",
    "json_cache_dir": "/tmp",
    "log_dir": "~/code/jobs/sidequest/logs"
  },

  "project_categories": {
    "Data & Analytics": {
      "keywords": ["scraper", "analytics", "bot", "data"],
      "description": "Data collection, scraping, analytics pipelines"
    },
    "Personal Sites": {
      "keywords": ["personalsite", "github.io", "portfolio", "blog"],
      "description": "Personal websites and blogs"
    },
    "Infrastructure": {
      "keywords": ["integrity", "studio", "visualizer", "tool", "dotfiles"],
      "description": "DevOps, tooling, development infrastructure"
    },
    "MCP Servers": {
      "keywords": ["mcp", "server", "context", "protocol"],
      "description": "Model Context Protocol server implementations"
    },
    "Client Work": {
      "keywords": ["client", "leora", "jobs"],
      "description": "Client projects and career tracking"
    },
    "Business Apps": {
      "keywords": ["inventory", "financial", "business"],
      "description": "Business applications and SaaS products"
    },
    "Legacy": {
      "keywords": ["old", "archive", "deprecated"],
      "description": "Archived or minimal-maintenance projects",
      "min_commits": 0,
      "max_commits": 5
    }
  },

  "visualizations": {
    "enabled": true,
    "formats": ["svg"],
    "charts": {
      "monthly_commits": {
        "enabled": true,
        "type": "pie",
        "filename": "monthly-commits.svg",
        "width": 800,
        "height": 600
      },
      "project_categories": {
        "enabled": true,
        "type": "pie",
        "filename": "project-categories.svg",
        "width": 800,
        "height": 600
      },
      "top_repositories": {
        "enabled": true,
        "type": "bar",
        "filename": "top-10-repos.svg",
        "width": 800,
        "height": 500,
        "count": 10
      },
      "language_distribution": {
        "enabled": true,
        "type": "pie",
        "filename": "language-distribution.svg",
        "width": 900,
        "height": 600
      }
    },
    "color_schemes": {
      "default": ["#0066cc", "#4da6ff", "#99ccff", "#00994d", "#ffcc00", "#ff6600", "#cc0000", "#9966cc", "#66cc99", "#ff6699"]
    }
  },

  "language_mapping": {
    "Python": [".py", ".pyw"],
    "JavaScript": [".js", ".mjs", ".cjs"],
    "TypeScript": [".ts", ".tsx"],
    "Ruby": [".rb", ".rake", ".gemspec"],
    "HTML": [".html", ".htm"],
    "CSS/SCSS": [".css", ".scss", ".sass", ".less"],
    "Markdown": [".md", ".markdown"],
    "JSON": [".json"],
    "YAML": [".yml", ".yaml"],
    "Shell": [".sh", ".bash", ".zsh"],
    "SQL": [".sql"],
    "Go": [".go"],
    "Rust": [".rs"],
    "C/C++": [".c", ".cpp", ".cc", ".h", ".hpp"],
    "Java": [".java"],
    "PHP": [".php"],
    "Lock Files": [".lock", "package-lock.json", "Gemfile.lock", "yarn.lock", "pnpm-lock.yaml"],
    "SVG": [".svg"],
    "Images": [".png", ".jpg", ".jpeg", ".gif", ".webp", ".ico", ".bmp"],
    "Data Files": [".csv", ".xml", ".tsv", ".parquet"],
    "Text Files": [".txt", ".log", ".ini", ".conf"]
  },

  "notifications": {
    "enabled": false,
    "methods": {
      "email": {
        "enabled": false,
        "to": "your-email@example.com",
        "subject": "Weekly Git Activity Report"
      },
      "slack": {
        "enabled": false,
        "webhook_url": "",
        "channel": "#dev-updates"
      },
      "discord": {
        "enabled": false,
        "webhook_url": ""
      }
    }
  },

  "git": {
    "auto_commit": false,
    "commit_message_template": "Add {period} git activity report: {start_date} to {end_date}\n\nü§ñ Generated with [Claude Code](https://claude.com/claude-code)\n\nCo-Authored-By: Claude <noreply@anthropic.com>",
    "auto_push": false
  },

  "cron": {
    "weekly": "0 20 * * 0",
    "monthly": "0 8 1 * *",
    "quarterly": "0 8 1 1,4,7,10 *"
  },

  "notes": [
    "This configuration file controls the automated git activity report generation.",
    "Edit the values above to customize the behavior for your workflow.",
    "The git activity reporter is now integrated with AlephAuto job queue framework.",
    "Usage: npm run git:weekly (weekly), npm run git:monthly (monthly), npm run git:schedule (scheduled mode)",
    "For production: pm2 start git-activity-pipeline.js --name git-activity",
    "Environment variable: GIT_CRON_SCHEDULE='0 20 * * 0' for custom schedule"
  ]
}
</file>

<file path="plugin-management-audit.sh">
set -euo pipefail
if [[ "${BASH_VERSINFO[0]}" -lt 4 ]]; then
    echo "Error: This script requires bash 4 or higher (current: $BASH_VERSION)"
    echo "macOS users can install with: brew install bash"
    echo "Then run with: /usr/local/bin/bash $0 $@"
    exit 1
fi
CLAUDE_CONFIG="${HOME}/.claude/settings.json"
OUTPUT_FORMAT="human"
DETAILED=false
while [[ $
    case $1 in
        --json)
            OUTPUT_FORMAT="json"
            shift
            ;;
        --detailed)
            DETAILED=true
            shift
            ;;
        *)
            echo "Unknown option: $1"
            echo "Usage: $0 [--json] [--detailed]"
            exit 1
            ;;
    esac
done
if [[ ! -f "$CLAUDE_CONFIG" ]]; then
    echo "Error: Claude config not found at $CLAUDE_CONFIG"
    exit 1
fi
ENABLED_PLUGINS=$(jq -r '.enabledPlugins | keys[]' "$CLAUDE_CONFIG" 2>/dev/null || echo "")
PLUGIN_COUNT=$(echo "$ENABLED_PLUGINS" | grep -v '^$' | wc -l | tr -d ' ')
declare -A categories
while IFS= read -r plugin; do
    [[ -z "$plugin" ]] && continue
    if [[ "$plugin" =~ documentation|document ]]; then
        categories["documentation"]+="$plugin "
    elif [[ "$plugin" =~ git|github ]]; then
        categories["git"]+="$plugin "
    elif [[ "$plugin" =~ test|testing ]]; then
        categories["testing"]+="$plugin "
    elif [[ "$plugin" =~ deploy|deployment ]]; then
        categories["deployment"]+="$plugin "
    elif [[ "$plugin" =~ lint|format ]]; then
        categories["linting"]+="$plugin "
    elif [[ "$plugin" =~ docker|container ]]; then
        categories["containers"]+="$plugin "
    elif [[ "$plugin" =~ api|rest|graphql ]]; then
        categories["api"]+="$plugin "
    elif [[ "$plugin" =~ db|database|sql ]]; then
        categories["database"]+="$plugin "
    fi
done <<< "$ENABLED_PLUGINS"
if [[ "$OUTPUT_FORMAT" == "json" ]]; then
    echo "{"
    echo "  \"total_enabled\": $PLUGIN_COUNT,"
    echo "  \"enabled_plugins\": ["
    first=true
    while IFS= read -r plugin; do
        [[ -z "$plugin" ]] && continue
        [[ "$first" == false ]] && echo ","
        echo -n "    \"$plugin\""
        first=false
    done <<< "$ENABLED_PLUGINS"
    echo ""
    echo "  ],"
    echo "  \"potential_duplicates\": {"
    first=true
    for category in "${!categories[@]}"; do
        count=$(echo "${categories[$category]}" | wc -w | tr -d ' ')
        if [[ $count -gt 1 ]]; then
            [[ "$first" == false ]] && echo ","
            echo -n "    \"$category\": ["
            inner_first=true
            for plugin in ${categories[$category]}; do
                [[ "$inner_first" == false ]] && echo -n ", "
                echo -n "\"$plugin\""
                inner_first=false
            done
            echo -n "]"
            first=false
        fi
    done
    echo ""
    echo "  }"
    echo "}"
else
    # Human-readable output
    echo "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó"
    echo "‚ïë          Claude Code Plugin Management Audit                   ‚ïë"
    echo "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù"
    echo ""
    echo "Total Enabled Plugins: $PLUGIN_COUNT"
    echo ""
    if [[ "$DETAILED" == true ]]; then
        echo "Enabled Plugins:"
        echo "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ"
        while IFS= read -r plugin; do
            [[ -z "$plugin" ]] && continue
            echo "  ‚Ä¢ $plugin"
        done <<< "$ENABLED_PLUGINS"
        echo ""
    fi
    # Show potential duplicates
    has_duplicates=false
    for category in "${!categories[@]}"; do
        count=$(echo "${categories[$category]}" | wc -w | tr -d ' ')
        if [[ $count -gt 1 ]]; then
            has_duplicates=true
        fi
    done
    if [[ "$has_duplicates" == true ]]; then
        echo "‚ö†Ô∏è  Potential Duplicate Categories:"
        echo "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ"
        for category in "${!categories[@]}"; do
            count=$(echo "${categories[$category]}" | wc -w | tr -d ' ')
            if [[ $count -gt 1 ]]; then
                echo ""
                echo "  Category: $category ($count plugins)"
                for plugin in ${categories[$category]}; do
                    echo "    ‚Ä¢ $plugin"
                done
            fi
        done
        echo ""
        echo "üí° Consider reviewing these categories for consolidation."
    else
        echo "‚úÖ No obvious duplicate categories detected."
    fi
    echo ""
    echo "Recommendations:"
    echo "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ"
    if [[ $PLUGIN_COUNT -gt 30 ]]; then
        echo "  ‚Ä¢ High plugin count ($PLUGIN_COUNT). Consider disabling unused plugins."
    fi
    if [[ "$has_duplicates" == true ]]; then
        echo "  ‚Ä¢ Review duplicate categories above."
        echo "  ‚Ä¢ Keep only the plugins you actively use in each category."
    fi
    echo "  ‚Ä¢ Run: npm run status to see plugin usage statistics"
    echo "  ‚Ä¢ Backup before changes: npm run backup"
    echo ""
fi
# Exit with status
if [[ "$has_duplicates" == true ]] || [[ $PLUGIN_COUNT -gt 30 ]]; then
    exit 1
else
    exit 0
fi
</file>

<file path="repomix.config.json">
{
  "$schema": "https://repomix.com/schemas/latest/schema.json",
  "input": {
    "maxFileSize": 52428800
  },
  "output": {
    "filePath": "repomix-output.xml",
    "style": "xml",
    "parsableStyle": false,
    "directoryStructure": true,
    "files": true,
    "removeComments": true,
    "removeEmptyLines": true,
    "compress": true,
    "topFilesLength": 5,
    "showLineNumbers": false,
    "truncateBase64": false,
    "copyToClipboard": false,
    "includeFullDirectoryStructure": false,
    "tokenCountTree": false,
    "includeEmptyDirectories": false,
    "git": {
      "sortByChanges": true,
      "sortByChangesMaxCommits": 100,
      "includeDiffs": false,
      "includeLogs": false,
      "includeLogsCount": 50
    }
  },
  "include": [],
  "ignore": {
    "useGitignore": true,
    "useDefaultPatterns": true,
    "customPatterns": [
      "logs/**",
      "node_modules/**",
      "*.log",
      "directory-scan-reports/**",
      "document-enhancement-impact-measurement/**",
      "**/go/pkg/mod/**",
      "**/pyenv/**",
      "**/python/pyenv/**",
      "**/vim/bundle/**",
      "**/vim/autoload/**",
      "repomix-output.xml",
      "repomix-output.txt"
    ]
  },
  "security": {
    "enableSecurityCheck": true
  },
  "tokenCount": {
    "encoding": "o200k_base"
  }
}
</file>

</files>
