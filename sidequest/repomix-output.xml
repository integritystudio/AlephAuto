This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: logs/**, node_modules/**, *.log, directory-scan-reports/**, document-enhancement-impact-measurement/**, **/go/pkg/mod/**, **/pyenv/**, **/python/pyenv/**, **/vim/bundle/**, **/vim/autoload/**, repomix-output.xml, repomix-output.txt, **/README.md, **/README.MD, **/*.md
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.claude/
  settings.local.json
doc-enhancement/
  readme-scanner.js
  schema-enhancement-worker.js
  schema-mcp-tools.js
.env.example
.gitignore
claude-health-worker.js
collect_git_activity.py
config.js
data-discovery-report-pipeline.js
directory-scanner.js
git-activity-worker.js
git-report-config.json
gitignore-repomix-updater.js
gitignore-worker.js
index.js
logger.js
plugin-management-audit.sh
plugin-manager.js
repomix-worker.js
repomix.config.json
server.js
universal-repo-cleanup.sh
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "Bash(node gitignore-repomix-updater.js:*)"
    ],
    "deny": [],
    "ask": []
  }
}
</file>

<file path="doc-enhancement/readme-scanner.js">
// @ts-nocheck
import fs from 'fs/promises';
import path from 'path';
import { createComponentLogger } from '../logger.js';

const logger = createComponentLogger('READMEScanner');

/**
 * READMEScanner - Recursively scans for README.md files
 */
export class READMEScanner {
  constructor(options = {}) {
    this.baseDir = options.baseDir || process.cwd();
    this.excludeDirs = new Set(options.excludeDirs || [
      'node_modules',
      '.git',
      'dist',
      'build',
      'coverage',
      '.next',
      '.nuxt',
      'vendor',
      '__pycache__',
      '.venv',
      'venv',
      '_site',
      '.cache',
      'target',
      '.idea',
      '.vscode',
      'jobs',
    ]);
    this.maxDepth = options.maxDepth || 10;
    this.readmePatterns = options.readmePatterns || [
      'README.md',
      'readme.md',
      'Readme.md',
      'README_ENHANCED.md',
    ];
  }

  /**
   * Scan all README files recursively
   */
  async scanREADMEs() {
    const readmes = [];
    await this.scanRecursive(this.baseDir, '', 0, readmes);
    return readmes;
  }

  /**
   * Recursively scan a directory for README files
   */
  async scanRecursive(currentPath, relativePath, depth, results) {
    // Check depth limit
    if (depth > this.maxDepth) {
      return;
    }

    try {
      const entries = await fs.readdir(currentPath, { withFileTypes: true });

      for (const entry of entries) {
        if (entry.isDirectory()) {
          // Skip excluded directories
          if (this.excludeDirs.has(entry.name) || entry.name.startsWith('.')) {
            continue;
          }

          const fullPath = path.join(currentPath, entry.name);
          const newRelativePath = relativePath
            ? path.join(relativePath, entry.name)
            : entry.name;

          // Recurse into subdirectories
          await this.scanRecursive(fullPath, newRelativePath, depth + 1, results);
        } else if (entry.isFile() && this.isREADMEFile(entry.name)) {
          const fullPath = path.join(currentPath, entry.name);
          const fileRelativePath = relativePath
            ? path.join(relativePath, entry.name)
            : entry.name;

          results.push({
            fullPath,
            relativePath: fileRelativePath,
            fileName: entry.name,
            dirPath: currentPath,
            depth,
          });
        }
      }
    } catch (error) {
      // Log but don't fail on permission errors
      logger.warn({
        path: currentPath,
        error: error.message
      }, 'Cannot access directory');
    }
  }

  /**
   * Check if filename matches README patterns
   */
  isREADMEFile(filename) {
    return this.readmePatterns.includes(filename);
  }

  /**
   * Check if README already has schema markup
   */
  async hasSchemaMarkup(readmePath) {
    try {
      const content = await fs.readFile(readmePath, 'utf-8');
      return content.includes('<script type="application/ld+json">');
    } catch (error) {
      return false;
    }
  }

  /**
   * Read README content
   */
  async readREADME(readmePath) {
    try {
      return await fs.readFile(readmePath, 'utf-8');
    } catch (error) {
      throw new Error(`Failed to read README at ${readmePath}: ${error.message}`);
    }
  }

  /**
   * Gather context about a directory
   */
  async gatherContext(dirPath) {
    const context = {
      languages: new Set(),
      gitRemote: null,
      hasPackageJson: false,
      hasPyproject: false,
      projectType: 'unknown',
    };

    try {
      const entries = await fs.readdir(dirPath);

      for (const entry of entries) {
        const fullPath = path.join(dirPath, entry);
        const stat = await fs.stat(fullPath);

        if (stat.isFile()) {
          // Detect languages by file extensions
          const ext = path.extname(entry);

          if (ext === '.py') context.languages.add('Python');
          if (['.ts', '.tsx'].includes(ext)) context.languages.add('TypeScript');
          if (['.js', '.jsx'].includes(ext)) context.languages.add('JavaScript');
          if (['.java'].includes(ext)) context.languages.add('Java');
          if (['.go'].includes(ext)) context.languages.add('Go');
          if (['.rs'].includes(ext)) context.languages.add('Rust');
          if (['.rb'].includes(ext)) context.languages.add('Ruby');

          // Check for project markers
          if (entry === 'package.json') {
            context.hasPackageJson = true;
            context.projectType = 'nodejs';
          }
          if (entry === 'pyproject.toml' || entry === 'setup.py') {
            context.hasPyproject = true;
            context.projectType = 'python';
          }
        }
      }

      // Try to get git remote
      context.gitRemote = await this.getGitRemote(dirPath);
    } catch (error) {
      logger.warn({
        dirPath,
        error: error.message
      }, 'Error gathering context');
    }

    return context;
  }

  /**
   * Get git remote URL for a directory
   */
  async getGitRemote(dirPath) {
    try {
      const { exec } = await import('child_process');
      const { promisify } = await import('util');
      const execAsync = promisify(exec);

      const { stdout } = await execAsync('git remote get-url origin', {
        cwd: dirPath,
        timeout: 5000,
      });

      return stdout.trim();
    } catch (error) {
      return null;
    }
  }

  /**
   * Get statistics about scanned READMEs
   */
  async getStats(readmes) {
    const stats = {
      total: readmes.length,
      withSchema: 0,
      withoutSchema: 0,
      byDepth: {},
    };

    for (const readme of readmes) {
      const hasSchema = await this.hasSchemaMarkup(readme.fullPath);
      if (hasSchema) {
        stats.withSchema++;
      } else {
        stats.withoutSchema++;
      }

      const depth = readme.depth;
      stats.byDepth[depth] = (stats.byDepth[depth] || 0) + 1;
    }

    return stats;
  }
}
</file>

<file path="doc-enhancement/schema-enhancement-worker.js">
import { SidequestServer } from '../server.js';
import { SchemaMCPTools } from './schema-mcp-tools.js';
import { READMEScanner } from './readme-scanner.js';
import { createComponentLogger } from '../logger.js';
import fs from 'fs/promises';
import path from 'path';

const logger = createComponentLogger('SchemaEnhancementWorker');

/**
 * SchemaEnhancementWorker - Enhances README files with Schema.org markup
 */
export class SchemaEnhancementWorker extends SidequestServer {
  constructor(options = {}) {
    super(options);
    this.outputBaseDir = options.outputBaseDir || './document-enhancement-impact-measurement';
    this.mcpTools = new SchemaMCPTools(options);
    this.scanner = new READMEScanner(options);
    this.dryRun = options.dryRun || false;
    this.stats = {
      enhanced: 0,
      skipped: 0,
      failed: 0,
    };
  }

  /**
   * Run enhancement for a specific README file
   */
  async runJobHandler(job) {
    const { readmePath, relativePath, context } = job.data;

    logger.info({ jobId: job.id, readmePath }, 'Enhancing README');

    try {
      // Read README content
      const originalContent = await fs.readFile(readmePath, 'utf-8');

      // Check if already has schema
      if (originalContent.includes('<script type="application/ld+json">')) {
        logger.info({ jobId: job.id }, 'Skipped - already has schema markup');
        this.stats.skipped++;
        return {
          status: 'skipped',
          reason: 'Already has schema markup',
          readmePath,
          relativePath,
        };
      }

      // Get appropriate schema type
      const schemaType = await this.mcpTools.getSchemaType(
        readmePath,
        originalContent,
        context
      );

      logger.info({ jobId: job.id, schemaType }, 'Schema type detected');

      // Generate schema markup
      const schema = await this.mcpTools.generateSchema(
        readmePath,
        originalContent,
        context,
        schemaType
      );

      // Validate schema
      const validation = await this.mcpTools.validateSchema(schema);
      if (!validation.valid) {
        throw new Error(`Schema validation failed: ${validation.errors.join(', ')}`);
      }

      if (validation.warnings.length > 0) {
        logger.warn({
          jobId: job.id,
          warnings: validation.warnings
        }, 'Schema validation warnings');
      }

      // Inject schema into content
      const enhancedContent = this.mcpTools.injectSchema(originalContent, schema);

      // Analyze impact
      const impact = await this.mcpTools.analyzeSchemaImpact(
        originalContent,
        enhancedContent,
        schema
      );

      logger.info({
        jobId: job.id,
        impactScore: impact.impactScore,
        rating: impact.rating
      }, 'Impact analysis complete');

      // Save enhanced README
      if (!this.dryRun) {
        await fs.writeFile(readmePath, enhancedContent, 'utf-8');
        logger.info({ jobId: job.id }, 'Enhanced README saved');
      } else {
        logger.info({ jobId: job.id }, 'Dry run - no changes made');
      }

      // Save impact report
      await this.saveImpactReport(relativePath, schema, impact);

      // Save enhanced copy to output directory
      await this.saveEnhancedCopy(relativePath, enhancedContent);

      this.stats.enhanced++;

      return {
        status: 'enhanced',
        readmePath,
        relativePath,
        schemaType,
        schema,
        impact,
        validation,
        timestamp: new Date().toISOString(),
      };

    } catch (error) {
      this.stats.failed++;
      throw error;
    }
  }

  /**
   * Save impact report to output directory
   */
  async saveImpactReport(relativePath, schema, impact) {
    const reportDir = path.join(
      this.outputBaseDir,
      'impact-reports',
      path.dirname(relativePath)
    );

    await fs.mkdir(reportDir, { recursive: true });

    const reportPath = path.join(
      reportDir,
      `${path.basename(relativePath, '.md')}-impact.json`
    );

    const report = {
      relativePath,
      schema,
      impact,
      timestamp: new Date().toISOString(),
    };

    await fs.writeFile(reportPath, JSON.stringify(report, null, 2));
  }

  /**
   * Save enhanced copy to output directory
   */
  async saveEnhancedCopy(relativePath, enhancedContent) {
    const outputDir = path.join(
      this.outputBaseDir,
      'enhanced-readmes',
      path.dirname(relativePath)
    );

    await fs.mkdir(outputDir, { recursive: true });

    const outputPath = path.join(outputDir, path.basename(relativePath));

    await fs.writeFile(outputPath, enhancedContent, 'utf-8');
  }

  /**
   * Create an enhancement job for a README
   */
  createEnhancementJob(readme, context) {
    const jobId = `schema-${readme.relativePath.replace(/\//g, '-')}-${Date.now()}`;

    return this.createJob(jobId, {
      readmePath: readme.fullPath,
      relativePath: readme.relativePath,
      context,
      type: 'schema-enhancement',
    });
  }

  /**
   * Get enhancement statistics
   */
  getEnhancementStats() {
    return {
      ...this.stats,
      total: this.stats.enhanced + this.stats.skipped + this.stats.failed,
      successRate: this.stats.enhanced > 0
        ? ((this.stats.enhanced / (this.stats.enhanced + this.stats.failed)) * 100).toFixed(2)
        : 0,
    };
  }

  /**
   * Generate enhancement summary report
   */
  async generateSummaryReport() {
    const stats = this.getEnhancementStats();
    const jobStats = this.getStats();

    const summary = {
      timestamp: new Date().toISOString(),
      enhancement: stats,
      jobs: jobStats,
      outputDirectory: this.outputBaseDir,
    };

    const summaryPath = path.join(
      this.outputBaseDir,
      `enhancement-summary-${Date.now()}.json`
    );

    await fs.mkdir(this.outputBaseDir, { recursive: true });
    await fs.writeFile(summaryPath, JSON.stringify(summary, null, 2));

    return summary;
  }
}
</file>

<file path="doc-enhancement/schema-mcp-tools.js">
import { config } from '../config.js';

/**
 * Schema.org MCP Tools Integration
 * Provides wrapper methods for Schema.org MCP server tools
 */

export class SchemaMCPTools {
  constructor(options = {}) {
    this.mcpServerUrl = options.mcpServerUrl || config.schemaMcpUrl;
    this.useRealMCP = options.useRealMCP || false;
  }

  /**
   * Get appropriate schema type for content
   * Maps to MCP tool: get_schema_type
   */
  async getSchemaType(readmePath, content, context) {
    // In real implementation, this would call the MCP server
    // For now, we'll use heuristics like the Python version

    const pathLower = readmePath.toLowerCase();
    const contentLower = content.toLowerCase();

    // Test documentation
    if (pathLower.includes('test') || contentLower.includes('testing guide')) {
      return 'HowTo';
    }

    // API documentation
    if (pathLower.includes('api') ||
        contentLower.includes('api reference') ||
        contentLower.includes('endpoints')) {
      return 'APIReference';
    }

    // Software application
    if (context.hasPackageJson || context.hasPyproject) {
      return 'SoftwareApplication';
    }

    // Tutorial/Guide
    if (contentLower.includes('tutorial') ||
        contentLower.includes('getting started') ||
        contentLower.includes('guide')) {
      return 'HowTo';
    }

    // Code repository/technical documentation
    if (context.gitRemote) {
      return 'SoftwareSourceCode';
    }

    // Default to TechArticle
    return 'TechArticle';
  }

  /**
   * Generate JSON-LD schema markup
   * Maps to MCP tool: generate_example
   */
  async generateSchema(readmePath, content, context, schemaType) {
    const schema = {
      '@context': 'https://schema.org',
      '@type': schemaType,
    };

    // Extract title from first heading
    const titleMatch = content.match(/^#\s+(.+)$/m);
    if (titleMatch) {
      schema.name = titleMatch[1].trim();
    } else {
      // Fallback to directory name
      const dirName = readmePath.split('/').slice(-2, -1)[0];
      schema.name = dirName || 'Documentation';
    }

    // Extract description from content
    const description = this.extractDescription(content);
    if (description) {
      schema.description = description;
    }

    // Add common properties based on schema type
    if (schemaType === 'SoftwareApplication' || schemaType === 'SoftwareSourceCode') {
      if (context.gitRemote) {
        schema.codeRepository = context.gitRemote;
      }

      if (context.languages && context.languages.length > 0) {
        schema.programmingLanguage = context.languages.map(lang => ({
          '@type': 'ComputerLanguage',
          name: lang,
        }));
      }

      if (schemaType === 'SoftwareApplication') {
        schema.applicationCategory = 'DeveloperApplication';
        schema.operatingSystem = 'Cross-platform';
      }
    }

    if (schemaType === 'TechArticle' || schemaType === 'HowTo') {
      schema.dateModified = new Date().toISOString();
      schema.inLanguage = 'en-US';
    }

    if (schemaType === 'APIReference') {
      schema.additionalType = 'https://schema.org/TechArticle';
      if (context.gitRemote) {
        schema.url = context.gitRemote;
      }
    }

    return schema;
  }

  /**
   * Extract description from README content
   */
  extractDescription(content) {
    // Try to get the first paragraph after the title
    const lines = content.split('\n');
    let foundTitle = false;
    let description = '';

    for (const line of lines) {
      const trimmed = line.trim();

      // Skip title
      if (trimmed.startsWith('#')) {
        foundTitle = true;
        continue;
      }

      // Skip empty lines and code blocks
      if (!trimmed || trimmed.startsWith('```') || trimmed.startsWith('<')) {
        if (description) break; // Stop at first empty line after description
        continue;
      }

      // Found description
      if (foundTitle && trimmed.length > 10) {
        description = trimmed;
        break;
      }
    }

    // Limit description length
    if (description.length > 200) {
      description = description.substring(0, 197) + '...';
    }

    return description || 'Technical documentation and guides';
  }

  /**
   * Validate schema markup
   * Maps to Schema.org validation tools
   */
  async validateSchema(schema) {
    // Basic validation
    const errors = [];
    const warnings = [];

    // Check required fields
    if (!schema['@context']) {
      errors.push('Missing @context');
    }
    if (!schema['@type']) {
      errors.push('Missing @type');
    }
    if (!schema.name) {
      warnings.push('Missing name property');
    }
    if (!schema.description) {
      warnings.push('Missing description property');
    }

    // Validate JSON-LD format
    try {
      JSON.stringify(schema);
    } catch (e) {
      errors.push(`Invalid JSON: ${e.message}`);
    }

    return {
      valid: errors.length === 0,
      errors,
      warnings,
    };
  }

  /**
   * Analyze schema impact on SEO/performance
   * Maps to MCP tool: analyze_schema_impact
   */
  async analyzeSchemaImpact(originalContent, enhancedContent, schema) {
    const impact = {
      timestamp: new Date().toISOString(),
      schemaType: schema['@type'],
      metrics: {
        contentSize: {
          original: originalContent.length,
          enhanced: enhancedContent.length,
          increase: enhancedContent.length - originalContent.length,
        },
        schemaProperties: Object.keys(schema).length,
        structuredDataAdded: true,
      },
      seoImprovements: [],
      richResultsEligibility: [],
    };

    // Analyze SEO improvements
    if (schema.name) {
      impact.seoImprovements.push('Added structured name/title');
    }
    if (schema.description) {
      impact.seoImprovements.push('Added structured description');
    }
    if (schema.codeRepository) {
      impact.seoImprovements.push('Linked to code repository');
    }
    if (schema.programmingLanguage) {
      impact.seoImprovements.push('Specified programming languages');
    }

    // Check Rich Results eligibility
    const schemaType = schema['@type'];
    if (schemaType === 'HowTo') {
      impact.richResultsEligibility.push('How-to rich results');
    }
    if (schemaType === 'SoftwareApplication') {
      impact.richResultsEligibility.push('Software app rich results');
    }
    if (schemaType === 'TechArticle') {
      impact.richResultsEligibility.push('Article rich results');
    }

    // Calculate impact score (0-100)
    let score = 0;
    score += impact.seoImprovements.length * 15;
    score += impact.richResultsEligibility.length * 20;
    score += schema.description ? 20 : 0;
    score += schema.codeRepository ? 15 : 0;

    impact.impactScore = Math.min(100, score);
    impact.rating = this.getRating(impact.impactScore);

    return impact;
  }

  /**
   * Get rating based on impact score
   */
  getRating(score) {
    if (score >= 80) return 'Excellent';
    if (score >= 60) return 'Good';
    if (score >= 40) return 'Fair';
    return 'Needs Improvement';
  }

  /**
   * Create JSON-LD script tag
   */
  createJSONLDScript(schema) {
    const jsonStr = JSON.stringify(schema, null, 2);
    return `<script type="application/ld+json">\n${jsonStr}\n</script>`;
  }

  /**
   * Inject schema into README content
   */
  injectSchema(content, schema) {
    const jsonldScript = this.createJSONLDScript(schema);
    const lines = content.split('\n');

    // Find first heading
    let insertIndex = 0;
    for (let i = 0; i < lines.length; i++) {
      if (lines[i].trim().startsWith('#')) {
        insertIndex = i + 1;
        break;
      }
    }

    // Insert schema after first heading with blank lines
    lines.splice(insertIndex, 0, '', jsonldScript, '');

    return lines.join('\n');
  }
}
</file>

<file path=".env.example">
# AlephAuto Configuration
# Copy this file to .env and customize for your environment

# ============================================
# Base Directories
# ============================================

# Base directory containing all code repositories to process
# Default: ~/code
CODE_BASE_DIR=/Users/username/code

# Output directory for repomix condensed code
# Default: ./output/condense (relative to project root)
OUTPUT_BASE_DIR=./output/condense

# Directory for application logs
# Default: ./logs
LOG_DIR=./logs

# Directory for directory scan reports
# Default: ./output/directory-scan-reports
SCAN_REPORTS_DIR=./output/directory-scan-reports

# ============================================
# Job Processing
# ============================================

# Maximum number of concurrent jobs (1-50)
# Default: 5
MAX_CONCURRENT=5

# ============================================
# Monitoring & Error Tracking
# ============================================

# Sentry DSN for error tracking (optional)
# Get from: https://sentry.io/settings/projects/
SENTRY_DSN=https://your-sentry-dsn@sentry.io/project-id

# Node environment (development, production)
# Default: production
NODE_ENV=production

# ============================================
# Scheduling
# ============================================

# Cron schedule for repomix pipeline (default: 2 AM daily)
# Format: minute hour day month weekday
# Examples:
#   "0 2 * * *"  - 2 AM daily
#   "0 */6 * * *" - Every 6 hours
#   "0 0 * * 0"  - Midnight every Sunday
CRON_SCHEDULE="0 2 * * *"

# Cron schedule for documentation enhancement pipeline (default: 3 AM daily)
DOC_CRON_SCHEDULE="0 3 * * *"

# Run jobs immediately on startup (true/false)
# Default: false
RUN_ON_STARTUP=false

# ============================================
# Repomix Settings
# ============================================

# Timeout for repomix command in milliseconds (default: 10 minutes)
REPOMIX_TIMEOUT=600000

# Maximum buffer size for repomix output in bytes (default: 50MB)
REPOMIX_MAX_BUFFER=52428800

# ============================================
# Schema.org Integration
# ============================================

# URL for Schema.org MCP server (optional)
SCHEMA_MCP_URL=http://localhost:3001

# Force schema enhancement even if schema already exists
# Default: false
FORCE_ENHANCEMENT=false

# ============================================
# Logging
# ============================================

# Log level: debug, info, warn, error
# Default: info
LOG_LEVEL=info

# ============================================
# Health Check
# ============================================

# Port for health check HTTP server
# Default: 3000
HEALTH_CHECK_PORT=3000
</file>

<file path=".gitignore">
# Dependencies
node_modules/

# Logs
logs/
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# Output artifacts
output/
condense/
directory-scan-reports/
gitignore-update-report-*.json
repomix-output.xml
doc-enhancement/repomix-output.xml

# Environment
.env
.env.local
.env.*.local

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# OS
.DS_Store
Thumbs.db

# Sentry
.sentryclirc

# Testing
coverage/
.vitest/

# Build
dist/
build/
*.tsbuildinfo

# Temporary files
*.tmp
*.temp
</file>

<file path="claude-health-worker.js">
// @ts-nocheck
/**
 * Claude Health Check Worker - AlephAuto Integration
 *
 * Comprehensive health monitoring for Claude Code environment including:
 * - Skills, hooks, agents, commands inventory
 * - Configuration validation (settings.json, skill-rules.json)
 * - Hook permissions and registration
 * - Plugin analysis and duplicate detection
 * - Directory structure verification
 * - Environment variable status
 * - Performance log analysis
 *
 * @extends SidequestServer
 */

import { SidequestServer } from './server.js';
import { config } from './config.js';
import { createComponentLogger } from './logger.js';
import { exec } from 'child_process';
import { promisify } from 'util';
import fs from 'fs/promises';
import path from 'path';

const execAsync = promisify(exec);
const logger = createComponentLogger('ClaudeHealth');

class ClaudeHealthWorker extends SidequestServer {
  constructor(options = {}) {
    super({
      maxConcurrent: options.maxConcurrent ?? 1, // Single concurrent health check
      ...options
    });

    this.claudeDir = path.join(process.env.HOME, '.claude');
    this.devDir = path.join(process.env.HOME, 'dev');
    this.healthScriptPath = path.join(
      process.env.HOME,
      'code/jobs/sidequest/claude-health-check.sh'
    );

    this.thresholds = {
      maxPlugins: 30,
      warnPlugins: 20,
      maxHookExecutionTime: 1000, // ms
      minDiskSpace: 1024 * 1024 * 100, // 100MB
      maxLogSize: 1024 * 1024 * 10 // 10MB
    };

    logger.info('Claude Health Worker initialized', {
      claudeDir: this.claudeDir,
      healthScriptPath: this.healthScriptPath
    });
  }

  /**
   * Run comprehensive health check job
   * @param {Object} job - Job configuration
   * @param {boolean} job.detailed - Include detailed component listing
   * @param {boolean} job.validateConfig - Run configuration validation
   * @param {boolean} job.checkPerformance - Analyze hook performance logs
   * @param {boolean} job.analyzePlugins - Run plugin duplicate detection
   * @returns {Promise<Object>} Health check results
   */
  async runJobHandler(job) {
    const startTime = Date.now();
    logger.info('Starting Claude health check', { jobId: job.id });

    try {
      const checks = await this.runAllChecks(job.data || {});
      const analysis = this.analyzeResults(checks);
      const recommendations = this.generateRecommendations(analysis);

      const result = {
        success: true,
        timestamp: new Date().toISOString(),
        duration: Date.now() - startTime,
        checks,
        analysis,
        recommendations,
        summary: this.generateSummary(analysis, recommendations)
      };

      logger.info('Claude health check completed', {
        jobId: job.id,
        issueCount: recommendations.filter(r => r.priority === 'high').length,
        duration: result.duration
      });

      return result;
    } catch (error) {
      logger.error({ err: error, jobId: job.id }, 'Claude health check failed');
      throw error;
    }
  }

  /**
   * Run all health checks
   * @param {Object} options - Check options
   * @returns {Promise<Object>} Check results
   */
  async runAllChecks(options = {}) {
    const checks = {};

    // Run checks in parallel where possible
    const [
      environment,
      directories,
      configuration,
      hooks,
      components,
      plugins,
      performance
    ] = await Promise.all([
      this.checkEnvironment(),
      this.checkDirectories(),
      options.validateConfig !== false ? this.checkConfiguration() : null,
      this.checkHooks(),
      this.checkComponents(options.detailed),
      options.analyzePlugins !== false ? this.checkPlugins() : null,
      options.checkPerformance !== false ? this.checkPerformance() : null
    ]);

    return {
      environment,
      directories,
      configuration,
      hooks,
      components,
      plugins,
      performance
    };
  }

  /**
   * Check environment variables and dependencies
   * @returns {Promise<Object>}
   */
  async checkEnvironment() {
    const checks = {
      direnv: false,
      direnvConfigured: false,
      direnvAllowed: false,
      nodeVersion: null,
      npmVersion: null,
      envVars: {}
    };

    try {
      // Check direnv
      try {
        await execAsync('which direnv');
        checks.direnv = true;
      } catch (e) {
        checks.direnv = false;
      }

      // Check if direnv hook is in shell config
      try {
        const { stdout } = await execAsync('grep -q "direnv hook" ~/.zshrc && echo "configured" || echo "not configured"');
        checks.direnvConfigured = stdout.trim() === 'configured';
      } catch (e) {
        checks.direnvConfigured = false;
      }

      // Check Node.js version
      const { stdout: nodeVersion } = await execAsync('node --version');
      checks.nodeVersion = nodeVersion.trim();

      // Check npm version
      const { stdout: npmVersion } = await execAsync('npm --version');
      checks.npmVersion = npmVersion.trim();

      // Check critical environment variables
      const varNames = [
        'CLAUDE_CONFIG_DIR',
        'CLAUDE_PROJECT_DIR',
        'CLAUDE_ACTIVE_DOCS',
        'CLAUDE_HOOKS_DIR'
      ];

      for (const varName of varNames) {
        checks.envVars[varName] = process.env[varName] || null;
      }

      checks.direnvAllowed = Object.values(checks.envVars).every(v => v !== null);

    } catch (error) {
      logger.warn({ err: error }, 'Error checking environment');
    }

    return checks;
  }

  /**
   * Check directory structure
   * @returns {Promise<Object>}
   */
  async checkDirectories() {
    const requiredDirs = {
      '.claude': this.claudeDir,
      'skills': path.join(this.claudeDir, 'skills'),
      'hooks': path.join(this.claudeDir, 'hooks'),
      'agents': path.join(this.claudeDir, 'agents'),
      'commands': path.join(this.claudeDir, 'commands'),
      'scripts': path.join(this.claudeDir, 'scripts'),
      'logs': path.join(this.claudeDir, 'logs'),
      'dev': this.devDir,
      'dev/active': path.join(this.devDir, 'active'),
      'dev/archive': path.join(this.devDir, 'archive'),
      'dev/templates': path.join(this.devDir, 'templates')
    };

    const checks = {};

    for (const [name, dirPath] of Object.entries(requiredDirs)) {
      try {
        const stats = await fs.stat(dirPath);
        checks[name] = {
          exists: stats.isDirectory(),
          path: dirPath,
          size: null
        };
      } catch (error) {
        checks[name] = {
          exists: false,
          path: dirPath,
          error: error.code
        };
      }
    }

    return checks;
  }

  /**
   * Check configuration files
   * @returns {Promise<Object>}
   */
  async checkConfiguration() {
    const checks = {
      settingsJson: {},
      skillRulesJson: {},
      packageJson: {},
      envrc: {}
    };

    // Check settings.json
    const settingsPath = path.join(this.claudeDir, 'settings.json');
    try {
      const data = await fs.readFile(settingsPath, 'utf-8');
      const settings = JSON.parse(data);
      checks.settingsJson = {
        exists: true,
        valid: true,
        hooks: settings.hooks ? Object.keys(settings.hooks).length : 0,
        enabledPlugins: settings.enabledPlugins ? Object.keys(settings.enabledPlugins).length : 0
      };
    } catch (error) {
      checks.settingsJson = {
        exists: error.code !== 'ENOENT',
        valid: false,
        error: error.message
      };
    }

    // Check skill-rules.json
    const skillRulesPath = path.join(this.claudeDir, 'skills', 'skill-rules.json');
    try {
      const data = await fs.readFile(skillRulesPath, 'utf-8');
      const skillRules = JSON.parse(data);
      checks.skillRulesJson = {
        exists: true,
        valid: true,
        skillCount: skillRules.skills ? Object.keys(skillRules.skills).length : 0
      };
    } catch (error) {
      checks.skillRulesJson = {
        exists: error.code !== 'ENOENT',
        valid: false,
        error: error.message
      };
    }

    // Check package.json
    const packagePath = path.join(this.claudeDir, 'package.json');
    try {
      const data = await fs.readFile(packagePath, 'utf-8');
      const pkg = JSON.parse(data);
      checks.packageJson = {
        exists: true,
        valid: true,
        scripts: pkg.scripts ? Object.keys(pkg.scripts).length : 0
      };
    } catch (error) {
      checks.packageJson = {
        exists: error.code !== 'ENOENT',
        valid: false,
        error: error.message
      };
    }

    // Check .envrc
    const envrcPath = path.join(this.claudeDir, '.envrc');
    try {
      await fs.access(envrcPath);
      checks.envrc = { exists: true };
    } catch (error) {
      checks.envrc = { exists: false };
    }

    return checks;
  }

  /**
   * Check hooks
   * @returns {Promise<Object>}
   */
  async checkHooks() {
    const hooksDir = path.join(this.claudeDir, 'hooks');
    const checks = {
      totalHooks: 0,
      executableHooks: 0,
      registeredHooks: 0,
      hooks: []
    };

    try {
      const files = await fs.readdir(hooksDir);
      const shellHooks = files.filter(f => f.endsWith('.sh'));

      checks.totalHooks = shellHooks.length;

      for (const hook of shellHooks) {
        const hookPath = path.join(hooksDir, hook);
        const stats = await fs.stat(hookPath);
        const isExecutable = (stats.mode & 0o111) !== 0;

        if (isExecutable) checks.executableHooks++;

        checks.hooks.push({
          name: hook,
          executable: isExecutable,
          size: stats.size
        });
      }

      // Check registered hooks
      const settingsPath = path.join(this.claudeDir, 'settings.json');
      try {
        const data = await fs.readFile(settingsPath, 'utf-8');
        const settings = JSON.parse(data);
        checks.registeredHooks = settings.hooks ? Object.keys(settings.hooks).length : 0;
      } catch (e) {
        logger.warn({ err: e }, 'Could not check registered hooks');
      }

    } catch (error) {
      logger.warn({ err: error }, 'Error checking hooks');
    }

    return checks;
  }

  /**
   * Check component counts
   * @param {boolean} detailed - Include detailed listing
   * @returns {Promise<Object>}
   */
  async checkComponents(detailed = false) {
    const counts = {
      skills: 0,
      agents: 0,
      commands: 0,
      activeTasks: 0,
      archivedTasks: 0,
      templates: 0
    };

    try {
      // Count skills
      const skillsDir = path.join(this.claudeDir, 'skills');
      const skillFiles = await fs.readdir(skillsDir);
      counts.skills = skillFiles.filter(f => f.endsWith('.md') || fs.stat(path.join(skillsDir, f)).then(s => s.isDirectory())).length;

      // Count agents
      const agentsDir = path.join(this.claudeDir, 'agents');
      const agentFiles = await fs.readdir(agentsDir);
      counts.agents = agentFiles.filter(f => f.endsWith('.md')).length;

      // Count commands
      const commandsDir = path.join(this.claudeDir, 'commands');
      const commandFiles = await fs.readdir(commandsDir);
      counts.commands = commandFiles.filter(f => f.endsWith('.md')).length;

      // Count active tasks
      const activeDir = path.join(this.devDir, 'active');
      try {
        const activeTasks = await fs.readdir(activeDir);
        counts.activeTasks = activeTasks.filter(f => !f.startsWith('.')).length;
      } catch (e) {
        counts.activeTasks = 0;
      }

      // Count archived tasks
      const archiveDir = path.join(this.devDir, 'archive');
      try {
        const archivedTasks = await fs.readdir(archiveDir);
        counts.archivedTasks = archivedTasks.filter(f => !f.startsWith('.')).length;
      } catch (e) {
        counts.archivedTasks = 0;
      }

      // Count templates
      const templatesDir = path.join(this.devDir, 'templates');
      try {
        const templates = await fs.readdir(templatesDir);
        counts.templates = templates.filter(f => f.endsWith('.md')).length;
      } catch (e) {
        counts.templates = 0;
      }

    } catch (error) {
      logger.warn({ err: error }, 'Error checking components');
    }

    return counts;
  }

  /**
   * Check plugins (integrate with plugin manager)
   * @returns {Promise<Object>}
   */
  async checkPlugins() {
    try {
      const settingsPath = path.join(this.claudeDir, 'settings.json');
      const data = await fs.readFile(settingsPath, 'utf-8');
      const settings = JSON.parse(data);

      const enabledPlugins = settings.enabledPlugins || {};
      const totalPlugins = Object.keys(enabledPlugins).length;

      // Basic duplicate detection (category-based)
      const categories = {};
      for (const plugin of Object.keys(enabledPlugins)) {
        const category = plugin.split(/[@-]/)[0];
        if (!categories[category]) categories[category] = [];
        categories[category].push(plugin);
      }

      const duplicateCategories = Object.entries(categories)
        .filter(([_, plugins]) => plugins.length > 1)
        .map(([category, plugins]) => ({ category, plugins, count: plugins.length }));

      return {
        totalPlugins,
        enabledPlugins: Object.keys(enabledPlugins),
        duplicateCategories,
        exceededThresholds: {
          maxPlugins: totalPlugins > this.thresholds.maxPlugins,
          warnPlugins: totalPlugins > this.thresholds.warnPlugins
        }
      };
    } catch (error) {
      logger.warn({ err: error }, 'Error checking plugins');
      return null;
    }
  }

  /**
   * Check performance logs
   * @returns {Promise<Object>}
   */
  async checkPerformance() {
    const perfLogPath = path.join(this.claudeDir, 'logs', 'hook-performance.log');

    try {
      const stats = await fs.stat(perfLogPath);
      const logSize = stats.size;

      // Read recent entries
      const data = await fs.readFile(perfLogPath, 'utf-8');
      const lines = data.split('\n').filter(l => l.trim());
      const recentEntries = lines.slice(-100); // Last 100 entries

      // Parse slow hooks
      const slowHooks = recentEntries
        .filter(line => line.includes('SLOW'))
        .map(line => {
          const match = line.match(/\[([^\]]+)\].*?(\d+)ms/);
          return match ? { hook: match[1], duration: parseInt(match[2]) } : null;
        })
        .filter(Boolean);

      // Parse failures
      const failures = recentEntries.filter(line => line.includes('failed')).length;

      return {
        logExists: true,
        logSize,
        totalEntries: lines.length,
        recentEntries: recentEntries.length,
        slowHooks: slowHooks.length,
        failures,
        slowHookDetails: slowHooks.slice(0, 5) // Top 5 slowest
      };
    } catch (error) {
      if (error.code === 'ENOENT') {
        return {
          logExists: false,
          message: 'Performance log not created yet'
        };
      }
      logger.warn({ err: error }, 'Error checking performance');
      return null;
    }
  }

  /**
   * Analyze health check results
   * @param {Object} checks - All check results
   * @returns {Object} Analysis
   */
  analyzeResults(checks) {
    const issues = [];
    const warnings = [];
    const successes = [];

    // Environment analysis
    if (!checks.environment?.direnv) {
      warnings.push({ type: 'environment', message: 'direnv not installed' });
    } else if (!checks.environment?.direnvConfigured) {
      warnings.push({ type: 'environment', message: 'direnv hook not configured in shell' });
    } else if (!checks.environment?.direnvAllowed) {
      warnings.push({ type: 'environment', message: 'Environment variables not loaded (run: direnv allow)' });
    } else {
      successes.push({ type: 'environment', message: 'Environment properly configured' });
    }

    // Directory analysis
    const missingDirs = Object.entries(checks.directories || {})
      .filter(([_, info]) => !info.exists)
      .map(([name, _]) => name);

    if (missingDirs.length > 0) {
      issues.push({ type: 'directories', message: `Missing directories: ${missingDirs.join(', ')}` });
    } else {
      successes.push({ type: 'directories', message: 'All required directories exist' });
    }

    // Configuration analysis
    if (!checks.configuration?.settingsJson?.valid) {
      issues.push({ type: 'configuration', message: 'settings.json is invalid or missing' });
    }
    if (!checks.configuration?.skillRulesJson?.valid) {
      issues.push({ type: 'configuration', message: 'skill-rules.json is invalid or missing' });
    }
    if (checks.configuration?.settingsJson?.valid && checks.configuration?.skillRulesJson?.valid) {
      successes.push({ type: 'configuration', message: 'Configuration files valid' });
    }

    // Hooks analysis
    const nonExecutableHooks = (checks.hooks?.totalHooks || 0) - (checks.hooks?.executableHooks || 0);
    if (nonExecutableHooks > 0) {
      issues.push({
        type: 'hooks',
        message: `${nonExecutableHooks} hook(s) not executable`,
        action: 'Run: chmod +x ~/.claude/hooks/*.sh'
      });
    }

    if ((checks.hooks?.registeredHooks || 0) === 0) {
      warnings.push({ type: 'hooks', message: 'No hooks registered in settings.json' });
    }

    // Plugin analysis
    if (checks.plugins?.exceededThresholds?.maxPlugins) {
      issues.push({
        type: 'plugins',
        message: `Too many plugins enabled (${checks.plugins.totalPlugins} > ${this.thresholds.maxPlugins})`,
        action: 'Review and disable unused plugins'
      });
    } else if (checks.plugins?.exceededThresholds?.warnPlugins) {
      warnings.push({
        type: 'plugins',
        message: `High plugin count (${checks.plugins.totalPlugins})`,
        action: 'Consider reviewing plugin usage'
      });
    }

    if (checks.plugins?.duplicateCategories?.length > 0) {
      warnings.push({
        type: 'plugins',
        message: `Found ${checks.plugins.duplicateCategories.length} categories with duplicate plugins`,
        action: 'Review and consolidate duplicate plugins'
      });
    }

    // Performance analysis
    if (checks.performance?.logExists) {
      if (checks.performance.slowHooks > 0) {
        warnings.push({
          type: 'performance',
          message: `${checks.performance.slowHooks} slow hook executions detected`,
          action: 'Review hook performance'
        });
      }
      if (checks.performance.failures > 0) {
        issues.push({
          type: 'performance',
          message: `${checks.performance.failures} hook failures detected`,
          action: 'Check hook logs for errors'
        });
      }
    }

    return {
      issues,
      warnings,
      successes,
      healthScore: this.calculateHealthScore(issues, warnings, successes)
    };
  }

  /**
   * Calculate overall health score (0-100)
   * @param {Array} issues - Critical issues
   * @param {Array} warnings - Warnings
   * @param {Array} successes - Successful checks
   * @returns {number} Health score
   */
  calculateHealthScore(issues, warnings, successes) {
    const totalChecks = issues.length + warnings.length + successes.length;
    if (totalChecks === 0) return 100;

    const issueWeight = 20;
    const warningWeight = 5;
    const successWeight = 10;

    const deductions = (issues.length * issueWeight) + (warnings.length * warningWeight);
    const additions = successes.length * successWeight;

    return Math.max(0, Math.min(100, 100 - deductions + (additions / totalChecks)));
  }

  /**
   * Generate recommendations
   * @param {Object} analysis - Analysis results
   * @returns {Array<Object>} Recommendations
   */
  generateRecommendations(analysis) {
    const recommendations = [];

    // Critical issues first
    for (const issue of analysis.issues) {
      recommendations.push({
        priority: 'high',
        type: issue.type,
        message: issue.message,
        action: issue.action || 'Review and fix'
      });
    }

    // Warnings
    for (const warning of analysis.warnings) {
      recommendations.push({
        priority: 'medium',
        type: warning.type,
        message: warning.message,
        action: warning.action || 'Review recommended'
      });
    }

    // Success message if health score is high
    if (analysis.healthScore >= 90 && recommendations.length === 0) {
      recommendations.push({
        priority: 'info',
        type: 'healthy',
        message: `Claude environment is healthy (Score: ${analysis.healthScore}/100)`,
        action: 'No action needed'
      });
    }

    return recommendations;
  }

  /**
   * Generate summary
   * @param {Object} analysis - Analysis results
   * @param {Array} recommendations - Recommendations
   * @returns {Object} Summary
   */
  generateSummary(analysis, recommendations) {
    const critical = recommendations.filter(r => r.priority === 'high').length;
    const warnings = recommendations.filter(r => r.priority === 'medium').length;

    return {
      healthScore: analysis.healthScore,
      status: analysis.healthScore >= 90 ? 'healthy' : analysis.healthScore >= 70 ? 'warning' : 'critical',
      criticalIssues: critical,
      warnings,
      message: this.getStatusMessage(analysis.healthScore, critical, warnings)
    };
  }

  /**
   * Get status message based on health score
   * @param {number} healthScore - Health score
   * @param {number} critical - Critical issue count
   * @param {number} warnings - Warning count
   * @returns {string} Status message
   */
  getStatusMessage(healthScore, critical, warnings) {
    if (healthScore >= 90 && critical === 0 && warnings === 0) {
      return '‚úÖ Claude environment is healthy';
    } else if (healthScore >= 70) {
      return `‚ö†Ô∏è  Claude environment has ${warnings} warning(s)`;
    } else {
      return `üî¥ Claude environment has ${critical} critical issue(s)`;
    }
  }

  /**
   * Create a health check job
   * @param {Object} options - Job options
   * @returns {Object} Created job
   */
  addJob(options = {}) {
    const jobId = `claude-health-${Date.now()}`;
    return this.createJob(jobId, {
      detailed: options.detailed || false,
      validateConfig: options.validateConfig !== false,
      checkPerformance: options.checkPerformance !== false,
      analyzePlugins: options.analyzePlugins !== false
    });
  }
}

// Export worker class
export { ClaudeHealthWorker };
</file>

<file path="collect_git_activity.py">
#!/usr/bin/env python3
"""
Comprehensive Git Activity Report Generator

Scans multiple repositories, analyzes commits, generates visualizations,
and creates a formatted markdown report for Jekyll sites.

Usage:
    python3 collect_git_activity.py --start-date 2025-07-07 --end-date 2025-11-16
    python3 collect_git_activity.py --days 7  # Last 7 days
    python3 collect_git_activity.py --weekly  # Last week
"""

import argparse
import json
import math
import os
import subprocess
from collections import defaultdict
from datetime import datetime, timedelta
from pathlib import Path


# Configuration
CODE_DIR = Path.home() / 'code'
EXCLUDE_PATTERNS = ['vim/bundle', 'node_modules', '.git', 'venv', '.venv']
DEFAULT_MAX_DEPTH = 2

# Language/File Type Mapping
LANGUAGE_EXTENSIONS = {
    'Python': ['.py', '.pyw'],
    'JavaScript': ['.js', '.mjs', '.cjs'],
    'TypeScript': ['.ts', '.tsx'],
    'Ruby': ['.rb', '.rake', '.gemspec'],
    'HTML': ['.html', '.htm'],
    'CSS/SCSS': ['.css', '.scss', '.sass', '.less'],
    'Markdown': ['.md', '.markdown'],
    'JSON': ['.json'],
    'YAML': ['.yml', '.yaml'],
    'Shell': ['.sh', '.bash', '.zsh'],
    'SQL': ['.sql'],
    'Go': ['.go'],
    'Rust': ['.rs'],
    'C/C++': ['.c', '.cpp', '.cc', '.h', '.hpp'],
    'Java': ['.java'],
    'PHP': ['.php'],
    'Lock Files': ['.lock', 'package-lock.json', 'Gemfile.lock', 'yarn.lock', 'pnpm-lock.yaml'],
    'SVG': ['.svg'],
    'Images': ['.png', '.jpg', '.jpeg', '.gif', '.webp', '.ico', '.bmp'],
    'Data Files': ['.csv', '.xml', '.tsv', '.parquet'],
    'Text Files': ['.txt', '.log', '.ini', '.conf']
}


def find_git_repos(max_depth=DEFAULT_MAX_DEPTH):
    """Find all git repositories, excluding specified patterns"""
    print(f"Scanning for repositories in {CODE_DIR} (depth: {max_depth})...")
    cmd = f"find {CODE_DIR} -maxdepth {max_depth} -name .git -type d"
    result = subprocess.run(cmd.split(), capture_output=True, text=True)

    repos = []
    for line in result.stdout.strip().split('\n'):
        if line:
            repo_path = Path(line).parent
            # Exclude patterns
            if not any(pattern in str(repo_path) for pattern in EXCLUDE_PATTERNS):
                repos.append(repo_path)

    print(f"Found {len(repos)} repositories")
    return sorted(repos)


def get_repo_stats(repo_path, since_date, until_date=None):
    """Get commit statistics for a repository"""
    try:
        os.chdir(repo_path)

        # Build git log command
        git_cmd = f"git log --since={since_date} --all --oneline"
        if until_date:
            git_cmd += f" --until={until_date}"

        result = subprocess.run(git_cmd.split(), capture_output=True, text=True)
        commits = len(result.stdout.strip().split('\n')) if result.stdout.strip() else 0

        # Get file changes for language analysis
        git_cmd = f"git log --since={since_date} --all --name-only --pretty=format:"
        if until_date:
            git_cmd += f" --until={until_date}"

        result = subprocess.run(git_cmd.split(), capture_output=True, text=True)
        files = [f for f in result.stdout.strip().split('\n') if f]

        # Get parent directory (for organization/grouping)
        parent = repo_path.parent.name if repo_path.parent != CODE_DIR else None

        return {
            'path': str(repo_path),
            'name': repo_path.name,
            'parent': parent,
            'commits': commits,
            'files': files
        }
    except Exception as e:
        print(f"Error processing {repo_path}: {e}")
        return None


def analyze_languages(all_files):
    """Analyze file changes by programming language"""
    language_stats = defaultdict(int)

    for file_path in all_files:
        file_ext = Path(file_path).suffix.lower()
        file_name = Path(file_path).name

        # Map to language
        found = False
        for language, extensions in LANGUAGE_EXTENSIONS.items():
            if file_ext in extensions or file_name in extensions:
                language_stats[language] += 1
                found = True
                break

        if not found and file_ext:
            language_stats['Other'] += 1

    return dict(language_stats)


def find_project_websites(repositories):
    """Scan for CNAME files to discover GitHub Pages websites"""
    websites = {}
    for repo in repositories:
        repo_path = Path(repo['path'])
        cname_file = repo_path / 'CNAME'

        if cname_file.exists():
            try:
                website = cname_file.read_text().strip()
                if website and '.' in website:  # Basic validation
                    websites[repo['name']] = f"https://{website}"
            except Exception:
                pass

    return websites


def categorize_repositories(repositories):
    """Categorize repositories by project type"""
    categories = {
        'Data & Analytics': [],
        'Personal Sites': [],
        'Infrastructure': [],
        'MCP Servers': [],
        'Client Work': [],
        'Business Apps': [],
        'Legacy': []
    }

    for repo in repositories:
        name = repo['name'].lower()
        commits = repo['commits']

        # Categorization logic (customize based on your projects)
        if 'scraper' in name or 'analytics' in name or 'bot' in name:
            categories['Data & Analytics'].append(repo)
        elif 'personalsite' in name or 'github.io' in name:
            categories['Personal Sites'].append(repo)
        elif 'mcp' in name or 'server' in name:
            categories['MCP Servers'].append(repo)
        elif commits < 5:
            categories['Legacy'].append(repo)
        elif 'integrity' in name or 'studio' in name or 'visualizer' in name:
            categories['Infrastructure'].append(repo)
        elif 'inventory' in name or 'financial' in name:
            categories['Business Apps'].append(repo)
        else:
            categories['Client Work'].append(repo)

    return categories


def create_pie_chart_svg(data, title, output_file, width=800, height=600):
    """Create SVG pie chart without matplotlib dependency"""
    cx, cy = width / 2, height / 2
    radius = min(width, height) / 3

    colors = [
        '#0066cc', '#4da6ff', '#99ccff', '#00994d', '#ffcc00',
        '#ff6600', '#cc0000', '#9966cc', '#66cc99', '#ff6699'
    ]

    total = sum(data.values())
    if total == 0:
        return

    svg_parts = [
        f'<svg width="{width}" height="{height}" xmlns="http://www.w3.org/2000/svg">',
        f'<text x="{cx}" y="30" text-anchor="middle" font-size="20" font-weight="bold">{title}</text>'
    ]

    start_angle = 0
    legend_y = 50

    for i, (label, value) in enumerate(data.items()):
        if value == 0:
            continue

        percent = (value / total) * 100
        angle = (value / total) * 360
        end_angle = start_angle + angle

        # Convert to radians
        start_rad = math.radians(start_angle - 90)
        end_rad = math.radians(end_angle - 90)

        # Calculate arc path
        x1 = cx + radius * math.cos(start_rad)
        y1 = cy + radius * math.sin(start_rad)
        x2 = cx + radius * math.cos(end_rad)
        y2 = cy + radius * math.sin(end_rad)

        large_arc = 1 if angle > 180 else 0

        # Create pie slice
        path = f'M {cx},{cy} L {x1},{y1} A {radius},{radius} 0 {large_arc},1 {x2},{y2} Z'
        color = colors[i % len(colors)]
        svg_parts.append(f'<path d="{path}" fill="{color}" stroke="white" stroke-width="2"/>')

        # Add legend
        legend_x = width - 200
        svg_parts.append(f'<rect x="{legend_x}" y="{legend_y}" width="15" height="15" fill="{color}"/>')
        svg_parts.append(f'<text x="{legend_x + 20}" y="{legend_y + 12}" font-size="12">{label}: {value} ({percent:.1f}%)</text>')
        legend_y += 25

        start_angle = end_angle

    svg_parts.append('</svg>')

    # Write to file
    output_file.parent.mkdir(parents=True, exist_ok=True)
    output_file.write_text('\n'.join(svg_parts))
    print(f"Created: {output_file.name}")


def create_bar_chart_svg(data, title, output_file, width=800, height=600):
    """Create SVG horizontal bar chart"""
    max_value = max(data.values()) if data else 1
    bar_height = 30
    spacing = 10
    chart_height = len(data) * (bar_height + spacing)
    margin_left = 250
    margin_top = 50

    actual_height = chart_height + margin_top + 50

    svg_parts = [
        f'<svg width="{width}" height="{actual_height}" xmlns="http://www.w3.org/2000/svg">',
        f'<text x="{width/2}" y="30" text-anchor="middle" font-size="20" font-weight="bold">{title}</text>'
    ]

    for i, (label, value) in enumerate(data.items()):
        y = margin_top + i * (bar_height + spacing)
        bar_width = ((width - margin_left - 100) * value / max_value)

        # Bar
        svg_parts.append(f'<rect x="{margin_left}" y="{y}" width="{bar_width}" height="{bar_height}" fill="#0066cc" stroke="#333" stroke-width="1"/>')

        # Label
        svg_parts.append(f'<text x="{margin_left - 10}" y="{y + bar_height/2 + 5}" text-anchor="end" font-size="14">{label}</text>')

        # Value
        svg_parts.append(f'<text x="{margin_left + bar_width + 5}" y="{y + bar_height/2 + 5}" font-size="14" font-weight="bold">{value}</text>')

    svg_parts.append('</svg>')

    # Write to file
    output_file.parent.mkdir(parents=True, exist_ok=True)
    output_file.write_text('\n'.join(svg_parts))
    print(f"Created: {output_file.name}")


def generate_visualizations(data, output_dir):
    """Generate all SVG visualizations"""
    print("\nGenerating SVG visualizations...")

    # Monthly commits (if available in data)
    if 'monthly' in data:
        monthly_data = {month: count for month, count in data['monthly'].items()}
        create_pie_chart_svg(
            monthly_data,
            f"Commits by Month ({data['total_commits']} total)",
            output_dir / 'monthly-commits.svg'
        )

    # Top 10 repositories
    top_10 = {}
    for repo in data['repositories'][:10]:
        name = repo['name']
        if repo['parent']:
            name = f"{repo['parent']}/{name}"
        top_10[name] = repo['commits']

    create_bar_chart_svg(
        top_10,
        'Top 10 Repositories by Commits',
        output_dir / 'top-10-repos.svg'
    )

    # Project categories
    if 'categories' in data:
        category_data = {cat: len(repos) for cat, repos in data['categories'].items() if repos}
        create_pie_chart_svg(
            category_data,
            f"Project Categories ({len(data['repositories'])} repos)",
            output_dir / 'project-categories.svg'
        )

    # Language distribution
    if 'languages' in data:
        language_data = data['languages']
        create_pie_chart_svg(
            language_data,
            f"File Changes by Language ({sum(language_data.values())} total)",
            output_dir / 'language-distribution.svg',
            width=900
        )


def main():
    parser = argparse.ArgumentParser(description='Generate comprehensive git activity report')
    parser.add_argument('--start-date', help='Start date (YYYY-MM-DD)')
    parser.add_argument('--end-date', help='End date (YYYY-MM-DD)', default=None)
    parser.add_argument('--days', type=int, help='Number of days back from today')
    parser.add_argument('--weekly', action='store_true', help='Last 7 days')
    parser.add_argument('--monthly', action='store_true', help='Last 30 days')
    parser.add_argument('--max-depth', type=int, default=DEFAULT_MAX_DEPTH, help='Max directory depth')
    parser.add_argument('--output-dir', help='Output directory for visualizations')
    parser.add_argument('--json-output', help='Output JSON data file')

    args = parser.parse_args()

    # Calculate date range
    if args.weekly:
        args.days = 7
    elif args.monthly:
        args.days = 30

    if args.days:
        end_date = datetime.now()
        start_date = end_date - timedelta(days=args.days)
        since_date = start_date.strftime('%Y-%m-%d')
        until_date = end_date.strftime('%Y-%m-%d')
    elif args.start_date:
        since_date = args.start_date
        until_date = args.end_date
    else:
        print("Error: Must specify --start-date, --days, --weekly, or --monthly")
        return 1

    print(f"\n{'='*60}")
    print(f"Git Activity Report Generator")
    print(f"{'='*60}")
    print(f"Date range: {since_date} to {until_date or 'now'}")
    print(f"Scan depth: {args.max_depth} directories")
    print(f"{'='*60}\n")

    # Find repositories
    repos = find_git_repos(args.max_depth)

    # Collect statistics
    print("\nCollecting commit statistics...")
    repositories = []
    all_files = []

    for repo in repos:
        stats = get_repo_stats(repo, since_date, until_date)
        if stats and stats['commits'] > 0:
            repositories.append(stats)
            all_files.extend(stats['files'])

    repositories.sort(key=lambda x: x['commits'], reverse=True)

    # Analyze languages
    print("\nAnalyzing programming languages...")
    language_stats = analyze_languages(all_files)

    # Find websites
    print("\nDiscovering project websites...")
    websites = find_project_websites(repositories)

    # Categorize projects
    print("\nCategorizing projects...")
    categories = categorize_repositories(repositories)

    # Compile data
    data = {
        'date_range': {
            'start': since_date,
            'end': until_date or datetime.now().strftime('%Y-%m-%d')
        },
        'total_commits': sum(r['commits'] for r in repositories),
        'total_repositories': len(repositories),
        'total_files': len(all_files),
        'repositories': repositories,
        'languages': language_stats,
        'websites': websites,
        'categories': {cat: [{'name': r['name'], 'commits': r['commits']}
                             for r in repos]
                       for cat, repos in categories.items()}
    }

    # Save JSON
    json_file = args.json_output or '/tmp/git_activity_comprehensive.json'
    with open(json_file, 'w') as f:
        json.dump(data, f, indent=2)
    print(f"\n‚úÖ Saved data to: {json_file}")

    # Generate visualizations
    if args.output_dir:
        output_dir = Path(args.output_dir)
    else:
        year = datetime.now().year
        output_dir = Path.home() / 'code' / 'PersonalSite' / 'assets' / 'images' / f'git-activity-{year}'

    generate_visualizations(data, output_dir)

    # Print summary
    print(f"\n{'='*60}")
    print(f"Summary")
    print(f"{'='*60}")
    print(f"Total commits: {data['total_commits']}")
    print(f"Active repositories: {len(repositories)}")
    print(f"File changes: {len(all_files)}")
    print(f"Languages detected: {len(language_stats)}")
    print(f"Websites found: {len(websites)}")
    print(f"\nTop 5 repositories:")
    for i, repo in enumerate(repositories[:5], 1):
        print(f"  {i}. {repo['name']}: {repo['commits']} commits")

    print(f"\nTop 5 languages:")
    sorted_langs = sorted(language_stats.items(), key=lambda x: x[1], reverse=True)
    for i, (lang, count) in enumerate(sorted_langs[:5], 1):
        print(f"  {i}. {lang}: {count} files")

    print(f"\n‚úÖ Complete! Visualizations saved to: {output_dir}")
    print(f"{'='*60}\n")

    return 0


if __name__ == '__main__':
    exit(main())
</file>

<file path="config.js">
import path from 'path';
import os from 'os';
import { fileURLToPath } from 'url';
import { dirname } from 'path';

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

/**
 * Centralized configuration for AlephAuto
 * All paths are resolved to absolute paths for consistency
 */
export const config = {
  // Base directories
  codeBaseDir: process.env.CODE_BASE_DIR || path.join(os.homedir(), 'code'),

  // Output directories (relative to project root or absolute)
  outputBaseDir: process.env.OUTPUT_BASE_DIR
    ? path.resolve(process.env.OUTPUT_BASE_DIR)
    : path.resolve(__dirname, 'output', 'condense'),

  logDir: process.env.LOG_DIR
    ? path.resolve(process.env.LOG_DIR)
    : path.resolve(__dirname, 'logs'),

  scanReportsDir: process.env.SCAN_REPORTS_DIR
    ? path.resolve(process.env.SCAN_REPORTS_DIR)
    : path.resolve(__dirname, 'output', 'directory-scan-reports'),

  // Job processing
  maxConcurrent: parseInt(process.env.MAX_CONCURRENT || '5', 10),

  // Sentry monitoring
  sentryDsn: process.env.SENTRY_DSN,
  nodeEnv: process.env.NODE_ENV || 'production',

  // Cron schedules (default: 2 AM for repomix, 3 AM for docs)
  repomixSchedule: process.env.CRON_SCHEDULE || '0 2 * * *',
  docSchedule: process.env.DOC_CRON_SCHEDULE || '0 3 * * *',

  // Feature flags
  runOnStartup: process.env.RUN_ON_STARTUP === 'true',
  forceEnhancement: process.env.FORCE_ENHANCEMENT === 'true',

  // Repomix settings
  repomixTimeout: parseInt(process.env.REPOMIX_TIMEOUT || '600000', 10), // 10 minutes
  repomixMaxBuffer: parseInt(process.env.REPOMIX_MAX_BUFFER || '52428800', 10), // 50MB
  repomixIgnorePatterns: process.env.REPOMIX_IGNORE_PATTERNS
    ? process.env.REPOMIX_IGNORE_PATTERNS.split(',')
    : ['**/README.md', '**/README.MD', '**/*.md'], // Skip README and markdown files by default

  // Schema.org MCP integration / Documentation Enhancement
  schemaMcpUrl: process.env.SCHEMA_MCP_URL,
  skipDocEnhancement: process.env.SKIP_DOC_ENHANCEMENT === 'true', // Skip README Schema.org enhancement

  // Logging
  logLevel: process.env.LOG_LEVEL || 'info',

  // Directory scanner exclusions
  excludeDirs: [
    'node_modules',
    '.git',
    'dist',
    'build',
    'coverage',
    '.next',
    '.nuxt',
    'vendor',
    '__pycache__',
    '.venv',
    'venv',
    'target',
    '.idea',
    '.vscode',
    'jobs',
    '.DS_Store',
    'Thumbs.db',
    '*.swp',
    '*.swo'
  ],

  // Health check server
  healthCheckPort: parseInt(process.env.HEALTH_CHECK_PORT || '3000', 10),

  // API server port
  apiPort: parseInt(process.env.JOBS_API_PORT || '8080', 10),

  // Project root directory
  projectRoot: __dirname,
};

/**
 * Validate configuration on import
 */
function validateConfig() {
  const errors = [];

  if (config.maxConcurrent < 1 || config.maxConcurrent > 50) {
    errors.push('MAX_CONCURRENT must be between 1 and 50');
  }

  if (config.repomixTimeout < 1000) {
    errors.push('REPOMIX_TIMEOUT must be at least 1000ms');
  }

  if (config.repomixMaxBuffer < 1024) {
    errors.push('REPOMIX_MAX_BUFFER must be at least 1024 bytes');
  }

  if (errors.length > 0) {
    throw new Error(`Configuration validation failed:\n${errors.join('\n')}`);
  }
}

// Validate on import
validateConfig();

export default config;
</file>

<file path="data-discovery-report-pipeline.js">
import cron from 'node-cron';
import { SchemaEnhancementWorker } from './doc-enhancement/schema-enhancement-worker.js';
import { READMEScanner } from './doc-enhancement/readme-scanner.js';
import { config } from './config.js';
import { createComponentLogger } from './logger.js';
import path from 'path';
import os from 'os';

const logger = createComponentLogger('DocEnhancementPipeline');

/**
 * Documentation Enhancement Pipeline
 * Automatically adds Schema.org markup to README files
 */
class DocEnhancementPipeline {
  constructor(options = {}) {
    this.targetDir = options.targetDir || path.join(os.homedir(), 'code', 'Inventory');
    this.dryRun = options.dryRun || false;

    this.worker = new SchemaEnhancementWorker({
      maxConcurrent: config.maxConcurrent,
      outputBaseDir: config.outputBaseDir,
      logDir: config.logDir,
      sentryDsn: config.sentryDsn,
      dryRun: this.dryRun,
    });

    this.scanner = new READMEScanner({
      baseDir: this.targetDir,
      excludeDirs: config.excludeDirs,
    });

    this.setupEventListeners();
  }

  /**
   * Setup event listeners for job events
   */
  setupEventListeners() {
    this.worker.on('job:created', (job) => {
      logger.info({ jobId: job.id }, 'Job created');
    });

    this.worker.on('job:started', (job) => {
      logger.info({
        jobId: job.id,
        relativePath: job.data.relativePath
      }, 'Job started');
    });

    this.worker.on('job:completed', (job) => {
      const duration = job.completedAt - job.startedAt;
      if (job.result.status === 'enhanced') {
        logger.info({
          jobId: job.id,
          duration,
          schemaType: job.result.schemaType,
          impactScore: job.result.impact.impactScore,
          rating: job.result.impact.rating
        }, 'Job completed - enhanced');
      } else {
        logger.info({
          jobId: job.id,
          duration,
          status: job.result.status,
          reason: job.result.reason
        }, 'Job completed - skipped');
      }
    });

    this.worker.on('job:failed', (job) => {
      logger.error({
        jobId: job.id,
        error: job.error
      }, 'Job failed');
    });
  }

  /**
   * Run enhancement on all README files
   */
  async runEnhancementPipeline() {
    // Check if documentation enhancement should be skipped
    if (config.skipDocEnhancement) {
      logger.info('Documentation enhancement is disabled (SKIP_DOC_ENHANCEMENT=true)');
      console.log('\n‚è≠Ô∏è  README enhancement pipeline skipped (SKIP_DOC_ENHANCEMENT=true)');
      console.log('To enable: unset SKIP_DOC_ENHANCEMENT or set to false\n');
      return;
    }

    logger.info({
      targetDir: this.targetDir,
      dryRun: this.dryRun
    }, 'Starting documentation enhancement pipeline');

    const startTime = Date.now();

    try {
      // Scan for README files
      logger.info('Scanning for README files');
      const readmes = await this.scanner.scanREADMEs();
      logger.info({ count: readmes.length }, 'README files found');

      if (readmes.length === 0) {
        logger.warn('No README files found to process');
        return;
      }

      // Get initial stats
      const scanStats = await this.scanner.getStats(readmes);
      logger.info({
        total: scanStats.total,
        withSchema: scanStats.withSchema,
        withoutSchema: scanStats.withoutSchema
      }, 'Scan statistics');

      // Create jobs for each README
      logger.info('Creating enhancement jobs');
      for (const readme of readmes) {
        // Skip if already has schema (unless explicitly overriding)
        const hasSchema = await this.scanner.hasSchemaMarkup(readme.fullPath);
        if (hasSchema && !config.forceEnhancement) {
          logger.debug({
            relativePath: readme.relativePath
          }, 'Skipping - already has schema');
          continue;
        }

        // Gather context for this README
        const context = await this.scanner.gatherContext(readme.dirPath);

        // Create enhancement job
        this.worker.createEnhancementJob(readme, context);
      }

      // Wait for all jobs to complete
      await this.waitForCompletion();

      const duration = Date.now() - startTime;
      const stats = this.worker.getEnhancementStats();

      logger.info({
        duration: Math.round(duration / 1000),
        enhanced: stats.enhanced,
        skipped: stats.skipped,
        failed: stats.failed,
        successRate: stats.successRate
      }, 'Enhancement pipeline complete');

      // Generate summary report
      const summary = await this.worker.generateSummaryReport();
      logger.info({
        outputDirectory: summary.outputDirectory
      }, 'Summary report saved');

    } catch (error) {
      logger.error({ err: error }, 'Error during enhancement pipeline');
      throw error;
    }
  }

  /**
   * Wait for all jobs to complete
   */
  async waitForCompletion() {
    return new Promise((resolve) => {
      const checkInterval = setInterval(() => {
        const stats = this.worker.getStats();
        if (stats.active === 0 && stats.queued === 0) {
          clearInterval(checkInterval);
          resolve();
        }
      }, 1000);
    });
  }

  /**
   * Setup cron job
   */
  setupCronJob(schedule = '0 3 * * *') {
    // Default: Run at 3 AM every day
    logger.info({ schedule }, 'Setting up cron job');

    cron.schedule(schedule, async () => {
      logger.info({ timestamp: new Date().toISOString() }, 'Cron job triggered');
      try {
        await this.runEnhancementPipeline();
      } catch (error) {
        logger.error({ err: error }, 'Cron job failed');
      }
    });

    logger.info('Cron job scheduled successfully');
  }

  /**
   * Start the pipeline
   */
  async start() {
    logger.info({
      targetDir: this.targetDir,
      outputBaseDir: this.worker.outputBaseDir,
      logDir: this.worker.logDir,
      dryRun: this.dryRun
    }, 'Documentation Enhancement Pipeline Server starting');

    // Setup cron job
    // Schedule: '0 3 * * *' = 3 AM daily
    // For testing: '*/10 * * * *' = every 10 minutes
    this.setupCronJob(config.docSchedule);

    // Run immediately on startup if requested
    if (config.runOnStartup) {
      logger.info('Running immediately (RUN_ON_STARTUP=true)');
      await this.runEnhancementPipeline();
    }

    logger.info('Server running. Press Ctrl+C to exit.');
  }
}

// Parse command line arguments
const args = process.argv.slice(2);
const options = {};

for (let i = 0; i < args.length; i++) {
  if (args[i] === '--target-dir' && args[i + 1]) {
    options.targetDir = args[i + 1];
    i++;
  } else if (args[i] === '--dry-run') {
    options.dryRun = true;
  }
}

// Start the pipeline
const pipeline = new DocEnhancementPipeline(options);
pipeline.start().catch((error) => {
  logger.error({ err: error }, 'Fatal error');
  process.exit(1);
});
</file>

<file path="directory-scanner.js">
import fs from 'fs/promises';
import path from 'path';
import os from 'os';
import { createComponentLogger } from './logger.js';

const logger = createComponentLogger('DirectoryScanner');

/**
 * DirectoryScanner - Recursively scans directories
 */
export class DirectoryScanner {
  constructor(options = {}) {
    this.baseDir = options.baseDir || path.join(os.homedir(), 'code');
    this.outputDir = options.outputDir || './directory-scan-reports';
    this.excludeDirs = new Set(options.excludeDirs || [
      'node_modules',
      '.git',
      'dist',
      'build',
      'coverage',
      '.next',
      '.nuxt',
      'vendor',
      '__pycache__',
      '.venv',
      'venv',
      'jobs',
      'logs',
      '.claude',
      'python',
      'node',
      'go',
      'php',
      'rust',
      'recovery',
    ]);
    this.maxDepth = options.maxDepth || 10;
  }

  /**
   * Scan for git repository root directories only
   */
  async scanDirectories() {
    const directories = [];
    await this.scanRecursive(this.baseDir, '', 0, directories);
    return directories;
  }

  /**
   * Check if a directory is a git repository root
   */
  async isGitRepository(dirPath) {
    try {
      const gitPath = path.join(dirPath, '.git');
      const stat = await fs.stat(gitPath);
      return stat.isDirectory();
    } catch (error) {
      return false;
    }
  }

  /**
   * Recursively scan for git repository root directories
   */
  async scanRecursive(currentPath, relativePath, depth, results) {
    // Check depth limit
    if (depth > this.maxDepth) {
      return;
    }

    try {
      // Check if current directory is a git repository
      const isGitRepo = await this.isGitRepository(currentPath);

      if (isGitRepo) {
        // This is a git repository root - add it and stop recursing
        results.push({
          fullPath: currentPath,
          relativePath: relativePath || path.basename(currentPath),
          name: path.basename(currentPath),
          depth,
          isGitRepo: true,
        });
        logger.info({ path: currentPath, relativePath }, 'Found git repository');
        return; // Don't scan subdirectories of git repos
      }

      const entries = await fs.readdir(currentPath, { withFileTypes: true });

      for (const entry of entries) {
        if (!entry.isDirectory()) continue;

        // Skip excluded directories
        if (this.excludeDirs.has(entry.name)) {
          continue;
        }

        // Skip hidden directories (except .git is checked separately)
        if (entry.name.startsWith('.')) {
          continue;
        }

        const fullPath = path.join(currentPath, entry.name);
        const newRelativePath = relativePath
          ? path.join(relativePath, entry.name)
          : entry.name;

        // Recurse into subdirectories
        await this.scanRecursive(fullPath, newRelativePath, depth + 1, results);
      }
    } catch (error) {
      // Log but don't fail on permission errors
      logger.warn({ path: currentPath, error: error.message }, 'Cannot access directory');
    }
  }

  /**
   * Check if a directory should be processed
   */
  async shouldProcess(dirPath) {
    try {
      const stat = await fs.stat(dirPath);
      if (!stat.isDirectory()) return false;

      // Check if directory has any files (not just subdirectories)
      const entries = await fs.readdir(dirPath);
      return entries.length > 0;
    } catch (error) {
      return false;
    }
  }

  /**
   * Get directory info
   */
  async getDirectoryInfo(dirPath) {
    const stat = await fs.stat(dirPath);
    const entries = await fs.readdir(dirPath);

    return {
      path: dirPath,
      size: stat.size,
      fileCount: entries.length,
      modifiedAt: stat.mtime,
    };
  }

  /**
   * Generate scan statistics
   */
  generateScanStats(directories) {
    const stats = {
      total: directories.length,
      byDepth: {},
      totalSize: 0,
      byName: {},
    };

    for (const dir of directories) {
      // Count by depth
      stats.byDepth[dir.depth] = (stats.byDepth[dir.depth] || 0) + 1;

      // Count by name (for detecting common project types)
      stats.byName[dir.name] = (stats.byName[dir.name] || 0) + 1;
    }

    // Get top directory names
    const sortedNames = Object.entries(stats.byName)
      .sort((a, b) => b[1] - a[1])
      .slice(0, 10);

    stats.topDirectoryNames = sortedNames.map(([name, count]) => ({ name, count }));

    return stats;
  }

  /**
   * Save scan report to output directory
   */
  async saveScanReport(directories, stats) {
    await fs.mkdir(this.outputDir, { recursive: true });

    const timestamp = Date.now();
    const report = {
      timestamp: new Date().toISOString(),
      baseDir: this.baseDir,
      scanStats: stats,
      directories: directories.map(d => ({
        relativePath: d.relativePath,
        name: d.name,
        depth: d.depth,
      })),
    };

    const reportPath = path.join(this.outputDir, `scan-report-${timestamp}.json`);
    await fs.writeFile(reportPath, JSON.stringify(report, null, 2));

    return reportPath;
  }

  /**
   * Generate directory tree visualization
   */
  generateDirectoryTree(directories) {
    const tree = [];

    // Group by depth for easier visualization
    const byDepth = {};
    for (const dir of directories) {
      if (!byDepth[dir.depth]) byDepth[dir.depth] = [];
      byDepth[dir.depth].push(dir);
    }

    // Generate tree structure
    tree.push('Directory Tree:');
    tree.push('==============');
    tree.push('');
    tree.push(this.baseDir);

    for (const dir of directories) {
      const indent = '  '.repeat(dir.depth + 1);
      const prefix = dir.depth === 0 ? '‚îú‚îÄ‚îÄ ' : '‚îî‚îÄ‚îÄ ';
      tree.push(`${indent}${prefix}${dir.name}/`);
    }

    return tree.join('\n');
  }

  /**
   * Save directory tree to file
   */
  async saveDirectoryTree(directories) {
    await fs.mkdir(this.outputDir, { recursive: true });

    const tree = this.generateDirectoryTree(directories);
    const timestamp = Date.now();
    const treePath = path.join(this.outputDir, `directory-tree-${timestamp}.txt`);

    await fs.writeFile(treePath, tree);

    return treePath;
  }

  /**
   * Generate and save complete scan results
   */
  async generateAndSaveScanResults(directories) {
    const stats = this.generateScanStats(directories);

    // Save JSON report
    const reportPath = await this.saveScanReport(directories, stats);

    // Save tree visualization
    const treePath = await this.saveDirectoryTree(directories);

    // Create summary
    const summary = {
      timestamp: new Date().toISOString(),
      baseDir: this.baseDir,
      totalDirectories: directories.length,
      maxDepth: Math.max(...directories.map(d => d.depth)),
      reportPath,
      treePath,
      stats,
    };

    const summaryPath = path.join(this.outputDir, `scan-summary-${Date.now()}.json`);
    await fs.writeFile(summaryPath, JSON.stringify(summary, null, 2));

    return {
      summary,
      reportPath,
      treePath,
      summaryPath,
    };
  }
}
</file>

<file path="git-activity-worker.js">
// @ts-nocheck
import { SidequestServer } from './server.js';
import { spawn } from 'child_process';
import fs from 'fs/promises';
import path from 'path';
import os from 'os';
import { createComponentLogger } from './logger.js';

const logger = createComponentLogger('GitActivityWorker');

/**
 * GitActivityWorker - Executes git activity report jobs
 *
 * Integrates the Python git activity collector into the AlephAuto framework,
 * providing job queue management, event tracking, and Sentry error monitoring.
 */
export class GitActivityWorker extends SidequestServer {
  constructor(options = {}) {
    super(options);
    this.codeBaseDir = options.codeBaseDir || path.join(os.homedir(), 'code');
    this.pythonScript = options.pythonScript || path.join(
      path.dirname(new URL(import.meta.url).pathname),
      'collect_git_activity.py'
    );
    this.personalSiteDir = options.personalSiteDir || path.join(
      os.homedir(),
      'code',
      'PersonalSite'
    );
    this.outputDir = options.outputDir || '/tmp';
    this.logDir = options.logDir || path.join(
      path.dirname(new URL(import.meta.url).pathname),
      'logs'
    );
  }

  /**
   * Run git activity report job
   */
  async runJobHandler(job) {
    const {
      reportType,
      days,
      sinceDate,
      untilDate,
      outputFormat = 'json',
      generateVisualizations = true
    } = job.data;

    logger.info({
      jobId: job.id,
      reportType,
      days,
      sinceDate,
      untilDate
    }, 'Running git activity report');

    try {
      // Build command arguments
      const args = this.#buildPythonArgs({
        reportType,
        days,
        sinceDate,
        untilDate,
        outputFormat,
        generateVisualizations
      });

      // Execute Python script
      const { stdout, stderr, outputFiles } = await this.#runPythonScript(args);

      if (stderr) {
        logger.warn({ jobId: job.id, stderr }, 'Git activity warnings');
      }

      // Parse JSON output to get statistics
      const stats = this.#parseStats(stdout);

      // Verify output files exist
      const verifiedFiles = await this.#verifyOutputFiles(outputFiles);

      logger.info({
        jobId: job.id,
        stats,
        filesGenerated: verifiedFiles.length
      }, 'Git activity report completed');

      return {
        reportType,
        days: days || this.#calculateDays(sinceDate, untilDate),
        sinceDate,
        untilDate,
        stats,
        outputFiles: verifiedFiles,
        timestamp: new Date().toISOString(),
      };
    } catch (error) {
      logger.error({
        jobId: job.id,
        error: error.message,
        stack: error.stack
      }, 'Git activity report failed');
      throw error;
    }
  }

  /**
   * Build Python script arguments
   * @private
   */
  #buildPythonArgs({ reportType, days, sinceDate, untilDate, outputFormat, generateVisualizations }) {
    const args = [];

    // Add date range arguments
    if (sinceDate && untilDate) {
      args.push('--since', sinceDate);
      args.push('--until', untilDate);
    } else if (reportType === 'weekly' || days === 7) {
      args.push('--weekly');
    } else if (reportType === 'monthly' || days === 30) {
      args.push('--monthly');
    } else if (days) {
      args.push('--days', String(days));
    } else {
      // Default to weekly
      args.push('--weekly');
    }

    // Add output format
    if (outputFormat !== 'json') {
      args.push('--output-format', outputFormat);
    }

    // Add visualization flag
    if (!generateVisualizations) {
      args.push('--no-visualizations');
    }

    return args;
  }

  /**
   * Execute Python script
   * @private
   */
  #runPythonScript(args) {
    return new Promise((resolve, reject) => {
      logger.debug({ args }, 'Executing Python script');

      const proc = spawn('python3', [this.pythonScript, ...args], {
        cwd: path.dirname(this.pythonScript),
        timeout: 300000, // 5 minute timeout
        maxBuffer: 10 * 1024 * 1024, // 10MB buffer
      });

      let stdout = '';
      let stderr = '';

      proc.stdout.on('data', (data) => {
        stdout += data.toString();
      });

      proc.stderr.on('data', (data) => {
        stderr += data.toString();
      });

      proc.on('close', (code) => {
        if (code === 0) {
          // Extract output files from stdout if present
          const outputFiles = this.#extractOutputFiles(stdout);
          resolve({ stdout, stderr, outputFiles });
        } else {
          const error = new Error(`Python script exited with code ${code}`);
          error.code = code;
          error.stdout = stdout;
          error.stderr = stderr;
          reject(error);
        }
      });

      proc.on('error', (error) => {
        error.stdout = stdout;
        error.stderr = stderr;
        reject(error);
      });
    });
  }

  /**
   * Extract output file paths from script output
   * @private
   */
  #extractOutputFiles(stdout) {
    const files = [];

    // Look for JSON output file
    const jsonMatch = stdout.match(/(?:Saving|Saved) (?:to|data to):\s*(.+\.json)/i);
    if (jsonMatch) {
      files.push(jsonMatch[1].trim());
    }

    // Look for visualization files
    const svgMatches = stdout.matchAll(/(?:Generating|Generated|Saving).*?:\s*(.+\.svg)/gi);
    for (const match of svgMatches) {
      files.push(match[1].trim());
    }

    return files;
  }

  /**
   * Parse statistics from script output
   * @private
   */
  #parseStats(stdout) {
    const stats = {
      totalCommits: 0,
      totalRepositories: 0,
      linesAdded: 0,
      linesDeleted: 0,
    };

    // Try to parse from JSON output in stdout
    try {
      const jsonMatch = stdout.match(/\{[\s\S]*"total_commits"[\s\S]*\}/);
      if (jsonMatch) {
        const data = JSON.parse(jsonMatch[0]);
        stats.totalCommits = data.total_commits || 0;
        stats.totalRepositories = data.repositories?.length || 0;
        stats.linesAdded = data.total_additions || 0;
        stats.linesDeleted = data.total_deletions || 0;
      }
    } catch (error) {
      logger.warn({ error: error.message }, 'Could not parse stats from output');
    }

    return stats;
  }

  /**
   * Calculate days between two dates
   * @private
   */
  #calculateDays(sinceDate, untilDate) {
    if (!sinceDate || !untilDate) return null;

    const since = new Date(sinceDate);
    const until = new Date(untilDate);
    const diffTime = Math.abs(until - since);
    const diffDays = Math.ceil(diffTime / (1000 * 60 * 60 * 24));

    return diffDays;
  }

  /**
   * Verify output files exist
   * @private
   */
  async #verifyOutputFiles(files) {
    const verified = [];

    for (const file of files) {
      try {
        await fs.access(file);
        const stats = await fs.stat(file);
        verified.push({
          path: file,
          size: stats.size,
          exists: true,
        });
      } catch (error) {
        logger.warn({ file, error: error.message }, 'Output file not found');
        verified.push({
          path: file,
          exists: false,
        });
      }
    }

    return verified;
  }

  /**
   * Create a weekly report job
   */
  createWeeklyReportJob() {
    const jobId = `git-activity-weekly-${Date.now()}`;

    return this.createJob(jobId, {
      reportType: 'weekly',
      days: 7,
      type: 'git-activity-report',
    });
  }

  /**
   * Create a monthly report job
   */
  createMonthlyReportJob() {
    const jobId = `git-activity-monthly-${Date.now()}`;

    return this.createJob(jobId, {
      reportType: 'monthly',
      days: 30,
      type: 'git-activity-report',
    });
  }

  /**
   * Create a custom date range report job
   */
  createCustomReportJob(sinceDate, untilDate) {
    const jobId = `git-activity-custom-${Date.now()}`;

    return this.createJob(jobId, {
      reportType: 'custom',
      sinceDate,
      untilDate,
      type: 'git-activity-report',
    });
  }

  /**
   * Create a report job with custom parameters
   */
  createReportJob(options = {}) {
    const jobId = options.jobId || `git-activity-${Date.now()}`;

    return this.createJob(jobId, {
      reportType: options.reportType || 'weekly',
      days: options.days,
      sinceDate: options.sinceDate,
      untilDate: options.untilDate,
      outputFormat: options.outputFormat || 'json',
      generateVisualizations: options.generateVisualizations !== false,
      type: 'git-activity-report',
    });
  }
}
</file>

<file path="git-report-config.json">
{
  "description": "Configuration for automated git activity reports",
  "version": "1.0.0",

  "scanning": {
    "code_directory": "~/code",
    "max_depth": 2,
    "exclude_patterns": [
      "vim/bundle",
      "node_modules",
      ".git",
      "venv",
      ".venv",
      "vendor",
      "target",
      "build",
      "dist"
    ]
  },

  "reports": {
    "weekly": {
      "enabled": true,
      "schedule": "Sunday 20:00",
      "days_back": 7,
      "auto_commit": false,
      "output_format": "markdown"
    },
    "monthly": {
      "enabled": true,
      "schedule": "1st of month 08:00",
      "days_back": 30,
      "auto_commit": false,
      "output_format": "markdown"
    },
    "quarterly": {
      "enabled": false,
      "schedule": "First Monday of quarter",
      "days_back": 90,
      "auto_commit": false,
      "output_format": "markdown"
    }
  },

  "output": {
    "personalsite_dir": "~/code/PersonalSite",
    "work_collection": "_work",
    "visualization_dir": "assets/images/git-activity-{year}",
    "json_cache_dir": "/tmp",
    "log_dir": "~/code/jobs/sidequest/logs"
  },

  "project_categories": {
    "Data & Analytics": {
      "keywords": ["scraper", "analytics", "bot", "data"],
      "description": "Data collection, scraping, analytics pipelines"
    },
    "Personal Sites": {
      "keywords": ["personalsite", "github.io", "portfolio", "blog"],
      "description": "Personal websites and blogs"
    },
    "Infrastructure": {
      "keywords": ["integrity", "studio", "visualizer", "tool", "dotfiles"],
      "description": "DevOps, tooling, development infrastructure"
    },
    "MCP Servers": {
      "keywords": ["mcp", "server", "context", "protocol"],
      "description": "Model Context Protocol server implementations"
    },
    "Client Work": {
      "keywords": ["client", "leora", "jobs"],
      "description": "Client projects and career tracking"
    },
    "Business Apps": {
      "keywords": ["inventory", "financial", "business"],
      "description": "Business applications and SaaS products"
    },
    "Legacy": {
      "keywords": ["old", "archive", "deprecated"],
      "description": "Archived or minimal-maintenance projects",
      "min_commits": 0,
      "max_commits": 5
    }
  },

  "visualizations": {
    "enabled": true,
    "formats": ["svg"],
    "charts": {
      "monthly_commits": {
        "enabled": true,
        "type": "pie",
        "filename": "monthly-commits.svg",
        "width": 800,
        "height": 600
      },
      "project_categories": {
        "enabled": true,
        "type": "pie",
        "filename": "project-categories.svg",
        "width": 800,
        "height": 600
      },
      "top_repositories": {
        "enabled": true,
        "type": "bar",
        "filename": "top-10-repos.svg",
        "width": 800,
        "height": 500,
        "count": 10
      },
      "language_distribution": {
        "enabled": true,
        "type": "pie",
        "filename": "language-distribution.svg",
        "width": 900,
        "height": 600
      }
    },
    "color_schemes": {
      "default": ["#0066cc", "#4da6ff", "#99ccff", "#00994d", "#ffcc00", "#ff6600", "#cc0000", "#9966cc", "#66cc99", "#ff6699"]
    }
  },

  "language_mapping": {
    "Python": [".py", ".pyw"],
    "JavaScript": [".js", ".mjs", ".cjs"],
    "TypeScript": [".ts", ".tsx"],
    "Ruby": [".rb", ".rake", ".gemspec"],
    "HTML": [".html", ".htm"],
    "CSS/SCSS": [".css", ".scss", ".sass", ".less"],
    "Markdown": [".md", ".markdown"],
    "JSON": [".json"],
    "YAML": [".yml", ".yaml"],
    "Shell": [".sh", ".bash", ".zsh"],
    "SQL": [".sql"],
    "Go": [".go"],
    "Rust": [".rs"],
    "C/C++": [".c", ".cpp", ".cc", ".h", ".hpp"],
    "Java": [".java"],
    "PHP": [".php"],
    "Lock Files": [".lock", "package-lock.json", "Gemfile.lock", "yarn.lock", "pnpm-lock.yaml"],
    "SVG": [".svg"],
    "Images": [".png", ".jpg", ".jpeg", ".gif", ".webp", ".ico", ".bmp"],
    "Data Files": [".csv", ".xml", ".tsv", ".parquet"],
    "Text Files": [".txt", ".log", ".ini", ".conf"]
  },

  "notifications": {
    "enabled": false,
    "methods": {
      "email": {
        "enabled": false,
        "to": "your-email@example.com",
        "subject": "Weekly Git Activity Report"
      },
      "slack": {
        "enabled": false,
        "webhook_url": "",
        "channel": "#dev-updates"
      },
      "discord": {
        "enabled": false,
        "webhook_url": ""
      }
    }
  },

  "git": {
    "auto_commit": false,
    "commit_message_template": "Add {period} git activity report: {start_date} to {end_date}\n\nü§ñ Generated with [Claude Code](https://claude.com/claude-code)\n\nCo-Authored-By: Claude <noreply@anthropic.com>",
    "auto_push": false
  },

  "cron": {
    "weekly": "0 20 * * 0",
    "monthly": "0 8 1 * *",
    "quarterly": "0 8 1 1,4,7,10 *"
  },

  "notes": [
    "This configuration file controls the automated git activity report generation.",
    "Edit the values above to customize the behavior for your workflow.",
    "The git activity reporter is now integrated with AlephAuto job queue framework.",
    "Usage: npm run git:weekly (weekly), npm run git:monthly (monthly), npm run git:schedule (scheduled mode)",
    "For production: pm2 start git-activity-pipeline.js --name git-activity",
    "Environment variable: GIT_CRON_SCHEDULE='0 20 * * 0' for custom schedule"
  ]
}
</file>

<file path="gitignore-repomix-updater.js">
import fs from 'fs/promises';
import path from 'path';
import os from 'os';
import { createComponentLogger } from './logger.js';

const logger = createComponentLogger('GitignoreRepomixUpdater');

/**
 * GitignoreRepomixUpdater - Adds repomix-output.xml to .gitignore in all git repositories
 */
export class GitignoreRepomixUpdater {
  constructor(options = {}) {
    this.baseDir = options.baseDir || path.join(os.homedir(), 'code');
    this.excludeDirs = new Set(options.excludeDirs || [
      'node_modules',
      '.git',
      'dist',
      'build',
      'coverage',
      '.next',
      '.nuxt',
      'vendor',
      '__pycache__',
      '.venv',
      'venv',
    ]);
    this.maxDepth = options.maxDepth || 10;
    this.dryRun = options.dryRun || false;
    this.gitignoreEntry = 'repomix-output.xml';
  }

  /**
   * Find all git repositories recursively
   */
  async findGitRepositories() {
    const repositories = [];
    await this.scanForGitRepos(this.baseDir, 0, repositories);
    return repositories;
  }

  /**
   * Recursively scan for git repositories
   */
  async scanForGitRepos(currentPath, depth, results) {
    // Check depth limit
    if (depth > this.maxDepth) {
      return;
    }

    try {
      const entries = await fs.readdir(currentPath, { withFileTypes: true });

      // Check if current directory is a git repository
      const hasGit = entries.some(entry => entry.name === '.git' && entry.isDirectory());

      if (hasGit) {
        results.push({
          fullPath: currentPath,
          depth,
        });
        // Don't scan subdirectories of a git repo for nested repos
        // (remove this return if you want to find nested repos)
        return;
      }

      // Scan subdirectories
      for (const entry of entries) {
        if (!entry.isDirectory()) continue;

        // Skip excluded directories
        if (this.excludeDirs.has(entry.name)) {
          continue;
        }

        // Skip hidden directories except .git
        if (entry.name.startsWith('.') && entry.name !== '.git') {
          continue;
        }

        const fullPath = path.join(currentPath, entry.name);
        await this.scanForGitRepos(fullPath, depth + 1, results);
      }
    } catch (error) {
      // Log but don't fail on permission errors
      logger.warn({ path: currentPath, error: error.message }, 'Cannot access directory');
    }
  }

  /**
   * Check if .gitignore already contains the entry
   */
  async gitignoreContainsEntry(gitignorePath) {
    try {
      const content = await fs.readFile(gitignorePath, 'utf8');
      const lines = content.split('\n').map(line => line.trim());

      // Check for exact match or pattern that would match
      return lines.some(line =>
        line === this.gitignoreEntry ||
        line === `/${this.gitignoreEntry}` ||
        line === `**/${this.gitignoreEntry}` ||
        line === `**/repomix-output.xml`
      );
    } catch (error) {
      if (error.code === 'ENOENT') {
        return false; // File doesn't exist
      }
      throw error;
    }
  }

  /**
   * Add entry to .gitignore file
   */
  async addToGitignore(repoPath) {
    const gitignorePath = path.join(repoPath, '.gitignore');

    try {
      // Check if entry already exists
      const alreadyExists = await this.gitignoreContainsEntry(gitignorePath);

      if (alreadyExists) {
        return {
          path: gitignorePath,
          action: 'skipped',
          reason: 'Entry already exists',
        };
      }

      if (this.dryRun) {
        return {
          path: gitignorePath,
          action: 'would_add',
          reason: 'Dry run mode',
        };
      }

      // Read existing content or start with empty string
      let content = '';
      try {
        content = await fs.readFile(gitignorePath, 'utf8');
      } catch (error) {
        if (error.code !== 'ENOENT') {
          throw error;
        }
      }

      // Ensure content ends with newline before adding new entry
      if (content.length > 0 && !content.endsWith('\n')) {
        content += '\n';
      }

      // Add comment and entry
      if (content.length > 0) {
        content += '\n';
      }
      content += '# Repomix output files\n';
      content += `${this.gitignoreEntry}\n`;

      // Write back to file
      await fs.writeFile(gitignorePath, content, 'utf8');

      return {
        path: gitignorePath,
        action: 'added',
        reason: 'Entry added successfully',
      };
    } catch (error) {
      return {
        path: gitignorePath,
        action: 'error',
        reason: error.message,
      };
    }
  }

  /**
   * Process all repositories
   */
  async processRepositories() {
    logger.info({
      baseDir: this.baseDir,
      dryRun: this.dryRun
    }, 'Scanning for git repositories');

    const repositories = await this.findGitRepositories();
    logger.info({ count: repositories.length }, 'Git repositories found');

    const results = [];

    for (const repo of repositories) {
      logger.info({ repository: repo.fullPath }, 'Processing repository');
      const result = await this.addToGitignore(repo.fullPath);
      results.push({
        repository: repo.fullPath,
        ...result,
      });
      logger.info({
        repository: repo.fullPath,
        action: result.action,
        reason: result.reason
      }, 'Repository processed');
    }

    return {
      totalRepositories: repositories.length,
      results,
      summary: this.generateSummary(results),
    };
  }

  /**
   * Generate summary statistics
   */
  generateSummary(results) {
    const summary = {
      added: 0,
      skipped: 0,
      would_add: 0,
      error: 0,
    };

    for (const result of results) {
      if (summary.hasOwnProperty(result.action)) {
        summary[result.action]++;
      }
    }

    return summary;
  }

  /**
   * Save results to JSON file
   */
  async saveResults(results, outputPath) {
    const report = {
      timestamp: new Date().toISOString(),
      baseDir: this.baseDir,
      dryRun: this.dryRun,
      gitignoreEntry: this.gitignoreEntry,
      ...results,
    };

    await fs.writeFile(outputPath, JSON.stringify(report, null, 2));
    logger.info({ outputPath }, 'Results saved');

    return report;
  }
}

/**
 * Main execution function
 */
export async function main() {
  const args = process.argv.slice(2);
  const dryRun = args.includes('--dry-run') || args.includes('-d');
  const baseDir = args.find(arg => !arg.startsWith('-')) || path.join(os.homedir(), 'code');

  const updater = new GitignoreRepomixUpdater({
    baseDir,
    dryRun,
  });

  try {
    const results = await updater.processRepositories();

    // Print summary
    logger.info({
      totalRepositories: results.totalRepositories,
      added: results.summary.added,
      skipped: results.summary.skipped,
      wouldAdd: results.summary.would_add,
      errors: results.summary.error
    }, 'Summary');

    // Save results
    const timestamp = Date.now();
    const outputPath = path.join(
      process.cwd(),
      `gitignore-update-report-${timestamp}.json`
    );
    await updater.saveResults(results, outputPath);

  } catch (error) {
    logger.error({ err: error }, 'Fatal error');
    process.exit(1);
  }
}

// Run if called directly
if (import.meta.url === `file://${process.argv[1]}`) {
  main();
}
</file>

<file path="gitignore-worker.js">
// @ts-nocheck
import { SidequestServer } from './server.js';
import { GitignoreRepomixUpdater } from './gitignore-repomix-updater.js';
import { createComponentLogger } from './logger.js';
import * as Sentry from '@sentry/node';
import path from 'path';
import os from 'os';

const logger = createComponentLogger('GitignoreWorker');

/**
 * GitignoreWorker - Executes gitignore update jobs
 *
 * Integrates GitignoreRepomixUpdater into the AlephAuto framework,
 * providing job queue management, event tracking, and Sentry error monitoring.
 */
export class GitignoreWorker extends SidequestServer {
  constructor(options = {}) {
    super(options);
    this.baseDir = options.baseDir || path.join(os.homedir(), 'code');
    this.excludeDirs = options.excludeDirs || [
      'node_modules',
      '.git',
      'dist',
      'build',
      'coverage',
      '.next',
      '.nuxt',
      'vendor',
      '__pycache__',
      '.venv',
      'venv',
    ];
    this.maxDepth = options.maxDepth || 10;
    this.gitignoreEntry = options.gitignoreEntry || 'repomix-output.xml';
  }

  /**
   * Run gitignore update job
   */
  async runJobHandler(job) {
    const {
      baseDir = this.baseDir,
      excludeDirs = this.excludeDirs,
      maxDepth = this.maxDepth,
      gitignoreEntry = this.gitignoreEntry,
      dryRun = false,
      repositories = null, // Optional: specific repositories to process
    } = job.data;

    logger.info({
      jobId: job.id,
      baseDir,
      dryRun,
      specificRepos: repositories ? repositories.length : 'all'
    }, 'Running gitignore update job');

    try {
      // Create updater instance
      const updater = new GitignoreRepomixUpdater({
        baseDir,
        excludeDirs,
        maxDepth,
        dryRun,
      });

      // Override gitignore entry if specified
      if (gitignoreEntry) {
        updater.gitignoreEntry = gitignoreEntry;
      }

      let results;
      if (repositories && repositories.length > 0) {
        // Process specific repositories
        results = await this.#processSpecificRepositories(updater, repositories);
      } else {
        // Process all repositories
        results = await updater.processRepositories();
      }

      logger.info({
        jobId: job.id,
        totalRepositories: results.totalRepositories,
        added: results.summary.added,
        skipped: results.summary.skipped,
        wouldAdd: results.summary.would_add,
        errors: results.summary.error
      }, 'Gitignore update job completed');

      return {
        ...results,
        timestamp: new Date().toISOString(),
        dryRun,
        gitignoreEntry,
      };
    } catch (error) {
      logger.error({
        jobId: job.id,
        error: error.message,
        stack: error.stack
      }, 'Gitignore update job failed');

      Sentry.captureException(error, {
        tags: {
          component: 'gitignore-worker',
          job_id: job.id,
        },
        extra: {
          baseDir,
          dryRun,
          gitignoreEntry,
        },
      });

      throw error;
    }
  }

  /**
   * Process specific repositories
   * @private
   */
  async #processSpecificRepositories(updater, repositories) {
    const results = [];

    logger.info({
      count: repositories.length
    }, 'Processing specific repositories');

    for (const repoPath of repositories) {
      logger.info({ repository: repoPath }, 'Processing repository');
      const result = await updater.addToGitignore(repoPath);
      results.push({
        repository: repoPath,
        ...result,
      });
      logger.info({
        repository: repoPath,
        action: result.action,
        reason: result.reason
      }, 'Repository processed');
    }

    return {
      totalRepositories: repositories.length,
      results,
      summary: updater.generateSummary(results),
    };
  }

  /**
   * Create a job to update all repositories
   */
  createUpdateAllJob(options = {}) {
    const jobId = `gitignore-update-all-${Date.now()}`;

    return this.createJob(jobId, {
      type: 'gitignore-update',
      baseDir: options.baseDir || this.baseDir,
      dryRun: options.dryRun ?? false,
      gitignoreEntry: options.gitignoreEntry || this.gitignoreEntry,
      excludeDirs: options.excludeDirs || this.excludeDirs,
      maxDepth: options.maxDepth || this.maxDepth,
    });
  }

  /**
   * Create a job to update specific repositories
   */
  createUpdateRepositoriesJob(repositories, options = {}) {
    const jobId = `gitignore-update-repos-${Date.now()}`;

    return this.createJob(jobId, {
      type: 'gitignore-update',
      repositories,
      dryRun: options.dryRun ?? false,
      gitignoreEntry: options.gitignoreEntry || this.gitignoreEntry,
    });
  }

  /**
   * Create a dry-run job to preview changes
   */
  createDryRunJob(options = {}) {
    const jobId = `gitignore-dryrun-${Date.now()}`;

    return this.createJob(jobId, {
      type: 'gitignore-update',
      baseDir: options.baseDir || this.baseDir,
      dryRun: true,
      gitignoreEntry: options.gitignoreEntry || this.gitignoreEntry,
      excludeDirs: options.excludeDirs || this.excludeDirs,
      maxDepth: options.maxDepth || this.maxDepth,
    });
  }
}
</file>

<file path="index.js">
import cron from 'node-cron';
import { RepomixWorker } from './repomix-worker.js';
import { DirectoryScanner } from './directory-scanner.js';
import { config } from './config.js';
import path from 'path';
import fs from 'fs/promises';
import { createComponentLogger } from './logger.js';

const logger = createComponentLogger('RepomixCronApp');

/**
 * Main application entry point
 */
class RepomixCronApp {
  constructor() {
    this.worker = new RepomixWorker({
      maxConcurrent: config.maxConcurrent,
      outputBaseDir: config.outputBaseDir,
      codeBaseDir: config.codeBaseDir,
      logDir: config.logDir,
      sentryDsn: config.sentryDsn,
    });

    this.scanner = new DirectoryScanner({
      baseDir: config.codeBaseDir,
      outputDir: config.scanReportsDir,
      excludeDirs: config.excludeDirs,
    });

    this.setupEventListeners();
  }

  /**
   * Setup event listeners for job events
   */
  setupEventListeners() {
    this.worker.on('job:created', (job) => {
      logger.info({ jobId: job.id }, 'Job created');
    });

    this.worker.on('job:started', (job) => {
      logger.info({ jobId: job.id, relativePath: job.data.relativePath }, 'Job started');
    });

    this.worker.on('job:completed', (job) => {
      const duration = job.completedAt - job.startedAt;
      logger.info({
        jobId: job.id,
        relativePath: job.data.relativePath,
        duration
      }, 'Job completed');
    });

    this.worker.on('job:failed', (job) => {
      logger.error({
        jobId: job.id,
        relativePath: job.data.relativePath,
        error: job.error
      }, 'Job failed');
    });
  }

  /**
   * Run repomix on all directories
   */
  async runRepomixOnAllDirectories() {
    logger.info({ baseDir: this.scanner.baseDir }, 'Starting repomix run');

    const startTime = Date.now();

    try {
      // Scan all directories
      const directories = await this.scanner.scanDirectories();
      logger.info({ directoryCount: directories.length }, 'Directories found');

      // Save scan results
      logger.info('Saving scan results');
      const scanResults = await this.scanner.generateAndSaveScanResults(directories);
      logger.info({
        reportPath: scanResults.reportPath,
        treePath: scanResults.treePath,
        summaryPath: scanResults.summaryPath,
        maxDepth: scanResults.summary.maxDepth,
        topDirectories: scanResults.summary.stats.topDirectoryNames.slice(0, 3).map(d => d.name)
      }, 'Scan results saved');

      // Create jobs for each directory
      let jobCount = 0;
      for (const dir of directories) {
        this.worker.createRepomixJob(dir.fullPath, dir.relativePath);
        jobCount++;
      }

      logger.info({ jobCount }, 'Jobs created');

      // Wait for all jobs to complete
      await this.waitForCompletion();

      const duration = Date.now() - startTime;
      const stats = this.worker.getStats();

      logger.info({
        durationSeconds: Math.round(duration / 1000),
        totalJobs: stats.total,
        completed: stats.completed,
        failed: stats.failed
      }, 'Repomix run complete');

      // Save run summary
      await this.saveRunSummary(stats, duration);

    } catch (error) {
      logger.error({ err: error }, 'Error during repomix run');
      throw error;
    }
  }

  /**
   * Wait for all jobs to complete
   */
  async waitForCompletion() {
    return new Promise((resolve) => {
      const checkInterval = setInterval(() => {
        const stats = this.worker.getStats();
        if (stats.active === 0 && stats.queued === 0) {
          clearInterval(checkInterval);
          resolve();
        }
      }, 1000);
    });
  }

  /**
   * Save run summary to logs
   */
  async saveRunSummary(stats, duration) {
    const summary = {
      timestamp: new Date().toISOString(),
      duration,
      stats,
    };

    const summaryPath = path.join('../logs', `run-summary-${Date.now()}.json`);
    await fs.writeFile(summaryPath, JSON.stringify(summary, null, 2));
  }

  /**
   * Setup cron job
   */
  setupCronJob(schedule = '0 2 * * *') {
    // Default: Run at 2 AM every day
    logger.info({ schedule }, 'Setting up cron job');

    cron.schedule(schedule, async () => {
      logger.info({ triggerTime: new Date().toISOString() }, 'Cron job triggered');
      try {
        await this.runRepomixOnAllDirectories();
      } catch (error) {
        logger.error({ err: error }, 'Cron job failed');
      }
    });

    logger.info('Cron job scheduled successfully');
  }

  /**
   * Start the application
   */
  async start() {
    logger.info({
      codeDirectory: this.scanner.baseDir,
      outputDirectory: this.worker.outputBaseDir,
      logDirectory: this.worker.logDir
    }, 'Repomix Cron Sidequest Server starting');

    // Setup cron job
    // Schedule: '0 2 * * *' = 2 AM daily
    // For testing: '*/5 * * * *' = every 5 minutes
    this.setupCronJob(config.repomixSchedule);

    // Run immediately on startup if requested
    if (config.runOnStartup) {
      logger.info('Running immediately (RUN_ON_STARTUP=true)');
      await this.runRepomixOnAllDirectories();
    }

    logger.info('Server running. Press Ctrl+C to exit');
  }
}

// Start the application
const app = new RepomixCronApp();
app.start().catch((error) => {
  logger.error({ err: error }, 'Fatal error');
  process.exit(1);
});
</file>

<file path="logger.js">
import pino from 'pino';
import { config } from './config.js';

/**
 * Create a structured logger instance using Pino
 *
 * Logs are JSON-formatted for easy parsing and analysis.
 * Use pino-pretty in development for human-readable output.
 *
 * Usage:
 *   logger.info('Simple message');
 *   logger.info({ jobId: 'job-123', path: '/foo' }, 'Job started');
 *   logger.error({ err }, 'Operation failed');
 */
export const logger = pino({
  level: config.logLevel,

  // Base fields included in every log
  base: {
    pid: process.pid,
    hostname: undefined, // Exclude hostname for cleaner logs
  },

  // Timestamp format
  timestamp: pino.stdTimeFunctions.isoTime,

  // Error serialization
  serializers: {
    err: pino.stdSerializers.err,
    error: pino.stdSerializers.err,
  },

  // Pretty printing for development (disabled in production for performance)
  transport: config.nodeEnv !== 'production' ? {
    target: 'pino-pretty',
    options: {
      colorize: true,
      translateTime: 'SYS:standard',
      ignore: 'pid,hostname',
      singleLine: false,
      messageFormat: '{levelLabel} - {msg}',
    }
  } : undefined,
});

/**
 * Create a child logger with additional context
 *
 * @param {Object} bindings - Context to add to all logs from this child
 * @returns {pino.Logger} Child logger instance
 *
 * @example
 * const jobLogger = createChildLogger({ jobId: 'job-123' });
 * jobLogger.info('Job started'); // Automatically includes jobId in log
 */
export function createChildLogger(bindings) {
  return logger.child(bindings);
}

/**
 * Create a logger for a specific component
 *
 * @param {string} component - Component name (e.g., 'RepomixWorker', 'DirectoryScanner')
 * @returns {pino.Logger} Component logger instance
 *
 * @example
 * const workerLogger = createComponentLogger('RepomixWorker');
 * workerLogger.info('Worker initialized');
 */
export function createComponentLogger(component) {
  return logger.child({ component });
}

export default logger;
</file>

<file path="plugin-management-audit.sh">
#!/usr/bin/env bash
#
# Plugin Management Audit Script
# Analyzes enabled plugins and identifies potential cleanup opportunities
#
# Usage: ./plugin-management-audit.sh [--json] [--detailed]
#
# Note: Requires bash 4+ for associative arrays
#       macOS users: brew install bash

set -euo pipefail

# Check bash version
if [[ "${BASH_VERSINFO[0]}" -lt 4 ]]; then
    echo "Error: This script requires bash 4 or higher (current: $BASH_VERSION)"
    echo "macOS users can install with: brew install bash"
    echo "Then run with: /usr/local/bin/bash $0 $@"
    exit 1
fi

CLAUDE_CONFIG="${HOME}/.claude/settings.json"
OUTPUT_FORMAT="human"
DETAILED=false

# Parse arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        --json)
            OUTPUT_FORMAT="json"
            shift
            ;;
        --detailed)
            DETAILED=true
            shift
            ;;
        *)
            echo "Unknown option: $1"
            echo "Usage: $0 [--json] [--detailed]"
            exit 1
            ;;
    esac
done

# Check if config exists
if [[ ! -f "$CLAUDE_CONFIG" ]]; then
    echo "Error: Claude config not found at $CLAUDE_CONFIG"
    exit 1
fi

# Get enabled plugins
ENABLED_PLUGINS=$(jq -r '.enabledPlugins | keys[]' "$CLAUDE_CONFIG" 2>/dev/null || echo "")
PLUGIN_COUNT=$(echo "$ENABLED_PLUGINS" | grep -v '^$' | wc -l | tr -d ' ')

# Identify potential duplicates by category
declare -A categories
while IFS= read -r plugin; do
    [[ -z "$plugin" ]] && continue

    # Extract category hints from plugin names
    if [[ "$plugin" =~ documentation|document ]]; then
        categories["documentation"]+="$plugin "
    elif [[ "$plugin" =~ git|github ]]; then
        categories["git"]+="$plugin "
    elif [[ "$plugin" =~ test|testing ]]; then
        categories["testing"]+="$plugin "
    elif [[ "$plugin" =~ deploy|deployment ]]; then
        categories["deployment"]+="$plugin "
    elif [[ "$plugin" =~ lint|format ]]; then
        categories["linting"]+="$plugin "
    elif [[ "$plugin" =~ docker|container ]]; then
        categories["containers"]+="$plugin "
    elif [[ "$plugin" =~ api|rest|graphql ]]; then
        categories["api"]+="$plugin "
    elif [[ "$plugin" =~ db|database|sql ]]; then
        categories["database"]+="$plugin "
    fi
done <<< "$ENABLED_PLUGINS"

# Generate output
if [[ "$OUTPUT_FORMAT" == "json" ]]; then
    # JSON output
    echo "{"
    echo "  \"total_enabled\": $PLUGIN_COUNT,"
    echo "  \"enabled_plugins\": ["
    first=true
    while IFS= read -r plugin; do
        [[ -z "$plugin" ]] && continue
        [[ "$first" == false ]] && echo ","
        echo -n "    \"$plugin\""
        first=false
    done <<< "$ENABLED_PLUGINS"
    echo ""
    echo "  ],"
    echo "  \"potential_duplicates\": {"
    first=true
    for category in "${!categories[@]}"; do
        count=$(echo "${categories[$category]}" | wc -w | tr -d ' ')
        if [[ $count -gt 1 ]]; then
            [[ "$first" == false ]] && echo ","
            echo -n "    \"$category\": ["
            inner_first=true
            for plugin in ${categories[$category]}; do
                [[ "$inner_first" == false ]] && echo -n ", "
                echo -n "\"$plugin\""
                inner_first=false
            done
            echo -n "]"
            first=false
        fi
    done
    echo ""
    echo "  }"
    echo "}"
else
    # Human-readable output
    echo "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó"
    echo "‚ïë          Claude Code Plugin Management Audit                   ‚ïë"
    echo "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù"
    echo ""
    echo "Total Enabled Plugins: $PLUGIN_COUNT"
    echo ""

    if [[ "$DETAILED" == true ]]; then
        echo "Enabled Plugins:"
        echo "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ"
        while IFS= read -r plugin; do
            [[ -z "$plugin" ]] && continue
            echo "  ‚Ä¢ $plugin"
        done <<< "$ENABLED_PLUGINS"
        echo ""
    fi

    # Show potential duplicates
    has_duplicates=false
    for category in "${!categories[@]}"; do
        count=$(echo "${categories[$category]}" | wc -w | tr -d ' ')
        if [[ $count -gt 1 ]]; then
            has_duplicates=true
        fi
    done

    if [[ "$has_duplicates" == true ]]; then
        echo "‚ö†Ô∏è  Potential Duplicate Categories:"
        echo "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ"
        for category in "${!categories[@]}"; do
            count=$(echo "${categories[$category]}" | wc -w | tr -d ' ')
            if [[ $count -gt 1 ]]; then
                echo ""
                echo "  Category: $category ($count plugins)"
                for plugin in ${categories[$category]}; do
                    echo "    ‚Ä¢ $plugin"
                done
            fi
        done
        echo ""
        echo "üí° Consider reviewing these categories for consolidation."
    else
        echo "‚úÖ No obvious duplicate categories detected."
    fi

    echo ""
    echo "Recommendations:"
    echo "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ"

    if [[ $PLUGIN_COUNT -gt 30 ]]; then
        echo "  ‚Ä¢ High plugin count ($PLUGIN_COUNT). Consider disabling unused plugins."
    fi

    if [[ "$has_duplicates" == true ]]; then
        echo "  ‚Ä¢ Review duplicate categories above."
        echo "  ‚Ä¢ Keep only the plugins you actively use in each category."
    fi

    echo "  ‚Ä¢ Run: npm run status to see plugin usage statistics"
    echo "  ‚Ä¢ Backup before changes: npm run backup"
    echo ""
fi

# Exit with status
if [[ "$has_duplicates" == true ]] || [[ $PLUGIN_COUNT -gt 30 ]]; then
    exit 1
else
    exit 0
fi
</file>

<file path="plugin-manager.js">
// @ts-nocheck
/**
 * Plugin Management Worker - AlephAuto Integration
 *
 * Monitors and audits Claude Code plugin configurations.
 * Identifies duplicate plugins, unused plugins, and provides cleanup recommendations.
 *
 * @extends SidequestServer
 */

import { SidequestServer } from './server.js';
import { config } from './config.js';
import { createComponentLogger } from './logger.js';
import { exec } from 'child_process';
import { promisify } from 'util';
import fs from 'fs/promises';
import path from 'path';

const execAsync = promisify(exec);
const logger = createComponentLogger('PluginManager');

class PluginManagerWorker extends SidequestServer {
  constructor(options = {}) {
    super({
      maxConcurrent: options.maxConcurrent ?? 1, // Single concurrent audit
      ...options
    });

    this.auditScriptPath = path.join(
      process.env.HOME,
      'code/jobs/sidequest/plugin-management-audit.sh'
    );
    this.configPath = path.join(process.env.HOME, '.claude/settings.json');
    this.thresholds = {
      maxPlugins: 30,
      warnPlugins: 20
    };

    logger.info('Plugin Manager Worker initialized', {
      auditScriptPath: this.auditScriptPath
    });
  }

  /**
   * Run plugin audit job
   * @param {Object} job - Job configuration
   * @param {boolean} job.detailed - Include detailed plugin listing
   * @param {boolean} job.autoCleanup - Attempt automatic cleanup (future)
   * @returns {Promise<Object>} Audit results
   */
  async runJobHandler(job) {
    const startTime = Date.now();
    logger.info('Starting plugin audit', { jobId: job.id, detailed: job.data?.detailed });

    try {
      // Run audit script
      const auditResults = await this.runAuditScript(job.data?.detailed || false);

      // Parse results
      const analysis = await this.analyzeResults(auditResults);

      // Generate recommendations
      const recommendations = this.generateRecommendations(analysis);

      const result = {
        success: true,
        timestamp: new Date().toISOString(),
        duration: Date.now() - startTime,
        ...analysis,
        recommendations
      };

      logger.info('Plugin audit completed', {
        jobId: job.id,
        totalPlugins: analysis.totalPlugins,
        duplicateCount: analysis.duplicateCategories?.length || 0,
        duration: result.duration
      });

      return result;
    } catch (error) {
      logger.error({ err: error, jobId: job.id }, 'Plugin audit failed');
      throw error;
    }
  }

  /**
   * Run the audit shell script
   * @param {boolean} detailed - Include detailed listing
   * @returns {Promise<Object>} Parsed JSON results
   */
  async runAuditScript(detailed = false) {
    try {
      const args = ['--json'];
      if (detailed) args.push('--detailed');

      // Use Homebrew bash if available
      const bashPath = '/opt/homebrew/bin/bash';
      const cmd = `${bashPath} ${this.auditScriptPath} ${args.join(' ')}`;

      const { stdout, stderr } = await execAsync(cmd);

      if (stderr) {
        logger.warn('Audit script warnings', { stderr });
      }

      return JSON.parse(stdout);
    } catch (error) {
      // Script exits with 1 if issues found, but still provides JSON
      if (error.stdout) {
        try {
          return JSON.parse(error.stdout);
        } catch (parseError) {
          logger.error({ err: parseError }, 'Failed to parse audit output');
          throw parseError;
        }
      }
      throw error;
    }
  }

  /**
   * Analyze audit results
   * @param {Object} auditResults - Raw audit results
   * @returns {Promise<Object>} Analysis
   */
  async analyzeResults(auditResults) {
    const { total_enabled, enabled_plugins, potential_duplicates } = auditResults;

    // Load plugin metadata if available
    const pluginMetadata = await this.loadPluginMetadata();

    // Identify duplicate categories
    const duplicateCategories = Object.entries(potential_duplicates || {}).map(
      ([category, plugins]) => ({
        category,
        plugins,
        count: plugins.length
      })
    );

    // Check thresholds
    const exceededThresholds = {
      maxPlugins: total_enabled > this.thresholds.maxPlugins,
      warnPlugins: total_enabled > this.thresholds.warnPlugins
    };

    return {
      totalPlugins: total_enabled,
      enabledPlugins: enabled_plugins,
      duplicateCategories,
      exceededThresholds,
      pluginMetadata
    };
  }

  /**
   * Load plugin metadata from Claude config
   * @returns {Promise<Object>} Plugin metadata
   */
  async loadPluginMetadata() {
    try {
      const configData = await fs.readFile(this.configPath, 'utf-8');
      const config = JSON.parse(configData);

      // Extract useful metadata if available
      return {
        enabledPlugins: config.enabledPlugins || {},
        pluginSettings: config.pluginSettings || {},
        lastModified: (await fs.stat(this.configPath)).mtime
      };
    } catch (error) {
      logger.warn({ err: error }, 'Failed to load plugin metadata');
      return {};
    }
  }

  /**
   * Generate cleanup recommendations
   * @param {Object} analysis - Analysis results
   * @returns {Array<Object>} Recommendations
   */
  generateRecommendations(analysis) {
    const recommendations = [];

    // High plugin count warning
    if (analysis.exceededThresholds.maxPlugins) {
      recommendations.push({
        priority: 'high',
        type: 'plugin_count',
        message: `You have ${analysis.totalPlugins} enabled plugins (threshold: ${this.thresholds.maxPlugins})`,
        action: 'Review and disable unused plugins to reduce overhead'
      });
    } else if (analysis.exceededThresholds.warnPlugins) {
      recommendations.push({
        priority: 'medium',
        type: 'plugin_count',
        message: `You have ${analysis.totalPlugins} enabled plugins (warning: ${this.thresholds.warnPlugins})`,
        action: 'Consider reviewing plugin usage'
      });
    }

    // Duplicate category recommendations
    if (analysis.duplicateCategories.length > 0) {
      recommendations.push({
        priority: 'medium',
        type: 'duplicate_categories',
        message: `Found ${analysis.duplicateCategories.length} categories with multiple plugins`,
        action: 'Review duplicate categories and consolidate',
        details: analysis.duplicateCategories.map(cat => ({
          category: cat.category,
          plugins: cat.plugins,
          suggestion: `Keep only the plugin you actively use in ${cat.category}`
        }))
      });
    }

    // Success message if no issues
    if (recommendations.length === 0) {
      recommendations.push({
        priority: 'info',
        type: 'healthy',
        message: `Plugin configuration looks healthy (${analysis.totalPlugins} plugins)`,
        action: 'No action needed'
      });
    }

    return recommendations;
  }

  /**
   * Create a plugin audit job
   * @param {Object} options - Job options
   * @returns {Object} Created job
   */
  addJob(options = {}) {
    const jobId = `plugin-audit-${Date.now()}`;
    return this.createJob(jobId, {
      detailed: options.detailed || false
    });
  }
}

// Export worker class
export { PluginManagerWorker };
</file>

<file path="repomix-worker.js">
// @ts-nocheck
import { SidequestServer } from './server.js';
import { spawn } from 'child_process';
import fs from 'fs/promises';
import path from 'path';
import os from 'os';
import { createComponentLogger } from './logger.js';
import { execSync } from 'child_process';
import { config } from './config.js';

const logger = createComponentLogger('RepomixWorker');

/**
 * RepomixWorker - Executes repomix jobs
 *
 * Respects .gitignore files by default - any directories or files listed
 * in .gitignore will be automatically excluded from processing.
 *
 * Options:
 * - respectGitignore: Respect .gitignore files (default: true)
 * - additionalIgnorePatterns: Array of additional patterns to ignore (default: [])
 * - outputBaseDir: Base directory for output files
 * - codeBaseDir: Base directory for source code
 */
export class RepomixWorker extends SidequestServer {
  constructor(options = {}) {
    super(options);
    this.outputBaseDir = options.outputBaseDir || './condense';
    this.codeBaseDir = options.codeBaseDir || path.join(os.homedir(), 'code');

    // Gitignore handling (respects .gitignore by default)
    this.respectGitignore = options.respectGitignore !== false; // Default: true

    // Default ignore patterns from config (includes README.md and markdown files)
    // Can be overridden via options or environment variable
    this.additionalIgnorePatterns = options.additionalIgnorePatterns || config.repomixIgnorePatterns || [];

    if (this.additionalIgnorePatterns.length > 0) {
      logger.info(
        { patterns: this.additionalIgnorePatterns },
        'RepomixWorker configured with ignore patterns (README files will be skipped)'
      );
    }

    // Pre-flight check: Verify repomix is available
    this.#verifyRepomixAvailable();
  }

  /**
   * Verify repomix is available via npx
   * Throws if repomix cannot be found
   * @private
   */
  #verifyRepomixAvailable() {
    try {
      execSync('npx repomix --version', { stdio: 'ignore', timeout: 5000 });
      logger.info('Pre-flight check: repomix is available');
    } catch (error) {
      const errorMessage =
        'repomix is not available. Please install it:\n' +
        '  npm install\n' +
        'Or verify package.json includes "repomix" dependency.';
      logger.error({ error }, errorMessage);
      throw new Error(errorMessage);
    }
  }

  /**
   * Run repomix for a specific directory
   */
  async runJobHandler(job) {
    const { sourceDir, relativePath } = job.data;

    // Create output directory matching the source structure
    const outputDir = path.join(this.outputBaseDir, relativePath);
    await fs.mkdir(outputDir, { recursive: true });

    const outputFile = path.join(outputDir, 'repomix-output.txt');

    logger.info({ jobId: job.id, sourceDir, outputFile }, 'Running repomix');

    try {
      const { stdout, stderr } = await this.#runRepomixCommand(sourceDir);

      // Save the output to the appropriate location
      await fs.writeFile(outputFile, stdout);

      if (stderr) {
        logger.warn({ jobId: job.id, stderr }, 'Repomix warnings');
      }

      return {
        sourceDir,
        outputFile,
        relativePath,
        size: (await fs.stat(outputFile)).size,
        timestamp: new Date().toISOString(),
      };
    } catch (error) {
      // Even if command fails, try to save any output
      if (error.stdout) {
        await fs.writeFile(outputFile, error.stdout);
      }
      throw error;
    }
  }

  /**
   * Securely run repomix command using npx (prevents command injection)
   * Uses npx to ensure local repomix from node_modules is used
   *
   * By default, repomix respects .gitignore files and excludes:
   * - Files and directories listed in .gitignore
   * - Common patterns (node_modules, .git, etc.)
   *
   * @param {string} cwd - Current working directory to run repomix in
   * @private
   */
  #runRepomixCommand(cwd) {
    return new Promise((resolve, reject) => {
      // Build repomix arguments
      const args = ['repomix'];

      // Disable gitignore if requested (NOT recommended)
      if (!this.respectGitignore) {
        args.push('--no-gitignore');
        logger.warn({ cwd }, 'Running repomix with --no-gitignore (not recommended)');
      }

      // Add additional ignore patterns if specified
      if (this.additionalIgnorePatterns.length > 0) {
        args.push('--ignore', this.additionalIgnorePatterns.join(','));
        logger.info(
          { cwd, patterns: this.additionalIgnorePatterns },
          'Adding additional ignore patterns'
        );
      }

      logger.debug({ cwd, args }, 'Spawning repomix with arguments');

      const proc = spawn('npx', args, {
        cwd,
        timeout: 600000, // 10 minute timeout
        maxBuffer: 50 * 1024 * 1024, // 50MB buffer
      });

      let stdout = '';
      let stderr = '';

      proc.stdout.on('data', (data) => {
        stdout += data.toString();
      });

      proc.stderr.on('data', (data) => {
        stderr += data.toString();
      });

      proc.on('close', (code) => {
        if (code === 0) {
          resolve({ stdout, stderr });
        } else {
          const error = new Error(`repomix exited with code ${code}`);
          error.code = code;
          error.stdout = stdout;
          error.stderr = stderr;
          reject(error);
        }
      });

      proc.on('error', (error) => {
        error.stdout = stdout;
        error.stderr = stderr;
        reject(error);
      });
    });
  }

  /**
   * Create a repomix job for a directory
   */
  createRepomixJob(sourceDir, relativePath) {
    const jobId = `repomix-${relativePath.replace(/\//g, '-')}-${Date.now()}`;

    return this.createJob(jobId, {
      sourceDir,
      relativePath,
      type: 'repomix',
    });
  }
}
</file>

<file path="repomix.config.json">
{
  "$schema": "https://repomix.com/schemas/latest/schema.json",
  "input": {
    "maxFileSize": 52428800
  },
  "output": {
    "filePath": "repomix-output.xml",
    "style": "xml",
    "parsableStyle": false,
    "fileSummary": true,
    "directoryStructure": true,
    "files": true,
    "removeComments": false,
    "removeEmptyLines": false,
    "compress": false,
    "topFilesLength": 5,
    "showLineNumbers": false,
    "truncateBase64": false,
    "copyToClipboard": false,
    "includeFullDirectoryStructure": false,
    "tokenCountTree": false,
    "git": {
      "sortByChanges": true,
      "sortByChangesMaxCommits": 100,
      "includeDiffs": false,
      "includeLogs": false,
      "includeLogsCount": 50
    }
  },
  "include": [],
  "ignore": {
    "useGitignore": true,
    "useDefaultPatterns": true,
    "customPatterns": [
      "logs/**",
      "node_modules/**",
      "*.log",
      "directory-scan-reports/**",
      "document-enhancement-impact-measurement/**",
      "**/go/pkg/mod/**",
      "**/pyenv/**",
      "**/python/pyenv/**",
      "**/vim/bundle/**",
      "**/vim/autoload/**",
      "repomix-output.xml",
      "repomix-output.txt"
    ]
  },
  "security": {
    "enableSecurityCheck": true
  },
  "tokenCount": {
    "encoding": "o200k_base"
  }
}
</file>

<file path="server.js">
import { EventEmitter } from 'events';
import * as Sentry from '@sentry/node';
import { config } from './config.js';
import fs from 'fs/promises';
import path from 'path';
import { createComponentLogger } from './logger.js';

const logger = createComponentLogger('SidequestServer');

/**
 * SidequestServer - Manages job execution with Sentry logging
 */
export class SidequestServer extends EventEmitter {
  constructor(options = {}) {
    super();
    this.jobs = new Map();
    this.jobHistory = [];
    this.maxConcurrent = options.maxConcurrent ?? 5;
    this.activeJobs = 0;
    this.queue = [];
    this.logDir = options.logDir || './logs';

    // Initialize Sentry
    Sentry.init({
      dsn: options.sentryDsn || config.sentryDsn,
      environment: config.nodeEnv,
      tracesSampleRate: 1.0,
    });
  }

  /**
   * Create a new job
   */
  createJob(jobId, jobData) {
    const job = {
      id: jobId,
      status: 'queued',
      data: jobData,
      createdAt: new Date(),
      startedAt: null,
      completedAt: null,
      error: null,
      result: null,
    };

    this.jobs.set(jobId, job);
    this.queue.push(jobId);

    Sentry.addBreadcrumb({
      category: 'job',
      message: `Job ${jobId} created`,
      level: 'info',
      data: { jobId, jobData },
    });

    this.emit('job:created', job);
    this.processQueue();

    return job;
  }

  /**
   * Process the job queue
   */
  async processQueue() {
    while (this.queue.length > 0 && this.activeJobs < this.maxConcurrent) {
      const jobId = this.queue.shift();
      const job = this.jobs.get(jobId);

      if (!job) continue;

      this.activeJobs++;
      this.executeJob(jobId).catch(error => {
        logger.error({ err: error, jobId }, 'Error executing job');
      });
    }
  }

  /**
   * Execute a job
   */
  async executeJob(jobId) {
    const job = this.jobs.get(jobId);
    if (!job) return;

    // Use Sentry v8 API
    return await Sentry.startSpan({
      op: 'job.execute',
      name: `Execute Job: ${jobId}`,
    }, async () => {
      try {
        job.status = 'running';
        job.startedAt = new Date();
        this.emit('job:started', job);

        Sentry.addBreadcrumb({
          category: 'job',
          message: `Job ${jobId} started`,
          level: 'info',
        });

        // Execute the job's handler
        const result = await this.runJobHandler(job);

        job.status = 'completed';
        job.completedAt = new Date();
        job.result = result;

        this.emit('job:completed', job);
        this.jobHistory.push({ ...job });

        // Log to file
        await this.logJobCompletion(job);

        Sentry.addBreadcrumb({
          category: 'job',
          message: `Job ${jobId} completed`,
          level: 'info',
        });

      } catch (error) {
        job.status = 'failed';
        job.completedAt = new Date();
        job.error = error.message;

        this.emit('job:failed', job);
        this.jobHistory.push({ ...job });

        // Log error to Sentry
        Sentry.captureException(error, {
          tags: {
            jobId: job.id,
            jobType: 'repomix',
          },
          contexts: {
            job: {
              id: job.id,
              data: job.data,
              startedAt: job.startedAt,
            },
          },
        });

        // Log to file
        await this.logJobFailure(job, error);

        logger.error({ err: error, jobId, jobData: job.data }, 'Job failed');
      } finally {
        this.activeJobs--;
        this.processQueue();
      }
    });
  }

  /**
   * Override this method to define job execution logic
   * @param {any} job - The job to execute
   * @returns {Promise<any>} - The result of the job execution
   */
  async runJobHandler(job) {
    throw new Error('runJobHandler must be implemented by subclass');
  }

  /**
   * Log job completion to file
   */
  async logJobCompletion(job) {
    const logPath = path.join(this.logDir, `${job.id}.json`);
    await fs.writeFile(logPath, JSON.stringify(job, null, 2));
  }

  /**
   * Log job failure to file
   */
  async logJobFailure(job, error) {
    const logPath = path.join(this.logDir, `${job.id}.error.json`);
    await fs.writeFile(logPath, JSON.stringify({
      ...job,
      error: {
        message: error.message,
        stack: error.stack,
      },
    }, null, 2));
  }

  /**
   * Get job status
   */
  getJob(jobId) {
    return this.jobs.get(jobId);
  }

  /**
   * Get all jobs
   */
  getAllJobs() {
    return Array.from(this.jobs.values());
  }

  /**
   * Get job statistics
   */
  getStats() {
    return {
      total: this.jobs.size,
      queued: this.queue.length,
      active: this.activeJobs,
      completed: this.jobHistory.filter(j => j.status === 'completed').length,
      failed: this.jobHistory.filter(j => j.status === 'failed').length,
    };
  }
}
</file>

<file path="universal-repo-cleanup.sh">
#!/bin/bash

################################################################################
# Universal Repository Cleanup Script
#
# Purpose: Remove common bloat files and directories from any repository
# Created: 2025-11-17
#
# This script performs cleanup tasks common to most repositories:
# 1. Remove Python virtual environments
# 2. Remove .DS_Store files (macOS system files)
# 3. Remove build artifacts and temporary files
# 4. Remove common duplicate/redundant directories
#
# Usage:
#   ./universal-repo-cleanup.sh [directory]
#
#   If no directory is provided, uses current working directory
#
# Examples:
#   ./universal-repo-cleanup.sh                    # Clean current directory
#   ./universal-repo-cleanup.sh /path/to/repo      # Clean specific directory
#   ./universal-repo-cleanup.sh ~/projects/myapp   # Clean using home path
################################################################################

set -e  # Exit on error
set -u  # Exit on undefined variable

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Target directory (first argument or current directory)
TARGET_DIR="${1:-$(pwd)}"

# Resolve to absolute path
TARGET_DIR="$(cd "$TARGET_DIR" 2>/dev/null && pwd)" || {
    echo -e "${RED}‚úó Error: Directory '$1' does not exist${NC}"
    exit 1
}

################################################################################
# Configuration - Customize what gets cleaned
################################################################################

# Python virtual environment directory names (common patterns)
VENV_PATTERNS=(
    "venv"
    ".venv"
    "env"
    ".env"
    "virtualenv"
    "*.venv"
    "personal_site"  # Example from original repo
)

# Build artifact patterns
BUILD_ARTIFACTS=(
    ".jekyll-cache"
    ".sass-cache"
    ".bundle"
    "node_modules/.cache"
    "dist"
    "build"
    ".next"
    ".nuxt"
    "out"
    ".output"
    "target"
    ".gradle"
)

# Temporary/cache file patterns
TEMP_FILE_PATTERNS=(
    ".DS_Store"
    "*.pyc"
    "*.pyo"
    "__pycache__"
    "*.swp"
    "*.swo"
    "*~"
    ".*.swp"
    "Thumbs.db"
    "desktop.ini"
)

# Output file patterns (files that are generated)
OUTPUT_FILE_PATTERNS=(
    "repomix-output.xml"
    "*.log"
    "npm-debug.log*"
    "yarn-debug.log*"
    "yarn-error.log*"
)

# Common redundant directory names
REDUNDANT_DIRS=(
    "drafts"
    "temp"
    "tmp"
    "backup"
    "backups"
    "old"
    "archive"
    "deprecated"
)

################################################################################
# Helper Functions
################################################################################

print_header() {
    echo -e "\n${BLUE}========================================${NC}"
    echo -e "${BLUE}$1${NC}"
    echo -e "${BLUE}========================================${NC}\n"
}

print_success() {
    echo -e "${GREEN}‚úì $1${NC}"
}

print_warning() {
    echo -e "${YELLOW}‚ö† $1${NC}"
}

print_error() {
    echo -e "${RED}‚úó $1${NC}"
}

print_info() {
    echo -e "${BLUE}‚Üí $1${NC}"
}

# Get directory/file size in human readable format
get_size() {
    local path="$1"
    if [ -e "$path" ]; then
        du -sh "$path" 2>/dev/null | cut -f1
    else
        echo "N/A"
    fi
}

# Count files in directory or matching pattern
count_files() {
    local path="$1"
    if [ -d "$path" ]; then
        find "$path" -type f 2>/dev/null | wc -l | tr -d ' '
    else
        echo "0"
    fi
}

################################################################################
# Scanning Functions
################################################################################

scan_venvs() {
    print_info "Scanning for Python virtual environments..."
    local found=()

    for pattern in "${VENV_PATTERNS[@]}"; do
        while IFS= read -r -d '' dir; do
            # Skip if inside node_modules
            if [[ ! "$dir" =~ node_modules ]]; then
                found+=("$dir")
            fi
        done < <(find "$TARGET_DIR" -maxdepth 3 -type d -name "$pattern" -print0 2>/dev/null)
    done

    # Remove duplicates
    printf '%s\n' "${found[@]}" | sort -u
}

scan_temp_files() {
    print_info "Scanning for temporary/cache files..."
    local found=()

    for pattern in "${TEMP_FILE_PATTERNS[@]}"; do
        while IFS= read -r -d '' file; do
            found+=("$file")
        done < <(find "$TARGET_DIR" -name "$pattern" -print0 2>/dev/null)
    done

    printf '%s\n' "${found[@]}" | sort -u
}

scan_output_files() {
    print_info "Scanning for output/generated files..."
    local found=()

    for pattern in "${OUTPUT_FILE_PATTERNS[@]}"; do
        while IFS= read -r -d '' file; do
            # Keep root-level repomix-output.xml
            if [[ "$file" != "$TARGET_DIR/repomix-output.xml" ]]; then
                found+=("$file")
            fi
        done < <(find "$TARGET_DIR" -name "$pattern" -print0 2>/dev/null)
    done

    printf '%s\n' "${found[@]}" | sort -u
}

scan_build_artifacts() {
    print_info "Scanning for build artifacts..."
    local found=()

    for artifact in "${BUILD_ARTIFACTS[@]}"; do
        local path="$TARGET_DIR/$artifact"
        if [ -d "$path" ]; then
            found+=("$path")
        fi
    done

    printf '%s\n' "${found[@]}" | sort -u
}

scan_redundant_dirs() {
    print_info "Scanning for redundant directories..."
    local found=()

    for dir_name in "${REDUNDANT_DIRS[@]}"; do
        local path="$TARGET_DIR/$dir_name"
        if [ -d "$path" ]; then
            # Check if it's in .gitignore
            if [ -f "$TARGET_DIR/.gitignore" ] && grep -q "^${dir_name}/\?$" "$TARGET_DIR/.gitignore" 2>/dev/null; then
                found+=("$path (in .gitignore)")
            else
                found+=("$path")
            fi
        fi
    done

    printf '%s\n' "${found[@]}" | sort -u
}

################################################################################
# Confirmation and Preview
################################################################################

show_preview() {
    print_header "Repository Cleanup Preview"

    echo "Target Directory: $TARGET_DIR"
    echo "Current Size: $(get_size "$TARGET_DIR")"
    echo ""

    # Scan all categories
    local venvs=($(scan_venvs))
    local temp_files=($(scan_temp_files))
    local output_files=($(scan_output_files))
    local build_artifacts=($(scan_build_artifacts))
    local redundant_dirs=($(scan_redundant_dirs))

    local total_items=0

    # Python venvs
    if [ ${#venvs[@]} -gt 0 ]; then
        echo -e "${YELLOW}Python Virtual Environments (${#venvs[@]} found):${NC}"
        for venv in "${venvs[@]}"; do
            [ -n "$venv" ] && echo "  - $venv ($(get_size "$venv"))" && ((total_items++))
        done
        echo ""
    fi

    # Temporary files
    if [ ${#temp_files[@]} -gt 0 ]; then
        echo -e "${YELLOW}Temporary/Cache Files (${#temp_files[@]} found):${NC}"
        local count=0
        for file in "${temp_files[@]}"; do
            [ -n "$file" ] && ((count++)) && ((total_items++))
        done
        echo "  - $count files (.DS_Store, __pycache__, .swp, etc.)"
        echo ""
    fi

    # Output files
    if [ ${#output_files[@]} -gt 0 ]; then
        echo -e "${YELLOW}Output/Generated Files (${#output_files[@]} found):${NC}"
        for file in "${output_files[@]}"; do
            [ -n "$file" ] && echo "  - $file" && ((total_items++))
        done
        echo ""
    fi

    # Build artifacts
    if [ ${#build_artifacts[@]} -gt 0 ]; then
        echo -e "${YELLOW}Build Artifacts (${#build_artifacts[@]} found):${NC}"
        for artifact in "${build_artifacts[@]}"; do
            [ -n "$artifact" ] && echo "  - $artifact ($(get_size "$artifact"))" && ((total_items++))
        done
        echo ""
    fi

    # Redundant directories
    if [ ${#redundant_dirs[@]} -gt 0 ]; then
        echo -e "${YELLOW}Redundant Directories (${#redundant_dirs[@]} found):${NC}"
        for dir in "${redundant_dirs[@]}"; do
            [ -n "$dir" ] && echo "  - $dir" && ((total_items++))
        done
        echo ""
    fi

    if [ $total_items -eq 0 ]; then
        print_success "No items found to clean - repository is already clean!"
        exit 0
    fi

    echo -e "${YELLOW}Total items to remove: $total_items${NC}"
    echo ""

    # Store for cleanup functions
    export FOUND_VENVS="${venvs[*]}"
    export FOUND_TEMP_FILES="${temp_files[*]}"
    export FOUND_OUTPUT_FILES="${output_files[*]}"
    export FOUND_BUILD_ARTIFACTS="${build_artifacts[*]}"
    export FOUND_REDUNDANT_DIRS="${redundant_dirs[*]}"
}

confirm_cleanup() {
    read -p "Do you want to proceed with cleanup? (yes/no): " response

    if [[ ! "$response" =~ ^[Yy][Ee][Ss]$ ]]; then
        print_warning "Cleanup cancelled by user"
        exit 0
    fi

    echo ""
}

################################################################################
# Cleanup Functions
################################################################################

cleanup_venvs() {
    print_header "Step 1: Removing Python Virtual Environments"

    local venvs=($FOUND_VENVS)
    local removed=0

    if [ ${#venvs[@]} -eq 0 ] || [ -z "${venvs[0]}" ]; then
        print_info "No virtual environments to remove"
        return
    fi

    for venv in "${venvs[@]}"; do
        if [ -n "$venv" ] && [ -d "$venv" ]; then
            local size=$(get_size "$venv")
            print_info "Removing $(basename "$venv") ($size)..."
            rm -rf "$venv"
            print_success "Removed $venv"
            ((removed++))
        fi
    done

    print_success "Removed $removed virtual environment(s)"
}

cleanup_temp_files() {
    print_header "Step 2: Removing Temporary/Cache Files"

    local temp_files=($FOUND_TEMP_FILES)
    local removed=0

    if [ ${#temp_files[@]} -eq 0 ] || [ -z "${temp_files[0]}" ]; then
        print_info "No temporary files to remove"
        return
    fi

    for file in "${temp_files[@]}"; do
        if [ -n "$file" ] && [ -e "$file" ]; then
            rm -rf "$file" 2>/dev/null && ((removed++))
        fi
    done

    print_success "Removed $removed temporary file(s)"
}

cleanup_output_files() {
    print_header "Step 3: Removing Output/Generated Files"

    local output_files=($FOUND_OUTPUT_FILES)
    local removed=0

    if [ ${#output_files[@]} -eq 0 ] || [ -z "${output_files[0]}" ]; then
        print_info "No output files to remove"
        return
    fi

    for file in "${output_files[@]}"; do
        if [ -n "$file" ] && [ -f "$file" ]; then
            print_info "Removing $(basename "$file")..."
            rm -f "$file"
            print_success "Removed $file"
            ((removed++))
        fi
    done

    print_success "Removed $removed output file(s)"
}

cleanup_build_artifacts() {
    print_header "Step 4: Removing Build Artifacts"

    local build_artifacts=($FOUND_BUILD_ARTIFACTS)
    local removed=0

    if [ ${#build_artifacts[@]} -eq 0 ] || [ -z "${build_artifacts[0]}" ]; then
        print_info "No build artifacts to remove"
        return
    fi

    for artifact in "${build_artifacts[@]}"; do
        if [ -n "$artifact" ] && [ -d "$artifact" ]; then
            local size=$(get_size "$artifact")
            print_info "Removing $(basename "$artifact") ($size)..."
            rm -rf "$artifact"
            print_success "Removed $artifact"
            ((removed++))
        fi
    done

    print_success "Removed $removed build artifact(s)"
}

cleanup_redundant_dirs() {
    print_header "Step 5: Removing Redundant Directories"

    local redundant_dirs=($FOUND_REDUNDANT_DIRS)
    local removed=0

    if [ ${#redundant_dirs[@]} -eq 0 ] || [ -z "${redundant_dirs[0]}" ]; then
        print_info "No redundant directories to remove"
        return
    fi

    for dir in "${redundant_dirs[@]}"; do
        # Remove " (in .gitignore)" suffix if present
        dir="${dir% (in .gitignore)}"

        if [ -n "$dir" ] && [ -d "$dir" ]; then
            local size=$(get_size "$dir")
            print_info "Removing $(basename "$dir") ($size)..."
            rm -rf "$dir"
            print_success "Removed $dir"
            ((removed++))
        fi
    done

    print_success "Removed $removed redundant director(ies)"
}

################################################################################
# Summary
################################################################################

print_summary() {
    print_header "Cleanup Summary"

    local final_size=$(get_size "$TARGET_DIR")

    echo "Cleanup completed successfully!"
    echo ""
    echo "Target Directory: $TARGET_DIR"
    echo "Final Size: $final_size"
    echo ""
    echo "Cleaned up:"
    echo "  ‚úì Python virtual environments"
    echo "  ‚úì Temporary/cache files (.DS_Store, __pycache__, etc.)"
    echo "  ‚úì Output/generated files (logs, repomix files, etc.)"
    echo "  ‚úì Build artifacts (.jekyll-cache, dist/, etc.)"
    echo "  ‚úì Redundant directories (drafts/, temp/, backup/, etc.)"
    echo ""

    print_success "Repository cleanup completed!"
}

recommend_gitignore() {
    print_header "Recommendations"

    if [ ! -f "$TARGET_DIR/.gitignore" ]; then
        print_warning "No .gitignore found - consider creating one"
        return
    fi

    echo "Consider adding these patterns to .gitignore if not already present:"
    echo ""
    echo "  # Python"
    echo "  venv/"
    echo "  .venv/"
    echo "  *.pyc"
    echo "  __pycache__/"
    echo ""
    echo "  # System files"
    echo "  .DS_Store"
    echo "  Thumbs.db"
    echo ""
    echo "  # Build artifacts"
    echo "  dist/"
    echo "  build/"
    echo "  *.log"
    echo ""
    echo "  # Temporary"
    echo "  temp/"
    echo "  tmp/"
    echo "  *.swp"
    echo ""
}

################################################################################
# Main Execution
################################################################################

main() {
    print_header "Universal Repository Cleanup Script"

    echo "Target: $TARGET_DIR"
    echo ""

    # Show preview and get confirmation
    show_preview
    confirm_cleanup

    # Execute cleanup tasks
    cleanup_venvs
    cleanup_temp_files
    cleanup_output_files
    cleanup_build_artifacts
    cleanup_redundant_dirs

    # Print summary
    print_summary

    # Print recommendations
    recommend_gitignore

    print_success "All cleanup tasks completed successfully!"
}

# Run main function
main "$@"
</file>

</files>
